{"id": "439980566b115d8bfebdd8c67c7ec47579fea812", "text": "How to create a dynamic (self-constructing) chain | ü¶úÔ∏èüîó LangChain\n\nThis guide assumes familiarity with the following:\n\n- LangChain Expression Language (LCEL)\n- How to turn any function into a runnable\n\nSometimes we want to construct parts of a chain at runtime, depending on the chain inputs (routing is the most common example of this). We can create dynamic chains like this using a very useful property of RunnableLambda's, which is that if a RunnableLambda returns a Runnable, that Runnable is itself invoked. Let's see an example.\n\n```\npip install -qU \"langchain[google-genai]\"\n```\n\n```\nimport getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n```\n\n```\n# | echo: falsefrom langchain_anthropic import ChatAnthropicllm = ChatAnthropic(model=\"claude-3-7-sonnet-20250219\")\n```\n\n```\nfrom operator import itemgetterfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import Runnable, RunnablePassthrough, chaincontextualize_instructions = \"\"\"Convert the latest user question into a standalone question given the chat history. Don't answer the question, return the question and nothing else (no descriptive text).\"\"\"contextualize_prompt = ChatPromptTemplate.from_messages(    [        (\"system\", contextualize_instructions),        (\"placeholder\", \"{chat_history}\"),        (\"human\", \"{question}\"),    ])contextualize_question = contextualize_prompt | llm | StrOutputParser()qa_instructions = (    \"\"\"Answer the user question given the following context:\\n\\n{context}.\"\"\")qa_prompt = ChatPromptTemplate.from_messages(    [(\"system\", qa_instructions), (\"human\", \"{question}\")])@chaindef contextualize_if_needed(input_: dict) -> Runnable:    if input_.get(\"chat_history\"):        # NOTE: This is returning another Runnable, not an actual output.        return contextualize_question    else:        return RunnablePassthrough() | itemgetter(\"question\")@chaindef fake_retriever(input_: dict) -> str:    return \"egypt's population in 2024 is about 111 million\"full_chain = (    RunnablePassthrough.assign(question=contextualize_if_needed).assign(        context=fake_retriever    )    | qa_prompt    | llm    | StrOutputParser())full_chain.invoke(    {        \"question\": \"what about egypt\",        \"chat_history\": [            (\"human\", \"what's the population of indonesia\"),            (\"ai\", \"about 276 million\"),        ],    })\n```\n\n```\n\"Egypt's population in 2024 is about 111 million.\"\n```\n\nThe key here is that contextualize_if_needed returns another Runnable and not an actual output. This returned Runnable is itself run when the full chain is executed.\n\nLooking at the trace we can see that, since we passed in chat_history, we executed the contextualize_question chain as part of the full chain: https://smith.langchain.com/public/9e0ae34c-4082-4f3f-beed-34a2a2f4c991/r\n\nNote that the streaming, batching, etc. capabilities of the returned Runnable are all preserved\n\n```\nfor chunk in contextualize_if_needed.stream(    {        \"question\": \"what about egypt\",        \"chat_history\": [            (\"human\", \"what's the population of indonesia\"),            (\"ai\", \"about 276 million\"),        ],    }):    print(chunk)\n```\n\n```\nWhat is the population of Egypt?\n```", "url_final": "https://python.langchain.com/docs/how_to/dynamic_chain/", "url_citable": "https://python.langchain.com/docs/how_to/dynamic_chain/", "title": "How to create a dynamic (self-constructing) chain | ü¶úÔ∏èüîó LangChain", "section_level": 1, "section_heading": "How to create a dynamic (self-constructing) chain | ü¶úÔ∏èüîó LangChain", "section_anchor": null, "fetched_at": "2025-10-06T21:55:28.910496", "has_code": true}
{"id": "3ed0a298d2d3a222b0adfd498cbabff8b077eb90", "text": "Get started‚Äã\n\n```\n# Helper function for printing docsdef pretty_print_docs(docs):    print(        f\"\\n{'-' * 100}\\n\".join(            [f\"Document {i + 1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]        )    )\n```", "url_final": "https://python.langchain.com/docs/how_to/contextual_compression/", "url_citable": "https://python.langchain.com/docs/how_to/contextual_compression/#get-started", "title": "How to do retrieval with contextual compression | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Get started‚Äã", "section_anchor": "get-started", "fetched_at": "2025-10-06T21:55:10.096512", "has_code": true}
{"id": "8e3a0d31d40ad4e6b00a2683af9a5a128fdb40b0", "text": "Using a vanilla vector store retriever‚Äã\n\nLet's start by initializing a simple vector store retriever and storing the 2023 State of the Union speech (in chunks). We can see that given an example question our retriever returns one or two relevant docs and a few irrelevant docs. And even the relevant docs have a lot of irrelevant information in them.\n\n```\nfrom langchain_community.document_loaders import TextLoaderfrom langchain_community.vectorstores import FAISSfrom langchain_openai import OpenAIEmbeddingsfrom langchain_text_splitters import CharacterTextSplitterdocuments = TextLoader(\"state_of_the_union.txt\").load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)texts = text_splitter.split_documents(documents)retriever = FAISS.from_documents(texts, OpenAIEmbeddings()).as_retriever()docs = retriever.invoke(\"What did the president say about Ketanji Brown Jackson\")pretty_print_docs(docs)\n```\n\n```\nDocument 1:Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you‚Äôre at it, pass the Disclose Act so Americans can know who is funding our elections. Tonight, I‚Äôd like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer‚Äîan Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation‚Äôs top legal minds, who will continue Justice Breyer‚Äôs legacy of excellence.----------------------------------------------------------------------------------------------------Document 2:A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she‚Äôs been nominated, she‚Äôs received a broad range of support‚Äîfrom the Fraternal Order of Police to former judges appointed by Democrats and Republicans. And if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. We can do both. At our border, we‚Äôve installed new technology like cutting-edge scanners to better detect drug smuggling.  We‚Äôve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  We‚Äôre putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. We‚Äôre securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.----------------------------------------------------------------------------------------------------Document 3:And for our LGBTQ+ Americans, let‚Äôs finally get the bipartisan Equality Act to my desk. The onslaught of state laws targeting transgender Americans and their families is wrong. As I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential. While it often appears that we never agree, that isn‚Äôt true. I signed 80 bipartisan bills into law last year. From preventing government shutdowns to protecting Asian-Americans from still-too-common hate crimes to reforming military justice. And soon, we‚Äôll strengthen the Violence Against Women Act that I first wrote three decades ago. It is important for us to show the nation that we can come together and do big things. So tonight I‚Äôm offering a Unity Agenda for the Nation. Four big things we can do together.  First, beat the opioid epidemic.----------------------------------------------------------------------------------------------------Document 4:Tonight, I‚Äôm announcing a crackdown on these companies overcharging American businesses and consumers. And as Wall Street firms take over more nursing homes, quality in those homes has gone down and costs have gone up.  That ends on my watch. Medicare is going to set higher standards for nursing homes and make sure your loved ones get the care they deserve and expect. We‚Äôll also cut costs and keep the economy going strong by giving workers a fair shot, provide more training and apprenticeships, hire them based on their skills not degrees. Let‚Äôs pass the Paycheck Fairness Act and paid leave.  Raise the minimum wage to $15 an hour and extend the Child Tax Credit, so no one has to raise a family in poverty. Let‚Äôs increase Pell Grants and increase our historic support of HBCUs, and invest in what Jill‚Äîour First Lady who teaches full-time‚Äîcalls America‚Äôs best-kept secret: community colleges.\n```", "url_final": "https://python.langchain.com/docs/how_to/contextual_compression/", "url_citable": "https://python.langchain.com/docs/how_to/contextual_compression/#using-a-vanilla-vector-store-retriever", "title": "How to do retrieval with contextual compression | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Using a vanilla vector store retriever‚Äã", "section_anchor": "using-a-vanilla-vector-store-retriever", "fetched_at": "2025-10-06T21:55:10.096512", "has_code": true}
{"id": "1c8e165763205e24cbfed258b161c10f78cf80c1", "text": "Adding contextual compression with an LLMChainExtractor‚Äã\n\nNow let's wrap our base retriever with a ContextualCompressionRetriever. We'll add an LLMChainExtractor, which will iterate over the initially returned documents and extract from each only the content that is relevant to the query.\n\n```\nfrom langchain.retrievers import ContextualCompressionRetrieverfrom langchain.retrievers.document_compressors import LLMChainExtractorfrom langchain_openai import OpenAIllm = OpenAI(temperature=0)compressor = LLMChainExtractor.from_llm(llm)compression_retriever = ContextualCompressionRetriever(    base_compressor=compressor, base_retriever=retriever)compressed_docs = compression_retriever.invoke(    \"What did the president say about Ketanji Jackson Brown\")pretty_print_docs(compressed_docs)\n```\n\n```\nDocument 1:I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson.\n```", "url_final": "https://python.langchain.com/docs/how_to/contextual_compression/", "url_citable": "https://python.langchain.com/docs/how_to/contextual_compression/#adding-contextual-compression-with-an-llmchainextractor", "title": "How to do retrieval with contextual compression | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Adding contextual compression with an LLMChainExtractor‚Äã", "section_anchor": "adding-contextual-compression-with-an-llmchainextractor", "fetched_at": "2025-10-06T21:55:10.096512", "has_code": true}
{"id": "58e1254895c07ae09e279532dfd97fff30a81b61", "text": "More built-in compressors: filters‚Äã\n\nLLMChainFilter‚Äã\n\nThe LLMChainFilter is slightly simpler but more robust compressor that uses an LLM chain to decide which of the initially retrieved documents to filter out and which ones to return, without manipulating the document contents.\n\n```\nfrom langchain.retrievers.document_compressors import LLMChainFilter_filter = LLMChainFilter.from_llm(llm)compression_retriever = ContextualCompressionRetriever(    base_compressor=_filter, base_retriever=retriever)compressed_docs = compression_retriever.invoke(    \"What did the president say about Ketanji Jackson Brown\")pretty_print_docs(compressed_docs)\n```\n\n```\nDocument 1:Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you‚Äôre at it, pass the Disclose Act so Americans can know who is funding our elections. Tonight, I‚Äôd like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer‚Äîan Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation‚Äôs top legal minds, who will continue Justice Breyer‚Äôs legacy of excellence.\n```\n\nLLMListwiseRerank‚Äã\n\nLLMListwiseRerank uses zero-shot listwise document reranking and functions similarly to LLMChainFilter as a robust but more expensive option. It is recommended to use a more powerful LLM.\n\nNote that LLMListwiseRerank requires a model with the with_structured_output method implemented.\n\n```\nfrom langchain.retrievers.document_compressors import LLMListwiseRerankfrom langchain_openai import ChatOpenAIllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)_filter = LLMListwiseRerank.from_llm(llm, top_n=1)compression_retriever = ContextualCompressionRetriever(    base_compressor=_filter, base_retriever=retriever)compressed_docs = compression_retriever.invoke(    \"What did the president say about Ketanji Jackson Brown\")pretty_print_docs(compressed_docs)\n```\n\n```\nDocument 1:Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you‚Äôre at it, pass the Disclose Act so Americans can know who is funding our elections. Tonight, I‚Äôd like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer‚Äîan Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation‚Äôs top legal minds, who will continue Justice Breyer‚Äôs legacy of excellence.\n```", "url_final": "https://python.langchain.com/docs/how_to/contextual_compression/", "url_citable": "https://python.langchain.com/docs/how_to/contextual_compression/#more-built-in-compressors-filters", "title": "How to do retrieval with contextual compression | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "More built-in compressors: filters‚Äã", "section_anchor": "more-built-in-compressors-filters", "fetched_at": "2025-10-06T21:55:10.096512", "has_code": true}
{"id": "19c9b75cd8e4d1f1421b440f26d6c50e791416d0", "text": "EmbeddingsFilter‚Äã\n\nMaking an extra LLM call over each retrieved document is expensive and slow. The EmbeddingsFilter provides a cheaper and faster option by embedding the documents and query and only returning those documents which have sufficiently similar embeddings to the query.\n\n```\nfrom langchain.retrievers.document_compressors import EmbeddingsFilterfrom langchain_openai import OpenAIEmbeddingsembeddings = OpenAIEmbeddings()embeddings_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.76)compression_retriever = ContextualCompressionRetriever(    base_compressor=embeddings_filter, base_retriever=retriever)compressed_docs = compression_retriever.invoke(    \"What did the president say about Ketanji Jackson Brown\")pretty_print_docs(compressed_docs)\n```\n\n```\nDocument 1:Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you‚Äôre at it, pass the Disclose Act so Americans can know who is funding our elections. Tonight, I‚Äôd like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer‚Äîan Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation‚Äôs top legal minds, who will continue Justice Breyer‚Äôs legacy of excellence.----------------------------------------------------------------------------------------------------Document 2:A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she‚Äôs been nominated, she‚Äôs received a broad range of support‚Äîfrom the Fraternal Order of Police to former judges appointed by Democrats and Republicans. And if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. We can do both. At our border, we‚Äôve installed new technology like cutting-edge scanners to better detect drug smuggling.  We‚Äôve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  We‚Äôre putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. We‚Äôre securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.\n```", "url_final": "https://python.langchain.com/docs/how_to/contextual_compression/", "url_citable": "https://python.langchain.com/docs/how_to/contextual_compression/#embeddingsfilter", "title": "How to do retrieval with contextual compression | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "EmbeddingsFilter‚Äã", "section_anchor": "embeddingsfilter", "fetched_at": "2025-10-06T21:55:10.096512", "has_code": true}
{"id": "d9fd8e9b6bfb3f25a287206fc93f25cf786ebae1", "text": "Stringing compressors and document transformers together‚Äã\n\nUsing the DocumentCompressorPipeline we can also easily combine multiple compressors in sequence. Along with compressors we can add BaseDocumentTransformers to our pipeline, which don't perform any contextual compression but simply perform some transformation on a set of documents. For example TextSplitters can be used as document transformers to split documents into smaller pieces, and the EmbeddingsRedundantFilter can be used to filter out redundant documents based on embedding similarity between documents.\n\nBelow we create a compressor pipeline by first splitting our docs into smaller chunks, then removing redundant documents, and then filtering based on relevance to the query.\n\n```\nfrom langchain.retrievers.document_compressors import DocumentCompressorPipelinefrom langchain_community.document_transformers import EmbeddingsRedundantFilterfrom langchain_text_splitters import CharacterTextSplittersplitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0, separator=\". \")redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)relevant_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.76)pipeline_compressor = DocumentCompressorPipeline(    transformers=[splitter, redundant_filter, relevant_filter])\n```\n\n```\ncompression_retriever = ContextualCompressionRetriever(    base_compressor=pipeline_compressor, base_retriever=retriever)compressed_docs = compression_retriever.invoke(    \"What did the president say about Ketanji Jackson Brown\")pretty_print_docs(compressed_docs)\n```\n\n```\nDocument 1:One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson----------------------------------------------------------------------------------------------------Document 2:As I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential. While it often appears that we never agree, that isn‚Äôt true. I signed 80 bipartisan bills into law last year----------------------------------------------------------------------------------------------------Document 3:A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder----------------------------------------------------------------------------------------------------Document 4:Since she‚Äôs been nominated, she‚Äôs received a broad range of support‚Äîfrom the Fraternal Order of Police to former judges appointed by Democrats and Republicans. And if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. We can do both\n```\n\n- LLMChainFilter\n- LLMListwiseRerank\n- EmbeddingsFilter", "url_final": "https://python.langchain.com/docs/how_to/contextual_compression/", "url_citable": "https://python.langchain.com/docs/how_to/contextual_compression/#stringing-compressors-and-document-transformers-together", "title": "How to do retrieval with contextual compression | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Stringing compressors and document transformers together‚Äã", "section_anchor": "stringing-compressors-and-document-transformers-together", "fetched_at": "2025-10-06T21:55:10.096512", "has_code": true}
{"id": "1304c0712bd91a12f1d1c9b5c447e3cea98e1c76", "text": "Setup‚Äã\n\nDependencies‚Äã\n\nWe'll use the following packages:\n\n```\n%pip install --upgrade --quiet langchain langchain-community langchainhub beautifulsoup4\n```\n\nLangSmith‚Äã\n\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\n\nNote that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:\n\n```\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n```\n\nComponents‚Äã\n\nWe will need to select three components from LangChain's suite of integrations.\n\nA chat model:\n\n```\npip install -qU \"langchain[google-genai]\"\n```\n\n```\nimport getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n```\n\nAn embedding model:\n\n```\npip install -qU langchain-openai\n```\n\n```\nimport getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"):  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")from langchain_openai import OpenAIEmbeddingsembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n```\n\nAnd a vector store:\n\n```\npip install -qU langchain-core\n```\n\n```\nfrom langchain_core.vectorstores import InMemoryVectorStorevector_store = InMemoryVectorStore(embeddings)\n```", "url_final": "https://python.langchain.com/docs/how_to/qa_sources/", "url_citable": "https://python.langchain.com/docs/how_to/qa_sources/#setup", "title": "How to get your RAG application to return sources | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Setup‚Äã", "section_anchor": "setup", "fetched_at": "2025-10-06T21:56:11.899561", "has_code": true}
{"id": "6a05e74e1bca76e3277ff6451b0e23205b4a0115", "text": "RAG application‚Äã\n\nLet's reconstruct the Q&A app with sources we built over the LLM Powered Autonomous Agents blog post by Lilian Weng in the RAG tutorial.\n\nFirst we index our documents:\n\n```\nimport bs4from langchain import hubfrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_core.documents import Documentfrom langchain_text_splitters import RecursiveCharacterTextSplitterfrom typing_extensions import List, TypedDict# Load and chunk contents of the blogloader = WebBaseLoader(    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),    bs_kwargs=dict(        parse_only=bs4.SoupStrainer(            class_=(\"post-content\", \"post-title\", \"post-header\")        )    ),)docs = loader.load()text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)all_splits = text_splitter.split_documents(docs)\n```\n\n```\n# Index chunks_ = vector_store.add_documents(documents=all_splits)\n```\n\nNext we build the application:\n\n```\nfrom langchain import hubfrom langchain_core.documents import Documentfrom langgraph.graph import START, StateGraphfrom typing_extensions import List, TypedDict# Define prompt for question-answeringprompt = hub.pull(\"rlm/rag-prompt\")# Define state for applicationclass State(TypedDict):    question: str    context: List[Document]    answer: str# Define application stepsdef retrieve(state: State):    retrieved_docs = vector_store.similarity_search(state[\"question\"])    return {\"context\": retrieved_docs}def generate(state: State):    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})    response = llm.invoke(messages)    return {\"answer\": response.content}# Compile application and testgraph_builder = StateGraph(State).add_sequence([retrieve, generate])graph_builder.add_edge(START, \"retrieve\")graph = graph_builder.compile()\n```\n\n```\nfrom IPython.display import Image, displaydisplay(Image(graph.get_graph().draw_mermaid_png()))\n```\n\nBecause we're tracking the retrieved context in our application's state, it is accessible after invoking the application:\n\n```\nresult = graph.invoke({\"question\": \"What is Task Decomposition?\"})print(f\"Context: {result['context']}\\n\\n\")print(f\"Answer: {result['answer']}\")\n```\n\n```\nContext: [Document(id='c8471b37-07d8-4d51-856e-4b2c22bca88d', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.'), Document(id='acb7eb6f-f252-4353-aec2-f459135354ba', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'), Document(id='4fae6668-7fec-4237-9b2d-78132f4f3f3f', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Resources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.'), Document(id='3c79dd86-595e-42e8-b64d-404780f9e2d9', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content=\"(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\")]Answer: Task Decomposition is the process of breaking down a complex task into smaller, manageable steps to facilitate execution. This can be achieved through techniques like Chain of Thought, which encourages step-by-step reasoning, or Tree of Thoughts, which explores multiple reasoning paths for each step. It can be implemented using simple prompts, specific instructions, or human input to effectively tackle the original task.\n```\n\nHere, \"context\" contains the sources that the LLM used in generating the response in \"answer\".", "url_final": "https://python.langchain.com/docs/how_to/qa_sources/", "url_citable": "https://python.langchain.com/docs/how_to/qa_sources/#rag-application", "title": "How to get your RAG application to return sources | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "RAG application‚Äã", "section_anchor": "rag-application", "fetched_at": "2025-10-06T21:56:11.899561", "has_code": true}
{"id": "a9491adf7bf8b08f6f5ec6ea4c7b0a0e628a6ccf", "text": "Structure sources in model response‚Äã\n\nUp to this point, we've simply propagated the documents returned from the retrieval step through to the final response. But this may not illustrate what subset of information the model relied on when generating its answer. Below, we show how to structure sources into the model response, allowing the model to report what specific context it relied on for its answer.\n\nIt is straightforward to extend the above LangGraph implementation. Below, we make a simple change: we use the model's tool-calling features to generate structured output, consisting of an answer and list of sources. The schema for the response is represented in the AnswerWithSources TypedDict, below.\n\n```\nfrom typing import Listfrom typing_extensions import Annotated, TypedDict# Desired schema for responseclass AnswerWithSources(TypedDict):    \"\"\"An answer to the question, with sources.\"\"\"    answer: str    sources: Annotated[        List[str],        ...,        \"List of sources (author + year) used to answer the question\",    ]class State(TypedDict):    question: str    context: List[Document]    answer: AnswerWithSourcesdef generate(state: State):    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})    structured_llm = llm.with_structured_output(AnswerWithSources)    response = structured_llm.invoke(messages)    return {\"answer\": response}graph_builder = StateGraph(State).add_sequence([retrieve, generate])graph_builder.add_edge(START, \"retrieve\")graph = graph_builder.compile()\n```\n\n```\nimport jsonresult = graph.invoke({\"question\": \"What is Chain of Thought?\"})print(json.dumps(result[\"answer\"], indent=2))\n```\n\n```\n{  \"answer\": \"Chain of Thought (CoT) is a prompting technique that enhances model performance by instructing it to think step by step, allowing the decomposition of complex tasks into smaller, manageable steps. This method not only aids in task execution but also provides insights into the model's reasoning process. CoT has become a standard approach in improving how language models handle intricate problem-solving tasks.\",  \"sources\": [    \"Wei et al. 2022\"  ]}\n```\n\nView LangSmith trace.", "url_final": "https://python.langchain.com/docs/how_to/qa_sources/", "url_citable": "https://python.langchain.com/docs/how_to/qa_sources/#structure-sources-in-model-response", "title": "How to get your RAG application to return sources | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Structure sources in model response‚Äã", "section_anchor": "structure-sources-in-model-response", "fetched_at": "2025-10-06T21:56:11.899561", "has_code": true}
{"id": "1b36308a4d9e55b298322902df6a48536c07fcba", "text": "Conversational RAG‚Äã\n\nPart 2 of the RAG tutorial implements a different architecture, in which steps in the RAG flow are represented via successive message objects. This leverages additional tool-calling features of chat models, and more naturally accommodates a \"back-and-forth\" conversational user experience.\n\nIn that tutorial (and below), we propagate the retrieved documents as artifacts on the tool messages. That makes it easy to pluck out the retrieved documents. Below, we add them as an additional key in the state, for convenience.\n\nNote that we define the response format of the tool as \"content_and_artifact\":\n\n```\nfrom langchain_core.tools import tool@tool(response_format=\"content_and_artifact\")def retrieve(query: str):    \"\"\"Retrieve information related to a query.\"\"\"    retrieved_docs = vector_store.similarity_search(query, k=2)    serialized = \"\\n\\n\".join(        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")        for doc in retrieved_docs    )    return serialized, retrieved_docs\n```\n\nWe can now build and compile the exact same application as in Part 2 of the RAG tutorial, with two changes:\n\n- We add a context key of the state to store retrieved documents;\n- In the generate step, we pluck out the retrieved documents and populate them in the state.\n\nThese changes are highlighted below.\n\n```\nfrom langchain_core.messages import SystemMessagefrom langgraph.graph import END, MessagesState, StateGraphfrom langgraph.prebuilt import ToolNode, tools_conditionclass State(MessagesState):    context: List[Document]# Step 1: Generate an AIMessage that may include a tool-call to be sent.def query_or_respond(state: State):    \"\"\"Generate tool call for retrieval or respond.\"\"\"    llm_with_tools = llm.bind_tools([retrieve])    response = llm_with_tools.invoke(state[\"messages\"])    # MessagesState appends messages to state instead of overwriting    return {\"messages\": [response]}# Step 2: Execute the retrieval.tools = ToolNode([retrieve])# Step 3: Generate a response using the retrieved content.def generate(state: MessagesState):    \"\"\"Generate answer.\"\"\"    # Get generated ToolMessages    recent_tool_messages = []    for message in reversed(state[\"messages\"]):        if message.type == \"tool\":            recent_tool_messages.append(message)        else:            break    tool_messages = recent_tool_messages[::-1]    # Format into prompt    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)    system_message_content = (        \"You are an assistant for question-answering tasks. \"        \"Use the following pieces of retrieved context to answer \"        \"the question. If you don't know the answer, say that you \"        \"don't know. Use three sentences maximum and keep the \"        \"answer concise.\"        \"\\n\\n\"        f\"{docs_content}\"    )    conversation_messages = [        message        for message in state[\"messages\"]        if message.type in (\"human\", \"system\")        or (message.type == \"ai\" and not message.tool_calls)    ]    prompt = [SystemMessage(system_message_content)] + conversation_messages    # Run    response = llm.invoke(prompt)    context = []    for tool_message in tool_messages:        context.extend(tool_message.artifact)    return {\"messages\": [response], \"context\": context}\n```\n\nWe can compile the application as before:\n\n```\ngraph_builder = StateGraph(MessagesState)graph_builder.add_node(query_or_respond)graph_builder.add_node(tools)graph_builder.add_node(generate)graph_builder.set_entry_point(\"query_or_respond\")graph_builder.add_conditional_edges(    \"query_or_respond\",    tools_condition,    {END: END, \"tools\": \"tools\"},)graph_builder.add_edge(\"tools\", \"generate\")graph_builder.add_edge(\"generate\", END)graph = graph_builder.compile()\n```\n\n```\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n```\n\nInvoking our application, we see that the retrieved Document objects are accessible from the application state.\n\n```\ninput_message = \"What is Task Decomposition?\"for step in graph.stream(    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},    stream_mode=\"values\",):    step[\"messages\"][-1].pretty_print()\n```\n\n```\n================================\u001b[1m Human Message \u001b[0m=================================What is Task Decomposition?==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls:  retrieve (call_oA0XZ5hF70X0oW4ccNUFCFxX) Call ID: call_oA0XZ5hF70X0oW4ccNUFCFxX  Args:    query: Task Decomposition=================================\u001b[1m Tool Message \u001b[0m=================================Name: retrieveSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}Content: Fig. 1. Overview of a LLM-powered autonomous agent system.Component One: Planning#A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.Task Decomposition#Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.==================================\u001b[1m Ai Message \u001b[0m==================================Task Decomposition is the process of breaking down a complicated task into smaller, manageable steps. It often utilizes techniques like Chain of Thought (CoT) prompting, which encourages models to think step by step, enhancing performance on complex tasks. This approach helps clarify the model's reasoning and makes it easier to tackle difficult problems.\n```\n\n```\nstep[\"context\"]\n```\n\n```\n[Document(id='c8471b37-07d8-4d51-856e-4b2c22bca88d', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.'), Document(id='acb7eb6f-f252-4353-aec2-f459135354ba', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.')]\n```\n\nCheck out the LangSmith trace.\n\n- Dependencies\n- LangSmith\n- Components", "url_final": "https://python.langchain.com/docs/how_to/qa_sources/", "url_citable": "https://python.langchain.com/docs/how_to/qa_sources/#conversational-rag", "title": "How to get your RAG application to return sources | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Conversational RAG‚Äã", "section_anchor": "conversational-rag", "fetched_at": "2025-10-06T21:56:11.899561", "has_code": true}
{"id": "923aa8c95d929768eab39f9d1263e635c46d7784", "text": "High level‚Äã\n\n- Why LangChain?: Overview of the value that LangChain provides.\n- Architecture: How packages are organized in the LangChain ecosystem.", "url_final": "https://python.langchain.com/docs/concepts/", "url_citable": "https://python.langchain.com/docs/concepts/#high-level", "title": "Conceptual guide | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "High level‚Äã", "section_anchor": "high-level", "fetched_at": "2025-10-06T21:54:00.940163", "has_code": false}
{"id": "85770386e1210f36f39750357d67376bf80f9fbb", "text": "Concepts‚Äã\n\n- Chat models: LLMs exposed via a chat API that process sequences of messages as input and output a message.\n- Messages: The unit of communication in chat models, used to represent model input and output.\n- Chat history: A conversation represented as a sequence of messages, alternating between user messages and model responses.\n- Tools: A function with an associated schema defining the function's name, description, and the arguments it accepts.\n- Tool calling: A type of chat model API that accepts tool schemas, along with messages, as input and returns invocations of those tools as part of the output message.\n- Structured output: A technique to make a chat model respond in a structured format, such as JSON that matches a given schema.\n- Memory: Information about a conversation that is persisted so that it can be used in future conversations.\n- Multimodality: The ability to work with data that comes in different forms, such as text, audio, images, and video.\n- Runnable interface: The base abstraction that many LangChain components and the LangChain Expression Language are built on.\n- Streaming: LangChain streaming APIs for surfacing results as they are generated.\n- LangChain Expression Language (LCEL): A syntax for orchestrating LangChain components. Most useful for simpler applications.\n- Document loaders: Load a source as a list of documents.\n- Retrieval: Information retrieval systems can retrieve structured or unstructured data from a datasource in response to a query.\n- Text splitters: Split long text into smaller chunks that can be individually indexed to enable granular retrieval.\n- Embedding models: Models that represent data such as text or images in a vector space.\n- Vector stores: Storage of and efficient search over vectors and associated metadata.\n- Retriever: A component that returns relevant documents from a knowledge base in response to a query.\n- Retrieval Augmented Generation (RAG): A technique that enhances language models by combining them with external knowledge bases.\n- Agents: Use a language model to choose a sequence of actions to take. Agents can interact with external resources via tools.\n- Prompt templates: Component for factoring out the static parts of a model \"prompt\" (usually a sequence of messages). Useful for serializing, versioning, and reusing these static parts.\n- Output parsers: Responsible for taking the output of a model and transforming it into a more suitable format for downstream tasks. Output parsers were primarily useful prior to the general availability of tool calling and structured outputs.\n- Few-shot prompting: A technique for improving model performance by providing a few examples of the task to perform in the prompt.\n- Example selectors: Used to select the most relevant examples from a dataset based on a given input. Example selectors are used in few-shot prompting to select examples for a prompt.\n- Async programming: The basics that one should know to use LangChain in an asynchronous context.\n- Callbacks: Callbacks enable the execution of custom auxiliary code in built-in components. Callbacks are used to stream outputs from LLMs in LangChain, trace the intermediate steps of an application, and more.\n- Tracing: The process of recording the steps that an application takes to go from input to output. Tracing is essential for debugging and diagnosing issues in complex applications.\n- Evaluation: The process of assessing the performance and effectiveness of AI applications. This involves testing the model's responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose. This process is vital for building reliable applications.\n- Testing: The process of verifying that a component of an integration or application works as expected. Testing is essential for ensuring that the application behaves correctly and that changes to the codebase do not introduce new bugs.", "url_final": "https://python.langchain.com/docs/concepts/", "url_citable": "https://python.langchain.com/docs/concepts/#concepts", "title": "Conceptual guide | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Concepts‚Äã", "section_anchor": "concepts", "fetched_at": "2025-10-06T21:54:00.940163", "has_code": false}
{"id": "eda9081953e3218233722381d12a89abc415c344", "text": "Glossary‚Äã\n\n- AIMessageChunk: A partial response from an AI message. Used when streaming responses from a chat model.\n- AIMessage: Represents a complete response from an AI model.\n- astream_events: Stream granular information from LCEL chains.\n- BaseTool: The base class for all tools in LangChain.\n- batch: Used to execute a runnable with batch inputs.\n- bind_tools: Allows models to interact with tools.\n- Caching: Storing results to avoid redundant calls to a chat model.\n- Chat models: Chat models that handle multiple data modalities.\n- Configurable runnables: Creating configurable Runnables.\n- Context window: The maximum size of input a chat model can process.\n- Conversation patterns: Common patterns in chat interactions.\n- Document: LangChain's representation of a document.\n- Embedding models: Models that generate vector embeddings for various data types.\n- HumanMessage: Represents a message from a human user.\n- InjectedState: A state injected into a tool function.\n- InjectedStore: A store that can be injected into a tool for data persistence.\n- InjectedToolArg: Mechanism to inject arguments into tool functions.\n- input and output types: Types used for input and output in Runnables.\n- Integration packages: Third-party packages that integrate with LangChain.\n- Integration tests: Tests that verify the correctness of the interaction between components, usually run with access to the underlying API that powers an integration.\n- invoke: A standard method to invoke a Runnable.\n- JSON mode: Returning responses in JSON format.\n- langchain-community: Community-driven components for LangChain.\n- langchain-core: Core langchain package. Includes base interfaces and in-memory implementations.\n- langchain: A package for higher level components (e.g., some pre-built chains).\n- langgraph: Powerful orchestration layer for LangChain. Use to build complex pipelines and workflows.\n- langserve: Used to deploy LangChain Runnables as REST endpoints. Uses FastAPI. Works primarily for LangChain Runnables, does not currently integrate with LangGraph.\n- LLMs (legacy): Older language models that take a string as input and return a string as output.\n- Managing chat history: Techniques to maintain and manage the chat history.\n- OpenAI format: OpenAI's message format for chat models.\n- Propagation of RunnableConfig: Propagating configuration through Runnables. Read if working with python 3.9, 3.10 and async.\n- rate-limiting: Client side rate limiting for chat models.\n- RemoveMessage: An abstraction used to remove a message from chat history, used primarily in LangGraph.\n- role: Represents the role (e.g., user, assistant) of a chat message.\n- RunnableConfig: Use to pass run time information to Runnables (e.g., run_name, run_id, tags, metadata, max_concurrency, recursion_limit, configurable).\n- Standard parameters for chat models: Parameters such as API key, temperature, and max_tokens.\n- Standard tests: A defined set of unit and integration tests that all integrations must pass.\n- stream: Use to stream output from a Runnable or a graph.\n- Tokenization: The process of converting data into tokens and vice versa.\n- Tokens: The basic unit that a language model reads, processes, and generates under the hood.\n- Tool artifacts: Add artifacts to the output of a tool that will not be sent to the model, but will be available for downstream processing.\n- Tool binding: Binding tools to models.\n- @tool: Decorator for creating tools in LangChain.\n- Toolkits: A collection of tools that can be used together.\n- ToolMessage: Represents a message that contains the results of a tool execution.\n- Unit tests: Tests that verify the correctness of individual components, run in isolation without access to the Internet.\n- Vector stores: Datastores specialized for storing and efficiently searching vector embeddings.\n- with_structured_output: A helper method for chat models that natively support tool calling to get structured output matching a given schema specified via Pydantic, JSON schema or a function.\n- with_types: Method to overwrite the input and output types of a runnable. Useful when working with complex LCEL chains and deploying with LangServe.\n\n- High level\n- Concepts\n- Glossary", "url_final": "https://python.langchain.com/docs/concepts/", "url_citable": "https://python.langchain.com/docs/concepts/#glossary", "title": "Conceptual guide | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Glossary‚Äã", "section_anchor": "glossary", "fetched_at": "2025-10-06T21:54:00.940163", "has_code": false}
{"id": "893f0c30487a3148c357cc233f7e185ea8acdb0a", "text": "‚ö†Ô∏è Security note ‚ö†Ô∏è‚Äã\n\nBuilding Q&A systems of graph databases requires executing model-generated graph queries. There are inherent risks in doing this. Make sure that your database connection permissions are always scoped as narrowly as possible for your chain/agent's needs. This will mitigate though not eliminate the risks of building a model-driven system. For more on general security best practices, see here.", "url_final": "https://python.langchain.com/docs/tutorials/graph/", "url_citable": "https://python.langchain.com/docs/tutorials/graph/#Ô∏è-security-note-Ô∏è", "title": "Build a Question Answering application over a Graph Database | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "‚ö†Ô∏è Security note ‚ö†Ô∏è‚Äã", "section_anchor": "Ô∏è-security-note-Ô∏è", "fetched_at": "2025-10-06T21:54:04.421798", "has_code": false}
{"id": "a6741bceec7ef9f6e0806e9234ad0d31cfc7dd81", "text": "Architecture‚Äã\n\nAt a high-level, the steps of most graph chains are:\n\n- Convert question to a graph database query: Model converts user input to a graph database query (e.g. Cypher).\n- Execute graph database query: Execute the graph database query.\n- Answer the question: Model responds to user input using the query results.", "url_final": "https://python.langchain.com/docs/tutorials/graph/", "url_citable": "https://python.langchain.com/docs/tutorials/graph/#architecture", "title": "Build a Question Answering application over a Graph Database | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Architecture‚Äã", "section_anchor": "architecture", "fetched_at": "2025-10-06T21:54:04.421798", "has_code": false}
{"id": "dbab6a9f76ea5b4ff2ac71293c9a3385c6b90647", "text": "Setup‚Äã\n\nFirst, get required packages and set environment variables. In this example, we will be using Neo4j graph database.\n\n```\n%pip install --upgrade --quiet langchain langchain-neo4j langchain-openai langgraph\n```\n\nWe default to OpenAI models in this guide.\n\n```\nimport getpassimport osif \"OPENAI_API_KEY\" not in os.environ:    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")# Uncomment the below to use LangSmith. Not required.# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n```\n\n```\nEnter your OpenAI API key:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n```\n\nNext, we need to define Neo4j credentials. Follow these installation steps to set up a Neo4j database.\n\n```\nos.environ[\"NEO4J_URI\"] = \"bolt://localhost:7687\"os.environ[\"NEO4J_USERNAME\"] = \"neo4j\"os.environ[\"NEO4J_PASSWORD\"] = \"password\"\n```\n\nThe below example will create a connection with a Neo4j database and will populate it with example data about movies and their actors.\n\n```\nfrom langchain_neo4j import Neo4jGraphgraph = Neo4jGraph()# Import movie informationmovies_query = \"\"\"LOAD CSV WITH HEADERS FROM 'https://raw.githubusercontent.com/tomasonjo/blog-datasets/main/movies/movies_small.csv'AS rowMERGE (m:Movie {id:row.movieId})SET m.released = date(row.released),    m.title = row.title,    m.imdbRating = toFloat(row.imdbRating)FOREACH (director in split(row.director, '|') |     MERGE (p:Person {name:trim(director)})    MERGE (p)-[:DIRECTED]->(m))FOREACH (actor in split(row.actors, '|') |     MERGE (p:Person {name:trim(actor)})    MERGE (p)-[:ACTED_IN]->(m))FOREACH (genre in split(row.genres, '|') |     MERGE (g:Genre {name:trim(genre)})    MERGE (m)-[:IN_GENRE]->(g))\"\"\"graph.query(movies_query)\n```\n\n```\n[]\n```", "url_final": "https://python.langchain.com/docs/tutorials/graph/", "url_citable": "https://python.langchain.com/docs/tutorials/graph/#setup", "title": "Build a Question Answering application over a Graph Database | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Setup‚Äã", "section_anchor": "setup", "fetched_at": "2025-10-06T21:54:04.421798", "has_code": true}
{"id": "b3693a07e11a1caf0714bcb59c7e702a7c5e00be", "text": "Graph schema‚Äã\n\nIn order for an LLM to be able to generate a Cypher statement, it needs information about the graph schema. When you instantiate a graph object, it retrieves the information about the graph schema. If you later make any changes to the graph, you can run the refresh_schema method to refresh the schema information.\n\n```\ngraph.refresh_schema()print(graph.schema)\n```\n\n```\nNode properties:Person {name: STRING}Movie {id: STRING, released: DATE, title: STRING, imdbRating: FLOAT}Genre {name: STRING}Chunk {id: STRING, embedding: LIST, text: STRING, question: STRING, query: STRING}Relationship properties:The relationships:(:Person)-[:DIRECTED]->(:Movie)(:Person)-[:ACTED_IN]->(:Movie)(:Movie)-[:IN_GENRE]->(:Genre)\n```\n\nFor more involved schema information, you can use enhanced_schema option.\n\n```\nenhanced_graph = Neo4jGraph(enhanced_schema=True)print(enhanced_graph.schema)\n```\n\n```\nReceived notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The procedure has a deprecated field. ('config' used by 'apoc.meta.graphSample' is deprecated.)} {position: line: 1, column: 1, offset: 0} for query: \"CALL apoc.meta.graphSample() YIELD nodes, relationships RETURN nodes, [rel in relationships | {name:apoc.any.property(rel, 'type'), count: apoc.any.property(rel, 'count')}] AS relationships\"``````outputNode properties:- **Person**  - `name`: STRING Example: \"John Lasseter\"- **Movie**  - `id`: STRING Example: \"1\"  - `released`: DATE Min: 1964-12-16, Max: 1996-09-15  - `title`: STRING Example: \"Toy Story\"  - `imdbRating`: FLOAT Min: 2.4, Max: 9.3- **Genre**  - `name`: STRING Example: \"Adventure\"- **Chunk**  - `id`: STRING Available options: ['d66006059fd78d63f3df90cc1059639a', '0e3dcb4502853979d12357690a95ec17', 'c438c6bcdcf8e4fab227f29f8e7ff204', '97fe701ec38057594464beaa2df0710e', 'b54f9286e684373498c4504b4edd9910', '5b50a72c3a4954b0ff7a0421be4f99b9', 'fb28d41771e717255f0d8f6c799ede32', '58e6f14dd2e6c6702cf333f2335c499c']  - `text`: STRING Available options: ['How many artists are there?', 'Which actors played in the movie Casino?', 'How many movies has Tom Hanks acted in?', \"List all the genres of the movie Schindler's List\", 'Which actors have worked in movies from both the c', 'Which directors have made movies with at least thr', 'Identify movies where directors also played a role', 'Find the actor with the highest number of movies i']  - `question`: STRING Available options: ['How many artists are there?', 'Which actors played in the movie Casino?', 'How many movies has Tom Hanks acted in?', \"List all the genres of the movie Schindler's List\", 'Which actors have worked in movies from both the c', 'Which directors have made movies with at least thr', 'Identify movies where directors also played a role', 'Find the actor with the highest number of movies i']  - `query`: STRING Available options: ['MATCH (a:Person)-[:ACTED_IN]->(:Movie) RETURN coun', \"MATCH (m:Movie {title: 'Casino'})<-[:ACTED_IN]-(a)\", \"MATCH (a:Person {name: 'Tom Hanks'})-[:ACTED_IN]->\", \"MATCH (m:Movie {title: 'Schindler's List'})-[:IN_G\", 'MATCH (a:Person)-[:ACTED_IN]->(:Movie)-[:IN_GENRE]', 'MATCH (d:Person)-[:DIRECTED]->(m:Movie)<-[:ACTED_I', 'MATCH (p:Person)-[:DIRECTED]->(m:Movie), (p)-[:ACT', 'MATCH (a:Actor)-[:ACTED_IN]->(m:Movie) RETURN a.na']Relationship properties:The relationships:(:Person)-[:DIRECTED]->(:Movie)(:Person)-[:ACTED_IN]->(:Movie)(:Movie)-[:IN_GENRE]->(:Genre)\n```\n\nThe enhanced_schema option enriches property information by including details such as minimum and maximum values for floats and dates, as well as example values for string properties. This additional context helps guide the LLM toward generating more accurate and effective queries.\n\nGreat! We've got a graph database that we can query. Now let's try hooking it up to an LLM.", "url_final": "https://python.langchain.com/docs/tutorials/graph/", "url_citable": "https://python.langchain.com/docs/tutorials/graph/#graph-schema", "title": "Build a Question Answering application over a Graph Database | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Graph schema‚Äã", "section_anchor": "graph-schema", "fetched_at": "2025-10-06T21:54:04.421798", "has_code": true}
{"id": "2fb79ad5dd26dab971ce7df90715ee6af93fb633", "text": "GraphQACypherChain‚Äã\n\nLet's use a simple out-of-the-box chain that takes a question, turns it into a Cypher query, executes the query, and uses the result to answer the original question.\n\nLangChain comes with a built-in chain for this workflow that is designed to work with Neo4j: GraphCypherQAChain\n\n```\nfrom langchain_neo4j import GraphCypherQAChainfrom langchain_openai import ChatOpenAIllm = ChatOpenAI(model=\"gpt-4o\", temperature=0)chain = GraphCypherQAChain.from_llm(    graph=enhanced_graph, llm=llm, verbose=True, allow_dangerous_requests=True)response = chain.invoke({\"query\": \"What was the cast of the Casino?\"})response\n```\n\n```\n\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0mGenerated Cypher:\u001b[32;1m\u001b[1;3mcypherMATCH (p:Person)-[:ACTED_IN]->(m:Movie {title: \"Casino\"})RETURN p.name\u001b[0mFull Context:\u001b[32;1m\u001b[1;3m[{'p.name': 'Robert De Niro'}, {'p.name': 'Joe Pesci'}, {'p.name': 'Sharon Stone'}, {'p.name': 'James Woods'}]\u001b[0m\u001b[1m> Finished chain.\u001b[0m\n```\n\n```\n{'query': 'What was the cast of the Casino?', 'result': 'Robert De Niro, Joe Pesci, Sharon Stone, and James Woods were the cast of Casino.'}\n```", "url_final": "https://python.langchain.com/docs/tutorials/graph/", "url_citable": "https://python.langchain.com/docs/tutorials/graph/#graphqacypherchain", "title": "Build a Question Answering application over a Graph Database | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "GraphQACypherChain‚Äã", "section_anchor": "graphqacypherchain", "fetched_at": "2025-10-06T21:54:04.421798", "has_code": true}
{"id": "9c9c571ea9fbfc7a3f6b765d56c91245c15d0e19", "text": "Advanced implementation with LangGraph‚Äã\n\nWhile the GraphCypherQAChain is effective for quick demonstrations, it may face challenges in production environments. Transitioning to LangGraph can enhance the workflow, but implementing natural language to query flows in production remains a complex task. Nevertheless, there are several strategies to significantly improve accuracy and reliability, which we will explore next.\n\nHere is the visualized LangGraph flow we will implement:\n\nWe will begin by defining the Input, Output, and Overall state of the LangGraph application.\n\n```\nfrom operator import addfrom typing import Annotated, Listfrom typing_extensions import TypedDictclass InputState(TypedDict):    question: strclass OverallState(TypedDict):    question: str    next_action: str    cypher_statement: str    cypher_errors: List[str]    database_records: List[dict]    steps: Annotated[List[str], add]class OutputState(TypedDict):    answer: str    steps: List[str]    cypher_statement: str\n```\n\nThe first step is a simple guardrails step, where we validate whether the question pertains to movies or their cast. If it doesn't, we notify the user that we cannot answer any other questions. Otherwise, we move on to the Cypher generation step.\n\n```\nfrom typing import Literalfrom langchain_core.prompts import ChatPromptTemplatefrom pydantic import BaseModel, Fieldguardrails_system = \"\"\"As an intelligent assistant, your primary objective is to decide whether a given question is related to movies or not. If the question is related to movies, output \"movie\". Otherwise, output \"end\".To make this decision, assess the content of the question and determine if it refers to any movie, actor, director, film industry, or related topics. Provide only the specified output: \"movie\" or \"end\".\"\"\"guardrails_prompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            guardrails_system,        ),        (            \"human\",            (\"{question}\"),        ),    ])class GuardrailsOutput(BaseModel):    decision: Literal[\"movie\", \"end\"] = Field(        description=\"Decision on whether the question is related to movies\"    )guardrails_chain = guardrails_prompt | llm.with_structured_output(GuardrailsOutput)def guardrails(state: InputState) -> OverallState:    \"\"\"    Decides if the question is related to movies or not.    \"\"\"    guardrails_output = guardrails_chain.invoke({\"question\": state.get(\"question\")})    database_records = None    if guardrails_output.decision == \"end\":        database_records = \"This questions is not about movies or their cast. Therefore I cannot answer this question.\"    return {        \"next_action\": guardrails_output.decision,        \"database_records\": database_records,        \"steps\": [\"guardrail\"],    }\n```", "url_final": "https://python.langchain.com/docs/tutorials/graph/", "url_citable": "https://python.langchain.com/docs/tutorials/graph/#advanced-implementation-with-langgraph", "title": "Build a Question Answering application over a Graph Database | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Advanced implementation with LangGraph‚Äã", "section_anchor": "advanced-implementation-with-langgraph", "fetched_at": "2025-10-06T21:54:04.421798", "has_code": true}
{"id": "32bda811a51fcfd54028e6c73b1cebbd42106976", "text": "Few-shot prompting‚Äã\n\nConverting natural language into accurate queries is challenging. One way to enhance this process is by providing relevant few-shot examples to guide the LLM in query generation. To achieve this, we will use the SemanticSimilarityExampleSelector to dynamically select the most relevant examples.\n\n```\nfrom langchain_core.example_selectors import SemanticSimilarityExampleSelectorfrom langchain_neo4j import Neo4jVectorfrom langchain_openai import OpenAIEmbeddingsexamples = [    {        \"question\": \"How many artists are there?\",        \"query\": \"MATCH (a:Person)-[:ACTED_IN]->(:Movie) RETURN count(DISTINCT a)\",    },    {        \"question\": \"Which actors played in the movie Casino?\",        \"query\": \"MATCH (m:Movie {title: 'Casino'})<-[:ACTED_IN]-(a) RETURN a.name\",    },    {        \"question\": \"How many movies has Tom Hanks acted in?\",        \"query\": \"MATCH (a:Person {name: 'Tom Hanks'})-[:ACTED_IN]->(m:Movie) RETURN count(m)\",    },    {        \"question\": \"List all the genres of the movie Schindler's List\",        \"query\": \"MATCH (m:Movie {title: 'Schindler's List'})-[:IN_GENRE]->(g:Genre) RETURN g.name\",    },    {        \"question\": \"Which actors have worked in movies from both the comedy and action genres?\",        \"query\": \"MATCH (a:Person)-[:ACTED_IN]->(:Movie)-[:IN_GENRE]->(g1:Genre), (a)-[:ACTED_IN]->(:Movie)-[:IN_GENRE]->(g2:Genre) WHERE g1.name = 'Comedy' AND g2.name = 'Action' RETURN DISTINCT a.name\",    },    {        \"question\": \"Which directors have made movies with at least three different actors named 'John'?\",        \"query\": \"MATCH (d:Person)-[:DIRECTED]->(m:Movie)<-[:ACTED_IN]-(a:Person) WHERE a.name STARTS WITH 'John' WITH d, COUNT(DISTINCT a) AS JohnsCount WHERE JohnsCount >= 3 RETURN d.name\",    },    {        \"question\": \"Identify movies where directors also played a role in the film.\",        \"query\": \"MATCH (p:Person)-[:DIRECTED]->(m:Movie), (p)-[:ACTED_IN]->(m) RETURN m.title, p.name\",    },    {        \"question\": \"Find the actor with the highest number of movies in the database.\",        \"query\": \"MATCH (a:Actor)-[:ACTED_IN]->(m:Movie) RETURN a.name, COUNT(m) AS movieCount ORDER BY movieCount DESC LIMIT 1\",    },]example_selector = SemanticSimilarityExampleSelector.from_examples(    examples, OpenAIEmbeddings(), Neo4jVector, k=5, input_keys=[\"question\"])\n```\n\nNext, we implement the Cypher generation chain, also known as text2cypher. The prompt includes an enhanced graph schema, dynamically selected few-shot examples, and the user‚Äôs question. This combination enables the generation of a Cypher query to retrieve relevant information from the database.\n\n```\nfrom langchain_core.output_parsers import StrOutputParsertext2cypher_prompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            (                \"Given an input question, convert it to a Cypher query. No pre-amble.\"                \"Do not wrap the response in any backticks or anything else. Respond with a Cypher statement only!\"            ),        ),        (            \"human\",            (                \"\"\"You are a Neo4j expert. Given an input question, create a syntactically correct Cypher query to run.Do not wrap the response in any backticks or anything else. Respond with a Cypher statement only!Here is the schema information{schema}Below are a number of examples of questions and their corresponding Cypher queries.{fewshot_examples}User input: {question}Cypher query:\"\"\"            ),        ),    ])text2cypher_chain = text2cypher_prompt | llm | StrOutputParser()def generate_cypher(state: OverallState) -> OverallState:    \"\"\"    Generates a cypher statement based on the provided schema and user input    \"\"\"    NL = \"\\n\"    fewshot_examples = (NL * 2).join(        [            f\"Question: {el['question']}{NL}Cypher:{el['query']}\"            for el in example_selector.select_examples(                {\"question\": state.get(\"question\")}            )        ]    )    generated_cypher = text2cypher_chain.invoke(        {            \"question\": state.get(\"question\"),            \"fewshot_examples\": fewshot_examples,            \"schema\": enhanced_graph.schema,        }    )    return {\"cypher_statement\": generated_cypher, \"steps\": [\"generate_cypher\"]}\n```", "url_final": "https://python.langchain.com/docs/tutorials/graph/", "url_citable": "https://python.langchain.com/docs/tutorials/graph/#few-shot-prompting", "title": "Build a Question Answering application over a Graph Database | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Few-shot prompting‚Äã", "section_anchor": "few-shot-prompting", "fetched_at": "2025-10-06T21:54:04.421798", "has_code": true}
{"id": "43c6e321f9e4e474a10950ef549c1eae791a1c2a", "text": "Query validation‚Äã\n\nThe next step is to validate the generated Cypher statement and ensuring that all property values are accurate. While numbers and dates typically don‚Äôt require validation, strings such as movie titles or people‚Äôs names do. In this example, we‚Äôll use a basic CONTAINS clause for validation, though more advanced mapping and validation techniques can be implemented if needed.\n\nFirst, we will create a chain that detects any errors in the Cypher statement and extracts the property values it references.\n\n```\nfrom typing import List, Optionalvalidate_cypher_system = \"\"\"You are a Cypher expert reviewing a statement written by a junior developer.\"\"\"validate_cypher_user = \"\"\"You must check the following:* Are there any syntax errors in the Cypher statement?* Are there any missing or undefined variables in the Cypher statement?* Are any node labels missing from the schema?* Are any relationship types missing from the schema?* Are any of the properties not included in the schema?* Does the Cypher statement include enough information to answer the question?Examples of good errors:* Label (:Foo) does not exist, did you mean (:Bar)?* Property bar does not exist for label Foo, did you mean baz?* Relationship FOO does not exist, did you mean FOO_BAR?Schema:{schema}The question is:{question}The Cypher statement is:{cypher}Make sure you don't make any mistakes!\"\"\"validate_cypher_prompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            validate_cypher_system,        ),        (            \"human\",            (validate_cypher_user),        ),    ])class Property(BaseModel):    \"\"\"    Represents a filter condition based on a specific node property in a graph in a Cypher statement.    \"\"\"    node_label: str = Field(        description=\"The label of the node to which this property belongs.\"    )    property_key: str = Field(description=\"The key of the property being filtered.\")    property_value: str = Field(        description=\"The value that the property is being matched against.\"    )class ValidateCypherOutput(BaseModel):    \"\"\"    Represents the validation result of a Cypher query's output,    including any errors and applied filters.    \"\"\"    errors: Optional[List[str]] = Field(        description=\"A list of syntax or semantical errors in the Cypher statement. Always explain the discrepancy between schema and Cypher statement\"    )    filters: Optional[List[Property]] = Field(        description=\"A list of property-based filters applied in the Cypher statement.\"    )validate_cypher_chain = validate_cypher_prompt | llm.with_structured_output(    ValidateCypherOutput)\n```\n\nLLMs often struggle with correctly determining relationship directions in generated Cypher statements. Since we have access to the schema, we can deterministically correct these directions using the CypherQueryCorrector.\n\nNote: The CypherQueryCorrector is an experimental feature and doesn't support all the newest Cypher syntax.\n\n```\nfrom langchain_neo4j.chains.graph_qa.cypher_utils import CypherQueryCorrector, Schema# Cypher query corrector is experimentalcorrector_schema = [    Schema(el[\"start\"], el[\"type\"], el[\"end\"])    for el in enhanced_graph.structured_schema.get(\"relationships\")]cypher_query_corrector = CypherQueryCorrector(corrector_schema)\n```\n\nNow we can implement the Cypher validation step. First, we use the EXPLAIN method to detect any syntax errors. Next, we leverage the LLM to identify potential issues and extract the properties used for filtering. For string properties, we validate them against the database using a simple CONTAINS clause.\n\nBased on the validation results, the process can take the following paths:\n\n- If value mapping fails, we end the conversation and inform the user that we couldn't identify a specific property value (e.g., a person or movie title).\n- If errors are found, we route the query for correction.\n- If no issues are detected, we proceed to the Cypher execution step.\n\n```\nfrom neo4j.exceptions import CypherSyntaxErrordef validate_cypher(state: OverallState) -> OverallState:    \"\"\"    Validates the Cypher statements and maps any property values to the database.    \"\"\"    errors = []    mapping_errors = []    # Check for syntax errors    try:        enhanced_graph.query(f\"EXPLAIN {state.get('cypher_statement')}\")    except CypherSyntaxError as e:        errors.append(e.message)    # Experimental feature for correcting relationship directions    corrected_cypher = cypher_query_corrector(state.get(\"cypher_statement\"))    if not corrected_cypher:        errors.append(\"The generated Cypher statement doesn't fit the graph schema\")    if not corrected_cypher == state.get(\"cypher_statement\"):        print(\"Relationship direction was corrected\")    # Use LLM to find additional potential errors and get the mapping for values    llm_output = validate_cypher_chain.invoke(        {            \"question\": state.get(\"question\"),            \"schema\": enhanced_graph.schema,            \"cypher\": state.get(\"cypher_statement\"),        }    )    if llm_output.errors:        errors.extend(llm_output.errors)    if llm_output.filters:        for filter in llm_output.filters:            # Do mapping only for string values            if (                not [                    prop                    for prop in enhanced_graph.structured_schema[\"node_props\"][                        filter.node_label                    ]                    if prop[\"property\"] == filter.property_key                ][0][\"type\"]                == \"STRING\"            ):                continue            mapping = enhanced_graph.query(                f\"MATCH (n:{filter.node_label}) WHERE toLower(n.`{filter.property_key}`) = toLower($value) RETURN 'yes' LIMIT 1\",                {\"value\": filter.property_value},            )            if not mapping:                print(                    f\"Missing value mapping for {filter.node_label} on property {filter.property_key} with value {filter.property_value}\"                )                mapping_errors.append(                    f\"Missing value mapping for {filter.node_label} on property {filter.property_key} with value {filter.property_value}\"                )    if mapping_errors:        next_action = \"end\"    elif errors:        next_action = \"correct_cypher\"    else:        next_action = \"execute_cypher\"    return {        \"next_action\": next_action,        \"cypher_statement\": corrected_cypher,        \"cypher_errors\": errors,        \"steps\": [\"validate_cypher\"],    }\n```\n\nThe Cypher correction step takes the existing Cypher statement, any identified errors, and the original question to generate a corrected version of the query.\n\n```\ncorrect_cypher_prompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            (                \"You are a Cypher expert reviewing a statement written by a junior developer. \"                \"You need to correct the Cypher statement based on the provided errors. No pre-amble.\"                \"Do not wrap the response in any backticks or anything else. Respond with a Cypher statement only!\"            ),        ),        (            \"human\",            (                \"\"\"Check for invalid syntax or semantics and return a corrected Cypher statement.Schema:{schema}Note: Do not include any explanations or apologies in your responses.Do not wrap the response in any backticks or anything else.Respond with a Cypher statement only!Do not respond to any questions that might ask anything else than for you to construct a Cypher statement.The question is:{question}The Cypher statement is:{cypher}The errors are:{errors}Corrected Cypher statement: \"\"\"            ),        ),    ])correct_cypher_chain = correct_cypher_prompt | llm | StrOutputParser()def correct_cypher(state: OverallState) -> OverallState:    \"\"\"    Correct the Cypher statement based on the provided errors.    \"\"\"    corrected_cypher = correct_cypher_chain.invoke(        {            \"question\": state.get(\"question\"),            \"errors\": state.get(\"cypher_errors\"),            \"cypher\": state.get(\"cypher_statement\"),            \"schema\": enhanced_graph.schema,        }    )    return {        \"next_action\": \"validate_cypher\",        \"cypher_statement\": corrected_cypher,        \"steps\": [\"correct_cypher\"],    }\n```\n\nWe need to add a step that executes the given Cypher statement. If no results are returned, we should explicitly handle this scenario, as leaving the context empty can sometimes lead to LLM hallucinations.\n\n```\nno_results = \"I couldn't find any relevant information in the database\"def execute_cypher(state: OverallState) -> OverallState:    \"\"\"    Executes the given Cypher statement.    \"\"\"    records = enhanced_graph.query(state.get(\"cypher_statement\"))    return {        \"database_records\": records if records else no_results,        \"next_action\": \"end\",        \"steps\": [\"execute_cypher\"],    }\n```\n\nThe final step is to generate the answer. This involves combining the initial question with the database output to produce a relevant response.\n\n```\ngenerate_final_prompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"You are a helpful assistant\",        ),        (            \"human\",            (                \"\"\"Use the following results retrieved from a database to providea succinct, definitive answer to the user's question.Respond as if you are answering the question directly.Results: {results}Question: {question}\"\"\"            ),        ),    ])generate_final_chain = generate_final_prompt | llm | StrOutputParser()def generate_final_answer(state: OverallState) -> OutputState:    \"\"\"    Decides if the question is related to movies.    \"\"\"    final_answer = generate_final_chain.invoke(        {\"question\": state.get(\"question\"), \"results\": state.get(\"database_records\")}    )    return {\"answer\": final_answer, \"steps\": [\"generate_final_answer\"]}\n```\n\nNext, we will implement the LangGraph workflow, starting with defining the conditional edge functions.\n\n```\ndef guardrails_condition(    state: OverallState,) -> Literal[\"generate_cypher\", \"generate_final_answer\"]:    if state.get(\"next_action\") == \"end\":        return \"generate_final_answer\"    elif state.get(\"next_action\") == \"movie\":        return \"generate_cypher\"def validate_cypher_condition(    state: OverallState,) -> Literal[\"generate_final_answer\", \"correct_cypher\", \"execute_cypher\"]:    if state.get(\"next_action\") == \"end\":        return \"generate_final_answer\"    elif state.get(\"next_action\") == \"correct_cypher\":        return \"correct_cypher\"    elif state.get(\"next_action\") == \"execute_cypher\":        return \"execute_cypher\"\n```\n\nLet's put it all together now.\n\n```\nfrom IPython.display import Image, displayfrom langgraph.graph import END, START, StateGraphlanggraph = StateGraph(OverallState, input=InputState, output=OutputState)langgraph.add_node(guardrails)langgraph.add_node(generate_cypher)langgraph.add_node(validate_cypher)langgraph.add_node(correct_cypher)langgraph.add_node(execute_cypher)langgraph.add_node(generate_final_answer)langgraph.add_edge(START, \"guardrails\")langgraph.add_conditional_edges(    \"guardrails\",    guardrails_condition,)langgraph.add_edge(\"generate_cypher\", \"validate_cypher\")langgraph.add_conditional_edges(    \"validate_cypher\",    validate_cypher_condition,)langgraph.add_edge(\"execute_cypher\", \"generate_final_answer\")langgraph.add_edge(\"correct_cypher\", \"validate_cypher\")langgraph.add_edge(\"generate_final_answer\", END)langgraph = langgraph.compile()# Viewdisplay(Image(langgraph.get_graph().draw_mermaid_png()))\n```\n\nWe can now test the application by asking an irrelevant question.\n\n```\nlanggraph.invoke({\"question\": \"What's the weather in Spain?\"})\n```\n\n```\n{'answer': \"I'm sorry, but I cannot provide current weather information. Please check a reliable weather website or app for the latest updates on the weather in Spain.\", 'steps': ['guardrail', 'generate_final_answer']}\n```\n\nLet's now ask something relevant about the movies.\n\n```\nlanggraph.invoke({\"question\": \"What was the cast of the Casino?\"})\n```\n\n```\n{'answer': 'The cast of \"Casino\" includes Robert De Niro, Joe Pesci, Sharon Stone, and James Woods.', 'steps': ['guardrail',  'generate_cypher',  'validate_cypher',  'execute_cypher',  'generate_final_answer'], 'cypher_statement': \"MATCH (m:Movie {title: 'Casino'})<-[:ACTED_IN]-(a:Person) RETURN a.name\"}\n```", "url_final": "https://python.langchain.com/docs/tutorials/graph/", "url_citable": "https://python.langchain.com/docs/tutorials/graph/#query-validation", "title": "Build a Question Answering application over a Graph Database | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Query validation‚Äã", "section_anchor": "query-validation", "fetched_at": "2025-10-06T21:54:04.421798", "has_code": true}
{"id": "0f2b69713dcc81d0e9242e2a76173cf1beedf27c", "text": "Next steps‚Äã\n\nFor other graph techniques like this and more check out:\n\n- Semantic layer: Techniques for implementing semantic layers.\n- Constructing graphs: Techniques for constructing knowledge graphs.\n\n- Few-shot prompting\n- Query validation\n- Next steps", "url_final": "https://python.langchain.com/docs/tutorials/graph/", "url_citable": "https://python.langchain.com/docs/tutorials/graph/#next-steps", "title": "Build a Question Answering application over a Graph Database | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Next steps‚Äã", "section_anchor": "next-steps", "fetched_at": "2025-10-06T21:54:04.421798", "has_code": false}
{"id": "8ca6c344e331a5acb9642c175794fae8be90a10f", "text": "Using the constructor‚Äã\n\nBelow, we explicitly wrap our custom logic using the RunnableLambda constructor:\n\n```\n%pip install -qU langchain langchain_openaiimport osfrom getpass import getpassif \"OPENAI_API_KEY\" not in os.environ:    os.environ[\"OPENAI_API_KEY\"] = getpass()\n```\n\n```\nfrom operator import itemgetterfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnableLambdafrom langchain_openai import ChatOpenAIdef length_function(text):    return len(text)def _multiple_length_function(text1, text2):    return len(text1) * len(text2)def multiple_length_function(_dict):    return _multiple_length_function(_dict[\"text1\"], _dict[\"text2\"])model = ChatOpenAI()prompt = ChatPromptTemplate.from_template(\"what is {a} + {b}\")chain = (    {        \"a\": itemgetter(\"foo\") | RunnableLambda(length_function),        \"b\": {\"text1\": itemgetter(\"foo\"), \"text2\": itemgetter(\"bar\")}        | RunnableLambda(multiple_length_function),    }    | prompt    | model)chain.invoke({\"foo\": \"bar\", \"bar\": \"gah\"})\n```\n\n```\nAIMessage(content='3 + 9 equals 12.', response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 14, 'total_tokens': 22}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-73728de3-e483-49e3-ad54-51bd9570e71a-0')\n```", "url_final": "https://python.langchain.com/docs/how_to/functions/", "url_citable": "https://python.langchain.com/docs/how_to/functions/#using-the-constructor", "title": "How to run custom functions | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Using the constructor‚Äã", "section_anchor": "using-the-constructor", "fetched_at": "2025-10-06T21:54:31.171568", "has_code": true}
{"id": "bfc3e43b04f05fa1b6d1c26bfb6123094d246417", "text": "The convenience @chain decorator‚Äã\n\nYou can also turn an arbitrary function into a chain by adding a @chain decorator. This is functionally equivalent to wrapping the function in a RunnableLambda constructor as shown above. Here's an example:\n\n```\nfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.runnables import chainprompt1 = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")prompt2 = ChatPromptTemplate.from_template(\"What is the subject of this joke: {joke}\")@chaindef custom_chain(text):    prompt_val1 = prompt1.invoke({\"topic\": text})    output1 = ChatOpenAI().invoke(prompt_val1)    parsed_output1 = StrOutputParser().invoke(output1)    chain2 = prompt2 | ChatOpenAI() | StrOutputParser()    return chain2.invoke({\"joke\": parsed_output1})custom_chain.invoke(\"bears\")\n```\n\n```\n'The subject of the joke is the bear and his girlfriend.'\n```\n\nAbove, the @chain decorator is used to convert custom_chain into a runnable, which we invoke with the .invoke() method.\n\nIf you are using a tracing with LangSmith, you should see a custom_chain trace in there, with the calls to OpenAI nested underneath.", "url_final": "https://python.langchain.com/docs/how_to/functions/", "url_citable": "https://python.langchain.com/docs/how_to/functions/#the-convenience-chain-decorator", "title": "How to run custom functions | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "The convenience @chain decorator‚Äã", "section_anchor": "the-convenience-chain-decorator", "fetched_at": "2025-10-06T21:54:31.171568", "has_code": true}
{"id": "26dc86a30426de47871fe8300cb49a544b703a4a", "text": "Automatic coercion in chains‚Äã\n\nWhen using custom functions in chains with the pipe operator (|), you can omit the RunnableLambda or @chain constructor and rely on coercion. Here's a simple example with a function that takes the output from the model and returns the first five letters of it:\n\n```\nprompt = ChatPromptTemplate.from_template(\"tell me a story about {topic}\")model = ChatOpenAI()chain_with_coerced_function = prompt | model | (lambda x: x.content[:5])chain_with_coerced_function.invoke({\"topic\": \"bears\"})\n```\n\n```\n'Once '\n```\n\nNote that we didn't need to wrap the custom function (lambda x: x.content[:5]) in a RunnableLambda constructor because the model on the left of the pipe operator is already a Runnable. The custom function is coerced into a runnable. See this section for more information.", "url_final": "https://python.langchain.com/docs/how_to/functions/", "url_citable": "https://python.langchain.com/docs/how_to/functions/#automatic-coercion-in-chains", "title": "How to run custom functions | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Automatic coercion in chains‚Äã", "section_anchor": "automatic-coercion-in-chains", "fetched_at": "2025-10-06T21:54:31.171568", "has_code": true}
{"id": "3f872b8c3c5f897539f8fa78c2508134cf4c6adc", "text": "Passing run metadata‚Äã\n\nRunnable lambdas can optionally accept a RunnableConfig parameter, which they can use to pass callbacks, tags, and other configuration information to nested runs.\n\n```\nimport jsonfrom langchain_core.runnables import RunnableConfigdef parse_or_fix(text: str, config: RunnableConfig):    fixing_chain = (        ChatPromptTemplate.from_template(            \"Fix the following text:\\n\\n\\`\\`\\`text\\n{input}\\n\\`\\`\\`\\nError: {error}\"            \" Don't narrate, just respond with the fixed data.\"        )        | model        | StrOutputParser()    )    for _ in range(3):        try:            return json.loads(text)        except Exception as e:            text = fixing_chain.invoke({\"input\": text, \"error\": e}, config)    return \"Failed to parse\"from langchain_community.callbacks import get_openai_callbackwith get_openai_callback() as cb:    output = RunnableLambda(parse_or_fix).invoke(        \"{foo: bar}\", {\"tags\": [\"my-tag\"], \"callbacks\": [cb]}    )    print(output)    print(cb)\n```\n\n```\n{'foo': 'bar'}Tokens Used: 62\tPrompt Tokens: 56\tCompletion Tokens: 6Successful Requests: 1Total Cost (USD): $9.6e-05\n```\n\n```\nfrom langchain_community.callbacks import get_openai_callbackwith get_openai_callback() as cb:    output = RunnableLambda(parse_or_fix).invoke(        \"{foo: bar}\", {\"tags\": [\"my-tag\"], \"callbacks\": [cb]}    )    print(output)    print(cb)\n```\n\n```\n{'foo': 'bar'}Tokens Used: 62\tPrompt Tokens: 56\tCompletion Tokens: 6Successful Requests: 1Total Cost (USD): $9.6e-05\n```", "url_final": "https://python.langchain.com/docs/how_to/functions/", "url_citable": "https://python.langchain.com/docs/how_to/functions/#passing-run-metadata", "title": "How to run custom functions | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Passing run metadata‚Äã", "section_anchor": "passing-run-metadata", "fetched_at": "2025-10-06T21:54:31.171568", "has_code": true}
{"id": "d368bdbde91cf9c791722af27a34d5d9f2f7f3dc", "text": "Streaming‚Äã\n\nRunnableLambda is best suited for code that does not need to support streaming. If you need to support streaming (i.e., be able to operate on chunks of inputs and yield chunks of outputs), use RunnableGenerator instead as in the example below.\n\nYou can use generator functions (ie. functions that use the yield keyword, and behave like iterators) in a chain.\n\nThe signature of these generators should be Iterator[Input] -> Iterator[Output]. Or for async generators: AsyncIterator[Input] -> AsyncIterator[Output].\n\nThese are useful for:\n\n- implementing a custom output parser\n- modifying the output of a previous step, while preserving streaming capabilities\n\nHere's an example of a custom output parser for comma-separated lists. First, we create a chain that generates such a list as text:\n\n```\nfrom typing import Iterator, Listprompt = ChatPromptTemplate.from_template(    \"Write a comma-separated list of 5 animals similar to: {animal}. Do not include numbers\")str_chain = prompt | model | StrOutputParser()for chunk in str_chain.stream({\"animal\": \"bear\"}):    print(chunk, end=\"\", flush=True)\n```\n\n```\nlion, tiger, wolf, gorilla, panda\n```\n\nNext, we define a custom function that will aggregate the currently streamed output and yield it when the model generates the next comma in the list:\n\n```\n# This is a custom parser that splits an iterator of llm tokens# into a list of strings separated by commasdef split_into_list(input: Iterator[str]) -> Iterator[List[str]]:    # hold partial input until we get a comma    buffer = \"\"    for chunk in input:        # add current chunk to buffer        buffer += chunk        # while there are commas in the buffer        while \",\" in buffer:            # split buffer on comma            comma_index = buffer.index(\",\")            # yield everything before the comma            yield [buffer[:comma_index].strip()]            # save the rest for the next iteration            buffer = buffer[comma_index + 1 :]    # yield the last chunk    yield [buffer.strip()]list_chain = str_chain | split_into_listfor chunk in list_chain.stream({\"animal\": \"bear\"}):    print(chunk, flush=True)\n```\n\n```\n['lion']['tiger']['wolf']['gorilla']['raccoon']\n```\n\nInvoking it gives a full array of values:\n\n```\nlist_chain.invoke({\"animal\": \"bear\"})\n```\n\n```\n['lion', 'tiger', 'wolf', 'gorilla', 'raccoon']\n```", "url_final": "https://python.langchain.com/docs/how_to/functions/", "url_citable": "https://python.langchain.com/docs/how_to/functions/#streaming", "title": "How to run custom functions | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Streaming‚Äã", "section_anchor": "streaming", "fetched_at": "2025-10-06T21:54:31.171568", "has_code": true}
{"id": "6e749a9bbc5c4814a7f513bcb78f904905e42ed8", "text": "Async version‚Äã\n\nIf you are working in an async environment, here is an async version of the above example:\n\n```\nfrom typing import AsyncIteratorasync def asplit_into_list(    input: AsyncIterator[str],) -> AsyncIterator[List[str]]:  # async def    buffer = \"\"    async for (        chunk    ) in input:  # `input` is a `async_generator` object, so use `async for`        buffer += chunk        while \",\" in buffer:            comma_index = buffer.index(\",\")            yield [buffer[:comma_index].strip()]            buffer = buffer[comma_index + 1 :]    yield [buffer.strip()]list_chain = str_chain | asplit_into_listasync for chunk in list_chain.astream({\"animal\": \"bear\"}):    print(chunk, flush=True)\n```\n\n```\n['lion']['tiger']['wolf']['gorilla']['panda']\n```\n\n```\nawait list_chain.ainvoke({\"animal\": \"bear\"})\n```\n\n```\n['lion', 'tiger', 'wolf', 'gorilla', 'panda']\n```", "url_final": "https://python.langchain.com/docs/how_to/functions/", "url_citable": "https://python.langchain.com/docs/how_to/functions/#async-version", "title": "How to run custom functions | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Async version‚Äã", "section_anchor": "async-version", "fetched_at": "2025-10-06T21:54:31.171568", "has_code": true}
{"id": "32c0a6dd7c5fc5e5638c876974e79142fd8aa42a", "text": "Next steps‚Äã\n\nNow you've learned a few different ways to use custom logic within your chains, and how to implement streaming.\n\nTo learn more, see the other how-to guides on runnables in this section.\n\n- Using the constructor\n- The convenience @chain decorator\n- Automatic coercion in chains\n- Passing run metadata\n- Streaming\n- Async version\n- Next steps", "url_final": "https://python.langchain.com/docs/how_to/functions/", "url_citable": "https://python.langchain.com/docs/how_to/functions/#next-steps", "title": "How to run custom functions | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Next steps‚Äã", "section_anchor": "next-steps", "fetched_at": "2025-10-06T21:54:31.171568", "has_code": false}
{"id": "e66ef19db57aab7e98f687cd3debaa2f21a67b1b", "text": "Basic usage‚Äã\n\nBelow we demonstrate ensembling of a BM25Retriever with a retriever derived from the FAISS vector store.\n\n```\n%pip install --upgrade --quiet  rank_bm25 > /dev/null\n```\n\n```\nfrom langchain.retrievers import EnsembleRetrieverfrom langchain_community.retrievers import BM25Retrieverfrom langchain_community.vectorstores import FAISSfrom langchain_openai import OpenAIEmbeddingsdoc_list_1 = [    \"I like apples\",    \"I like oranges\",    \"Apples and oranges are fruits\",]# initialize the bm25 retriever and faiss retrieverbm25_retriever = BM25Retriever.from_texts(    doc_list_1, metadatas=[{\"source\": 1}] * len(doc_list_1))bm25_retriever.k = 2doc_list_2 = [    \"You like apples\",    \"You like oranges\",]embedding = OpenAIEmbeddings()faiss_vectorstore = FAISS.from_texts(    doc_list_2, embedding, metadatas=[{\"source\": 2}] * len(doc_list_2))faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 2})# initialize the ensemble retrieverensemble_retriever = EnsembleRetriever(    retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5])\n```\n\n```\ndocs = ensemble_retriever.invoke(\"apples\")docs\n```\n\n```\n[Document(page_content='I like apples', metadata={'source': 1}), Document(page_content='You like apples', metadata={'source': 2}), Document(page_content='Apples and oranges are fruits', metadata={'source': 1}), Document(page_content='You like oranges', metadata={'source': 2})]\n```", "url_final": "https://python.langchain.com/docs/how_to/ensemble_retriever/", "url_citable": "https://python.langchain.com/docs/how_to/ensemble_retriever/#basic-usage", "title": "How to combine results from multiple retrievers | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Basic usage‚Äã", "section_anchor": "basic-usage", "fetched_at": "2025-10-06T21:55:30.913556", "has_code": true}
{"id": "cbb2d748c762e9c34e48e35f6d38dc88d080bc32", "text": "Runtime Configuration‚Äã\n\nWe can also configure the individual retrievers at runtime using configurable fields. Below we update the \"top-k\" parameter for the FAISS retriever specifically:\n\n```\nfrom langchain_core.runnables import ConfigurableFieldfaiss_retriever = faiss_vectorstore.as_retriever(    search_kwargs={\"k\": 2}).configurable_fields(    search_kwargs=ConfigurableField(        id=\"search_kwargs_faiss\",        name=\"Search Kwargs\",        description=\"The search kwargs to use\",    ))ensemble_retriever = EnsembleRetriever(    retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5])\n```\n\n```\nconfig = {\"configurable\": {\"search_kwargs_faiss\": {\"k\": 1}}}docs = ensemble_retriever.invoke(\"apples\", config=config)docs\n```\n\n```\n[Document(page_content='I like apples', metadata={'source': 1}), Document(page_content='You like apples', metadata={'source': 2}), Document(page_content='Apples and oranges are fruits', metadata={'source': 1})]\n```\n\nNotice that this only returns one source from the FAISS retriever, because we pass in the relevant configuration at run time\n\n- Basic usage\n- Runtime Configuration", "url_final": "https://python.langchain.com/docs/how_to/ensemble_retriever/", "url_citable": "https://python.langchain.com/docs/how_to/ensemble_retriever/#runtime-configuration", "title": "How to combine results from multiple retrievers | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Runtime Configuration‚Äã", "section_anchor": "runtime-configuration", "fetched_at": "2025-10-06T21:55:30.913556", "has_code": true}
{"id": "eff1e0f6f3037edeff756af1c172fd4b02dd93cb", "text": "How to force models to call a tool | ü¶úÔ∏èüîó LangChain\n\nThis guide assumes familiarity with the following concepts:\n\n- Chat models\n- LangChain Tools\n- How to use a model to call tools\n\nIn order to force our LLM to select a specific tool, we can use the tool_choice parameter to ensure certain behavior. First, let's define our model and tools:\n\n```\nfrom langchain_core.tools import tool@tooldef add(a: int, b: int) -> int:    \"\"\"Adds a and b.\"\"\"    return a + b@tooldef multiply(a: int, b: int) -> int:    \"\"\"Multiplies a and b.\"\"\"    return a * btools = [add, multiply]\n```\n\nFor example, we can force our tool to call the multiply tool by using the following code:\n\n```\nllm_forced_to_multiply = llm.bind_tools(tools, tool_choice=\"multiply\")llm_forced_to_multiply.invoke(\"what is 2 + 4\")\n```\n\n```\nAIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_9cViskmLvPnHjXk9tbVla5HA', 'function': {'arguments': '{\"a\":2,\"b\":4}', 'name': 'Multiply'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 103, 'total_tokens': 112}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-095b827e-2bdd-43bb-8897-c843f4504883-0', tool_calls=[{'name': 'Multiply', 'args': {'a': 2, 'b': 4}, 'id': 'call_9cViskmLvPnHjXk9tbVla5HA'}], usage_metadata={'input_tokens': 103, 'output_tokens': 9, 'total_tokens': 112})\n```\n\nEven if we pass it something that doesn't require multiplcation - it will still call the tool!\n\nWe can also just force our tool to select at least one of our tools by passing in the \"any\" (or \"required\" which is OpenAI specific) keyword to the tool_choice parameter.\n\n```\nllm_forced_to_use_tool = llm.bind_tools(tools, tool_choice=\"any\")llm_forced_to_use_tool.invoke(\"What day is today?\")\n```\n\n```\nAIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_mCSiJntCwHJUBfaHZVUB2D8W', 'function': {'arguments': '{\"a\":1,\"b\":2}', 'name': 'Add'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 94, 'total_tokens': 109}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-28f75260-9900-4bed-8cd3-f1579abb65e5-0', tool_calls=[{'name': 'Add', 'args': {'a': 1, 'b': 2}, 'id': 'call_mCSiJntCwHJUBfaHZVUB2D8W'}], usage_metadata={'input_tokens': 94, 'output_tokens': 15, 'total_tokens': 109})\n```", "url_final": "https://python.langchain.com/docs/how_to/tool_choice/", "url_citable": "https://python.langchain.com/docs/how_to/tool_choice/", "title": "How to force models to call a tool | ü¶úÔ∏èüîó LangChain", "section_level": 1, "section_heading": "How to force models to call a tool | ü¶úÔ∏èüîó LangChain", "section_anchor": null, "fetched_at": "2025-10-06T21:56:35.588582", "has_code": true}
{"id": "cdf2041d411e6b8c412aa1668c586fbb6318b808", "text": "Dependencies‚Äã\n\nNote: this guide requires langchain-core >= 0.2.13. We will also use OpenAI for embeddings, but any LangChain embeddings should suffice. We will use a simple LangGraph agent for demonstration purposes.\n\n```\n%%capture --no-stderr%pip install -U langchain-core langchain-openai langgraph\n```\n\nLangChain tools are interfaces that an agent, chain, or chat model can use to interact with the world. See here for how-to guides covering tool-calling, built-in tools, custom tools, and more information.\n\nLangChain tools-- instances of BaseTool-- are Runnables with additional constraints that enable them to be invoked effectively by language models:\n\n- Their inputs are constrained to be serializable, specifically strings and Python dict objects;\n- They contain names and descriptions indicating how and when they should be used;\n- They may contain a detailed args_schema for their arguments. That is, while a tool (as a Runnable) might accept a single dict input, the specific keys and type information needed to populate a dict should be specified in the args_schema.\n\nRunnables that accept string or dict input can be converted to tools using the as_tool method, which allows for the specification of names, descriptions, and additional schema information for arguments.", "url_final": "https://python.langchain.com/docs/how_to/convert_runnable_to_tool/", "url_citable": "https://python.langchain.com/docs/how_to/convert_runnable_to_tool/#dependencies", "title": "How to convert Runnables to Tools | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Dependencies‚Äã", "section_anchor": "dependencies", "fetched_at": "2025-10-06T21:55:11.098328", "has_code": true}
{"id": "c795267c17f121f2e05c5c4c9df0b422263fca36", "text": "Basic usage‚Äã\n\nWith typed dict input:\n\n```\nfrom typing import Listfrom langchain_core.runnables import RunnableLambdafrom typing_extensions import TypedDictclass Args(TypedDict):    a: int    b: List[int]def f(x: Args) -> str:    return str(x[\"a\"] * max(x[\"b\"]))runnable = RunnableLambda(f)as_tool = runnable.as_tool(    name=\"My tool\",    description=\"Explanation of when to use tool.\",)\n```\n\n```\nprint(as_tool.description)as_tool.args_schema.model_json_schema()\n```\n\n```\nExplanation of when to use tool.\n```\n\n```\n{'properties': {'a': {'title': 'A', 'type': 'integer'},  'b': {'items': {'type': 'integer'}, 'title': 'B', 'type': 'array'}}, 'required': ['a', 'b'], 'title': 'My tool', 'type': 'object'}\n```\n\n```\nas_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n```\n\n```\n'6'\n```\n\nWithout typing information, arg types can be specified via arg_types:\n\n```\nfrom typing import Any, Dictdef g(x: Dict[str, Any]) -> str:    return str(x[\"a\"] * max(x[\"b\"]))runnable = RunnableLambda(g)as_tool = runnable.as_tool(    name=\"My tool\",    description=\"Explanation of when to use tool.\",    arg_types={\"a\": int, \"b\": List[int]},)\n```\n\nAlternatively, the schema can be fully specified by directly passing the desired args_schema for the tool:\n\n```\nfrom pydantic import BaseModel, Fieldclass GSchema(BaseModel):    \"\"\"Apply a function to an integer and list of integers.\"\"\"    a: int = Field(..., description=\"Integer\")    b: List[int] = Field(..., description=\"List of ints\")runnable = RunnableLambda(g)as_tool = runnable.as_tool(GSchema)\n```\n\nString input is also supported:\n\n```\ndef f(x: str) -> str:    return x + \"a\"def g(x: str) -> str:    return x + \"z\"runnable = RunnableLambda(f) | gas_tool = runnable.as_tool()\n```\n\n```\nas_tool.invoke(\"b\")\n```\n\n```\n'baz'\n```", "url_final": "https://python.langchain.com/docs/how_to/convert_runnable_to_tool/", "url_citable": "https://python.langchain.com/docs/how_to/convert_runnable_to_tool/#basic-usage", "title": "How to convert Runnables to Tools | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Basic usage‚Äã", "section_anchor": "basic-usage", "fetched_at": "2025-10-06T21:55:11.098328", "has_code": true}
{"id": "8baf7a965db877d41823adae1634dc11c291db0f", "text": "In agents‚Äã\n\nBelow we will incorporate LangChain Runnables as tools in an agent application. We will demonstrate with:\n\n- a document retriever;\n- a simple RAG chain, allowing an agent to delegate relevant queries to it.\n\nWe first instantiate a chat model that supports tool calling:\n\n```\npip install -qU \"langchain[google-genai]\"\n```\n\n```\nimport getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n```\n\nFollowing the RAG tutorial, let's first construct a retriever:\n\n```\nfrom langchain_core.documents import Documentfrom langchain_core.vectorstores import InMemoryVectorStorefrom langchain_openai import OpenAIEmbeddingsdocuments = [    Document(        page_content=\"Dogs are great companions, known for their loyalty and friendliness.\",    ),    Document(        page_content=\"Cats are independent pets that often enjoy their own space.\",    ),]vectorstore = InMemoryVectorStore.from_documents(    documents, embedding=OpenAIEmbeddings())retriever = vectorstore.as_retriever(    search_type=\"similarity\",    search_kwargs={\"k\": 1},)\n```\n\nWe next create use a simple pre-built LangGraph agent and provide it the tool:\n\n```\nfrom langgraph.prebuilt import create_react_agenttools = [    retriever.as_tool(        name=\"pet_info_retriever\",        description=\"Get information about pets.\",    )]agent = create_react_agent(llm, tools)\n```\n\n```\nfor chunk in agent.stream({\"messages\": [(\"human\", \"What are dogs known for?\")]}):    print(chunk)    print(\"----\")\n```\n\n```\n{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_W8cnfOjwqEn4cFcg19LN9mYD', 'function': {'arguments': '{\"__arg1\":\"dogs\"}', 'name': 'pet_info_retriever'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 60, 'total_tokens': 79}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-d7f81de9-1fb7-4caf-81ed-16dcdb0b2ab4-0', tool_calls=[{'name': 'pet_info_retriever', 'args': {'__arg1': 'dogs'}, 'id': 'call_W8cnfOjwqEn4cFcg19LN9mYD'}], usage_metadata={'input_tokens': 60, 'output_tokens': 19, 'total_tokens': 79})]}}----{'tools': {'messages': [ToolMessage(content=\"[Document(id='86f835fe-4bbe-4ec6-aeb4-489a8b541707', page_content='Dogs are great companions, known for their loyalty and friendliness.')]\", name='pet_info_retriever', tool_call_id='call_W8cnfOjwqEn4cFcg19LN9mYD')]}}----{'agent': {'messages': [AIMessage(content='Dogs are known for being great companions, known for their loyalty and friendliness.', response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 134, 'total_tokens': 152}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-9ca5847a-a5eb-44c0-a774-84cc2c5bbc5b-0', usage_metadata={'input_tokens': 134, 'output_tokens': 18, 'total_tokens': 152})]}}----\n```\n\nSee LangSmith trace for the above run.\n\nGoing further, we can create a simple RAG chain that takes an additional parameter-- here, the \"style\" of the answer.\n\n```\nfrom operator import itemgetterfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnablePassthroughsystem_prompt = \"\"\"You are an assistant for question-answering tasks.Use the below context to answer the question. Ifyou don't know the answer, say you don't know.Use three sentences maximum and keep the answerconcise.Answer in the style of {answer_style}.Question: {question}Context: {context}\"\"\"prompt = ChatPromptTemplate.from_messages([(\"system\", system_prompt)])rag_chain = (    {        \"context\": itemgetter(\"question\") | retriever,        \"question\": itemgetter(\"question\"),        \"answer_style\": itemgetter(\"answer_style\"),    }    | prompt    | llm    | StrOutputParser())\n```\n\nNote that the input schema for our chain contains the required arguments, so it converts to a tool without further specification:\n\n```\nrag_chain.input_schema.model_json_schema()\n```\n\n```\n{'properties': {'question': {'title': 'Question'},  'answer_style': {'title': 'Answer Style'}}, 'required': ['question', 'answer_style'], 'title': 'RunnableParallel<context,question,answer_style>Input', 'type': 'object'}\n```\n\n```\nrag_tool = rag_chain.as_tool(    name=\"pet_expert\",    description=\"Get information about pets.\",)\n```\n\nBelow we again invoke the agent. Note that the agent populates the required parameters in its tool_calls:\n\n```\nagent = create_react_agent(llm, [rag_tool])for chunk in agent.stream(    {\"messages\": [(\"human\", \"What would a pirate say dogs are known for?\")]}):    print(chunk)    print(\"----\")\n```\n\n```\n{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_17iLPWvOD23zqwd1QVQ00Y63', 'function': {'arguments': '{\"question\":\"What are dogs known for according to pirates?\",\"answer_style\":\"quote\"}', 'name': 'pet_expert'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 59, 'total_tokens': 87}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-7fef44f3-7bba-4e63-8c51-2ad9c5e65e2e-0', tool_calls=[{'name': 'pet_expert', 'args': {'question': 'What are dogs known for according to pirates?', 'answer_style': 'quote'}, 'id': 'call_17iLPWvOD23zqwd1QVQ00Y63'}], usage_metadata={'input_tokens': 59, 'output_tokens': 28, 'total_tokens': 87})]}}----{'tools': {'messages': [ToolMessage(content='\"Dogs are known for their loyalty and friendliness, making them great companions for pirates on long sea voyages.\"', name='pet_expert', tool_call_id='call_17iLPWvOD23zqwd1QVQ00Y63')]}}----{'agent': {'messages': [AIMessage(content='According to pirates, dogs are known for their loyalty and friendliness, making them great companions for pirates on long sea voyages.', response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 119, 'total_tokens': 146}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5a30edc3-7be0-4743-b980-ca2f8cad9b8d-0', usage_metadata={'input_tokens': 119, 'output_tokens': 27, 'total_tokens': 146})]}}----\n```\n\nSee LangSmith trace for the above run.\n\n- Dependencies\n- Basic usage\n- In agents", "url_final": "https://python.langchain.com/docs/how_to/convert_runnable_to_tool/", "url_citable": "https://python.langchain.com/docs/how_to/convert_runnable_to_tool/#in-agents", "title": "How to convert Runnables to Tools | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "In agents‚Äã", "section_anchor": "in-agents", "fetched_at": "2025-10-06T21:55:11.098328", "has_code": true}
{"id": "05241dce53dfe3026e67a9aa7365734a19f27908", "text": "Concepts‚Äã\n\nThis guide focuses on retrieval of text data. We will cover the following concepts:\n\n- Documents and document loaders;\n- Text splitters;\n- Embeddings;\n- Vector stores and retrievers.", "url_final": "https://python.langchain.com/docs/tutorials/retrievers/", "url_citable": "https://python.langchain.com/docs/tutorials/retrievers/#concepts", "title": "Build a semantic search engine | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Concepts‚Äã", "section_anchor": "concepts", "fetched_at": "2025-10-06T21:54:12.441555", "has_code": false}
{"id": "4588aefc4ed0b0ecc19aae65261189a0307bd045", "text": "Setup‚Äã\n\nJupyter Notebook‚Äã\n\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebook. See here for instructions on how to install.\n\nInstallation‚Äã\n\nThis tutorial requires the langchain-community and pypdf packages:\n\n- Pip\n- Conda\n\n```\npip install langchain-community pypdf\n```\n\n```\nconda install langchain-community pypdf -c conda-forge\n```\n\nFor more details, see our Installation guide.\n\nLangSmith‚Äã\n\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\n\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\n\n```\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"\n```\n\nOr, if in a notebook, you can set them with:\n\n```\nimport getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n```", "url_final": "https://python.langchain.com/docs/tutorials/retrievers/", "url_citable": "https://python.langchain.com/docs/tutorials/retrievers/#setup", "title": "Build a semantic search engine | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Setup‚Äã", "section_anchor": "setup", "fetched_at": "2025-10-06T21:54:12.441555", "has_code": true}
{"id": "ccc3af16ed5e99f25cea84a6c9cd519e458e3ecf", "text": "Documents and Document Loaders‚Äã\n\nLangChain implements a Document abstraction, which is intended to represent a unit of text and associated metadata. It has three attributes:\n\n- page_content: a string representing the content;\n- metadata: a dict containing arbitrary metadata;\n- id: (optional) a string identifier for the document.\n\nThe metadata attribute can capture information about the source of the document, its relationship to other documents, and other information. Note that an individual Document object often represents a chunk of a larger document.\n\nWe can generate sample documents when desired:\n\n```\nfrom langchain_core.documents import Documentdocuments = [    Document(        page_content=\"Dogs are great companions, known for their loyalty and friendliness.\",        metadata={\"source\": \"mammal-pets-doc\"},    ),    Document(        page_content=\"Cats are independent pets that often enjoy their own space.\",        metadata={\"source\": \"mammal-pets-doc\"},    ),]\n```\n\nHowever, the LangChain ecosystem implements document loaders that integrate with hundreds of common sources. This makes it easy to incorporate data from these sources into your AI application.\n\nLoading documents‚Äã\n\nLet's load a PDF into a sequence of Document objects. There is a sample PDF in the LangChain repo here -- a 10-k filing for Nike from 2023. We can consult the LangChain documentation for available PDF document loaders. Let's select PyPDFLoader, which is fairly lightweight.\n\n```\nfrom langchain_community.document_loaders import PyPDFLoaderfile_path = \"../example_data/nke-10k-2023.pdf\"loader = PyPDFLoader(file_path)docs = loader.load()print(len(docs))\n```\n\n```\n107\n```\n\nSee this guide for more detail on PDF document loaders.\n\nPyPDFLoader loads one Document object per PDF page. For each, we can easily access:\n\n- The string content of the page;\n- Metadata containing the file name and page number.\n\n```\nprint(f\"{docs[0].page_content[:200]}\\n\")print(docs[0].metadata)\n```\n\n```\nTable of ContentsUNITED STATESSECURITIES AND EXCHANGE COMMISSIONWashington, D.C. 20549FORM 10-K(Mark One)‚òë ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(D) OF THE SECURITIES EXCHANGE ACT OF 1934FO{'source': '../example_data/nke-10k-2023.pdf', 'page': 0}\n```\n\nSplitting‚Äã\n\nFor both information retrieval and downstream question-answering purposes, a page may be too coarse a representation. Our goal in the end will be to retrieve Document objects that answer an input query, and further splitting our PDF will help ensure that the meanings of relevant portions of the document are not \"washed out\" by surrounding text.\n\nWe can use text splitters for this purpose. Here we will use a simple text splitter that partitions based on characters. We will split our documents into chunks of 1000 characters with 200 characters of overlap between chunks. The overlap helps mitigate the possibility of separating a statement from important context related to it. We use the RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\n\nWe set add_start_index=True so that the character index where each split Document starts within the initial Document is preserved as metadata attribute ‚Äústart_index‚Äù.\n\nSee this guide for more detail about working with PDFs, including how to extract text from specific sections and images.\n\n```\nfrom langchain_text_splitters import RecursiveCharacterTextSplittertext_splitter = RecursiveCharacterTextSplitter(    chunk_size=1000, chunk_overlap=200, add_start_index=True)all_splits = text_splitter.split_documents(docs)len(all_splits)\n```\n\n```\n514\n```", "url_final": "https://python.langchain.com/docs/tutorials/retrievers/", "url_citable": "https://python.langchain.com/docs/tutorials/retrievers/#documents-and-document-loaders", "title": "Build a semantic search engine | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Documents and Document Loaders‚Äã", "section_anchor": "documents-and-document-loaders", "fetched_at": "2025-10-06T21:54:12.441555", "has_code": true}
{"id": "6405ae3fc32c42efb367d7d94c342adc2ee8f2db", "text": "Embeddings‚Äã\n\nVector search is a common way to store and search over unstructured data (such as unstructured text). The idea is to store numeric vectors that are associated with the text. Given a query, we can embed it as a vector of the same dimension and use vector similarity metrics (such as cosine similarity) to identify related text.\n\nLangChain supports embeddings from dozens of providers. These models specify how text should be converted into a numeric vector. Let's select a model:\n\n```\npip install -qU langchain-openai\n```\n\n```\nimport getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"):  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")from langchain_openai import OpenAIEmbeddingsembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n```\n\n```\nvector_1 = embeddings.embed_query(all_splits[0].page_content)vector_2 = embeddings.embed_query(all_splits[1].page_content)assert len(vector_1) == len(vector_2)print(f\"Generated vectors of length {len(vector_1)}\\n\")print(vector_1[:10])\n```\n\n```\nGenerated vectors of length 1536[-0.008586574345827103, -0.03341241180896759, -0.008936782367527485, -0.0036674530711025, 0.010564599186182022, 0.009598285891115665, -0.028587326407432556, -0.015824200585484505, 0.0030416189692914486, -0.012899317778646946]\n```\n\nArmed with a model for generating text embeddings, we can next store them in a special data structure that supports efficient similarity search.", "url_final": "https://python.langchain.com/docs/tutorials/retrievers/", "url_citable": "https://python.langchain.com/docs/tutorials/retrievers/#embeddings", "title": "Build a semantic search engine | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Embeddings‚Äã", "section_anchor": "embeddings", "fetched_at": "2025-10-06T21:54:12.441555", "has_code": true}
{"id": "ad5678252e195917b3051ef6e0f114e30b25b25d", "text": "Vector stores‚Äã\n\nLangChain VectorStore objects contain methods for adding text and Document objects to the store, and querying them using various similarity metrics. They are often initialized with embedding models, which determine how text data is translated to numeric vectors.\n\nLangChain includes a suite of integrations with different vector store technologies. Some vector stores are hosted by a provider (e.g., various cloud providers) and require specific credentials to use; some (such as Postgres) run in separate infrastructure that can be run locally or via a third-party; others can run in-memory for lightweight workloads. Let's select a vector store:\n\n```\npip install -qU langchain-core\n```\n\n```\nfrom langchain_core.vectorstores import InMemoryVectorStorevector_store = InMemoryVectorStore(embeddings)\n```\n\nHaving instantiated our vector store, we can now index the documents.\n\n```\nids = vector_store.add_documents(documents=all_splits)\n```\n\nNote that most vector store implementations will allow you to connect to an existing vector store-- e.g., by providing a client, index name, or other information. See the documentation for a specific integration for more detail.\n\nOnce we've instantiated a VectorStore that contains documents, we can query it. VectorStore includes methods for querying:\n\n- Synchronously and asynchronously;\n- By string query and by vector;\n- With and without returning similarity scores;\n- By similarity and maximum marginal relevance (to balance similarity with query to diversity in retrieved results).\n\nThe methods will generally include a list of Document objects in their outputs.", "url_final": "https://python.langchain.com/docs/tutorials/retrievers/", "url_citable": "https://python.langchain.com/docs/tutorials/retrievers/#vector-stores", "title": "Build a semantic search engine | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Vector stores‚Äã", "section_anchor": "vector-stores", "fetched_at": "2025-10-06T21:54:12.441555", "has_code": true}
{"id": "5cf2b1477b49566552cab42002a1f81370e538b2", "text": "Usage‚Äã\n\nEmbeddings typically represent text as a \"dense\" vector such that texts with similar meanings are geometrically close. This lets us retrieve relevant information just by passing in a question, without knowledge of any specific key-terms used in the document.\n\nReturn documents based on similarity to a string query:\n\n```\nresults = vector_store.similarity_search(    \"How many distribution centers does Nike have in the US?\")print(results[0])\n```\n\n```\npage_content='direct to consumer operations sell products through the following number of retail stores in the United States:U.S. RETAIL STORES NUMBERNIKE Brand factory stores 213 NIKE Brand in-line stores (including employee-only stores) 74 Converse stores (including factory stores) 82 TOTAL 369 In the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.2023 FORM 10-K 2' metadata={'page': 4, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 3125}\n```\n\nAsync query:\n\n```\nresults = await vector_store.asimilarity_search(\"When was Nike incorporated?\")print(results[0])\n```\n\n```\npage_content='Table of ContentsPART IITEM 1. BUSINESSGENERALNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this \"Annual Report\"), the terms \"we,\" \"us,\" \"our,\"\"NIKE\" and the \"Company\" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.Our principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE isthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail storesand sales through our digital platforms (also referred to as \"NIKE Brand Digital\"), to retail accounts and to a mix of independent distributors, licensees and sales' metadata={'page': 3, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}\n```\n\nReturn scores:\n\n```\n# Note that providers implement different scores; the score here# is a distance metric that varies inversely with similarity.results = vector_store.similarity_search_with_score(\"What was Nike's revenue in 2023?\")doc, score = results[0]print(f\"Score: {score}\\n\")print(doc)\n```\n\n```\nScore: 0.23699893057346344page_content='Table of ContentsFISCAL 2023 NIKE BRAND REVENUE HIGHLIGHTSThe following tables present NIKE Brand revenues disaggregated by reportable operating segment, distribution channel and major product line:FISCAL 2023 COMPARED TO FISCAL 2022‚Ä¢NIKE, Inc. Revenues were $51.2 billion in fiscal 2023, which increased 10% and 16% compared to fiscal 2022 on a reported and currency-neutral basis, respectively.The increase was due to higher revenues in North America, Europe, Middle East & Africa (\"EMEA\"), APLA and Greater China, which contributed approximately 7, 6,2 and 1 percentage points to NIKE, Inc. Revenues, respectively.‚Ä¢NIKE Brand revenues, which represented over 90% of NIKE, Inc. Revenues, increased 10% and 16% on a reported and currency-neutral basis, respectively. Thisincrease was primarily due to higher revenues in Men's, the Jordan Brand, Women's and Kids' which grew 17%, 35%,11% and 10%, respectively, on a wholesaleequivalent basis.' metadata={'page': 35, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}\n```\n\nReturn documents based on similarity to an embedded query:\n\n```\nembedding = embeddings.embed_query(\"How were Nike's margins impacted in 2023?\")results = vector_store.similarity_search_by_vector(embedding)print(results[0])\n```\n\n```\npage_content='Table of ContentsGROSS MARGINFISCAL 2023 COMPARED TO FISCAL 2022For fiscal 2023, our consolidated gross profit increased 4% to $22,292 million compared to $21,479 million for fiscal 2022. Gross margin decreased 250 basis points to43.5% for fiscal 2023 compared to 46.0% for fiscal 2022 due to the following:*Wholesale equivalentThe decrease in gross margin for fiscal 2023 was primarily due to:‚Ä¢Higher NIKE Brand product costs, on a wholesale equivalent basis, primarily due to higher input costs and elevated inbound freight and logistics costs as well asproduct mix;‚Ä¢Lower margin in our NIKE Direct business, driven by higher promotional activity to liquidate inventory in the current period compared to lower promotional activity inthe prior period resulting from lower available inventory supply;‚Ä¢Unfavorable changes in net foreign currency exchange rates, including hedges; and‚Ä¢Lower off-price margin, on a wholesale equivalent basis.This was partially offset by:' metadata={'page': 36, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}\n```\n\nLearn more:\n\n- API reference\n- How-to guide\n- Integration-specific docs", "url_final": "https://python.langchain.com/docs/tutorials/retrievers/", "url_citable": "https://python.langchain.com/docs/tutorials/retrievers/#usage", "title": "Build a semantic search engine | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Usage‚Äã", "section_anchor": "usage", "fetched_at": "2025-10-06T21:54:12.441555", "has_code": true}
{"id": "d96a7eca7140eab0cfc81ba5b863ed17093c45e9", "text": "Retrievers‚Äã\n\nLangChain VectorStore objects do not subclass Runnable. LangChain Retrievers are Runnables, so they implement a standard set of methods (e.g., synchronous and asynchronous invoke and batch operations). Although we can construct retrievers from vector stores, retrievers can interface with non-vector store sources of data, as well (such as external APIs).\n\nWe can create a simple version of this ourselves, without subclassing Retriever. If we choose what method we wish to use to retrieve documents, we can create a runnable easily. Below we will build one around the similarity_search method:\n\n```\nfrom typing import Listfrom langchain_core.documents import Documentfrom langchain_core.runnables import chain@chaindef retriever(query: str) -> List[Document]:    return vector_store.similarity_search(query, k=1)retriever.batch(    [        \"How many distribution centers does Nike have in the US?\",        \"When was Nike incorporated?\",    ],)\n```\n\n```\n[[Document(metadata={'page': 4, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 3125}, page_content='direct to consumer operations sell products through the following number of retail stores in the United States:\\nU.S. RETAIL STORES NUMBER\\nNIKE Brand factory stores 213 \\nNIKE Brand in-line stores (including employee-only stores) 74 \\nConverse stores (including factory stores) 82 \\nTOTAL 369 \\nIn the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.\\n2023 FORM 10-K 2')], [Document(metadata={'page': 3, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}, page_content='Table of Contents\\nPART I\\nITEM 1. BUSINESS\\nGENERAL\\nNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this \"Annual Report\"), the terms \"we,\" \"us,\" \"our,\"\\n\"NIKE\" and the \"Company\" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\\nOur principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\\nthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\\nand sales through our digital platforms (also referred to as \"NIKE Brand Digital\"), to retail accounts and to a mix of independent distributors, licensees and sales')]]\n```\n\nVectorstores implement an as_retriever method that will generate a Retriever, specifically a VectorStoreRetriever. These retrievers include specific search_type and search_kwargs attributes that identify what methods of the underlying vector store to call, and how to parameterize them. For instance, we can replicate the above with the following:\n\n```\nretriever = vector_store.as_retriever(    search_type=\"similarity\",    search_kwargs={\"k\": 1},)retriever.batch(    [        \"How many distribution centers does Nike have in the US?\",        \"When was Nike incorporated?\",    ],)\n```\n\n```\n[[Document(metadata={'page': 4, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 3125}, page_content='direct to consumer operations sell products through the following number of retail stores in the United States:\\nU.S. RETAIL STORES NUMBER\\nNIKE Brand factory stores 213 \\nNIKE Brand in-line stores (including employee-only stores) 74 \\nConverse stores (including factory stores) 82 \\nTOTAL 369 \\nIn the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.\\n2023 FORM 10-K 2')], [Document(metadata={'page': 3, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}, page_content='Table of Contents\\nPART I\\nITEM 1. BUSINESS\\nGENERAL\\nNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this \"Annual Report\"), the terms \"we,\" \"us,\" \"our,\"\\n\"NIKE\" and the \"Company\" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\\nOur principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\\nthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\\nand sales through our digital platforms (also referred to as \"NIKE Brand Digital\"), to retail accounts and to a mix of independent distributors, licensees and sales')]]\n```\n\nVectorStoreRetriever supports search types of \"similarity\" (default), \"mmr\" (maximum marginal relevance, described above), and \"similarity_score_threshold\". We can use the latter to threshold documents output by the retriever by similarity score.\n\nRetrievers can easily be incorporated into more complex applications, such as retrieval-augmented generation (RAG) applications that combine a given question with retrieved context into a prompt for a LLM. To learn more about building such an application, check out the RAG tutorial tutorial.", "url_final": "https://python.langchain.com/docs/tutorials/retrievers/", "url_citable": "https://python.langchain.com/docs/tutorials/retrievers/#retrievers", "title": "Build a semantic search engine | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Retrievers‚Äã", "section_anchor": "retrievers", "fetched_at": "2025-10-06T21:54:12.441555", "has_code": true}
{"id": "4ca202e23dbfb95581ccaa4a91a41be317557543", "text": "Learn more:‚Äã\n\nRetrieval strategies can be rich and complex. For example:\n\n- We can infer hard rules and filters from a query (e.g., \"using documents published after 2020\");\n- We can return documents that are linked to the retrieved context in some way (e.g., via some document taxonomy);\n- We can generate multiple embeddings for each unit of context;\n- We can ensemble results from multiple retrievers;\n- We can assign weights to documents, e.g., to weigh recent documents higher.\n\nThe retrievers section of the how-to guides covers these and other built-in retrieval strategies.\n\nIt is also straightforward to extend the BaseRetriever class in order to implement custom retrievers. See our how-to guide here.", "url_final": "https://python.langchain.com/docs/tutorials/retrievers/", "url_citable": "https://python.langchain.com/docs/tutorials/retrievers/#learn-more", "title": "Build a semantic search engine | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Learn more:‚Äã", "section_anchor": "learn-more", "fetched_at": "2025-10-06T21:54:12.441555", "has_code": false}
{"id": "90528719823eb4a153a7b8fd481706623df18c97", "text": "Next steps‚Äã\n\nYou've now seen how to build a semantic search engine over a PDF document.\n\nFor more on document loaders:\n\n- Conceptual guide\n- How-to guides\n- Available integrations\n\nFor more on embeddings:\n\n- Conceptual guide\n- How-to guides\n- Available integrations\n\nFor more on vector stores:\n\n- Conceptual guide\n- How-to guides\n- Available integrations\n\nFor more on RAG, see:\n\n- Build a Retrieval Augmented Generation (RAG) App\n- Related how-to guides\n\n- Jupyter Notebook\n- Installation\n- LangSmith\n\n- Loading documents\n- Splitting\n\n- Usage\n\n- Learn more:", "url_final": "https://python.langchain.com/docs/tutorials/retrievers/", "url_citable": "https://python.langchain.com/docs/tutorials/retrievers/#next-steps", "title": "Build a semantic search engine | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Next steps‚Äã", "section_anchor": "next-steps", "fetched_at": "2025-10-06T21:54:12.441555", "has_code": false}
{"id": "f2186979675c0632ba5ce195cd51a28601936946", "text": "Formatting with RunnableParallels‚Äã\n\nRunnableParallels are useful for parallelizing operations, but can also be useful for manipulating the output of one Runnable to match the input format of the next Runnable in a sequence. You can use them to split or fork the chain so that multiple components can process the input in parallel. Later, other components can join or merge the results to synthesize a final response. This type of chain creates a computation graph that looks like the following:\n\n```\n     Input      / \\     /   \\ Branch1 Branch2     \\   /      \\ /      Combine\n```\n\nBelow, the input to prompt is expected to be a map with keys \"context\" and \"question\". The user input is just the question. So we need to get the context using our retriever and passthrough the user input under the \"question\" key.\n\n```\nfrom langchain_community.vectorstores import FAISSfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnablePassthroughfrom langchain_openai import ChatOpenAI, OpenAIEmbeddingsvectorstore = FAISS.from_texts(    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())retriever = vectorstore.as_retriever()template = \"\"\"Answer the question based only on the following context:{context}Question: {question}\"\"\"# The prompt expects input with keys for \"context\" and \"question\"prompt = ChatPromptTemplate.from_template(template)model = ChatOpenAI()retrieval_chain = (    {\"context\": retriever, \"question\": RunnablePassthrough()}    | prompt    | model    | StrOutputParser())retrieval_chain.invoke(\"where did harrison work?\")\n```\n\n```\n'Harrison worked at Kensho.'\n```\n\nNote that when composing a RunnableParallel with another Runnable we don't even need to wrap our dictionary in the RunnableParallel class ‚Äî the type conversion is handled for us. In the context of a chain, these are equivalent:\n\n```\n{\"context\": retriever, \"question\": RunnablePassthrough()}\n```\n\n```\nRunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\n```\n\n```\nRunnableParallel(context=retriever, question=RunnablePassthrough())\n```\n\nSee the section on coercion for more.", "url_final": "https://python.langchain.com/docs/how_to/parallel/", "url_citable": "https://python.langchain.com/docs/how_to/parallel/#formatting-with-runnableparallels", "title": "How to invoke runnables in parallel | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Formatting with RunnableParallels‚Äã", "section_anchor": "formatting-with-runnableparallels", "fetched_at": "2025-10-06T21:54:21.801412", "has_code": true}
{"id": "bef04e18ff7ea62b57cefb8a2fa51d6bf3fdf47b", "text": "Using itemgetter as shorthand‚Äã\n\nNote that you can use Python's itemgetter as shorthand to extract data from the map when combining with RunnableParallel. You can find more information about itemgetter in the Python Documentation.\n\nIn the example below, we use itemgetter to extract specific keys from the map:\n\n```\nfrom operator import itemgetterfrom langchain_community.vectorstores import FAISSfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnablePassthroughfrom langchain_openai import ChatOpenAI, OpenAIEmbeddingsvectorstore = FAISS.from_texts(    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())retriever = vectorstore.as_retriever()template = \"\"\"Answer the question based only on the following context:{context}Question: {question}Answer in the following language: {language}\"\"\"prompt = ChatPromptTemplate.from_template(template)chain = (    {        \"context\": itemgetter(\"question\") | retriever,        \"question\": itemgetter(\"question\"),        \"language\": itemgetter(\"language\"),    }    | prompt    | model    | StrOutputParser())chain.invoke({\"question\": \"where did harrison work\", \"language\": \"italian\"})\n```\n\n```\n'Harrison ha lavorato a Kensho.'\n```", "url_final": "https://python.langchain.com/docs/how_to/parallel/", "url_citable": "https://python.langchain.com/docs/how_to/parallel/#using-itemgetter-as-shorthand", "title": "How to invoke runnables in parallel | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Using itemgetter as shorthand‚Äã", "section_anchor": "using-itemgetter-as-shorthand", "fetched_at": "2025-10-06T21:54:21.801412", "has_code": true}
{"id": "153829310f46371cfec5371ae8ac532e53ffebd6", "text": "Parallelize steps‚Äã\n\nRunnableParallels make it easy to execute multiple Runnables in parallel, and to return the output of these Runnables as a map.\n\n```\nfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnableParallelfrom langchain_openai import ChatOpenAImodel = ChatOpenAI()joke_chain = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | modelpoem_chain = (    ChatPromptTemplate.from_template(\"write a 2-line poem about {topic}\") | model)map_chain = RunnableParallel(joke=joke_chain, poem=poem_chain)map_chain.invoke({\"topic\": \"bear\"})\n```\n\n```\n{'joke': AIMessage(content=\"Why don't bears like fast food? Because they can't catch it!\", response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 13, 'total_tokens': 28}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_d9767fc5b9', 'finish_reason': 'stop', 'logprobs': None}, id='run-fe024170-c251-4b7a-bfd4-64a3737c67f2-0'), 'poem': AIMessage(content='In the quiet of the forest, the bear roams free\\nMajestic and wild, a sight to see.', response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 15, 'total_tokens': 39}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-2707913e-a743-4101-b6ec-840df4568a76-0')}\n```", "url_final": "https://python.langchain.com/docs/how_to/parallel/", "url_citable": "https://python.langchain.com/docs/how_to/parallel/#parallelize-steps", "title": "How to invoke runnables in parallel | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Parallelize steps‚Äã", "section_anchor": "parallelize-steps", "fetched_at": "2025-10-06T21:54:21.801412", "has_code": true}
{"id": "703b1992875d509f07e219da396e5f1fef8dfaf9", "text": "Parallelism‚Äã\n\nRunnableParallel are also useful for running independent processes in parallel, since each Runnable in the map is executed in parallel. For example, we can see our earlier joke_chain, poem_chain and map_chain all have about the same runtime, even though map_chain executes both of the other two.\n\n```\n%%timeitjoke_chain.invoke({\"topic\": \"bear\"})\n```\n\n```\n610 ms ¬± 64 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n```\n\n```\n%%timeitpoem_chain.invoke({\"topic\": \"bear\"})\n```\n\n```\n599 ms ¬± 73.3 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n```\n\n```\n%%timeitmap_chain.invoke({\"topic\": \"bear\"})\n```\n\n```\n643 ms ¬± 77.8 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n```", "url_final": "https://python.langchain.com/docs/how_to/parallel/", "url_citable": "https://python.langchain.com/docs/how_to/parallel/#parallelism", "title": "How to invoke runnables in parallel | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Parallelism‚Äã", "section_anchor": "parallelism", "fetched_at": "2025-10-06T21:54:21.801412", "has_code": true}
{"id": "77a02f561a1c4a7a0aa0caf099efab4e63c64e38", "text": "Next steps‚Äã\n\nYou now know some ways to format and parallelize chain steps with RunnableParallel.\n\nTo learn more, see the other how-to guides on runnables in this section.\n\n- Formatting with RunnableParallels\n- Using itemgetter as shorthand\n- Parallelize steps\n- Parallelism\n- Next steps", "url_final": "https://python.langchain.com/docs/how_to/parallel/", "url_citable": "https://python.langchain.com/docs/how_to/parallel/#next-steps", "title": "How to invoke runnables in parallel | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Next steps‚Äã", "section_anchor": "next-steps", "fetched_at": "2025-10-06T21:54:21.801412", "has_code": false}
{"id": "0ca00b3bd62ee96799d165bda222c7d4bfdd41af", "text": "LangChain Expression Language Cheatsheet | ü¶úÔ∏èüîó LangChain\n\nThis is a quick reference for all the most important LCEL primitives. For more advanced usage see the LCEL how-to guides and the full API reference.\n\nInvoke a runnable‚Äã\n\n```\nfrom langchain_core.runnables import RunnableLambdarunnable = RunnableLambda(lambda x: str(x))runnable.invoke(5)# Async variant:# await runnable.ainvoke(5)\n```\n\n```\n'5'\n```\n\nBatch a runnable‚Äã\n\n```\nfrom langchain_core.runnables import RunnableLambdarunnable = RunnableLambda(lambda x: str(x))runnable.batch([7, 8, 9])# Async variant:# await runnable.abatch([7, 8, 9])\n```\n\n```\n['7', '8', '9']\n```\n\nStream a runnable‚Äã\n\n```\nfrom langchain_core.runnables import RunnableLambdadef func(x):    for y in x:        yield str(y)runnable = RunnableLambda(func)for chunk in runnable.stream(range(5)):    print(chunk)# Async variant:# async for chunk in await runnable.astream(range(5)):#     print(chunk)\n```\n\n```\n01234\n```\n\nCompose runnables‚Äã\n\n```\nfrom langchain_core.runnables import RunnableLambdarunnable1 = RunnableLambda(lambda x: {\"foo\": x})runnable2 = RunnableLambda(lambda x: [x] * 2)chain = runnable1 | runnable2chain.invoke(2)\n```\n\n```\n[{'foo': 2}, {'foo': 2}]\n```\n\nInvoke runnables in parallel‚Äã\n\n```\nfrom langchain_core.runnables import RunnableLambda, RunnableParallelrunnable1 = RunnableLambda(lambda x: {\"foo\": x})runnable2 = RunnableLambda(lambda x: [x] * 2)chain = RunnableParallel(first=runnable1, second=runnable2)chain.invoke(2)\n```\n\n```\n{'first': {'foo': 2}, 'second': [2, 2]}\n```\n\nTurn any function into a runnable‚Äã\n\n```\nfrom langchain_core.runnables import RunnableLambdadef func(x):    return x + 5runnable = RunnableLambda(func)runnable.invoke(2)\n```\n\n```\n7\n```\n\nMerge input and output dicts‚Äã\n\n```\nfrom langchain_core.runnables import RunnableLambda, RunnablePassthroughrunnable1 = RunnableLambda(lambda x: x[\"foo\"] + 7)chain = RunnablePassthrough.assign(bar=runnable1)chain.invoke({\"foo\": 10})\n```\n\n```\n{'foo': 10, 'bar': 17}\n```\n\nInclude input dict in output dict‚Äã\n\n```\nfrom langchain_core.runnables import (    RunnableLambda,    RunnableParallel,    RunnablePassthrough,)runnable1 = RunnableLambda(lambda x: x[\"foo\"] + 7)chain = RunnableParallel(bar=runnable1, baz=RunnablePassthrough())chain.invoke({\"foo\": 10})\n```\n\n```\n{'bar': 17, 'baz': {'foo': 10}}\n```\n\nAdd default invocation args‚Äã\n\n```\nfrom typing import Optionalfrom langchain_core.runnables import RunnableLambdadef func(main_arg: dict, other_arg: Optional[str] = None) -> dict:    if other_arg:        return {**main_arg, **{\"foo\": other_arg}}    return main_argrunnable1 = RunnableLambda(func)bound_runnable1 = runnable1.bind(other_arg=\"bye\")bound_runnable1.invoke({\"bar\": \"hello\"})\n```\n\n```\n{'bar': 'hello', 'foo': 'bye'}\n```\n\nAdd fallbacks‚Äã\n\n```\nfrom langchain_core.runnables import RunnableLambdarunnable1 = RunnableLambda(lambda x: x + \"foo\")runnable2 = RunnableLambda(lambda x: str(x) + \"foo\")chain = runnable1.with_fallbacks([runnable2])chain.invoke(5)\n```\n\n```\n'5foo'\n```\n\nAdd retries‚Äã\n\n```\nfrom langchain_core.runnables import RunnableLambdacounter = -1def func(x):    global counter    counter += 1    print(f\"attempt with {counter=}\")    return x / counterchain = RunnableLambda(func).with_retry(stop_after_attempt=2)chain.invoke(2)\n```\n\n```\nattempt with counter=0attempt with counter=1\n```\n\n```\n2.0\n```\n\nConfigure runnable execution‚Äã\n\n```\nfrom langchain_core.runnables import RunnableLambda, RunnableParallelrunnable1 = RunnableLambda(lambda x: {\"foo\": x})runnable2 = RunnableLambda(lambda x: [x] * 2)runnable3 = RunnableLambda(lambda x: str(x))chain = RunnableParallel(first=runnable1, second=runnable2, third=runnable3)chain.invoke(7, config={\"max_concurrency\": 2})\n```\n\n```\n{'first': {'foo': 7}, 'second': [7, 7], 'third': '7'}\n```", "url_final": "https://python.langchain.com/docs/how_to/lcel_cheatsheet/", "url_citable": "https://python.langchain.com/docs/how_to/lcel_cheatsheet/", "title": "LangChain Expression Language Cheatsheet | ü¶úÔ∏èüîó LangChain", "section_level": 1, "section_heading": "LangChain Expression Language Cheatsheet | ü¶úÔ∏èüîó LangChain", "section_anchor": null, "fetched_at": "2025-10-06T21:55:45.896512", "has_code": true}
{"id": "bf2b833c76c4e4504abea8931889c43f87a25789", "text": "Add default config to runnable‚Äã\n\n```\nfrom langchain_core.runnables import RunnableLambda, RunnableParallelrunnable1 = RunnableLambda(lambda x: {\"foo\": x})runnable2 = RunnableLambda(lambda x: [x] * 2)runnable3 = RunnableLambda(lambda x: str(x))chain = RunnableParallel(first=runnable1, second=runnable2, third=runnable3)configured_chain = chain.with_config(max_concurrency=2)chain.invoke(7)\n```\n\n```\n{'first': {'foo': 7}, 'second': [7, 7], 'third': '7'}\n```", "url_final": "https://python.langchain.com/docs/how_to/lcel_cheatsheet/", "url_citable": "https://python.langchain.com/docs/how_to/lcel_cheatsheet/#add-default-config-to-runnable", "title": "LangChain Expression Language Cheatsheet | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Add default config to runnable‚Äã", "section_anchor": "add-default-config-to-runnable", "fetched_at": "2025-10-06T21:55:45.896512", "has_code": true}
{"id": "ac7ea6f282d79fccdb3138aa7b66c65cb001cd3a", "text": "Make runnable attributes configurable‚Äã\n\n```\nfrom typing import Any, Optionalfrom langchain_core.runnables import (    ConfigurableField,    RunnableConfig,    RunnableSerializable,)class FooRunnable(RunnableSerializable[dict, dict]):    output_key: str    def invoke(        self, input: Any, config: Optional[RunnableConfig] = None, **kwargs: Any    ) -> list:        return self._call_with_config(self.subtract_seven, input, config, **kwargs)    def subtract_seven(self, input: dict) -> dict:        return {self.output_key: input[\"foo\"] - 7}runnable1 = FooRunnable(output_key=\"bar\")configurable_runnable1 = runnable1.configurable_fields(    output_key=ConfigurableField(id=\"output_key\"))configurable_runnable1.invoke(    {\"foo\": 10}, config={\"configurable\": {\"output_key\": \"not bar\"}})\n```\n\n```\n{'not bar': 3}\n```\n\n```\nconfigurable_runnable1.invoke({\"foo\": 10})\n```\n\n```\n{'bar': 3}\n```\n\nMake chain components configurable‚Äã\n\n```\nfrom typing import Any, Optionalfrom langchain_core.runnables import RunnableConfig, RunnableLambda, RunnableParallelclass ListRunnable(RunnableSerializable[Any, list]):    def invoke(        self, input: Any, config: Optional[RunnableConfig] = None, **kwargs: Any    ) -> list:        return self._call_with_config(self.listify, input, config, **kwargs)    def listify(self, input: Any) -> list:        return [input]class StrRunnable(RunnableSerializable[Any, str]):    def invoke(        self, input: Any, config: Optional[RunnableConfig] = None, **kwargs: Any    ) -> list:        return self._call_with_config(self.strify, input, config, **kwargs)    def strify(self, input: Any) -> str:        return str(input)runnable1 = RunnableLambda(lambda x: {\"foo\": x})configurable_runnable = ListRunnable().configurable_alternatives(    ConfigurableField(id=\"second_step\"), default_key=\"list\", string=StrRunnable())chain = runnable1 | configurable_runnablechain.invoke(7, config={\"configurable\": {\"second_step\": \"string\"}})\n```\n\n```\n\"{'foo': 7}\"\n```\n\n```\nchain.invoke(7)\n```\n\n```\n[{'foo': 7}]\n```\n\nBuild a chain dynamically based on input‚Äã\n\n```\nfrom langchain_core.runnables import RunnableLambda, RunnableParallelrunnable1 = RunnableLambda(lambda x: {\"foo\": x})runnable2 = RunnableLambda(lambda x: [x] * 2)chain = RunnableLambda(lambda x: runnable1 if x > 6 else runnable2)chain.invoke(7)\n```\n\n```\n{'foo': 7}\n```\n\n```\nchain.invoke(5)\n```\n\n```\n[5, 5]\n```", "url_final": "https://python.langchain.com/docs/how_to/lcel_cheatsheet/", "url_citable": "https://python.langchain.com/docs/how_to/lcel_cheatsheet/", "title": "LangChain Expression Language Cheatsheet | ü¶úÔ∏èüîó LangChain", "section_level": 1, "section_heading": "LangChain Expression Language Cheatsheet | ü¶úÔ∏èüîó LangChain", "section_anchor": null, "fetched_at": "2025-10-06T21:55:45.896512", "has_code": true}
{"id": "40868fa60125dd38c10c5c6bc789a341df24765b", "text": "Generate a stream of events‚Äã\n\n```\n# | echo: falseimport nest_asyncionest_asyncio.apply()\n```\n\n```\nfrom langchain_core.runnables import RunnableLambda, RunnableParallelrunnable1 = RunnableLambda(lambda x: {\"foo\": x}, name=\"first\")async def func(x):    for _ in range(5):        yield xrunnable2 = RunnableLambda(func, name=\"second\")chain = runnable1 | runnable2async for event in chain.astream_events(\"bar\", version=\"v2\"):    print(f\"event={event['event']} | name={event['name']} | data={event['data']}\")\n```\n\n```\nevent=on_chain_start | name=RunnableSequence | data={'input': 'bar'}event=on_chain_start | name=first | data={}event=on_chain_stream | name=first | data={'chunk': {'foo': 'bar'}}event=on_chain_start | name=second | data={}event=on_chain_end | name=first | data={'output': {'foo': 'bar'}, 'input': 'bar'}event=on_chain_stream | name=second | data={'chunk': {'foo': 'bar'}}event=on_chain_stream | name=RunnableSequence | data={'chunk': {'foo': 'bar'}}event=on_chain_stream | name=second | data={'chunk': {'foo': 'bar'}}event=on_chain_stream | name=RunnableSequence | data={'chunk': {'foo': 'bar'}}event=on_chain_stream | name=second | data={'chunk': {'foo': 'bar'}}event=on_chain_stream | name=RunnableSequence | data={'chunk': {'foo': 'bar'}}event=on_chain_stream | name=second | data={'chunk': {'foo': 'bar'}}event=on_chain_stream | name=RunnableSequence | data={'chunk': {'foo': 'bar'}}event=on_chain_stream | name=second | data={'chunk': {'foo': 'bar'}}event=on_chain_stream | name=RunnableSequence | data={'chunk': {'foo': 'bar'}}event=on_chain_end | name=second | data={'output': {'foo': 'bar'}, 'input': {'foo': 'bar'}}event=on_chain_end | name=RunnableSequence | data={'output': {'foo': 'bar'}}\n```", "url_final": "https://python.langchain.com/docs/how_to/lcel_cheatsheet/", "url_citable": "https://python.langchain.com/docs/how_to/lcel_cheatsheet/#generate-a-stream-of-events", "title": "LangChain Expression Language Cheatsheet | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Generate a stream of events‚Äã", "section_anchor": "generate-a-stream-of-events", "fetched_at": "2025-10-06T21:55:45.896512", "has_code": true}
{"id": "b5272ecfa45d8df125a43791632e438bec20a452", "text": "Yield batched outputs as they complete‚Äã\n\n```\nimport timefrom langchain_core.runnables import RunnableLambda, RunnableParallelrunnable1 = RunnableLambda(lambda x: time.sleep(x) or print(f\"slept {x}\"))for idx, result in runnable1.batch_as_completed([5, 1]):    print(idx, result)# Async variant:# async for idx, result in runnable1.abatch_as_completed([5, 1]):#     print(idx, result)\n```\n\n```\nslept 11 Noneslept 50 None\n```\n\nReturn subset of output dict‚Äã\n\n```\nfrom langchain_core.runnables import RunnableLambda, RunnablePassthroughrunnable1 = RunnableLambda(lambda x: x[\"baz\"] + 5)chain = RunnablePassthrough.assign(foo=runnable1).pick([\"foo\", \"bar\"])chain.invoke({\"bar\": \"hi\", \"baz\": 2})\n```\n\n```\n{'foo': 7, 'bar': 'hi'}\n```\n\nDeclaratively make a batched version of a runnable‚Äã\n\n```\nfrom langchain_core.runnables import RunnableLambdarunnable1 = RunnableLambda(lambda x: list(range(x)))runnable2 = RunnableLambda(lambda x: x + 5)chain = runnable1 | runnable2.map()chain.invoke(3)\n```\n\n```\n[5, 6, 7]\n```\n\nGet a graph representation of a runnable‚Äã\n\n```\nfrom langchain_core.runnables import RunnableLambda, RunnableParallelrunnable1 = RunnableLambda(lambda x: {\"foo\": x})runnable2 = RunnableLambda(lambda x: [x] * 2)runnable3 = RunnableLambda(lambda x: str(x))chain = runnable1 | RunnableParallel(second=runnable2, third=runnable3)chain.get_graph().print_ascii()\n```\n\n```\n                             +-------------+                                                           | LambdaInput |                                                           +-------------+                                                                  *                                                                         *                                                                         *                                                         +------------------------------+                                          | Lambda(lambda x: {'foo': x}) |                                          +------------------------------+                                                          *                                                                         *                                                                         *                                                          +-----------------------------+                                           | Parallel<second,third>Input |                                           +-----------------------------+                                              ****                  ***                                             ****                         ****                                       **                                 **                   +---------------------------+               +--------------------------+  | Lambda(lambda x: [x] * 2) |               | Lambda(lambda x: str(x)) |  +---------------------------+               +--------------------------+                          ****                  ***                                                     ****          ****                                                            **      **                                                    +------------------------------+                                          | Parallel<second,third>Output |                                          +------------------------------+\n```", "url_final": "https://python.langchain.com/docs/how_to/lcel_cheatsheet/", "url_citable": "https://python.langchain.com/docs/how_to/lcel_cheatsheet/", "title": "LangChain Expression Language Cheatsheet | ü¶úÔ∏èüîó LangChain", "section_level": 1, "section_heading": "LangChain Expression Language Cheatsheet | ü¶úÔ∏èüîó LangChain", "section_anchor": null, "fetched_at": "2025-10-06T21:55:45.896512", "has_code": true}
{"id": "36caa1434b163d61e5e72b4caf3e400767307644", "text": "Get all prompts in a chain‚Äã\n\n```\nfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnableLambdaprompt1 = ChatPromptTemplate.from_messages(    [(\"system\", \"good ai\"), (\"human\", \"{input}\")])prompt2 = ChatPromptTemplate.from_messages(    [        (\"system\", \"really good ai\"),        (\"human\", \"{input}\"),        (\"ai\", \"{ai_output}\"),        (\"human\", \"{input2}\"),    ])fake_llm = RunnableLambda(lambda prompt: \"i am good ai\")chain = prompt1.assign(ai_output=fake_llm) | prompt2 | fake_llmfor i, prompt in enumerate(chain.get_prompts()):    print(f\"**prompt {i=}**\\n\")    print(prompt.pretty_repr())    print(\"\\n\" * 3)\n```\n\n```\n**prompt i=0**================================ System Message ================================good ai================================ Human Message ================================={input}**prompt i=1**================================ System Message ================================really good ai================================ Human Message ================================={input}================================== AI Message =================================={ai_output}================================ Human Message ================================={input2}\n```", "url_final": "https://python.langchain.com/docs/how_to/lcel_cheatsheet/", "url_citable": "https://python.langchain.com/docs/how_to/lcel_cheatsheet/#get-all-prompts-in-a-chain", "title": "LangChain Expression Language Cheatsheet | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Get all prompts in a chain‚Äã", "section_anchor": "get-all-prompts-in-a-chain", "fetched_at": "2025-10-06T21:55:45.896512", "has_code": true}
{"id": "fac608a2cc4daf1fc38c55e8e0d87b60bb19b63d", "text": "Add lifecycle listeners‚Äã\n\n```\nimport timefrom langchain_core.runnables import RunnableLambdafrom langchain_core.tracers.schemas import Rundef on_start(run_obj: Run):    print(\"start_time:\", run_obj.start_time)def on_end(run_obj: Run):    print(\"end_time:\", run_obj.end_time)runnable1 = RunnableLambda(lambda x: time.sleep(x))chain = runnable1.with_listeners(on_start=on_start, on_end=on_end)chain.invoke(2)\n```\n\n```\nstart_time: 2024-05-17 23:04:00.951065+00:00end_time: 2024-05-17 23:04:02.958765+00:00\n```\n\n- Invoke a runnable\n- Batch a runnable\n- Stream a runnable\n- Compose runnables\n- Invoke runnables in parallel\n- Turn any function into a runnable\n- Merge input and output dicts\n- Include input dict in output dict\n- Add default invocation args\n- Add fallbacks\n- Add retries\n- Configure runnable execution\n- Add default config to runnable\n- Make runnable attributes configurable\n- Make chain components configurable\n- Build a chain dynamically based on input\n- Generate a stream of events\n- Yield batched outputs as they complete\n- Return subset of output dict\n- Declaratively make a batched version of a runnable\n- Get a graph representation of a runnable\n- Get all prompts in a chain\n- Add lifecycle listeners", "url_final": "https://python.langchain.com/docs/how_to/lcel_cheatsheet/", "url_citable": "https://python.langchain.com/docs/how_to/lcel_cheatsheet/", "title": "LangChain Expression Language Cheatsheet | ü¶úÔ∏èüîó LangChain", "section_level": 1, "section_heading": "LangChain Expression Language Cheatsheet | ü¶úÔ∏èüîó LangChain", "section_anchor": null, "fetched_at": "2025-10-06T21:55:45.896512", "has_code": true}
{"id": "3ea4e8a4567af002c4cd3e636fc3c8eb1fc6c0ce", "text": "Next steps‚Äã\n\nYou've now learned how to pass callbacks into a constructor.\n\nNext, check out the other how-to guides in this section, such as how to pass callbacks at runtime.\n\n- Next steps", "url_final": "https://python.langchain.com/docs/how_to/callbacks_constructor/", "url_citable": "https://python.langchain.com/docs/how_to/callbacks_constructor/#next-steps", "title": "How to propagate callbacks  constructor | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Next steps‚Äã", "section_anchor": "next-steps", "fetched_at": "2025-10-06T21:54:59.935560", "has_code": false}
{"id": "041a955cfd8cfe3f2ac766845a4c78393b806e34", "text": "Load chat model‚Äã\n\nLet's first load a chat model:\n\n```\npip install -qU \"langchain[google-genai]\"\n```\n\n```\nimport getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n```\n\nSee also: How to summarize through parallelization and How to summarize through iterative refinement.", "url_final": "https://python.langchain.com/docs/how_to/summarize_stuff/", "url_citable": "https://python.langchain.com/docs/how_to/summarize_stuff/#load-chat-model", "title": "How to summarize text in a single LLM call | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Load chat model‚Äã", "section_anchor": "load-chat-model", "fetched_at": "2025-10-06T21:54:38.312792", "has_code": true}
{"id": "56db2574e3ea0388f410269b85e33886ce524780", "text": "Load documents‚Äã\n\nNext, we need some documents to summarize. Below, we generate some toy documents for illustrative purposes. See the document loader how-to guides and integration pages for additional sources of data. The summarization tutorial also includes an example summarizing a blog post.\n\n```\nfrom langchain_core.documents import Documentdocuments = [    Document(page_content=\"Apples are red\", metadata={\"title\": \"apple_book\"}),    Document(page_content=\"Blueberries are blue\", metadata={\"title\": \"blueberry_book\"}),    Document(page_content=\"Bananas are yelow\", metadata={\"title\": \"banana_book\"}),]\n```", "url_final": "https://python.langchain.com/docs/how_to/summarize_stuff/", "url_citable": "https://python.langchain.com/docs/how_to/summarize_stuff/#load-documents", "title": "How to summarize text in a single LLM call | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Load documents‚Äã", "section_anchor": "load-documents", "fetched_at": "2025-10-06T21:54:38.312792", "has_code": true}
{"id": "4b3cd380204462e047dbe329050d547563b98886", "text": "Load chain‚Äã\n\nBelow, we define a simple prompt and instantiate the chain with our chat model and documents:\n\n```\nfrom langchain.chains.combine_documents import create_stuff_documents_chainfrom langchain_core.prompts import ChatPromptTemplateprompt = ChatPromptTemplate.from_template(\"Summarize this content: {context}\")chain = create_stuff_documents_chain(llm, prompt)\n```", "url_final": "https://python.langchain.com/docs/how_to/summarize_stuff/", "url_citable": "https://python.langchain.com/docs/how_to/summarize_stuff/#load-chain", "title": "How to summarize text in a single LLM call | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Load chain‚Äã", "section_anchor": "load-chain", "fetched_at": "2025-10-06T21:54:38.312792", "has_code": true}
{"id": "9bd8a1e8d24376d87e9d14765336b7ec71929dd8", "text": "Invoke chain‚Äã\n\nBecause the chain is a Runnable, it implements the usual methods for invocation:\n\n```\nresult = chain.invoke({\"context\": documents})result\n```\n\n```\n'The content describes the colors of three fruits: apples are red, blueberries are blue, and bananas are yellow.'\n```\n\nStreaming‚Äã\n\nNote that the chain also supports streaming of individual output tokens:\n\n```\nfor chunk in chain.stream({\"context\": documents}):    print(chunk, end=\"|\")\n```\n\n```\n|The| content| describes| the| colors| of| three| fruits|:| apples| are| red|,| blueberries| are| blue|,| and| bananas| are| yellow|.||\n```", "url_final": "https://python.langchain.com/docs/how_to/summarize_stuff/", "url_citable": "https://python.langchain.com/docs/how_to/summarize_stuff/#invoke-chain", "title": "How to summarize text in a single LLM call | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Invoke chain‚Äã", "section_anchor": "invoke-chain", "fetched_at": "2025-10-06T21:54:38.312792", "has_code": true}
{"id": "ca355201a5d9fbd557247aeb7b81c9b4fd692a8e", "text": "Next steps‚Äã\n\nSee the summarization how-to guides for additional summarization strategies, including those designed for larger volumes of text.\n\nSee also this tutorial for more detail on summarization.\n\n- Streaming", "url_final": "https://python.langchain.com/docs/how_to/summarize_stuff/", "url_citable": "https://python.langchain.com/docs/how_to/summarize_stuff/#next-steps", "title": "How to summarize text in a single LLM call | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Next steps‚Äã", "section_anchor": "next-steps", "fetched_at": "2025-10-06T21:54:38.312792", "has_code": false}
{"id": "170d45a8ba760279f8eebd149c9d28e0b9d6d900", "text": "‚ö†Ô∏è Security note ‚ö†Ô∏è‚Äã\n\nBuilding Q&A systems of SQL databases requires executing model-generated SQL queries. There are inherent risks in doing this. Make sure that your database connection permissions are always scoped as narrowly as possible for your chain/agent's needs. This will mitigate though not eliminate the risks of building a model-driven system. For more on general security best practices, see here.", "url_final": "https://python.langchain.com/docs/tutorials/sql_qa/", "url_citable": "https://python.langchain.com/docs/tutorials/sql_qa/#Ô∏è-security-note-Ô∏è", "title": "Build a Question/Answering system over SQL data | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "‚ö†Ô∏è Security note ‚ö†Ô∏è‚Äã", "section_anchor": "Ô∏è-security-note-Ô∏è", "fetched_at": "2025-10-06T21:54:13.432579", "has_code": false}
{"id": "42cd6ec6bbc452989c16442bfb99104f1ee1c662", "text": "Architecture‚Äã\n\nAt a high-level, the steps of these systems are:\n\n- Convert question to SQL query: Model converts user input to a SQL query.\n- Execute SQL query: Execute the query.\n- Answer the question: Model responds to user input using the query results.\n\nNote that querying data in CSVs can follow a similar approach. See our how-to guide on question-answering over CSV data for more detail.", "url_final": "https://python.langchain.com/docs/tutorials/sql_qa/", "url_citable": "https://python.langchain.com/docs/tutorials/sql_qa/#architecture", "title": "Build a Question/Answering system over SQL data | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Architecture‚Äã", "section_anchor": "architecture", "fetched_at": "2025-10-06T21:54:13.432579", "has_code": false}
{"id": "6d622512d8d2fa9bf70d3fc37f13f9afa7b98878", "text": "Setup‚Äã\n\nFirst, get required packages and set environment variables:\n\n```\n%%capture --no-stderr%pip install --upgrade --quiet langchain-community langgraph\n```\n\n```\n# Comment out the below to opt-out of using LangSmith in this notebook. Not required.if not os.environ.get(\"LANGSMITH_API_KEY\"):    os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()    os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n```\n\nSample data‚Äã\n\nThe below example will use a SQLite connection with the Chinook database, which is a sample database that represents a digital media store. Follow these installation steps to create Chinook.db in the same directory as this notebook. You can also download and build the database via the command line:\n\n```\ncurl -s https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sql | sqlite3 Chinook.db\n```\n\nNow, Chinook.db is in our directory and we can interface with it using the SQLAlchemy-driven SQLDatabase class:\n\n```\nfrom langchain_community.utilities import SQLDatabasedb = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")print(db.dialect)print(db.get_usable_table_names())db.run(\"SELECT * FROM Artist LIMIT 10;\")\n```\n\n```\nsqlite['Album', 'Artist', 'Customer', 'Employee', 'Genre', 'Invoice', 'InvoiceLine', 'MediaType', 'Playlist', 'PlaylistTrack', 'Track']\n```\n\n```\n\"[(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains'), (6, 'Ant√¥nio Carlos Jobim'), (7, 'Apocalyptica'), (8, 'Audioslave'), (9, 'BackBeat'), (10, 'Billy Cobham')]\"\n```\n\nGreat! We've got a SQL database that we can query. Now let's try hooking it up to an LLM.", "url_final": "https://python.langchain.com/docs/tutorials/sql_qa/", "url_citable": "https://python.langchain.com/docs/tutorials/sql_qa/#setup", "title": "Build a Question/Answering system over SQL data | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Setup‚Äã", "section_anchor": "setup", "fetched_at": "2025-10-06T21:54:13.432579", "has_code": true}
{"id": "dcc69d22fb57de42a6926ad31871f2e9aceed4c1", "text": "Chains‚Äã\n\nChains are compositions of predictable steps. In LangGraph, we can represent a chain via simple sequence of nodes. Let's create a sequence of steps that, given a question, does the following:\n\n- converts the question into a SQL query;\n- executes the query;\n- uses the result to answer the original question.\n\nThere are scenarios not supported by this arrangement. For example, this system will execute a SQL query for any user input-- even \"hello\". Importantly, as we'll see below, some questions require more than one query to answer. We will address these scenarios in the Agents section.\n\nApplication state‚Äã\n\nThe LangGraph state of our application controls what data is input to the application, transferred between steps, and output by the application. It is typically a TypedDict, but can also be a Pydantic BaseModel.\n\nFor this application, we can just keep track of the input question, generated query, query result, and generated answer:\n\n```\nfrom typing_extensions import TypedDictclass State(TypedDict):    question: str    query: str    result: str    answer: str\n```\n\nNow we just need functions that operate on this state and populate its contents.", "url_final": "https://python.langchain.com/docs/tutorials/sql_qa/", "url_citable": "https://python.langchain.com/docs/tutorials/sql_qa/#chains", "title": "Build a Question/Answering system over SQL data | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Chains‚Äã", "section_anchor": "chains", "fetched_at": "2025-10-06T21:54:13.432579", "has_code": true}
{"id": "0847d8e5635163defa4cd3abca0c8262238d5f48", "text": "Convert question to SQL query‚Äã\n\nThe first step is to take the user input and convert it to a SQL query. To reliably obtain SQL queries (absent markdown formatting and explanations or clarifications), we will make use of LangChain's structured output abstraction.\n\nLet's select a chat model for our application:\n\n```\npip install -qU \"langchain[google-genai]\"\n```\n\n```\nimport getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n```\n\nLet's provide some instructions for our model:\n\n```\nfrom langchain_core.prompts import ChatPromptTemplatesystem_message = \"\"\"Given an input question, create a syntactically correct {dialect} query torun to help find the answer. Unless the user specifies in his question aspecific number of examples they wish to obtain, always limit your query toat most {top_k} results. You can order the results by a relevant column toreturn the most interesting examples in the database.Never query for all the columns from a specific table, only ask for a thefew relevant columns given the question.Pay attention to use only the column names that you can see in the schemadescription. Be careful to not query for columns that do not exist. Also,pay attention to which column is in which table.Only use the following tables:{table_info}\"\"\"user_prompt = \"Question: {input}\"query_prompt_template = ChatPromptTemplate(    [(\"system\", system_message), (\"user\", user_prompt)])for message in query_prompt_template.messages:    message.pretty_print()\n```\n\n```\n================================\u001b[1m System Message \u001b[0m================================Given an input question, create a syntactically correct \u001b[33;1m\u001b[1;3m{dialect}\u001b[0m query torun to help find the answer. Unless the user specifies in his question aspecific number of examples they wish to obtain, always limit your query toat most \u001b[33;1m\u001b[1;3m{top_k}\u001b[0m results. You can order the results by a relevant column toreturn the most interesting examples in the database.Never query for all the columns from a specific table, only ask for a thefew relevant columns given the question.Pay attention to use only the column names that you can see in the schemadescription. Be careful to not query for columns that do not exist. Also,pay attention to which column is in which table.Only use the following tables:\u001b[33;1m\u001b[1;3m{table_info}\u001b[0m================================\u001b[1m Human Message \u001b[0m=================================Question: \u001b[33;1m\u001b[1;3m{input}\u001b[0m\n```\n\nThe prompt includes several parameters we will need to populate, such as the SQL dialect and table schemas. LangChain's SQLDatabase object includes methods to help with this. Our write_query step will just populate these parameters and prompt a model to generate the SQL query:\n\n```\nfrom typing_extensions import Annotatedclass QueryOutput(TypedDict):    \"\"\"Generated SQL query.\"\"\"    query: Annotated[str, ..., \"Syntactically valid SQL query.\"]def write_query(state: State):    \"\"\"Generate SQL query to fetch information.\"\"\"    prompt = query_prompt_template.invoke(        {            \"dialect\": db.dialect,            \"top_k\": 10,            \"table_info\": db.get_table_info(),            \"input\": state[\"question\"],        }    )    structured_llm = llm.with_structured_output(QueryOutput)    result = structured_llm.invoke(prompt)    return {\"query\": result[\"query\"]}\n```\n\nLet's test it out:\n\n```\nwrite_query({\"question\": \"How many Employees are there?\"})\n```\n\n```\n{'query': 'SELECT COUNT(*) as employee_count FROM Employee;'}\n```", "url_final": "https://python.langchain.com/docs/tutorials/sql_qa/", "url_citable": "https://python.langchain.com/docs/tutorials/sql_qa/#convert-question-to-sql-query", "title": "Build a Question/Answering system over SQL data | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Convert question to SQL query‚Äã", "section_anchor": "convert-question-to-sql-query", "fetched_at": "2025-10-06T21:54:13.432579", "has_code": true}
{"id": "54946fcfaba7065ea5a156a86b9871a546d206d4", "text": "Execute query‚Äã\n\nThis is the most dangerous part of creating a SQL chain. Consider carefully if it is OK to run automated queries over your data. Minimize the database connection permissions as much as possible. Consider adding a human approval step to you chains before query execution (see below).\n\nTo execute the query, we will load a tool from langchain-community. Our execute_query node will just wrap this tool:\n\n```\nfrom langchain_community.tools.sql_database.tool import QuerySQLDatabaseTooldef execute_query(state: State):    \"\"\"Execute SQL query.\"\"\"    execute_query_tool = QuerySQLDatabaseTool(db=db)    return {\"result\": execute_query_tool.invoke(state[\"query\"])}\n```\n\nTesting this step:\n\n```\nexecute_query({\"query\": \"SELECT COUNT(EmployeeId) AS EmployeeCount FROM Employee;\"})\n```\n\n```\n{'result': '[(8,)]'}\n```", "url_final": "https://python.langchain.com/docs/tutorials/sql_qa/", "url_citable": "https://python.langchain.com/docs/tutorials/sql_qa/#execute-query", "title": "Build a Question/Answering system over SQL data | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Execute query‚Äã", "section_anchor": "execute-query", "fetched_at": "2025-10-06T21:54:13.432579", "has_code": true}
{"id": "5ecc2bdc83cab6decce3b06b31bb9edfe655d337", "text": "Generate answer‚Äã\n\nFinally, our last step generates an answer to the question given the information pulled from the database:\n\n```\ndef generate_answer(state: State):    \"\"\"Answer question using retrieved information as context.\"\"\"    prompt = (        \"Given the following user question, corresponding SQL query, \"        \"and SQL result, answer the user question.\\n\\n\"        f\"Question: {state['question']}\\n\"        f\"SQL Query: {state['query']}\\n\"        f\"SQL Result: {state['result']}\"    )    response = llm.invoke(prompt)    return {\"answer\": response.content}\n```", "url_final": "https://python.langchain.com/docs/tutorials/sql_qa/", "url_citable": "https://python.langchain.com/docs/tutorials/sql_qa/#generate-answer", "title": "Build a Question/Answering system over SQL data | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Generate answer‚Äã", "section_anchor": "generate-answer", "fetched_at": "2025-10-06T21:54:13.432579", "has_code": true}
{"id": "555a35d89d09b8fb8aec20cb8bdd29810afc3240", "text": "Orchestrating with LangGraph‚Äã\n\nFinally, we compile our application into a single graph object. In this case, we are just connecting the three steps into a single sequence.\n\n```\nfrom langgraph.graph import START, StateGraphgraph_builder = StateGraph(State).add_sequence(    [write_query, execute_query, generate_answer])graph_builder.add_edge(START, \"write_query\")graph = graph_builder.compile()\n```\n\nLangGraph also comes with built-in utilities for visualizing the control flow of your application:\n\n```\nfrom IPython.display import Image, displaydisplay(Image(graph.get_graph().draw_mermaid_png()))\n```\n\nLet's test our application! Note that we can stream the results of individual steps:\n\n```\nfor step in graph.stream(    {\"question\": \"How many employees are there?\"}, stream_mode=\"updates\"):    print(step)\n```\n\n```\n{'write_query': {'query': 'SELECT COUNT(*) as employee_count FROM Employee;'}}{'execute_query': {'result': '[(8,)]'}}{'generate_answer': {'answer': 'There are 8 employees in total.'}}\n```\n\nCheck out the LangSmith trace.", "url_final": "https://python.langchain.com/docs/tutorials/sql_qa/", "url_citable": "https://python.langchain.com/docs/tutorials/sql_qa/#orchestrating-with-langgraph", "title": "Build a Question/Answering system over SQL data | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Orchestrating with LangGraph‚Äã", "section_anchor": "orchestrating-with-langgraph", "fetched_at": "2025-10-06T21:54:13.432579", "has_code": true}
{"id": "f25e5acf02ccfe92128c7d64da342db76415b1c5", "text": "Human-in-the-loop‚Äã\n\nLangGraph supports a number of features that can be useful for this workflow. One of them is human-in-the-loop: we can interrupt our application before sensitive steps (such as the execution of a SQL query) for human review. This is enabled by LangGraph's persistence layer, which saves run progress to your storage of choice. Below, we specify storage in-memory:\n\n```\nfrom langgraph.checkpoint.memory import MemorySavermemory = MemorySaver()graph = graph_builder.compile(checkpointer=memory, interrupt_before=[\"execute_query\"])# Now that we're using persistence, we need to specify a thread ID# so that we can continue the run after review.config = {\"configurable\": {\"thread_id\": \"1\"}}\n```\n\n```\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n```\n\nLet's repeat the same run, adding in a simple yes/no approval step:\n\n```\nfor step in graph.stream(    {\"question\": \"How many employees are there?\"},    config,    stream_mode=\"updates\",):    print(step)try:    user_approval = input(\"Do you want to go to execute query? (yes/no): \")except Exception:    user_approval = \"no\"if user_approval.lower() == \"yes\":    # If approved, continue the graph execution    for step in graph.stream(None, config, stream_mode=\"updates\"):        print(step)else:    print(\"Operation cancelled by user.\")\n```\n\n```\n{'write_query': {'query': 'SELECT COUNT(EmployeeId) AS EmployeeCount FROM Employee;'}}{'__interrupt__': ()}``````outputDo you want to go to execute query? (yes/no):  yes``````output{'execute_query': {'result': '[(8,)]'}}{'generate_answer': {'answer': 'There are 8 employees.'}}\n```\n\nSee this LangGraph guide for more detail and examples.", "url_final": "https://python.langchain.com/docs/tutorials/sql_qa/", "url_citable": "https://python.langchain.com/docs/tutorials/sql_qa/#human-in-the-loop", "title": "Build a Question/Answering system over SQL data | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Human-in-the-loop‚Äã", "section_anchor": "human-in-the-loop", "fetched_at": "2025-10-06T21:54:13.432579", "has_code": true}
{"id": "9a967582106531f533d24a23ffb1183b42a64d9e", "text": "Next steps‚Äã\n\nFor more complex query-generation, we may want to create few-shot prompts or add query-checking steps. For advanced techniques like this and more check out:\n\n- Prompting strategies: Advanced prompt engineering techniques.\n- Query checking: Add query validation and error handling.\n- Large databases: Techniques for working with large databases.", "url_final": "https://python.langchain.com/docs/tutorials/sql_qa/", "url_citable": "https://python.langchain.com/docs/tutorials/sql_qa/#next-steps", "title": "Build a Question/Answering system over SQL data | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Next steps‚Äã", "section_anchor": "next-steps", "fetched_at": "2025-10-06T21:54:13.432579", "has_code": false}
{"id": "a06eb0a93b6dc6c2a4b8b6d10cf10e66ab6d2282", "text": "Agents‚Äã\n\nAgents leverage the reasoning capabilities of LLMs to make decisions during execution. Using agents allows you to offload additional discretion over the query generation and execution process. Although their behavior is less predictable than the above \"chain\", they feature some advantages:\n\n- They can query the database as many times as needed to answer the user question.\n- They can recover from errors by running a generated query, catching the traceback and regenerating it correctly.\n- They can answer questions based on the databases' schema as well as on the databases' content (like describing a specific table).\n\nBelow we assemble a minimal SQL agent. We will equip it with a set of tools using LangChain's SQLDatabaseToolkit. Using LangGraph's pre-built ReAct agent constructor, we can do this in one line.\n\nCheck out LangGraph's SQL Agent Tutorial for a more advanced formulation of a SQL agent.\n\nThe SQLDatabaseToolkit includes tools that can:\n\n- Create and execute queries\n- Check query syntax\n- Retrieve table descriptions\n- ... and more\n\n```\nfrom langchain_community.agent_toolkits import SQLDatabaseToolkittoolkit = SQLDatabaseToolkit(db=db, llm=llm)tools = toolkit.get_tools()tools\n```\n\n```\n[QuerySQLDatabaseTool(description=\"Input to this tool is a detailed and correct SQL query, output is a result from the database. If the query is not correct, an error message will be returned. If an error is returned, rewrite the query, check the query, and try again. If you encounter an issue with Unknown column 'xxxx' in 'field list', use sql_db_schema to query the correct table fields.\", db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x10d5f9120>), InfoSQLDatabaseTool(description='Input to this tool is a comma-separated list of tables, output is the schema and sample rows for those tables. Be sure that the tables actually exist by calling sql_db_list_tables first! Example Input: table1, table2, table3', db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x10d5f9120>), ListSQLDatabaseTool(db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x10d5f9120>), QuerySQLCheckerTool(description='Use this tool to double check if your query is correct before executing it. Always use this tool before executing a query with sql_db_query!', db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x10d5f9120>, llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x119315480>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x119317550>, root_client=<openai.OpenAI object at 0x10d5f8df0>, root_async_client=<openai.AsyncOpenAI object at 0x1193154e0>, model_name='gpt-4o', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['dialect', 'query'], input_types={}, partial_variables={}, template='\\n{query}\\nDouble check the {dialect} query above for common mistakes, including:\\n- Using NOT IN with NULL values\\n- Using UNION when UNION ALL should have been used\\n- Using BETWEEN for exclusive ranges\\n- Data type mismatch in predicates\\n- Properly quoting identifiers\\n- Using the correct number of arguments for functions\\n- Casting to the correct data type\\n- Using the proper columns for joins\\n\\nIf there are any of the above mistakes, rewrite the query. If there are no mistakes, just reproduce the original query.\\n\\nOutput the final SQL query only.\\n\\nSQL Query: '), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x119315480>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x119317550>, root_client=<openai.OpenAI object at 0x10d5f8df0>, root_async_client=<openai.AsyncOpenAI object at 0x1193154e0>, model_name='gpt-4o', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}))]\n```", "url_final": "https://python.langchain.com/docs/tutorials/sql_qa/", "url_citable": "https://python.langchain.com/docs/tutorials/sql_qa/#agents", "title": "Build a Question/Answering system over SQL data | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Agents‚Äã", "section_anchor": "agents", "fetched_at": "2025-10-06T21:54:13.432579", "has_code": true}
{"id": "a573d56c029422e6052199ae74d049499b4651fd", "text": "System Prompt‚Äã\n\nWe will also want to load a system prompt for our agent. This will consist of instructions for how to behave. Note that the prompt below has several parameters, which we assign below.\n\n```\nsystem_message = \"\"\"You are an agent designed to interact with a SQL database.Given an input question, create a syntactically correct {dialect} query to run,then look at the results of the query and return the answer. Unless the userspecifies a specific number of examples they wish to obtain, always limit yourquery to at most {top_k} results.You can order the results by a relevant column to return the most interestingexamples in the database. Never query for all the columns from a specific table,only ask for the relevant columns given the question.You MUST double check your query before executing it. If you get an error whileexecuting a query, rewrite the query and try again.DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to thedatabase.To start you should ALWAYS look at the tables in the database to see what youcan query. Do NOT skip this step.Then you should query the schema of the most relevant tables.\"\"\".format(    dialect=\"SQLite\",    top_k=5,)\n```", "url_final": "https://python.langchain.com/docs/tutorials/sql_qa/", "url_citable": "https://python.langchain.com/docs/tutorials/sql_qa/#system-prompt", "title": "Build a Question/Answering system over SQL data | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "System Prompt‚Äã", "section_anchor": "system-prompt", "fetched_at": "2025-10-06T21:54:13.432579", "has_code": true}
{"id": "584443148441f1765a7edb911c3b5cfc5ea2b6ad", "text": "Initializing agent‚Äã\n\nWe will use a prebuilt LangGraph agent to build our agent\n\n```\nfrom langchain_core.messages import HumanMessagefrom langgraph.prebuilt import create_react_agentagent_executor = create_react_agent(llm, tools, prompt=system_message)\n```\n\nConsider how the agent responds to the below question:\n\n```\nquestion = \"Which country's customers spent the most?\"for step in agent_executor.stream(    {\"messages\": [{\"role\": \"user\", \"content\": question}]},    stream_mode=\"values\",):    step[\"messages\"][-1].pretty_print()\n```\n\n```\n================================\u001b[1m Human Message \u001b[0m=================================Which country's customers spent the most?==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls:  sql_db_list_tables (call_tFp7HYD6sAAmCShgeqkVZH6Q) Call ID: call_tFp7HYD6sAAmCShgeqkVZH6Q  Args:=================================\u001b[1m Tool Message \u001b[0m=================================Name: sql_db_list_tablesAlbum, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls:  sql_db_schema (call_KJZ1Jx6JazyDdJa0uH1UeiOz) Call ID: call_KJZ1Jx6JazyDdJa0uH1UeiOz  Args:    table_names: Customer, Invoice=================================\u001b[1m Tool Message \u001b[0m=================================Name: sql_db_schemaCREATE TABLE \"Customer\" (\t\"CustomerId\" INTEGER NOT NULL, \t\"FirstName\" NVARCHAR(40) NOT NULL, \t\"LastName\" NVARCHAR(20) NOT NULL, \t\"Company\" NVARCHAR(80), \t\"Address\" NVARCHAR(70), \t\"City\" NVARCHAR(40), \t\"State\" NVARCHAR(40), \t\"Country\" NVARCHAR(40), \t\"PostalCode\" NVARCHAR(10), \t\"Phone\" NVARCHAR(24), \t\"Fax\" NVARCHAR(24), \t\"Email\" NVARCHAR(60) NOT NULL, \t\"SupportRepId\" INTEGER, \tPRIMARY KEY (\"CustomerId\"), \tFOREIGN KEY(\"SupportRepId\") REFERENCES \"Employee\" (\"EmployeeId\"))/*3 rows from Customer table:CustomerId\tFirstName\tLastName\tCompany\tAddress\tCity\tState\tCountry\tPostalCode\tPhone\tFax\tEmail\tSupportRepId1\tLu√≠s\tGon√ßalves\tEmbraer - Empresa Brasileira de Aeron√°utica S.A.\tAv. Brigadeiro Faria Lima, 2170\tS√£o Jos√© dos Campos\tSP\tBrazil\t12227-000\t+55 (12) 3923-5555\t+55 (12) 3923-5566\tluisg@embraer.com.br\t32\tLeonie\tK√∂hler\tNone\tTheodor-Heuss-Stra√üe 34\tStuttgart\tNone\tGermany\t70174\t+49 0711 2842222\tNone\tleonekohler@surfeu.de\t53\tFran√ßois\tTremblay\tNone\t1498 rue B√©langer\tMontr√©al\tQC\tCanada\tH2G 1A7\t+1 (514) 721-4711\tNone\tftremblay@gmail.com\t3*/CREATE TABLE \"Invoice\" (\t\"InvoiceId\" INTEGER NOT NULL, \t\"CustomerId\" INTEGER NOT NULL, \t\"InvoiceDate\" DATETIME NOT NULL, \t\"BillingAddress\" NVARCHAR(70), \t\"BillingCity\" NVARCHAR(40), \t\"BillingState\" NVARCHAR(40), \t\"BillingCountry\" NVARCHAR(40), \t\"BillingPostalCode\" NVARCHAR(10), \t\"Total\" NUMERIC(10, 2) NOT NULL, \tPRIMARY KEY (\"InvoiceId\"), \tFOREIGN KEY(\"CustomerId\") REFERENCES \"Customer\" (\"CustomerId\"))/*3 rows from Invoice table:InvoiceId\tCustomerId\tInvoiceDate\tBillingAddress\tBillingCity\tBillingState\tBillingCountry\tBillingPostalCode\tTotal1\t2\t2021-01-01 00:00:00\tTheodor-Heuss-Stra√üe 34\tStuttgart\tNone\tGermany\t70174\t1.982\t4\t2021-01-02 00:00:00\tUllev√•lsveien 14\tOslo\tNone\tNorway\t0171\t3.963\t8\t2021-01-03 00:00:00\tGr√©trystraat 63\tBrussels\tNone\tBelgium\t1000\t5.94*/==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls:  sql_db_query_checker (call_AQuTGbgH63u4gPgyV723yrjX) Call ID: call_AQuTGbgH63u4gPgyV723yrjX  Args:    query: SELECT c.Country, SUM(i.Total) as TotalSpent FROM Customer c JOIN Invoice i ON c.CustomerId = i.CustomerId GROUP BY c.Country ORDER BY TotalSpent DESC LIMIT 1;=================================\u001b[1m Tool Message \u001b[0m=================================Name: sql_db_query_checker\\`\\`\\`sqlSELECT c.Country, SUM(i.Total) as TotalSpent FROM Customer c JOIN Invoice i ON c.CustomerId = i.CustomerId GROUP BY c.Country ORDER BY TotalSpent DESC LIMIT 1;\\`\\`\\`==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls:  sql_db_query (call_B88EwU44nwwpQL5M9nlcemSU) Call ID: call_B88EwU44nwwpQL5M9nlcemSU  Args:    query: SELECT c.Country, SUM(i.Total) as TotalSpent FROM Customer c JOIN Invoice i ON c.CustomerId = i.CustomerId GROUP BY c.Country ORDER BY TotalSpent DESC LIMIT 1;=================================\u001b[1m Tool Message \u001b[0m=================================Name: sql_db_query[('USA', 523.06)]==================================\u001b[1m Ai Message \u001b[0m==================================The country whose customers spent the most is the USA, with a total spending of 523.06.\n```\n\nYou can also use the LangSmith trace to visualize these steps and associated metadata.\n\nNote that the agent executes multiple queries until it has the information it needs:\n\n- List available tables;\n- Retrieves the schema for three tables;\n- Queries multiple of the tables via a join operation.\n\nThe agent is then able to use the result of the final query to generate an answer to the original question.\n\nThe agent can similarly handle qualitative questions:\n\n```\nquestion = \"Describe the playlisttrack table\"for step in agent_executor.stream(    {\"messages\": [{\"role\": \"user\", \"content\": question}]},    stream_mode=\"values\",):    step[\"messages\"][-1].pretty_print()\n```\n\n```\n================================\u001b[1m Human Message \u001b[0m=================================Describe the playlisttrack table==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls:  sql_db_list_tables (call_fMF8eTmX5TJDJjc3Mhdg52TI) Call ID: call_fMF8eTmX5TJDJjc3Mhdg52TI  Args:=================================\u001b[1m Tool Message \u001b[0m=================================Name: sql_db_list_tablesAlbum, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls:  sql_db_schema (call_W8Vkk4NEodkAAIg8nexAszUH) Call ID: call_W8Vkk4NEodkAAIg8nexAszUH  Args:    table_names: PlaylistTrack=================================\u001b[1m Tool Message \u001b[0m=================================Name: sql_db_schemaCREATE TABLE \"PlaylistTrack\" (\t\"PlaylistId\" INTEGER NOT NULL, \t\"TrackId\" INTEGER NOT NULL, \tPRIMARY KEY (\"PlaylistId\", \"TrackId\"), \tFOREIGN KEY(\"TrackId\") REFERENCES \"Track\" (\"TrackId\"), \tFOREIGN KEY(\"PlaylistId\") REFERENCES \"Playlist\" (\"PlaylistId\"))/*3 rows from PlaylistTrack table:PlaylistId\tTrackId1\t34021\t33891\t3390*/==================================\u001b[1m Ai Message \u001b[0m==================================The `PlaylistTrack` table is designed to associate tracks with playlists. It has the following structure:- **PlaylistId**: An integer that serves as a foreign key referencing the `Playlist` table. It is part of the composite primary key.- **TrackId**: An integer that serves as a foreign key referencing the `Track` table. It is also part of the composite primary key.The primary key for this table is a composite key consisting of both `PlaylistId` and `TrackId`, ensuring that each track can be uniquely associated with a playlist. The table enforces referential integrity by linking to the `Track` and `Playlist` tables through foreign keys.\n```", "url_final": "https://python.langchain.com/docs/tutorials/sql_qa/", "url_citable": "https://python.langchain.com/docs/tutorials/sql_qa/#initializing-agent", "title": "Build a Question/Answering system over SQL data | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Initializing agent‚Äã", "section_anchor": "initializing-agent", "fetched_at": "2025-10-06T21:54:13.432579", "has_code": true}
{"id": "b9f039514d499e29f8182666b89ca940a027aec4", "text": "Dealing with high-cardinality columns‚Äã\n\nIn order to filter columns that contain proper nouns such as addresses, song names or artists, we first need to double-check the spelling in order to filter the data correctly.\n\nWe can achieve this by creating a vector store with all the distinct proper nouns that exist in the database. We can then have the agent query that vector store each time the user includes a proper noun in their question, to find the correct spelling for that word. In this way, the agent can make sure it understands which entity the user is referring to before building the target query.\n\nFirst we need the unique values for each entity we want, for which we define a function that parses the result into a list of elements:\n\n```\nimport astimport redef query_as_list(db, query):    res = db.run(query)    res = [el for sub in ast.literal_eval(res) for el in sub if el]    res = [re.sub(r\"\\b\\d+\\b\", \"\", string).strip() for string in res]    return list(set(res))artists = query_as_list(db, \"SELECT Name FROM Artist\")albums = query_as_list(db, \"SELECT Title FROM Album\")albums[:5]\n```\n\n```\n['In Through The Out Door', 'Transmission', 'Battlestar Galactica (Classic), Season', 'A Copland Celebration, Vol. I', 'Quiet Songs']\n```\n\nUsing this function, we can create a retriever tool that the agent can execute at its discretion.\n\nLet's select an embeddings model and vector store for this step:\n\nSelect an embedding model:\n\n```\npip install -qU langchain-openai\n```\n\n```\nimport getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"):  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")from langchain_openai import OpenAIEmbeddingsembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n```\n\nSelect a vector store:\n\n```\npip install -qU langchain-core\n```\n\n```\nfrom langchain_core.vectorstores import InMemoryVectorStorevector_store = InMemoryVectorStore(embeddings)\n```\n\nWe can now construct a retrieval tool that can search over relevant proper nouns in the database:\n\n```\nfrom langchain.agents.agent_toolkits import create_retriever_tool_ = vector_store.add_texts(artists + albums)retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})description = (    \"Use to look up values to filter on. Input is an approximate spelling \"    \"of the proper noun, output is valid proper nouns. Use the noun most \"    \"similar to the search.\")retriever_tool = create_retriever_tool(    retriever,    name=\"search_proper_nouns\",    description=description,)\n```\n\nLet's try it out:\n\n```\nprint(retriever_tool.invoke(\"Alice Chains\"))\n```\n\n```\nAlice In ChainsAlanis MorissettePearl JamPearl JamAudioslave\n```\n\nThis way, if the agent determines it needs to write a filter based on an artist along the lines of \"Alice Chains\", it can first use the retriever tool to observe relevant values of a column.\n\nPutting this together:\n\n```\n# Add to system messagesuffix = (    \"If you need to filter on a proper noun like a Name, you must ALWAYS first look up \"    \"the filter value using the 'search_proper_nouns' tool! Do not try to \"    \"guess at the proper name - use this function to find similar ones.\")system = f\"{system_message}\\n\\n{suffix}\"tools.append(retriever_tool)agent = create_react_agent(llm, tools, prompt=system)\n```\n\n```\nquestion = \"How many albums does alis in chain have?\"for step in agent.stream(    {\"messages\": [{\"role\": \"user\", \"content\": question}]},    stream_mode=\"values\",):    step[\"messages\"][-1].pretty_print()\n```\n\n```\n================================\u001b[1m Human Message \u001b[0m=================================How many albums does alis in chain have?==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls:  search_proper_nouns (call_8ryjsRPLAr79mM3Qvnq6gTOH) Call ID: call_8ryjsRPLAr79mM3Qvnq6gTOH  Args:    query: alis in chain=================================\u001b[1m Tool Message \u001b[0m=================================Name: search_proper_nounsAlice In ChainsAisha DuoXisDa Lama Ao CaosA-Sides==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls:  sql_db_list_tables (call_NJjtCpU89MBMplssjn1z0xzq) Call ID: call_NJjtCpU89MBMplssjn1z0xzq  Args:  search_proper_nouns (call_1BfrueC9koSIyi4OfMu2Ao8q) Call ID: call_1BfrueC9koSIyi4OfMu2Ao8q  Args:    query: Alice In Chains=================================\u001b[1m Tool Message \u001b[0m=================================Name: search_proper_nounsAlice In ChainsPearl JamPearl JamFoo FightersSoundgarden==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls:  sql_db_schema (call_Kn09w9jd9swcNzIZ1b5MlKID) Call ID: call_Kn09w9jd9swcNzIZ1b5MlKID  Args:    table_names: Album, Artist=================================\u001b[1m Tool Message \u001b[0m=================================Name: sql_db_schemaCREATE TABLE \"Album\" (\t\"AlbumId\" INTEGER NOT NULL, \t\"Title\" NVARCHAR(160) NOT NULL, \t\"ArtistId\" INTEGER NOT NULL, \tPRIMARY KEY (\"AlbumId\"), \tFOREIGN KEY(\"ArtistId\") REFERENCES \"Artist\" (\"ArtistId\"))/*3 rows from Album table:AlbumId\tTitle\tArtistId1\tFor Those About To Rock We Salute You\t12\tBalls to the Wall\t23\tRestless and Wild\t2*/CREATE TABLE \"Artist\" (\t\"ArtistId\" INTEGER NOT NULL, \t\"Name\" NVARCHAR(120), \tPRIMARY KEY (\"ArtistId\"))/*3 rows from Artist table:ArtistId\tName1\tAC/DC2\tAccept3\tAerosmith*/==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls:  sql_db_query (call_WkHRiPcBoGN9bc58MIupRHKP) Call ID: call_WkHRiPcBoGN9bc58MIupRHKP  Args:    query: SELECT COUNT(*) FROM Album WHERE ArtistId = (SELECT ArtistId FROM Artist WHERE Name = 'Alice In Chains')=================================\u001b[1m Tool Message \u001b[0m=================================Name: sql_db_query[(1,)]==================================\u001b[1m Ai Message \u001b[0m==================================Alice In Chains has released 1 album in the database.\n```\n\nAs we can see, both in the streamed steps and in the LangSmith trace, the agent used the search_proper_nouns tool in order to check how to correctly query the database for this specific artist.\n\n- Sample data\n\n- Application state\n- Convert question to SQL query\n- Execute query\n- Generate answer\n- Orchestrating with LangGraph\n- Human-in-the-loop\n- Next steps\n\n- System Prompt\n- Initializing agent\n- Dealing with high-cardinality columns", "url_final": "https://python.langchain.com/docs/tutorials/sql_qa/", "url_citable": "https://python.langchain.com/docs/tutorials/sql_qa/#dealing-with-high-cardinality-columns", "title": "Build a Question/Answering system over SQL data | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Dealing with high-cardinality columns‚Äã", "section_anchor": "dealing-with-high-cardinality-columns", "fetched_at": "2025-10-06T21:54:13.432579", "has_code": true}
{"id": "ce7c2811b7c202d0c168d99e78b722306d476135", "text": "Smaller chunks‚Äã\n\nOften times it can be useful to retrieve larger chunks of information, but embed smaller chunks. This allows for embeddings to capture the semantic meaning as closely as possible, but for as much context as possible to be passed downstream. Note that this is what the ParentDocumentRetriever does. Here we show what is going on under the hood.\n\nWe will make a distinction between the vector store, which indexes embeddings of the (sub) documents, and the document store, which houses the \"parent\" documents and associates them with an identifier.\n\n```\nimport uuidfrom langchain.retrievers.multi_vector import MultiVectorRetriever# The storage layer for the parent documentsstore = InMemoryByteStore()id_key = \"doc_id\"# The retriever (empty to start)retriever = MultiVectorRetriever(    vectorstore=vectorstore,    byte_store=store,    id_key=id_key,)doc_ids = [str(uuid.uuid4()) for _ in docs]\n```\n\nWe next generate the \"sub\" documents by splitting the original documents. Note that we store the document identifier in the metadata of the corresponding Document object.\n\n```\n# The splitter to use to create smaller chunkschild_text_splitter = RecursiveCharacterTextSplitter(chunk_size=400)sub_docs = []for i, doc in enumerate(docs):    _id = doc_ids[i]    _sub_docs = child_text_splitter.split_documents([doc])    for _doc in _sub_docs:        _doc.metadata[id_key] = _id    sub_docs.extend(_sub_docs)\n```\n\nFinally, we index the documents in our vector store and document store:\n\n```\nretriever.vectorstore.add_documents(sub_docs)retriever.docstore.mset(list(zip(doc_ids, docs)))\n```\n\nThe vector store alone will retrieve small chunks:\n\n```\nretriever.vectorstore.similarity_search(\"justice breyer\")[0]\n```\n\n```\nDocument(page_content='Tonight, I‚Äôd like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer‚Äîan Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\n\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.', metadata={'doc_id': '064eca46-a4c4-4789-8e3b-583f9597e54f', 'source': 'state_of_the_union.txt'})\n```\n\nWhereas the retriever will return the larger parent document:\n\n```\nlen(retriever.invoke(\"justice breyer\")[0].page_content)\n```\n\n```\n9875\n```\n\nThe default search type the retriever performs on the vector database is a similarity search. LangChain vector stores also support searching via Max Marginal Relevance. This can be controlled via the search_type parameter of the retriever:\n\n```\nfrom langchain.retrievers.multi_vector import SearchTyperetriever.search_type = SearchType.mmrlen(retriever.invoke(\"justice breyer\")[0].page_content)\n```\n\n```\n9875\n```", "url_final": "https://python.langchain.com/docs/how_to/multi_vector/", "url_citable": "https://python.langchain.com/docs/how_to/multi_vector/#smaller-chunks", "title": "How to retrieve using multiple vectors per document | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Smaller chunks‚Äã", "section_anchor": "smaller-chunks", "fetched_at": "2025-10-06T21:55:56.212574", "has_code": true}
{"id": "f237ba3e5ec1e1f72d833808bca95a7a118a6228", "text": "Associating summaries with a document for retrieval‚Äã\n\nA summary may be able to distill more accurately what a chunk is about, leading to better retrieval. Here we show how to create summaries, and then embed those.\n\nWe construct a simple chain that will receive an input Document object and generate a summary using a LLM.\n\n```\npip install -qU \"langchain[google-genai]\"\n```\n\n```\nimport getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n```\n\n```\nimport uuidfrom langchain_core.documents import Documentfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatechain = (    {\"doc\": lambda x: x.page_content}    | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")    | llm    | StrOutputParser())\n```\n\nNote that we can batch the chain across documents:\n\n```\nsummaries = chain.batch(docs, {\"max_concurrency\": 5})\n```\n\nWe can then initialize a MultiVectorRetriever as before, indexing the summaries in our vector store, and retaining the original documents in our document store:\n\n```\n# The vectorstore to use to index the child chunksvectorstore = Chroma(collection_name=\"summaries\", embedding_function=OpenAIEmbeddings())# The storage layer for the parent documentsstore = InMemoryByteStore()id_key = \"doc_id\"# The retriever (empty to start)retriever = MultiVectorRetriever(    vectorstore=vectorstore,    byte_store=store,    id_key=id_key,)doc_ids = [str(uuid.uuid4()) for _ in docs]summary_docs = [    Document(page_content=s, metadata={id_key: doc_ids[i]})    for i, s in enumerate(summaries)]retriever.vectorstore.add_documents(summary_docs)retriever.docstore.mset(list(zip(doc_ids, docs)))\n```\n\n```\n# # We can also add the original chunks to the vectorstore if we so want# for i, doc in enumerate(docs):#     doc.metadata[id_key] = doc_ids[i]# retriever.vectorstore.add_documents(docs)\n```\n\nQuerying the vector store will return summaries:\n\n```\nsub_docs = retriever.vectorstore.similarity_search(\"justice breyer\")sub_docs[0]\n```\n\n```\nDocument(page_content=\"President Biden recently nominated Judge Ketanji Brown Jackson to serve on the United States Supreme Court, emphasizing her qualifications and broad support. The President also outlined a plan to secure the border, fix the immigration system, protect women's rights, support LGBTQ+ Americans, and advance mental health services. He highlighted the importance of bipartisan unity in passing legislation, such as the Violence Against Women Act. The President also addressed supporting veterans, particularly those impacted by exposure to burn pits, and announced plans to expand benefits for veterans with respiratory cancers. Additionally, he proposed a plan to end cancer as we know it through the Cancer Moonshot initiative. President Biden expressed optimism about the future of America and emphasized the strength of the American people in overcoming challenges.\", metadata={'doc_id': '84015b1b-980e-400a-94d8-cf95d7e079bd'})\n```\n\nWhereas the retriever will return the larger source document:\n\n```\nretrieved_docs = retriever.invoke(\"justice breyer\")len(retrieved_docs[0].page_content)\n```\n\n```\n9194\n```", "url_final": "https://python.langchain.com/docs/how_to/multi_vector/", "url_citable": "https://python.langchain.com/docs/how_to/multi_vector/#associating-summaries-with-a-document-for-retrieval", "title": "How to retrieve using multiple vectors per document | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Associating summaries with a document for retrieval‚Äã", "section_anchor": "associating-summaries-with-a-document-for-retrieval", "fetched_at": "2025-10-06T21:55:56.212574", "has_code": true}
{"id": "7c1e18b40ee78b0f4f1d2f6c43a29215924012d3", "text": "Hypothetical Queries‚Äã\n\nAn LLM can also be used to generate a list of hypothetical questions that could be asked of a particular document, which might bear close semantic similarity to relevant queries in a RAG application. These questions can then be embedded and associated with the documents to improve retrieval.\n\nBelow, we use the with_structured_output method to structure the LLM output into a list of strings.\n\n```\nfrom typing import Listfrom pydantic import BaseModel, Fieldclass HypotheticalQuestions(BaseModel):    \"\"\"Generate hypothetical questions.\"\"\"    questions: List[str] = Field(..., description=\"List of questions\")chain = (    {\"doc\": lambda x: x.page_content}    # Only asking for 3 hypothetical questions, but this could be adjusted    | ChatPromptTemplate.from_template(        \"Generate a list of exactly 3 hypothetical questions that the below document could be used to answer:\\n\\n{doc}\"    )    | ChatOpenAI(max_retries=0, model=\"gpt-4o\").with_structured_output(        HypotheticalQuestions    )    | (lambda x: x.questions))\n```\n\nInvoking the chain on a single document demonstrates that it outputs a list of questions:\n\n```\nchain.invoke(docs[0])\n```\n\n```\n[\"What impact did the IBM 1401 have on the author's early programming experiences?\", \"How did the transition from using the IBM 1401 to microcomputers influence the author's programming journey?\", \"What role did Lisp play in shaping the author's understanding and approach to AI?\"]\n```\n\nWe can batch then batch the chain over all documents and assemble our vector store and document store as before:\n\n```\n# Batch chain over documents to generate hypothetical questionshypothetical_questions = chain.batch(docs, {\"max_concurrency\": 5})# The vectorstore to use to index the child chunksvectorstore = Chroma(    collection_name=\"hypo-questions\", embedding_function=OpenAIEmbeddings())# The storage layer for the parent documentsstore = InMemoryByteStore()id_key = \"doc_id\"# The retriever (empty to start)retriever = MultiVectorRetriever(    vectorstore=vectorstore,    byte_store=store,    id_key=id_key,)doc_ids = [str(uuid.uuid4()) for _ in docs]# Generate Document objects from hypothetical questionsquestion_docs = []for i, question_list in enumerate(hypothetical_questions):    question_docs.extend(        [Document(page_content=s, metadata={id_key: doc_ids[i]}) for s in question_list]    )retriever.vectorstore.add_documents(question_docs)retriever.docstore.mset(list(zip(doc_ids, docs)))\n```\n\nNote that querying the underlying vector store will retrieve hypothetical questions that are semantically similar to the input query:\n\n```\nsub_docs = retriever.vectorstore.similarity_search(\"justice breyer\")sub_docs\n```\n\n```\n[Document(page_content='What might be the potential benefits of nominating Circuit Court of Appeals Judge Ketanji Brown Jackson to the United States Supreme Court?', metadata={'doc_id': '43292b74-d1b8-4200-8a8b-ea0cb57fbcdb'}), Document(page_content='How might the Bipartisan Infrastructure Law impact the economic competition between the U.S. and China?', metadata={'doc_id': '66174780-d00c-4166-9791-f0069846e734'}), Document(page_content='What factors led to the creation of Y Combinator?', metadata={'doc_id': '72003c4e-4cc9-4f09-a787-0b541a65b38c'}), Document(page_content='How did the ability to publish essays online change the landscape for writers and thinkers?', metadata={'doc_id': 'e8d2c648-f245-4bcc-b8d3-14e64a164b64'})]\n```\n\nAnd invoking the retriever will return the corresponding document:\n\n```\nretrieved_docs = retriever.invoke(\"justice breyer\")len(retrieved_docs[0].page_content)\n```\n\n```\n9194\n```\n\n- Smaller chunks\n- Associating summaries with a document for retrieval\n- Hypothetical Queries", "url_final": "https://python.langchain.com/docs/how_to/multi_vector/", "url_citable": "https://python.langchain.com/docs/how_to/multi_vector/#hypothetical-queries", "title": "How to retrieve using multiple vectors per document | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Hypothetical Queries‚Äã", "section_anchor": "hypothetical-queries", "fetched_at": "2025-10-06T21:55:56.212574", "has_code": true}
{"id": "f4552965523ac284129f34b08a4b23e3e5569840", "text": "Overview of the Splitters‚Äã\n\nHTMLHeaderTextSplitter‚Äã\n\nUseful when you want to preserve the hierarchical structure of a document based on its headings.\n\nDescription: Splits HTML text based on header tags (e.g., <h1>, <h2>, <h3>, etc.), and adds metadata for each header relevant to any given chunk.\n\nCapabilities:\n\n- Splits text at the HTML element level.\n- Preserves context-rich information encoded in document structures.\n- Can return chunks element by element or combine elements with the same metadata.\n\nHTMLSectionSplitter‚Äã\n\nUseful when you want to split HTML documents into larger sections, such as <section>, <div>, or custom-defined sections.\n\nDescription: Similar to HTMLHeaderTextSplitter but focuses on splitting HTML into sections based on specified tags.\n\nCapabilities:\n\n- Uses XSLT transformations to detect and split sections.\n- Internally uses RecursiveCharacterTextSplitter for large sections.\n- Considers font sizes to determine sections.\n\nHTMLSemanticPreservingSplitter‚Äã\n\nIdeal when you need to ensure that structured elements are not split across chunks, preserving contextual relevancy.\n\nDescription: Splits HTML content into manageable chunks while preserving the semantic structure of important elements like tables, lists, and other HTML components.\n\nCapabilities:\n\n- Preserves tables, lists, and other specified HTML elements.\n- Allows custom handlers for specific HTML tags.\n- Ensures that the semantic meaning of the document is maintained.\n- Built in normalization & stopword removal\n\nChoosing the Right Splitter‚Äã\n\n- Use HTMLHeaderTextSplitter when: You need to split an HTML document based on its header hierarchy and maintain metadata about the headers.\n- Use HTMLSectionSplitter when: You need to split the document into larger, more general sections, possibly based on custom tags or font sizes.\n- Use HTMLSemanticPreservingSplitter when: You need to split the document into chunks while preserving semantic elements like tables and lists, ensuring that they are not split and that their context is maintained.", "url_final": "https://python.langchain.com/docs/how_to/split_html/", "url_citable": "https://python.langchain.com/docs/how_to/split_html/#overview-of-the-splitters", "title": "How to split HTML | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Overview of the Splitters‚Äã", "section_anchor": "overview-of-the-splitters", "fetched_at": "2025-10-06T21:56:23.329565", "has_code": false}
{"id": "e7c3b174575bea7487c80afe7079bf28c225f1f5", "text": "Example HTML Document‚Äã\n\nLet's use the following HTML document as an example:\n\n```\nhtml_string = \"\"\"<!DOCTYPE html>  <html lang='en'>  <head>    <meta charset='UTF-8'>    <meta name='viewport' content='width=device-width, initial-scale=1.0'>    <title>Fancy Example HTML Page</title>  </head>  <body>    <h1>Main Title</h1>    <p>This is an introductory paragraph with some basic content.</p>        <h2>Section 1: Introduction</h2>    <p>This section introduces the topic. Below is a list:</p>    <ul>      <li>First item</li>      <li>Second item</li>      <li>Third item with <strong>bold text</strong> and <a href='#'>a link</a></li>    </ul>        <h3>Subsection 1.1: Details</h3>    <p>This subsection provides additional details. Here's a table:</p>    <table border='1'>      <thead>        <tr>          <th>Header 1</th>          <th>Header 2</th>          <th>Header 3</th>        </tr>      </thead>      <tbody>        <tr>          <td>Row 1, Cell 1</td>          <td>Row 1, Cell 2</td>          <td>Row 1, Cell 3</td>        </tr>        <tr>          <td>Row 2, Cell 1</td>          <td>Row 2, Cell 2</td>          <td>Row 2, Cell 3</td>        </tr>      </tbody>    </table>        <h2>Section 2: Media Content</h2>    <p>This section contains an image and a video:</p>      <img src='example_image_link.mp4' alt='Example Image'>      <video controls width='250' src='example_video_link.mp4' type='video/mp4'>      Your browser does not support the video tag.    </video>    <h2>Section 3: Code Example</h2>    <p>This section contains a code block:</p>    <pre><code data-lang=\"html\">    &lt;div&gt;      &lt;p&gt;This is a paragraph inside a div.&lt;/p&gt;    &lt;/div&gt;    </code></pre>    <h2>Conclusion</h2>    <p>This is the conclusion of the document.</p>  </body>  </html>\"\"\"\n```", "url_final": "https://python.langchain.com/docs/how_to/split_html/", "url_citable": "https://python.langchain.com/docs/how_to/split_html/#example-html-document", "title": "How to split HTML | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Example HTML Document‚Äã", "section_anchor": "example-html-document", "fetched_at": "2025-10-06T21:56:23.329565", "has_code": true}
{"id": "174ee74c382ff92b724c73858024864e53b6f1de", "text": "Using HTMLHeaderTextSplitter‚Äã\n\nHTMLHeaderTextSplitter is a \"structure-aware\" text splitter that splits text at the HTML element level and adds metadata for each header \"relevant\" to any given chunk. It can return chunks element by element or combine elements with the same metadata, with the objectives of (a) keeping related text grouped (more or less) semantically and (b) preserving context-rich information encoded in document structures. It can be used with other text splitters as part of a chunking pipeline.\n\nIt is analogous to the MarkdownHeaderTextSplitter for markdown files.\n\nTo specify what headers to split on, specify headers_to_split_on when instantiating HTMLHeaderTextSplitter as shown below.\n\n```\nfrom langchain_text_splitters import HTMLHeaderTextSplitterheaders_to_split_on = [    (\"h1\", \"Header 1\"),    (\"h2\", \"Header 2\"),    (\"h3\", \"Header 3\"),]html_splitter = HTMLHeaderTextSplitter(headers_to_split_on)html_header_splits = html_splitter.split_text(html_string)html_header_splits\n```\n\n```\n[Document(metadata={'Header 1': 'Main Title'}, page_content='This is an introductory paragraph with some basic content.'), Document(metadata={'Header 1': 'Main Title', 'Header 2': 'Section 1: Introduction'}, page_content='This section introduces the topic. Below is a list:  \\nFirst item Second item Third item with bold text and a link'), Document(metadata={'Header 1': 'Main Title', 'Header 2': 'Section 1: Introduction', 'Header 3': 'Subsection 1.1: Details'}, page_content=\"This subsection provides additional details. Here's a table:\"), Document(metadata={'Header 1': 'Main Title', 'Header 2': 'Section 2: Media Content'}, page_content='This section contains an image and a video:'), Document(metadata={'Header 1': 'Main Title', 'Header 2': 'Section 3: Code Example'}, page_content='This section contains a code block:'), Document(metadata={'Header 1': 'Main Title', 'Header 2': 'Conclusion'}, page_content='This is the conclusion of the document.')]\n```\n\nTo return each element together with their associated headers, specify return_each_element=True when instantiating HTMLHeaderTextSplitter:\n\n```\nhtml_splitter = HTMLHeaderTextSplitter(    headers_to_split_on,    return_each_element=True,)html_header_splits_elements = html_splitter.split_text(html_string)\n```\n\nComparing with the above, where elements are aggregated by their headers:\n\n```\nfor element in html_header_splits[:2]:    print(element)\n```\n\n```\npage_content='This is an introductory paragraph with some basic content.' metadata={'Header 1': 'Main Title'}page_content='This section introduces the topic. Below is a list:  First item Second item Third item with bold text and a link' metadata={'Header 1': 'Main Title', 'Header 2': 'Section 1: Introduction'}\n```\n\nNow each element is returned as a distinct Document:\n\n```\nfor element in html_header_splits_elements[:3]:    print(element)\n```\n\n```\npage_content='This is an introductory paragraph with some basic content.' metadata={'Header 1': 'Main Title'}page_content='This section introduces the topic. Below is a list:' metadata={'Header 1': 'Main Title', 'Header 2': 'Section 1: Introduction'}page_content='First item Second item Third item with bold text and a link' metadata={'Header 1': 'Main Title', 'Header 2': 'Section 1: Introduction'}\n```\n\nHow to split from a URL or HTML file:‚Äã\n\nTo read directly from a URL, pass the URL string into the split_text_from_url method.\n\nSimilarly, a local HTML file can be passed to the split_text_from_file method.\n\n```\nurl = \"https://plato.stanford.edu/entries/goedel/\"headers_to_split_on = [    (\"h1\", \"Header 1\"),    (\"h2\", \"Header 2\"),    (\"h3\", \"Header 3\"),    (\"h4\", \"Header 4\"),]html_splitter = HTMLHeaderTextSplitter(headers_to_split_on)# for local file use html_splitter.split_text_from_file(<path_to_file>)html_header_splits = html_splitter.split_text_from_url(url)\n```", "url_final": "https://python.langchain.com/docs/how_to/split_html/", "url_citable": "https://python.langchain.com/docs/how_to/split_html/#using-htmlheadertextsplitter", "title": "How to split HTML | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Using HTMLHeaderTextSplitter‚Äã", "section_anchor": "using-htmlheadertextsplitter", "fetched_at": "2025-10-06T21:56:23.329565", "has_code": true}
{"id": "4940b9209563814b1dff66bd92c14bbc3a1a5073", "text": "How to constrain chunk sizes:‚Äã\n\nHTMLHeaderTextSplitter, which splits based on HTML headers, can be composed with another splitter which constrains splits based on character lengths, such as RecursiveCharacterTextSplitter.\n\nThis can be done using the .split_documents method of the second splitter:\n\n```\nfrom langchain_text_splitters import RecursiveCharacterTextSplitterchunk_size = 500chunk_overlap = 30text_splitter = RecursiveCharacterTextSplitter(    chunk_size=chunk_size, chunk_overlap=chunk_overlap)# Splitsplits = text_splitter.split_documents(html_header_splits)splits[80:85]\n```\n\n```\n[Document(metadata={'Header 1': 'Kurt G√∂del', 'Header 2': '2. G√∂del‚Äôs Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}, page_content='We see that G√∂del first tried to reduce the consistency problem for analysis to that of arithmetic. This seemed to require a truth definition for arithmetic, which in turn led to paradoxes, such as the Liar paradox (‚ÄúThis sentence is false‚Äù) and Berry‚Äôs paradox (‚ÄúThe least number not defined by an expression consisting of just fourteen English words‚Äù). G√∂del then noticed that such paradoxes would not necessarily arise if truth were replaced by provability. But this means that arithmetic truth'), Document(metadata={'Header 1': 'Kurt G√∂del', 'Header 2': '2. G√∂del‚Äôs Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}, page_content='means that arithmetic truth and arithmetic provability are not co-extensive ‚Äî whence the First Incompleteness Theorem.'), Document(metadata={'Header 1': 'Kurt G√∂del', 'Header 2': '2. G√∂del‚Äôs Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}, page_content='This account of G√∂del‚Äôs discovery was told to Hao Wang very much after the fact; but in G√∂del‚Äôs contemporary correspondence with Bernays and Zermelo, essentially the same description of his path to the theorems is given. (See G√∂del 2003a and G√∂del 2003b respectively.) From those accounts we see that the undefinability of truth in arithmetic, a result credited to Tarski, was likely obtained in some form by G√∂del by 1931. But he neither publicized nor published the result; the biases logicians'), Document(metadata={'Header 1': 'Kurt G√∂del', 'Header 2': '2. G√∂del‚Äôs Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}, page_content='result; the biases logicians had expressed at the time concerning the notion of truth, biases which came vehemently to the fore when Tarski announced his results on the undefinability of truth in formal systems 1935, may have served as a deterrent to G√∂del‚Äôs publication of that theorem.'), Document(metadata={'Header 1': 'Kurt G√∂del', 'Header 2': '2. G√∂del‚Äôs Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.2 The proof of the First Incompleteness Theorem'}, page_content='We now describe the proof of the two theorems, formulating G√∂del‚Äôs results in Peano arithmetic. G√∂del himself used a system related to that defined in Principia Mathematica, but containing Peano arithmetic. In our presentation of the First and Second Incompleteness Theorems we refer to Peano arithmetic as P, following G√∂del‚Äôs notation.')]\n```", "url_final": "https://python.langchain.com/docs/how_to/split_html/", "url_citable": "https://python.langchain.com/docs/how_to/split_html/#how-to-constrain-chunk-sizes", "title": "How to split HTML | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "How to constrain chunk sizes:‚Äã", "section_anchor": "how-to-constrain-chunk-sizes", "fetched_at": "2025-10-06T21:56:23.329565", "has_code": true}
{"id": "a8468576741d4fa9a4d861e50b33690f71a324e2", "text": "Limitations‚Äã\n\nThere can be quite a bit of structural variation from one HTML document to another, and while HTMLHeaderTextSplitter will attempt to attach all \"relevant\" headers to any given chunk, it can sometimes miss certain headers. For example, the algorithm assumes an informational hierarchy in which headers are always at nodes \"above\" associated text, i.e. prior siblings, ancestors, and combinations thereof. In the following news article (as of the writing of this document), the document is structured such that the text of the top-level headline, while tagged \"h1\", is in a distinct subtree from the text elements that we'd expect it to be \"above\"‚Äîso we can observe that the \"h1\" element and its associated text do not show up in the chunk metadata (but, where applicable, we do see \"h2\" and its associated text):\n\n```\nurl = \"https://www.cnn.com/2023/09/25/weather/el-nino-winter-us-climate/index.html\"headers_to_split_on = [    (\"h1\", \"Header 1\"),    (\"h2\", \"Header 2\"),]html_splitter = HTMLHeaderTextSplitter(headers_to_split_on)html_header_splits = html_splitter.split_text_from_url(url)print(html_header_splits[1].page_content[:500])\n```\n\n```\nNo two El Ni√±o winters are the same, but many have temperature and precipitation trends in common.  Average conditions during an El Ni√±o winter across the continental US.  One of the major reasons is the position of the jet stream, which often shifts south during an El Ni√±o winter. This shift typically brings wetter and cooler weather to the South while the North becomes drier and warmer, according to NOAA.  Because the jet stream is essentially a river of air that storms flow through, they c\n```", "url_final": "https://python.langchain.com/docs/how_to/split_html/", "url_citable": "https://python.langchain.com/docs/how_to/split_html/#limitations", "title": "How to split HTML | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Limitations‚Äã", "section_anchor": "limitations", "fetched_at": "2025-10-06T21:56:23.329565", "has_code": true}
{"id": "e51996b2f6e0470fab16bd8548e6555abfcb7114", "text": "Using HTMLSectionSplitter‚Äã\n\nSimilar in concept to the HTMLHeaderTextSplitter, the HTMLSectionSplitter is a \"structure-aware\" text splitter that splits text at the element level and adds metadata for each header \"relevant\" to any given chunk. It lets you split HTML by sections.\n\nIt can return chunks element by element or combine elements with the same metadata, with the objectives of (a) keeping related text grouped (more or less) semantically and (b) preserving context-rich information encoded in document structures.\n\nUse xslt_path to provide an absolute path to transform the HTML so that it can detect sections based on provided tags. The default is to use the converting_to_header.xslt file in the data_connection/document_transformers directory. This is for converting the html to a format/layout that is easier to detect sections. For example, span based on their font size can be converted to header tags to be detected as a section.\n\nHow to split HTML strings:‚Äã\n\n```\nfrom langchain_text_splitters import HTMLSectionSplitterheaders_to_split_on = [    (\"h1\", \"Header 1\"),    (\"h2\", \"Header 2\"),]html_splitter = HTMLSectionSplitter(headers_to_split_on)html_header_splits = html_splitter.split_text(html_string)html_header_splits\n```\n\n```\n[Document(metadata={'Header 1': 'Main Title'}, page_content='Main Title \\n This is an introductory paragraph with some basic content.'), Document(metadata={'Header 2': 'Section 1: Introduction'}, page_content=\"Section 1: Introduction \\n This section introduces the topic. Below is a list: \\n \\n First item \\n Second item \\n Third item with  bold text  and  a link \\n \\n \\n Subsection 1.1: Details \\n This subsection provides additional details. Here's a table: \\n \\n \\n \\n Header 1 \\n Header 2 \\n Header 3 \\n \\n \\n \\n \\n Row 1, Cell 1 \\n Row 1, Cell 2 \\n Row 1, Cell 3 \\n \\n \\n Row 2, Cell 1 \\n Row 2, Cell 2 \\n Row 2, Cell 3\"), Document(metadata={'Header 2': 'Section 2: Media Content'}, page_content='Section 2: Media Content \\n This section contains an image and a video: \\n \\n \\n      Your browser does not support the video tag.'), Document(metadata={'Header 2': 'Section 3: Code Example'}, page_content='Section 3: Code Example \\n This section contains a code block: \\n \\n    <div>\\n      <p>This is a paragraph inside a div.</p>\\n    </div>'), Document(metadata={'Header 2': 'Conclusion'}, page_content='Conclusion \\n This is the conclusion of the document.')]\n```", "url_final": "https://python.langchain.com/docs/how_to/split_html/", "url_citable": "https://python.langchain.com/docs/how_to/split_html/#using-htmlsectionsplitter", "title": "How to split HTML | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Using HTMLSectionSplitter‚Äã", "section_anchor": "using-htmlsectionsplitter", "fetched_at": "2025-10-06T21:56:23.329565", "has_code": true}
{"id": "5b746406d6409465d0850301d18d8603765c6cf4", "text": "How to constrain chunk sizes:‚Äã\n\nHTMLSectionSplitter can be used with other text splitters as part of a chunking pipeline. Internally, it uses the RecursiveCharacterTextSplitter when the section size is larger than the chunk size. It also considers the font size of the text to determine whether it is a section or not based on the determined font size threshold.\n\n```\nfrom langchain_text_splitters import RecursiveCharacterTextSplitterheaders_to_split_on = [    (\"h1\", \"Header 1\"),    (\"h2\", \"Header 2\"),    (\"h3\", \"Header 3\"),]html_splitter = HTMLSectionSplitter(headers_to_split_on)html_header_splits = html_splitter.split_text(html_string)chunk_size = 50chunk_overlap = 5text_splitter = RecursiveCharacterTextSplitter(    chunk_size=chunk_size, chunk_overlap=chunk_overlap)# Splitsplits = text_splitter.split_documents(html_header_splits)splits\n```\n\n```\n[Document(metadata={'Header 1': 'Main Title'}, page_content='Main Title'), Document(metadata={'Header 1': 'Main Title'}, page_content='This is an introductory paragraph with some'), Document(metadata={'Header 1': 'Main Title'}, page_content='some basic content.'), Document(metadata={'Header 2': 'Section 1: Introduction'}, page_content='Section 1: Introduction'), Document(metadata={'Header 2': 'Section 1: Introduction'}, page_content='This section introduces the topic. Below is a'), Document(metadata={'Header 2': 'Section 1: Introduction'}, page_content='is a list:'), Document(metadata={'Header 2': 'Section 1: Introduction'}, page_content='First item \\n Second item'), Document(metadata={'Header 2': 'Section 1: Introduction'}, page_content='Third item with  bold text  and  a link'), Document(metadata={'Header 3': 'Subsection 1.1: Details'}, page_content='Subsection 1.1: Details'), Document(metadata={'Header 3': 'Subsection 1.1: Details'}, page_content='This subsection provides additional details.'), Document(metadata={'Header 3': 'Subsection 1.1: Details'}, page_content=\"Here's a table:\"), Document(metadata={'Header 3': 'Subsection 1.1: Details'}, page_content='Header 1 \\n Header 2 \\n Header 3'), Document(metadata={'Header 3': 'Subsection 1.1: Details'}, page_content='Row 1, Cell 1 \\n Row 1, Cell 2'), Document(metadata={'Header 3': 'Subsection 1.1: Details'}, page_content='Row 1, Cell 3 \\n \\n \\n Row 2, Cell 1'), Document(metadata={'Header 3': 'Subsection 1.1: Details'}, page_content='Row 2, Cell 2 \\n Row 2, Cell 3'), Document(metadata={'Header 2': 'Section 2: Media Content'}, page_content='Section 2: Media Content'), Document(metadata={'Header 2': 'Section 2: Media Content'}, page_content='This section contains an image and a video:'), Document(metadata={'Header 2': 'Section 2: Media Content'}, page_content='Your browser does not support the video'), Document(metadata={'Header 2': 'Section 2: Media Content'}, page_content='tag.'), Document(metadata={'Header 2': 'Section 3: Code Example'}, page_content='Section 3: Code Example'), Document(metadata={'Header 2': 'Section 3: Code Example'}, page_content='This section contains a code block: \\n \\n    <div>'), Document(metadata={'Header 2': 'Section 3: Code Example'}, page_content='<p>This is a paragraph inside a div.</p>'), Document(metadata={'Header 2': 'Section 3: Code Example'}, page_content='</div>'), Document(metadata={'Header 2': 'Conclusion'}, page_content='Conclusion'), Document(metadata={'Header 2': 'Conclusion'}, page_content='This is the conclusion of the document.')]\n```", "url_final": "https://python.langchain.com/docs/how_to/split_html/", "url_citable": "https://python.langchain.com/docs/how_to/split_html/#how-to-constrain-chunk-sizes-1", "title": "How to split HTML | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "How to constrain chunk sizes:‚Äã", "section_anchor": "how-to-constrain-chunk-sizes-1", "fetched_at": "2025-10-06T21:56:23.329565", "has_code": true}
{"id": "c6056f4426d902476d1144e53cc9bd626b08c6dc", "text": "Using HTMLSemanticPreservingSplitter‚Äã\n\nThe HTMLSemanticPreservingSplitter is designed to split HTML content into manageable chunks while preserving the semantic structure of important elements like tables, lists, and other HTML components. This ensures that such elements are not split across chunks, causing loss of contextual relevancy such as table headers, list headers etc.\n\nThis splitter is designed at its heart, to create contextually relevant chunks. General Recursive splitting with HTMLHeaderTextSplitter can cause tables, lists and other structured elements to be split in the middle, losing significant context and creating bad chunks.\n\nThe HTMLSemanticPreservingSplitter is essential for splitting HTML content that includes structured elements like tables and lists, especially when it's critical to preserve these elements intact. Additionally, its ability to define custom handlers for specific HTML tags makes it a versatile tool for processing complex HTML documents.\n\nIMPORTANT: max_chunk_size is not a definite maximum size of a chunk, the calculation of max size, occurs when the preserved content is not apart of the chunk, to ensure it is not split. When we add the preserved data back in to the chunk, there is a chance the chunk size will exceed the max_chunk_size. This is crucial to ensure we maintain the structure of the original document\n\nNotes:\n\n- We have defined a custom handler to re-format the contents of code blocks\n- We defined a deny list for specific html elements, to decompose them and their contents pre-processing\n- We have intentionally set a small chunk size to demonstrate the non-splitting of elements\n\n```\n# BeautifulSoup is required to use the custom handlersfrom bs4 import Tagfrom langchain_text_splitters import HTMLSemanticPreservingSplitterheaders_to_split_on = [    (\"h1\", \"Header 1\"),    (\"h2\", \"Header 2\"),]def code_handler(element: Tag) -> str:    data_lang = element.get(\"data-lang\")    code_format = f\"<code:{data_lang}>{element.get_text()}</code>\"    return code_formatsplitter = HTMLSemanticPreservingSplitter(    headers_to_split_on=headers_to_split_on,    separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \"],    max_chunk_size=50,    preserve_images=True,    preserve_videos=True,    elements_to_preserve=[\"table\", \"ul\", \"ol\", \"code\"],    denylist_tags=[\"script\", \"style\", \"head\"],    custom_handlers={\"code\": code_handler},)documents = splitter.split_text(html_string)documents\n```\n\n```\n[Document(metadata={'Header 1': 'Main Title'}, page_content='This is an introductory paragraph with some basic content.'), Document(metadata={'Header 2': 'Section 1: Introduction'}, page_content='This section introduces the topic'), Document(metadata={'Header 2': 'Section 1: Introduction'}, page_content='. Below is a list: First item Second item Third item with bold text and a link Subsection 1.1: Details This subsection provides additional details'), Document(metadata={'Header 2': 'Section 1: Introduction'}, page_content=\". Here's a table: Header 1 Header 2 Header 3 Row 1, Cell 1 Row 1, Cell 2 Row 1, Cell 3 Row 2, Cell 1 Row 2, Cell 2 Row 2, Cell 3\"), Document(metadata={'Header 2': 'Section 2: Media Content'}, page_content='This section contains an image and a video: ![image:example_image_link.mp4](example_image_link.mp4) ![video:example_video_link.mp4](example_video_link.mp4)'), Document(metadata={'Header 2': 'Section 3: Code Example'}, page_content='This section contains a code block: <code:html> <div> <p>This is a paragraph inside a div.</p> </div> </code>'), Document(metadata={'Header 2': 'Conclusion'}, page_content='This is the conclusion of the document.')]\n```", "url_final": "https://python.langchain.com/docs/how_to/split_html/", "url_citable": "https://python.langchain.com/docs/how_to/split_html/#using-htmlsemanticpreservingsplitter", "title": "How to split HTML | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Using HTMLSemanticPreservingSplitter‚Äã", "section_anchor": "using-htmlsemanticpreservingsplitter", "fetched_at": "2025-10-06T21:56:23.329565", "has_code": true}
{"id": "7e8cbbdb2df3a8dde7ebd3485933da54a2fe5497", "text": "Preserving Tables and Lists‚Äã\n\nIn this example, we will demonstrate how the HTMLSemanticPreservingSplitter can preserve a table and a large list within an HTML document. The chunk size will be set to 50 characters to illustrate how the splitter ensures that these elements are not split, even when they exceed the maximum defined chunk size.\n\n```\nfrom langchain_text_splitters import HTMLSemanticPreservingSplitterhtml_string = \"\"\"<!DOCTYPE html><html>    <body>        <div>            <h1>Section 1</h1>            <p>This section contains an important table and list that should not be split across chunks.</p>            <table>                <tr>                    <th>Item</th>                    <th>Quantity</th>                    <th>Price</th>                </tr>                <tr>                    <td>Apples</td>                    <td>10</td>                    <td>$1.00</td>                </tr>                <tr>                    <td>Oranges</td>                    <td>5</td>                    <td>$0.50</td>                </tr>                <tr>                    <td>Bananas</td>                    <td>50</td>                    <td>$1.50</td>                </tr>            </table>            <h2>Subsection 1.1</h2>            <p>Additional text in subsection 1.1 that is separated from the table and list.</p>            <p>Here is a detailed list:</p>            <ul>                <li>Item 1: Description of item 1, which is quite detailed and important.</li>                <li>Item 2: Description of item 2, which also contains significant information.</li>                <li>Item 3: Description of item 3, another item that we don't want to split across chunks.</li>            </ul>        </div>    </body></html>\"\"\"headers_to_split_on = [(\"h1\", \"Header 1\"), (\"h2\", \"Header 2\")]splitter = HTMLSemanticPreservingSplitter(    headers_to_split_on=headers_to_split_on,    max_chunk_size=50,    elements_to_preserve=[\"table\", \"ul\"],)documents = splitter.split_text(html_string)print(documents)\n```\n\n```\n[Document(metadata={'Header 1': 'Section 1'}, page_content='This section contains an important table and list'), Document(metadata={'Header 1': 'Section 1'}, page_content='that should not be split across chunks.'), Document(metadata={'Header 1': 'Section 1'}, page_content='Item Quantity Price Apples 10 $1.00 Oranges 5 $0.50 Bananas 50 $1.50'), Document(metadata={'Header 2': 'Subsection 1.1'}, page_content='Additional text in subsection 1.1 that is'), Document(metadata={'Header 2': 'Subsection 1.1'}, page_content='separated from the table and list. Here is a'), Document(metadata={'Header 2': 'Subsection 1.1'}, page_content=\"detailed list: Item 1: Description of item 1, which is quite detailed and important. Item 2: Description of item 2, which also contains significant information. Item 3: Description of item 3, another item that we don't want to split across chunks.\")]\n```\n\nIn this example, the HTMLSemanticPreservingSplitter ensures that the entire table and the unordered list (<ul>) are preserved within their respective chunks. Even though the chunk size is set to 50 characters, the splitter recognizes that these elements should not be split and keeps them intact.\n\nThis is particularly important when dealing with data tables or lists, where splitting the content could lead to loss of context or confusion. The resulting Document objects retain the full structure of these elements, ensuring that the contextual relevance of the information is maintained.", "url_final": "https://python.langchain.com/docs/how_to/split_html/", "url_citable": "https://python.langchain.com/docs/how_to/split_html/#preserving-tables-and-lists", "title": "How to split HTML | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Preserving Tables and Lists‚Äã", "section_anchor": "preserving-tables-and-lists", "fetched_at": "2025-10-06T21:56:23.329565", "has_code": true}
{"id": "d6162eefffcb450aec2c321af05478ad44916fbe", "text": "Using a Custom Handler‚Äã\n\nThe HTMLSemanticPreservingSplitter allows you to define custom handlers for specific HTML elements. Some platforms, have custom HTML tags that are not natively parsed by BeautifulSoup, when this occurs, you can utilize custom handlers to add the formatting logic easily.\n\nThis can be particularly useful for elements that require special processing, such as <iframe> tags or specific 'data-' elements. In this example, we'll create a custom handler for iframe tags that converts them into Markdown-like links.\n\n```\ndef custom_iframe_extractor(iframe_tag):    iframe_src = iframe_tag.get(\"src\", \"\")    return f\"[iframe:{iframe_src}]({iframe_src})\"splitter = HTMLSemanticPreservingSplitter(    headers_to_split_on=headers_to_split_on,    max_chunk_size=50,    separators=[\"\\n\\n\", \"\\n\", \". \"],    elements_to_preserve=[\"table\", \"ul\", \"ol\"],    custom_handlers={\"iframe\": custom_iframe_extractor},)html_string = \"\"\"<!DOCTYPE html><html>    <body>        <div>            <h1>Section with Iframe</h1>            <iframe src=\"https://example.com/embed\"></iframe>            <p>Some text after the iframe.</p>            <ul>                <li>Item 1: Description of item 1, which is quite detailed and important.</li>                <li>Item 2: Description of item 2, which also contains significant information.</li>                <li>Item 3: Description of item 3, another item that we don't want to split across chunks.</li>            </ul>        </div>    </body></html>\"\"\"documents = splitter.split_text(html_string)print(documents)\n```\n\n```\n[Document(metadata={'Header 1': 'Section with Iframe'}, page_content='[iframe:https://example.com/embed](https://example.com/embed) Some text after the iframe'), Document(metadata={'Header 1': 'Section with Iframe'}, page_content=\". Item 1: Description of item 1, which is quite detailed and important. Item 2: Description of item 2, which also contains significant information. Item 3: Description of item 3, another item that we don't want to split across chunks.\")]\n```\n\nIn this example, we defined a custom handler for iframe tags that converts them into Markdown-like links. When the splitter processes the HTML content, it uses this custom handler to transform the iframe tags while preserving other elements like tables and lists. The resulting Document objects show how the iframe is handled according to the custom logic you provided.\n\nImportant: When presvering items such as links, you should be mindful not to include . in your separators, or leave separators blank. RecursiveCharacterTextSplitter splits on full stop, which will cut links in half. Ensure you provide a separator list with . instead.", "url_final": "https://python.langchain.com/docs/how_to/split_html/", "url_citable": "https://python.langchain.com/docs/how_to/split_html/#using-a-custom-handler", "title": "How to split HTML | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Using a Custom Handler‚Äã", "section_anchor": "using-a-custom-handler", "fetched_at": "2025-10-06T21:56:23.329565", "has_code": true}
{"id": "928a3a5faffdedb097bba32f460857d075150b55", "text": "Using a custom handler to analyze an image with an LLM‚Äã\n\nWith custom handler's, we can also override the default processing for any element. A great example of this, is inserting semantic analysis of an image within a document, directly in the chunking flow.\n\nSince our function is called when the tag is discovered, we can override the <img> tag and turn off preserve_images to insert any content we would like to embed in our chunks.\n\n```\n\"\"\"This example assumes you have helper methods `load_image_from_url` and an LLM agent `llm` that can process image data.\"\"\"from langchain.agents import AgentExecutor# This example needs to be replaced with your own agentllm = AgentExecutor(...)# This method is a placeholder for loading image data from a URL and is not implemented heredef load_image_from_url(image_url: str) -> bytes:    # Assuming this method fetches the image data from the URL    return b\"image_data\"html_string = \"\"\"<!DOCTYPE html><html>    <body>        <div>            <h1>Section with Image and Link</h1>            <p>                <img src=\"https://example.com/image.jpg\" alt=\"An example image\" />                Some text after the image.            </p>            <ul>                <li>Item 1: Description of item 1, which is quite detailed and important.</li>                <li>Item 2: Description of item 2, which also contains significant information.</li>                <li>Item 3: Description of item 3, another item that we don't want to split across chunks.</li>            </ul>        </div>    </body></html>\"\"\"def custom_image_handler(img_tag) -> str:    img_src = img_tag.get(\"src\", \"\")    img_alt = img_tag.get(\"alt\", \"No alt text provided\")    image_data = load_image_from_url(img_src)    semantic_meaning = llm.invoke(image_data)    markdown_text = f\"[Image Alt Text: {img_alt} | Image Source: {img_src} | Image Semantic Meaning: {semantic_meaning}]\"    return markdown_textsplitter = HTMLSemanticPreservingSplitter(    headers_to_split_on=headers_to_split_on,    max_chunk_size=50,    separators=[\"\\n\\n\", \"\\n\", \". \"],    elements_to_preserve=[\"ul\"],    preserve_images=False,    custom_handlers={\"img\": custom_image_handler},)documents = splitter.split_text(html_string)print(documents)\n```\n\n```\n[Document(metadata={'Header 1': 'Section with Image and Link'}, page_content='[Image Alt Text: An example image | Image Source: https://example.com/image.jpg | Image Semantic Meaning: semantic-meaning] Some text after the image'), Document(metadata={'Header 1': 'Section with Image and Link'}, page_content=\". Item 1: Description of item 1, which is quite detailed and important. Item 2: Description of item 2, which also contains significant information. Item 3: Description of item 3, another item that we don't want to split across chunks.\")]\n```\n\nWith our custom handler written to extract the specific fields from a <img> element in HTML, we can further process the data with our agent, and insert the result directly into our chunk. It is important to ensure preserve_images is set to False otherwise the default processing of <img> fields will take place.\n\n- HTMLHeaderTextSplitter\n- HTMLSectionSplitter\n- HTMLSemanticPreservingSplitter\n- Choosing the Right Splitter\n\n- How to split from a URL or HTML file:\n- How to constrain chunk sizes:\n- Limitations\n\n- How to split HTML strings:\n- How to constrain chunk sizes:\n\n- Preserving Tables and Lists\n- Using a Custom Handler\n- Using a custom handler to analyze an image with an LLM", "url_final": "https://python.langchain.com/docs/how_to/split_html/", "url_citable": "https://python.langchain.com/docs/how_to/split_html/#using-a-custom-handler-to-analyze-an-image-with-an-llm", "title": "How to split HTML | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Using a custom handler to analyze an image with an LLM‚Äã", "section_anchor": "using-a-custom-handler-to-analyze-an-image-with-an-llm", "fetched_at": "2025-10-06T21:56:23.329565", "has_code": true}
{"id": "c667ffa3447cca2b6764b431923544edfdd39e6c", "text": "The .with_structured_output() method‚Äã\n\nYou can find a list of models that support this method here.\n\nThis is the easiest and most reliable way to get structured outputs. with_structured_output() is implemented for models that provide native APIs for structuring outputs, like tool/function calling or JSON mode, and makes use of these capabilities under the hood.\n\nThis method takes a schema as input which specifies the names, types, and descriptions of the desired output attributes. The method returns a model-like Runnable, except that instead of outputting strings or messages it outputs objects corresponding to the given schema. The schema can be specified as a TypedDict class, JSON Schema or a Pydantic class. If TypedDict or JSON Schema are used then a dictionary will be returned by the Runnable, and if a Pydantic class is used then a Pydantic object will be returned.\n\nAs an example, let's get a model to generate a joke and separate the setup from the punchline:\n\n```\npip install -qU \"langchain[google-genai]\"\n```\n\n```\nimport getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n```\n\nPydantic class‚Äã\n\nIf we want the model to return a Pydantic object, we just need to pass in the desired Pydantic class. The key advantage of using Pydantic is that the model-generated output will be validated. Pydantic will raise an error if any required fields are missing or if any fields are of the wrong type.\n\n```\nfrom typing import Optionalfrom pydantic import BaseModel, Field# Pydanticclass Joke(BaseModel):    \"\"\"Joke to tell user.\"\"\"    setup: str = Field(description=\"The setup of the joke\")    punchline: str = Field(description=\"The punchline to the joke\")    rating: Optional[int] = Field(        default=None, description=\"How funny the joke is, from 1 to 10\"    )structured_llm = llm.with_structured_output(Joke)structured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```\nJoke(setup='Why was the cat sitting on the computer?', punchline='Because it wanted to keep an eye on the mouse!', rating=7)\n```\n\nBeyond just the structure of the Pydantic class, the name of the Pydantic class, the docstring, and the names and provided descriptions of parameters are very important. Most of the time with_structured_output is using a model's function/tool calling API, and you can effectively think of all of this information as being added to the model prompt.", "url_final": "https://python.langchain.com/docs/how_to/structured_output/", "url_citable": "https://python.langchain.com/docs/how_to/structured_output/#the-with_structured_output-method", "title": "How to return structured data from a model | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "The .with_structured_output() method‚Äã", "section_anchor": "the-with_structured_output-method", "fetched_at": "2025-10-06T21:54:35.265803", "has_code": true}
{"id": "b926cff0badd37253a2062c8a233df61e1e06035", "text": "TypedDict or JSON Schema‚Äã\n\nIf you don't want to use Pydantic, explicitly don't want validation of the arguments, or want to be able to stream the model outputs, you can define your schema using a TypedDict class. We can optionally use a special Annotated syntax supported by LangChain that allows you to specify the default value and description of a field. Note, the default value is not filled in automatically if the model doesn't generate it, it is only used in defining the schema that is passed to the model.\n\n- Core: langchain-core>=0.2.26\n- Typing extensions: It is highly recommended to import Annotated and TypedDict from typing_extensions instead of typing to ensure consistent behavior across Python versions.\n\n```\nfrom typing import Optionalfrom typing_extensions import Annotated, TypedDict# TypedDictclass Joke(TypedDict):    \"\"\"Joke to tell user.\"\"\"    setup: Annotated[str, ..., \"The setup of the joke\"]    # Alternatively, we could have specified setup as:    # setup: str                    # no default, no description    # setup: Annotated[str, ...]    # no default, no description    # setup: Annotated[str, \"foo\"]  # default, no description    punchline: Annotated[str, ..., \"The punchline of the joke\"]    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]structured_llm = llm.with_structured_output(Joke)structured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```\n{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the mouse!', 'rating': 7}\n```\n\nEquivalently, we can pass in a JSON Schema dict. This requires no imports or classes and makes it very clear exactly how each parameter is documented, at the cost of being a bit more verbose.\n\n```\njson_schema = {    \"title\": \"joke\",    \"description\": \"Joke to tell user.\",    \"type\": \"object\",    \"properties\": {        \"setup\": {            \"type\": \"string\",            \"description\": \"The setup of the joke\",        },        \"punchline\": {            \"type\": \"string\",            \"description\": \"The punchline to the joke\",        },        \"rating\": {            \"type\": \"integer\",            \"description\": \"How funny the joke is, from 1 to 10\",            \"default\": None,        },    },    \"required\": [\"setup\", \"punchline\"],}structured_llm = llm.with_structured_output(json_schema)structured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```\n{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the mouse!', 'rating': 7}\n```", "url_final": "https://python.langchain.com/docs/how_to/structured_output/", "url_citable": "https://python.langchain.com/docs/how_to/structured_output/#typeddict-or-json-schema", "title": "How to return structured data from a model | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "TypedDict or JSON Schema‚Äã", "section_anchor": "typeddict-or-json-schema", "fetched_at": "2025-10-06T21:54:35.265803", "has_code": true}
{"id": "c3efe8fd9c9bbaa8f1834ac3be0dfd097ee7c704", "text": "Choosing between multiple schemas‚Äã\n\nThe simplest way to let the model choose from multiple schemas is to create a parent schema that has a Union-typed attribute.\n\n```\nfrom typing import Unionclass Joke(BaseModel):    \"\"\"Joke to tell user.\"\"\"    setup: str = Field(description=\"The setup of the joke\")    punchline: str = Field(description=\"The punchline to the joke\")    rating: Optional[int] = Field(        default=None, description=\"How funny the joke is, from 1 to 10\"    )class ConversationalResponse(BaseModel):    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"    response: str = Field(description=\"A conversational response to the user's query\")class FinalResponse(BaseModel):    final_output: Union[Joke, ConversationalResponse]structured_llm = llm.with_structured_output(FinalResponse)structured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```\nFinalResponse(final_output=Joke(setup='Why was the cat sitting on the computer?', punchline='Because it wanted to keep an eye on the mouse!', rating=7))\n```\n\n```\nstructured_llm.invoke(\"How are you today?\")\n```\n\n```\nFinalResponse(final_output=ConversationalResponse(response=\"I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with whatever you need!\"))\n```\n\n```\nfrom typing import Optional, Unionfrom typing_extensions import Annotated, TypedDictclass Joke(TypedDict):    \"\"\"Joke to tell user.\"\"\"    setup: Annotated[str, ..., \"The setup of the joke\"]    punchline: Annotated[str, ..., \"The punchline of the joke\"]    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]class ConversationalResponse(TypedDict):    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"    response: Annotated[str, ..., \"A conversational response to the user's query\"]class FinalResponse(TypedDict):    final_output: Union[Joke, ConversationalResponse]structured_llm = llm.with_structured_output(FinalResponse)structured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```\n{'final_output': {'setup': 'Why was the cat sitting on the computer?',  'punchline': 'Because it wanted to keep an eye on the mouse!',  'rating': 7}}\n```\n\n```\nstructured_llm.invoke(\"How are you today?\")\n```\n\n```\n{'final_output': {'response': \"I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with whatever you need!\"}}\n```\n\nResponses shall be identical to the ones shown in the Pydantic example.\n\nAlternatively, you can use tool calling directly to allow the model to choose between options, if your chosen model supports it. This involves a bit more parsing and setup but in some instances leads to better performance because you don't have to use nested schemas. See this how-to guide for more details.", "url_final": "https://python.langchain.com/docs/how_to/structured_output/", "url_citable": "https://python.langchain.com/docs/how_to/structured_output/#choosing-between-multiple-schemas", "title": "How to return structured data from a model | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Choosing between multiple schemas‚Äã", "section_anchor": "choosing-between-multiple-schemas", "fetched_at": "2025-10-06T21:54:35.265803", "has_code": true}
{"id": "43ed18de7d8364f3e1129a0e4e5efa7442f2acc9", "text": "Streaming‚Äã\n\nWe can stream outputs from our structured model when the output type is a dict (i.e., when the schema is specified as a TypedDict class or JSON Schema dict).\n\nNote that what's yielded is already aggregated chunks, not deltas.\n\n```\nfrom typing_extensions import Annotated, TypedDict# TypedDictclass Joke(TypedDict):    \"\"\"Joke to tell user.\"\"\"    setup: Annotated[str, ..., \"The setup of the joke\"]    punchline: Annotated[str, ..., \"The punchline of the joke\"]    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]structured_llm = llm.with_structured_output(Joke)for chunk in structured_llm.stream(\"Tell me a joke about cats\"):    print(chunk)\n```\n\n```\n{}{'setup': ''}{'setup': 'Why'}{'setup': 'Why was'}{'setup': 'Why was the'}{'setup': 'Why was the cat'}{'setup': 'Why was the cat sitting'}{'setup': 'Why was the cat sitting on'}{'setup': 'Why was the cat sitting on the'}{'setup': 'Why was the cat sitting on the computer'}{'setup': 'Why was the cat sitting on the computer?'}{'setup': 'Why was the cat sitting on the computer?', 'punchline': ''}{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because'}{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it'}{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted'}{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to'}{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep'}{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an'}{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye'}{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on'}{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the'}{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the mouse'}{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the mouse!'}{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the mouse!', 'rating': 7}\n```", "url_final": "https://python.langchain.com/docs/how_to/structured_output/", "url_citable": "https://python.langchain.com/docs/how_to/structured_output/#streaming", "title": "How to return structured data from a model | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Streaming‚Äã", "section_anchor": "streaming", "fetched_at": "2025-10-06T21:54:35.265803", "has_code": true}
{"id": "003c26c36fa31e1747fbdcef2bcb75dde3f34c31", "text": "Few-shot prompting‚Äã\n\nFor more complex schemas it's very useful to add few-shot examples to the prompt. This can be done in a few ways.\n\nThe simplest and most universal way is to add examples to a system message in the prompt:\n\n```\nfrom langchain_core.prompts import ChatPromptTemplatesystem = \"\"\"You are a hilarious comedian. Your specialty is knock-knock jokes. \\Return a joke which has the setup (the response to \"Who's there?\") and the final punchline (the response to \"<setup> who?\").Here are some examples of jokes:example_user: Tell me a joke about planesexample_assistant: {{\"setup\": \"Why don't planes ever get tired?\", \"punchline\": \"Because they have rest wings!\", \"rating\": 2}}example_user: Tell me another joke about planesexample_assistant: {{\"setup\": \"Cargo\", \"punchline\": \"Cargo 'vroom vroom', but planes go 'zoom zoom'!\", \"rating\": 10}}example_user: Now about caterpillarsexample_assistant: {{\"setup\": \"Caterpillar\", \"punchline\": \"Caterpillar really slow, but watch me turn into a butterfly and steal the show!\", \"rating\": 5}}\"\"\"prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", \"{input}\")])few_shot_structured_llm = prompt | structured_llmfew_shot_structured_llm.invoke(\"what's something funny about woodpeckers\")\n```\n\n```\n{'setup': 'Woodpecker', 'punchline': \"Woodpecker you a joke, but I'm afraid it might be too 'hole-some'!\", 'rating': 7}\n```\n\nWhen the underlying method for structuring outputs is tool calling, we can pass in our examples as explicit tool calls. You can check if the model you're using makes use of tool calling in its API reference.\n\n```\nfrom langchain_core.messages import AIMessage, HumanMessage, ToolMessageexamples = [    HumanMessage(\"Tell me a joke about planes\", name=\"example_user\"),    AIMessage(        \"\",        name=\"example_assistant\",        tool_calls=[            {                \"name\": \"joke\",                \"args\": {                    \"setup\": \"Why don't planes ever get tired?\",                    \"punchline\": \"Because they have rest wings!\",                    \"rating\": 2,                },                \"id\": \"1\",            }        ],    ),    # Most tool-calling models expect a ToolMessage(s) to follow an AIMessage with tool calls.    ToolMessage(\"\", tool_call_id=\"1\"),    # Some models also expect an AIMessage to follow any ToolMessages,    # so you may need to add an AIMessage here.    HumanMessage(\"Tell me another joke about planes\", name=\"example_user\"),    AIMessage(        \"\",        name=\"example_assistant\",        tool_calls=[            {                \"name\": \"joke\",                \"args\": {                    \"setup\": \"Cargo\",                    \"punchline\": \"Cargo 'vroom vroom', but planes go 'zoom zoom'!\",                    \"rating\": 10,                },                \"id\": \"2\",            }        ],    ),    ToolMessage(\"\", tool_call_id=\"2\"),    HumanMessage(\"Now about caterpillars\", name=\"example_user\"),    AIMessage(        \"\",        name=\"example_assistant\",        tool_calls=[            {                \"name\": \"joke\",                \"args\": {                    \"setup\": \"Caterpillar\",                    \"punchline\": \"Caterpillar really slow, but watch me turn into a butterfly and steal the show!\",                    \"rating\": 5,                },                \"id\": \"3\",            }        ],    ),    ToolMessage(\"\", tool_call_id=\"3\"),]system = \"\"\"You are a hilarious comedian. Your specialty is knock-knock jokes. \\Return a joke which has the setup (the response to \"Who's there?\") \\and the final punchline (the response to \"<setup> who?\").\"\"\"prompt = ChatPromptTemplate.from_messages(    [(\"system\", system), (\"placeholder\", \"{examples}\"), (\"human\", \"{input}\")])few_shot_structured_llm = prompt | structured_llmfew_shot_structured_llm.invoke({\"input\": \"crocodiles\", \"examples\": examples})\n```\n\n```\n{'setup': 'Crocodile', 'punchline': 'Crocodile be seeing you later, alligator!', 'rating': 6}\n```\n\nFor more on few shot prompting when using tool calling, see here.", "url_final": "https://python.langchain.com/docs/how_to/structured_output/", "url_citable": "https://python.langchain.com/docs/how_to/structured_output/#few-shot-prompting", "title": "How to return structured data from a model | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Few-shot prompting‚Äã", "section_anchor": "few-shot-prompting", "fetched_at": "2025-10-06T21:54:35.265803", "has_code": true}
{"id": "b387f80fdf0635fcf849f1e75a623bed507a6d2c", "text": "(Advanced) Specifying the method for structuring outputs‚Äã\n\nFor models that support more than one means of structuring outputs (i.e., they support both tool calling and JSON mode), you can specify which method to use with the method= argument.\n\nIf using JSON mode you'll have to still specify the desired schema in the model prompt. The schema you pass to with_structured_output will only be used for parsing the model outputs, it will not be passed to the model the way it is with tool calling.\n\nTo see if the model you're using supports JSON mode, check its entry in the API reference.\n\n```\nstructured_llm = llm.with_structured_output(None, method=\"json_schema\")structured_llm.invoke(    \"Tell me a joke about cats, respond in JSON with `setup` and `punchline` keys\")\n```\n\n```\n{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the mouse!'}\n```", "url_final": "https://python.langchain.com/docs/how_to/structured_output/", "url_citable": "https://python.langchain.com/docs/how_to/structured_output/#advanced-specifying-the-method-for-structuring-outputs", "title": "How to return structured data from a model | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "(Advanced) Specifying the method for structuring outputs‚Äã", "section_anchor": "advanced-specifying-the-method-for-structuring-outputs", "fetched_at": "2025-10-06T21:54:35.265803", "has_code": true}
{"id": "9275aa21e38c06886722c4cece4711fa186a7eb5", "text": "(Advanced) Raw outputs‚Äã\n\nLLMs aren't perfect at generating structured output, especially as schemas become complex. You can avoid raising exceptions and handle the raw output yourself by passing include_raw=True. This changes the output format to contain the raw message output, the parsed value (if successful), and any resulting errors:\n\n```\nstructured_llm = llm.with_structured_output(Joke, include_raw=True)structured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```\n{'raw': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_f25ZRmh8u5vHlOWfTUw8sJFZ', 'function': {'arguments': '{\"setup\":\"Why was the cat sitting on the computer?\",\"punchline\":\"Because it wanted to keep an eye on the mouse!\",\"rating\":7}', 'name': 'Joke'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 93, 'total_tokens': 126}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_4e2b2da518', 'finish_reason': 'stop', 'logprobs': None}, id='run-d880d7e2-df08-4e9e-ad92-dfc29f2fd52f-0', tool_calls=[{'name': 'Joke', 'args': {'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the mouse!', 'rating': 7}, 'id': 'call_f25ZRmh8u5vHlOWfTUw8sJFZ', 'type': 'tool_call'}], usage_metadata={'input_tokens': 93, 'output_tokens': 33, 'total_tokens': 126}), 'parsed': {'setup': 'Why was the cat sitting on the computer?',  'punchline': 'Because it wanted to keep an eye on the mouse!',  'rating': 7}, 'parsing_error': None}\n```", "url_final": "https://python.langchain.com/docs/how_to/structured_output/", "url_citable": "https://python.langchain.com/docs/how_to/structured_output/#advanced-raw-outputs", "title": "How to return structured data from a model | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "(Advanced) Raw outputs‚Äã", "section_anchor": "advanced-raw-outputs", "fetched_at": "2025-10-06T21:54:35.265803", "has_code": true}
{"id": "ea5432f8b7f2cbcf9befcd6806e70c9ce65eed56", "text": "Prompting and parsing model outputs directly‚Äã\n\nNot all models support .with_structured_output(), since not all models have tool calling or JSON mode support. For such models you'll need to directly prompt the model to use a specific format, and use an output parser to extract the structured response from the raw model output.\n\nUsing PydanticOutputParser‚Äã\n\nThe following example uses the built-in PydanticOutputParser to parse the output of a chat model prompted to match the given Pydantic schema. Note that we are adding format_instructions directly to the prompt from a method on the parser:\n\n```\nfrom typing import Listfrom langchain_core.output_parsers import PydanticOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom pydantic import BaseModel, Fieldclass Person(BaseModel):    \"\"\"Information about a person.\"\"\"    name: str = Field(..., description=\"The name of the person\")    height_in_meters: float = Field(        ..., description=\"The height of the person expressed in meters.\"    )class People(BaseModel):    \"\"\"Identifying information about all people in a text.\"\"\"    people: List[Person]# Set up a parserparser = PydanticOutputParser(pydantic_object=People)# Promptprompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"Answer the user query. Wrap the output in `json` tags\\n{format_instructions}\",        ),        (\"human\", \"{query}\"),    ]).partial(format_instructions=parser.get_format_instructions())\n```\n\nLet‚Äôs take a look at what information is sent to the model:\n\n```\nquery = \"Anna is 23 years old and she is 6 feet tall\"print(prompt.invoke({\"query\": query}).to_string())\n```\n\n```\nSystem: Answer the user query. Wrap the output in `json` tagsThe output should be formatted as a JSON instance that conforms to the JSON schema below.As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.Here is the output schema:\\`\\`\\`{\"description\": \"Identifying information about all people in a text.\", \"properties\": {\"people\": {\"title\": \"People\", \"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Person\"}}}, \"required\": [\"people\"], \"definitions\": {\"Person\": {\"title\": \"Person\", \"description\": \"Information about a person.\", \"type\": \"object\", \"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"The name of the person\", \"type\": \"string\"}, \"height_in_meters\": {\"title\": \"Height In Meters\", \"description\": \"The height of the person expressed in meters.\", \"type\": \"number\"}}, \"required\": [\"name\", \"height_in_meters\"]}}}\\`\\`\\`Human: Anna is 23 years old and she is 6 feet tall\n```\n\nAnd now let's invoke it:\n\n```\nchain = prompt | llm | parserchain.invoke({\"query\": query})\n```\n\n```\nPeople(people=[Person(name='Anna', height_in_meters=1.8288)])\n```\n\nFor a deeper dive into using output parsers with prompting techniques for structured output, see this guide.", "url_final": "https://python.langchain.com/docs/how_to/structured_output/", "url_citable": "https://python.langchain.com/docs/how_to/structured_output/#prompting-and-parsing-model-outputs-directly", "title": "How to return structured data from a model | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Prompting and parsing model outputs directly‚Äã", "section_anchor": "prompting-and-parsing-model-outputs-directly", "fetched_at": "2025-10-06T21:54:35.265803", "has_code": true}
{"id": "ab95fb7bc8f19c00ea5bec990b985ef4c78aa406", "text": "Custom Parsing‚Äã\n\nYou can also create a custom prompt and parser with LangChain Expression Language (LCEL), using a plain function to parse the output from the model:\n\n```\nimport jsonimport refrom typing import Listfrom langchain_core.messages import AIMessagefrom langchain_core.prompts import ChatPromptTemplatefrom pydantic import BaseModel, Fieldclass Person(BaseModel):    \"\"\"Information about a person.\"\"\"    name: str = Field(..., description=\"The name of the person\")    height_in_meters: float = Field(        ..., description=\"The height of the person expressed in meters.\"    )class People(BaseModel):    \"\"\"Identifying information about all people in a text.\"\"\"    people: List[Person]# Promptprompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"Answer the user query. Output your answer as JSON that  \"            \"matches the given schema: \\`\\`\\`json\\n{schema}\\n\\`\\`\\`. \"            \"Make sure to wrap the answer in \\`\\`\\`json and \\`\\`\\` tags\",        ),        (\"human\", \"{query}\"),    ]).partial(schema=People.model_json_schema())# Custom parserdef extract_json(message: AIMessage) -> List[dict]:    \"\"\"Extracts JSON content from a string where JSON is embedded between \\`\\`\\`json and \\`\\`\\` tags.    Parameters:        text (str): The text containing the JSON content.    Returns:        list: A list of extracted JSON strings.    \"\"\"    text = message.content    # Define the regular expression pattern to match JSON blocks    pattern = r\"\\`\\`\\`json(.*?)\\`\\`\\`\"    # Find all non-overlapping matches of the pattern in the string    matches = re.findall(pattern, text, re.DOTALL)    # Return the list of matched JSON strings, stripping any leading or trailing whitespace    try:        return [json.loads(match.strip()) for match in matches]    except Exception:        raise ValueError(f\"Failed to parse: {message}\")\n```\n\nHere is the prompt sent to the model:\n\n```\nquery = \"Anna is 23 years old and she is 6 feet tall\"print(prompt.format_prompt(query=query).to_string())\n```\n\n```\nSystem: Answer the user query. Output your answer as JSON that  matches the given schema: \\`\\`\\`json{'title': 'People', 'description': 'Identifying information about all people in a text.', 'type': 'object', 'properties': {'people': {'title': 'People', 'type': 'array', 'items': {'$ref': '#/definitions/Person'}}}, 'required': ['people'], 'definitions': {'Person': {'title': 'Person', 'description': 'Information about a person.', 'type': 'object', 'properties': {'name': {'title': 'Name', 'description': 'The name of the person', 'type': 'string'}, 'height_in_meters': {'title': 'Height In Meters', 'description': 'The height of the person expressed in meters.', 'type': 'number'}}, 'required': ['name', 'height_in_meters']}}}\\`\\`\\`. Make sure to wrap the answer in \\`\\`\\`json and \\`\\`\\` tagsHuman: Anna is 23 years old and she is 6 feet tall\n```\n\nAnd here's what it looks like when we invoke it:\n\n```\nchain = prompt | llm | extract_jsonchain.invoke({\"query\": query})\n```\n\n```\n[{'people': [{'name': 'Anna', 'height_in_meters': 1.8288}]}]\n```", "url_final": "https://python.langchain.com/docs/how_to/structured_output/", "url_citable": "https://python.langchain.com/docs/how_to/structured_output/#custom-parsing", "title": "How to return structured data from a model | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Custom Parsing‚Äã", "section_anchor": "custom-parsing", "fetched_at": "2025-10-06T21:54:35.265803", "has_code": true}
{"id": "27d09dcc4a302ac8d5b064cdd3702bd184293c7f", "text": "Combining with Additional Tools‚Äã\n\nWhen you need to use both structured output and additional tools (like web search), note the order of operations:\n\nCorrect Order:\n\n```\n# 1. Bind tools firstllm_with_tools = llm.bind_tools([web_search_tool, calculator_tool])# 2. Apply structured outputstructured_llm = llm_with_tools.with_structured_output(MySchema)\n```\n\nIncorrect Order:\n\n```\n# This will fail with \"Tool 'MySchema' not found\" errorstructured_llm = llm.with_structured_output(MySchema)broken_llm = structured_llm.bind_tools([web_search_tool])\n```\n\nWhy Order Matters: with_structured_output() internally uses tool calling to enforce the schema. When you bind additional tools afterward, it creates a conflict in the tool resolution system.\n\nComplete Example:\n\n```\nfrom pydantic import BaseModel, Fieldfrom langchain_openai import ChatOpenAIclass SearchResult(BaseModel):    \"\"\"Structured search result.\"\"\"    query: str = Field(description=\"The search query\")    findings: str = Field(description=\"Summary of findings\")# Define toolssearch_tool = {    \"type\": \"function\",    \"function\": {        \"name\": \"web_search\",        \"description\": \"Search the web for information\",        \"parameters\": {            \"type\": \"object\",            \"properties\": {\"query\": {\"type\": \"string\", \"description\": \"Search query\"}},            \"required\": [\"query\"],        },    },}# Correct approachllm = ChatOpenAI()llm_with_search = llm.bind_tools([search_tool])structured_search_llm = llm_with_search.with_structured_output(SearchResult)# Now you can use both search and get structured outputresult = structured_search_llm.invoke(\"Search for latest AI research and summarize\")\n```\n\n- Pydantic class\n- TypedDict or JSON Schema\n- Choosing between multiple schemas\n- Streaming\n- Few-shot prompting\n- (Advanced) Specifying the method for structuring outputs\n- (Advanced) Raw outputs\n\n- Using PydanticOutputParser\n- Custom Parsing", "url_final": "https://python.langchain.com/docs/how_to/structured_output/", "url_citable": "https://python.langchain.com/docs/how_to/structured_output/#combining-with-additional-tools", "title": "How to return structured data from a model | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Combining with Additional Tools‚Äã", "section_anchor": "combining-with-additional-tools", "fetched_at": "2025-10-06T21:54:35.265803", "has_code": true}
{"id": "d9316bd0cfadb909f5156ebda8474c3aea9a8e1e", "text": "Setup‚Äã\n\nWe'll need to install the following packages:\n\n```\n%pip install --upgrade --quiet langchain langchain-community\n```\n\nIf you'd like to use LangSmith, uncomment the below:\n\n```\nimport getpassimport os# os.environ[\"LANGSMITH_TRACING\"] = \"true\"# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n```\n\nYou can select any of the given models for this how-to guide. Keep in mind that most of these models already support native tool calling, so using the prompting strategy shown here doesn't make sense for these models, and instead you should follow the how to use a chat model to call tools guide.\n\n```\npip install -qU \"langchain[google-genai]\"\n```\n\n```\nimport getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelmodel = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n```\n\nTo illustrate the idea, we'll use phi3 via Ollama, which does NOT have native support for tool calling. If you'd like to use Ollama as well follow these instructions.\n\n```\nfrom langchain_community.llms import Ollamamodel = Ollama(model=\"phi3\")\n```", "url_final": "https://python.langchain.com/docs/how_to/tools_prompting/", "url_citable": "https://python.langchain.com/docs/how_to/tools_prompting/#setup", "title": "How to add ad-hoc tool calling capability to LLMs and Chat Models | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Setup‚Äã", "section_anchor": "setup", "fetched_at": "2025-10-06T21:54:40.477833", "has_code": true}
{"id": "88bb9666b9fc4f6593591b58ecd40c2e57037e78", "text": "Create a tool‚Äã\n\nFirst, let's create an add and multiply tools. For more information on creating custom tools, please see this guide.\n\n```\nfrom langchain_core.tools import tool@tooldef multiply(x: float, y: float) -> float:    \"\"\"Multiply two numbers together.\"\"\"    return x * y@tooldef add(x: int, y: int) -> int:    \"Add two numbers.\"    return x + ytools = [multiply, add]# Let's inspect the toolsfor t in tools:    print(\"--\")    print(t.name)    print(t.description)    print(t.args)\n```\n\n```\n--multiplyMultiply two numbers together.{'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}--addAdd two numbers.{'x': {'title': 'X', 'type': 'integer'}, 'y': {'title': 'Y', 'type': 'integer'}}\n```\n\n```\nmultiply.invoke({\"x\": 4, \"y\": 5})\n```\n\n```\n20.0\n```", "url_final": "https://python.langchain.com/docs/how_to/tools_prompting/", "url_citable": "https://python.langchain.com/docs/how_to/tools_prompting/#create-a-tool", "title": "How to add ad-hoc tool calling capability to LLMs and Chat Models | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Create a tool‚Äã", "section_anchor": "create-a-tool", "fetched_at": "2025-10-06T21:54:40.477833", "has_code": true}
{"id": "5bb74834a46e7cf2623093f20de1706fe07f0ad9", "text": "Creating our prompt‚Äã\n\nWe'll want to write a prompt that specifies the tools the model has access to, the arguments to those tools, and the desired output format of the model. In this case we'll instruct it to output a JSON blob of the form {\"name\": \"...\", \"arguments\": {...}}.\n\n```\nfrom langchain_core.output_parsers import JsonOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.tools import render_text_descriptionrendered_tools = render_text_description(tools)print(rendered_tools)\n```\n\n```\nmultiply(x: float, y: float) -> float - Multiply two numbers together.add(x: int, y: int) -> int - Add two numbers.\n```\n\n```\nsystem_prompt = f\"\"\"\\You are an assistant that has access to the following set of tools. Here are the names and descriptions for each tool:{rendered_tools}Given the user input, return the name and input of the tool to use. Return your response as a JSON blob with 'name' and 'arguments' keys.The `arguments` should be a dictionary, with keys corresponding to the argument names and the values corresponding to the requested values.\"\"\"prompt = ChatPromptTemplate.from_messages(    [(\"system\", system_prompt), (\"user\", \"{input}\")])\n```\n\n```\nchain = prompt | modelmessage = chain.invoke({\"input\": \"what's 3 plus 1132\"})# Let's take a look at the output from the model# if the model is an LLM (not a chat model), the output will be a string.if isinstance(message, str):    print(message)else:  # Otherwise it's a chat model    print(message.content)\n```\n\n```\n{    \"name\": \"add\",    \"arguments\": {        \"x\": 3,        \"y\": 1132    }}\n```", "url_final": "https://python.langchain.com/docs/how_to/tools_prompting/", "url_citable": "https://python.langchain.com/docs/how_to/tools_prompting/#creating-our-prompt", "title": "How to add ad-hoc tool calling capability to LLMs and Chat Models | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Creating our prompt‚Äã", "section_anchor": "creating-our-prompt", "fetched_at": "2025-10-06T21:54:40.477833", "has_code": true}
{"id": "d27585074951f820dfdcce7f9f2c5ddd979db9a3", "text": "Adding an output parser‚Äã\n\nWe'll use the JsonOutputParser for parsing our models output to JSON.\n\n```\nfrom langchain_core.output_parsers import JsonOutputParserchain = prompt | model | JsonOutputParser()chain.invoke({\"input\": \"what's thirteen times 4\"})\n```\n\n```\n{'name': 'multiply', 'arguments': {'x': 13.0, 'y': 4.0}}\n```\n\nüéâ Amazing! üéâ We now instructed our model on how to request that a tool be invoked.\n\nNow, let's create some logic to actually run the tool!", "url_final": "https://python.langchain.com/docs/how_to/tools_prompting/", "url_citable": "https://python.langchain.com/docs/how_to/tools_prompting/#adding-an-output-parser", "title": "How to add ad-hoc tool calling capability to LLMs and Chat Models | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Adding an output parser‚Äã", "section_anchor": "adding-an-output-parser", "fetched_at": "2025-10-06T21:54:40.477833", "has_code": true}
{"id": "f2d29fc04ee1c69d8d1627f994a4d24f69deb8f5", "text": "Invoking the tool üèÉ‚Äã\n\nNow that the model can request that a tool be invoked, we need to write a function that can actually invoke the tool.\n\nThe function will select the appropriate tool by name, and pass to it the arguments chosen by the model.\n\n```\nfrom typing import Any, Dict, Optional, TypedDictfrom langchain_core.runnables import RunnableConfigclass ToolCallRequest(TypedDict):    \"\"\"A typed dict that shows the inputs into the invoke_tool function.\"\"\"    name: str    arguments: Dict[str, Any]def invoke_tool(    tool_call_request: ToolCallRequest, config: Optional[RunnableConfig] = None):    \"\"\"A function that we can use the perform a tool invocation.    Args:        tool_call_request: a dict that contains the keys name and arguments.            The name must match the name of a tool that exists.            The arguments are the arguments to that tool.        config: This is configuration information that LangChain uses that contains            things like callbacks, metadata, etc.See LCEL documentation about RunnableConfig.    Returns:        output from the requested tool    \"\"\"    tool_name_to_tool = {tool.name: tool for tool in tools}    name = tool_call_request[\"name\"]    requested_tool = tool_name_to_tool[name]    return requested_tool.invoke(tool_call_request[\"arguments\"], config=config)\n```\n\nLet's test this out üß™!\n\n```\ninvoke_tool({\"name\": \"multiply\", \"arguments\": {\"x\": 3, \"y\": 5}})\n```\n\n```\n15.0\n```", "url_final": "https://python.langchain.com/docs/how_to/tools_prompting/", "url_citable": "https://python.langchain.com/docs/how_to/tools_prompting/#invoking-the-tool-", "title": "How to add ad-hoc tool calling capability to LLMs and Chat Models | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Invoking the tool üèÉ‚Äã", "section_anchor": "invoking-the-tool-", "fetched_at": "2025-10-06T21:54:40.477833", "has_code": true}
{"id": "48b9ba60791da63fc5fd8f3858c069dda3e0f2f6", "text": "Let's put it together‚Äã\n\nLet's put it together into a chain that creates a calculator with add and multiplication capabilities.\n\n```\nchain = prompt | model | JsonOutputParser() | invoke_toolchain.invoke({\"input\": \"what's thirteen times 4.14137281\"})\n```\n\n```\n53.83784653\n```", "url_final": "https://python.langchain.com/docs/how_to/tools_prompting/", "url_citable": "https://python.langchain.com/docs/how_to/tools_prompting/#lets-put-it-together", "title": "How to add ad-hoc tool calling capability to LLMs and Chat Models | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Let's put it together‚Äã", "section_anchor": "lets-put-it-together", "fetched_at": "2025-10-06T21:54:40.477833", "has_code": true}
{"id": "987d6346bf311462b94282fcb7d8444c5a9f4386", "text": "Returning tool inputs‚Äã\n\nIt can be helpful to return not only tool outputs but also tool inputs. We can easily do this with LCEL by RunnablePassthrough.assign-ing the tool output. This will take whatever the input is to the RunnablePassrthrough components (assumed to be a dictionary) and add a key to it while still passing through everything that's currently in the input:\n\n```\nfrom langchain_core.runnables import RunnablePassthroughchain = (    prompt | model | JsonOutputParser() | RunnablePassthrough.assign(output=invoke_tool))chain.invoke({\"input\": \"what's thirteen times 4.14137281\"})\n```\n\n```\n{'name': 'multiply', 'arguments': {'x': 13, 'y': 4.14137281}, 'output': 53.83784653}\n```", "url_final": "https://python.langchain.com/docs/how_to/tools_prompting/", "url_citable": "https://python.langchain.com/docs/how_to/tools_prompting/#returning-tool-inputs", "title": "How to add ad-hoc tool calling capability to LLMs and Chat Models | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Returning tool inputs‚Äã", "section_anchor": "returning-tool-inputs", "fetched_at": "2025-10-06T21:54:40.477833", "has_code": true}
{"id": "39e406d4267a38ffb7c941f024022890e67b850c", "text": "What's next?‚Äã\n\nThis how-to guide shows the \"happy path\" when the model correctly outputs all the required tool information.\n\nIn reality, if you're using more complex tools, you will start encountering errors from the model, especially for models that have not been fine tuned for tool calling and for less capable models.\n\nYou will need to be prepared to add strategies to improve the output from the model; e.g.,\n\n- Provide few shot examples.\n- Add error handling (e.g., catch the exception and feed it back to the LLM to ask it to correct its previous output).\n\n- Setup\n- Create a tool\n- Creating our prompt\n- Adding an output parser\n- Invoking the tool üèÉ\n- Let's put it together\n- Returning tool inputs\n- What's next?", "url_final": "https://python.langchain.com/docs/how_to/tools_prompting/", "url_citable": "https://python.langchain.com/docs/how_to/tools_prompting/#whats-next", "title": "How to add ad-hoc tool calling capability to LLMs and Chat Models | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "What's next?‚Äã", "section_anchor": "whats-next", "fetched_at": "2025-10-06T21:54:40.477833", "has_code": false}
{"id": "152b8ba7c97881ff6d44af4096181983c44f3cc8", "text": "Implementation‚Äã\n\nThere are only two required things that a custom LLM needs to implement:\n\nOptional implementations:\n\nLet's implement a simple custom LLM that just returns the first n characters of the input.\n\n```\nfrom typing import Any, Dict, Iterator, List, Mapping, Optionalfrom langchain_core.callbacks.manager import CallbackManagerForLLMRunfrom langchain_core.language_models.llms import LLMfrom langchain_core.outputs import GenerationChunkclass CustomLLM(LLM):    \"\"\"A custom chat model that echoes the first `n` characters of the input.    When contributing an implementation to LangChain, carefully document    the model including the initialization parameters, include    an example of how to initialize the model and include any relevant    links to the underlying models documentation or API.    Example:        .. code-block:: python            model = CustomChatModel(n=2)            result = model.invoke([HumanMessage(content=\"hello\")])            result = model.batch([[HumanMessage(content=\"hello\")],                                 [HumanMessage(content=\"world\")]])    \"\"\"    n: int    \"\"\"The number of characters from the last message of the prompt to be echoed.\"\"\"    def _call(        self,        prompt: str,        stop: Optional[List[str]] = None,        run_manager: Optional[CallbackManagerForLLMRun] = None,        **kwargs: Any,    ) -> str:        \"\"\"Run the LLM on the given input.        Override this method to implement the LLM logic.        Args:            prompt: The prompt to generate from.            stop: Stop words to use when generating. Model output is cut off at the                first occurrence of any of the stop substrings.                If stop tokens are not supported consider raising NotImplementedError.            run_manager: Callback manager for the run.            **kwargs: Arbitrary additional keyword arguments. These are usually passed                to the model provider API call.        Returns:            The model output as a string. Actual completions SHOULD NOT include the prompt.        \"\"\"        if stop is not None:            raise ValueError(\"stop kwargs are not permitted.\")        return prompt[: self.n]    def _stream(        self,        prompt: str,        stop: Optional[List[str]] = None,        run_manager: Optional[CallbackManagerForLLMRun] = None,        **kwargs: Any,    ) -> Iterator[GenerationChunk]:        \"\"\"Stream the LLM on the given prompt.        This method should be overridden by subclasses that support streaming.        If not implemented, the default behavior of calls to stream will be to        fallback to the non-streaming version of the model and return        the output as a single chunk.        Args:            prompt: The prompt to generate from.            stop: Stop words to use when generating. Model output is cut off at the                first occurrence of any of these substrings.            run_manager: Callback manager for the run.            **kwargs: Arbitrary additional keyword arguments. These are usually passed                to the model provider API call.        Returns:            An iterator of GenerationChunks.        \"\"\"        for char in prompt[: self.n]:            chunk = GenerationChunk(text=char)            if run_manager:                run_manager.on_llm_new_token(chunk.text, chunk=chunk)            yield chunk    @property    def _identifying_params(self) -> Dict[str, Any]:        \"\"\"Return a dictionary of identifying parameters.\"\"\"        return {            # The model name allows users to specify custom token counting            # rules in LLM monitoring applications (e.g., in LangSmith users            # can provide per token pricing for their model and monitor            # costs for the given LLM.)            \"model_name\": \"CustomChatModel\",        }    @property    def _llm_type(self) -> str:        \"\"\"Get the type of language model used by this chat model. Used for logging purposes only.\"\"\"        return \"custom\"\n```", "url_final": "https://python.langchain.com/docs/how_to/custom_llm/", "url_citable": "https://python.langchain.com/docs/how_to/custom_llm/#implementation", "title": "How to create a custom LLM class | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Implementation‚Äã", "section_anchor": "implementation", "fetched_at": "2025-10-06T21:55:15.349107", "has_code": true}
{"id": "3238fc6d6c080a854e7da86325c08d7d08400a87", "text": "Let's test it üß™‚Äã\n\nThis LLM will implement the standard Runnable interface of LangChain which many of the LangChain abstractions support!\n\n```\nllm = CustomLLM(n=5)print(llm)\n```\n\n```\n\u001b[1mCustomLLM\u001b[0mParams: {'model_name': 'CustomChatModel'}\n```\n\n```\nllm.invoke(\"This is a foobar thing\")\n```\n\n```\n'This '\n```\n\n```\nawait llm.ainvoke(\"world\")\n```\n\n```\n'world'\n```\n\n```\nllm.batch([\"woof woof woof\", \"meow meow meow\"])\n```\n\n```\n['woof ', 'meow ']\n```\n\n```\nawait llm.abatch([\"woof woof woof\", \"meow meow meow\"])\n```\n\n```\n['woof ', 'meow ']\n```\n\n```\nasync for token in llm.astream(\"hello\"):    print(token, end=\"|\", flush=True)\n```\n\n```\nh|e|l|l|o|\n```\n\nLet's confirm that in integrates nicely with other LangChain APIs.\n\n```\nfrom langchain_core.prompts import ChatPromptTemplate\n```\n\n```\nprompt = ChatPromptTemplate.from_messages(    [(\"system\", \"you are a bot\"), (\"human\", \"{input}\")])\n```\n\n```\nllm = CustomLLM(n=7)chain = prompt | llm\n```\n\n```\nidx = 0async for event in chain.astream_events({\"input\": \"hello there!\"}, version=\"v1\"):    print(event)    idx += 1    if idx > 7:        # Truncate        break\n```\n\n```\n{'event': 'on_chain_start', 'run_id': '05f24b4f-7ea3-4fb6-8417-3aa21633462f', 'name': 'RunnableSequence', 'tags': [], 'metadata': {}, 'data': {'input': {'input': 'hello there!'}}}{'event': 'on_prompt_start', 'name': 'ChatPromptTemplate', 'run_id': '7e996251-a926-4344-809e-c425a9846d21', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'input': {'input': 'hello there!'}}}{'event': 'on_prompt_end', 'name': 'ChatPromptTemplate', 'run_id': '7e996251-a926-4344-809e-c425a9846d21', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'input': {'input': 'hello there!'}, 'output': ChatPromptValue(messages=[SystemMessage(content='you are a bot'), HumanMessage(content='hello there!')])}}{'event': 'on_llm_start', 'name': 'CustomLLM', 'run_id': 'a8766beb-10f4-41de-8750-3ea7cf0ca7e2', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'input': {'prompts': ['System: you are a bot\\nHuman: hello there!']}}}{'event': 'on_llm_stream', 'name': 'CustomLLM', 'run_id': 'a8766beb-10f4-41de-8750-3ea7cf0ca7e2', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': 'S'}}{'event': 'on_chain_stream', 'run_id': '05f24b4f-7ea3-4fb6-8417-3aa21633462f', 'tags': [], 'metadata': {}, 'name': 'RunnableSequence', 'data': {'chunk': 'S'}}{'event': 'on_llm_stream', 'name': 'CustomLLM', 'run_id': 'a8766beb-10f4-41de-8750-3ea7cf0ca7e2', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': 'y'}}{'event': 'on_chain_stream', 'run_id': '05f24b4f-7ea3-4fb6-8417-3aa21633462f', 'tags': [], 'metadata': {}, 'name': 'RunnableSequence', 'data': {'chunk': 'y'}}\n```", "url_final": "https://python.langchain.com/docs/how_to/custom_llm/", "url_citable": "https://python.langchain.com/docs/how_to/custom_llm/#lets-test-it-", "title": "How to create a custom LLM class | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Let's test it üß™‚Äã", "section_anchor": "lets-test-it-", "fetched_at": "2025-10-06T21:55:15.349107", "has_code": true}
{"id": "706c52cd5081ca574e03555749b83939c64090b4", "text": "Contributing‚Äã\n\nWe appreciate all chat model integration contributions.\n\nHere's a checklist to help make sure your contribution gets added to LangChain:\n\nDocumentation:\n\n- The model contains doc-strings for all initialization arguments, as these will be surfaced in the APIReference.\n- The class doc-string for the model contains a link to the model API if the model is powered by a service.\n\nTests:\n\n- Add unit or integration tests to the overridden methods. Verify that invoke, ainvoke, batch, stream work if you've over-ridden the corresponding code.\n\nStreaming (if you're implementing it):\n\n- Make sure to invoke the on_llm_new_token callback\n- on_llm_new_token is invoked BEFORE yielding the chunk\n\nStop Token Behavior:\n\n- Stop token should be respected\n- Stop token should be INCLUDED as part of the response\n\nSecret API Keys:\n\n- If your model connects to an API it will likely accept API keys as part of its initialization. Use Pydantic's SecretStr type for secrets, so they don't get accidentally printed out when folks print the model.\n\n- Let's test it üß™", "url_final": "https://python.langchain.com/docs/how_to/custom_llm/", "url_citable": "https://python.langchain.com/docs/how_to/custom_llm/#contributing", "title": "How to create a custom LLM class | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Contributing‚Äã", "section_anchor": "contributing", "fetched_at": "2025-10-06T21:55:15.349107", "has_code": false}
{"id": "d3a0c7893af26a3a8ff4bf3d8f58b3825c51f12d", "text": "Basic usage‚Äã\n\n```\nfrom langchain.chat_models import init_chat_model# Don't forget to set your environment variables for the API keys of the respective providers!# For example, you can set them in your terminal or in a .env file:# export OPENAI_API_KEY=\"your_openai_api_key\"# Returns a langchain_openai.ChatOpenAI instance.gpt_4o = init_chat_model(\"gpt-4o\", model_provider=\"openai\", temperature=0)# Returns a langchain_anthropic.ChatAnthropic instance.claude_opus = init_chat_model(    \"claude-3-opus-20240229\", model_provider=\"anthropic\", temperature=0)# Returns a langchain_google_vertexai.ChatVertexAI instance.gemini_15 = init_chat_model(    \"gemini-2.5-pro\", model_provider=\"google_genai\", temperature=0)# Since all model integrations implement the ChatModel interface, you can use them in the same way.print(\"GPT-4o: \" + gpt_4o.invoke(\"what's your name\").content + \"\\n\")print(\"Claude Opus: \" + claude_opus.invoke(\"what's your name\").content + \"\\n\")print(\"Gemini 2.5: \" + gemini_15.invoke(\"what's your name\").content + \"\\n\")\n```\n\n```\nGPT-4o: I‚Äôm called ChatGPT. How can I assist you today?Claude Opus: My name is Claude. It's nice to meet you!Gemini 2.5: I do not have a name. I am a large language model, trained by Google.\n```", "url_final": "https://python.langchain.com/docs/how_to/chat_models_universal_init/", "url_citable": "https://python.langchain.com/docs/how_to/chat_models_universal_init/#basic-usage", "title": "How to init any model in one line | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Basic usage‚Äã", "section_anchor": "basic-usage", "fetched_at": "2025-10-06T21:55:06.045587", "has_code": true}
{"id": "0e0615b995c16a2e390491f8cd12f38724f62dd4", "text": "Inferring model provider‚Äã\n\nFor common and distinct model names init_chat_model() will attempt to infer the model provider. See the API reference for a full list of inference behavior. E.g. any model that starts with gpt-3... or gpt-4... will be inferred as using model provider openai.\n\n```\ngpt_4o = init_chat_model(\"gpt-4o\", temperature=0)claude_opus = init_chat_model(\"claude-3-opus-20240229\", temperature=0)gemini_15 = init_chat_model(\"gemini-2.5-pro\", temperature=0)\n```", "url_final": "https://python.langchain.com/docs/how_to/chat_models_universal_init/", "url_citable": "https://python.langchain.com/docs/how_to/chat_models_universal_init/#inferring-model-provider", "title": "How to init any model in one line | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Inferring model provider‚Äã", "section_anchor": "inferring-model-provider", "fetched_at": "2025-10-06T21:55:06.045587", "has_code": true}
{"id": "e2441ebd63c0768a6e58a0b5ee7abcd7871c3ed3", "text": "Creating a configurable model‚Äã\n\nYou can also create a runtime-configurable model by specifying configurable_fields. If you don't specify a model value, then \"model\" and \"model_provider\" be configurable by default.\n\n```\nconfigurable_model = init_chat_model(temperature=0)configurable_model.invoke(    \"what's your name\", config={\"configurable\": {\"model\": \"gpt-4o\"}})\n```\n\n```\nAIMessage(content='I‚Äôm called ChatGPT. How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 11, 'total_tokens': 24, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_07871e2ad8', 'id': 'chatcmpl-BwCyyBpMqn96KED6zPhLm4k9SQMiQ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--fada10c3-4128-406c-b83d-a850d16b365f-0', usage_metadata={'input_tokens': 11, 'output_tokens': 13, 'total_tokens': 24, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n```\n\n```\nconfigurable_model.invoke(    \"what's your name\", config={\"configurable\": {\"model\": \"claude-3-5-sonnet-latest\"}})\n```\n\n```\nAIMessage(content=\"My name is Claude. It's nice to meet you!\", additional_kwargs={}, response_metadata={'id': 'msg_01VDGrG9D6yefanbBG9zPJrc', 'model': 'claude-3-5-sonnet-20240620', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 11, 'output_tokens': 15, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-5-sonnet-20240620'}, id='run--f0156087-debf-4b4b-9aaa-f3328a81ef92-0', usage_metadata={'input_tokens': 11, 'output_tokens': 15, 'total_tokens': 26, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})\n```\n\nConfigurable model with default values‚Äã\n\nWe can create a configurable model with default model values, specify which parameters are configurable, and add prefixes to configurable params:\n\n```\nfirst_llm = init_chat_model(    model=\"gpt-4o\",    temperature=0,    configurable_fields=(\"model\", \"model_provider\", \"temperature\", \"max_tokens\"),    config_prefix=\"first\",  # useful when you have a chain with multiple models)first_llm.invoke(\"what's your name\")\n```\n\n```\nAIMessage(content=\"I'm an AI created by OpenAI, and I don't have a personal name. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 11, 'total_tokens': 34}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_25624ae3a5', 'finish_reason': 'stop', 'logprobs': None}, id='run-3380f977-4b89-4f44-bc02-b64043b3166f-0', usage_metadata={'input_tokens': 11, 'output_tokens': 23, 'total_tokens': 34})\n```\n\n```\nfirst_llm.invoke(    \"what's your name\",    config={        \"configurable\": {            \"first_model\": \"claude-3-5-sonnet-latest\",            \"first_temperature\": 0.5,            \"first_max_tokens\": 100,        }    },)\n```\n\n```\nAIMessage(content=\"My name is Claude. It's nice to meet you!\", additional_kwargs={}, response_metadata={'id': 'msg_01EFKSWpmsn2PSYPQa4cNHWb', 'model': 'claude-3-5-sonnet-20240620', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 11, 'output_tokens': 15}}, id='run-3c58f47c-41b9-4e56-92e7-fb9602e3787c-0', usage_metadata={'input_tokens': 11, 'output_tokens': 15, 'total_tokens': 26})\n```", "url_final": "https://python.langchain.com/docs/how_to/chat_models_universal_init/", "url_citable": "https://python.langchain.com/docs/how_to/chat_models_universal_init/#creating-a-configurable-model", "title": "How to init any model in one line | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Creating a configurable model‚Äã", "section_anchor": "creating-a-configurable-model", "fetched_at": "2025-10-06T21:55:06.045587", "has_code": true}
{"id": "897cf0bedbc0d9c5ab51575b014481ec9d128b95", "text": "Using a configurable model declaratively‚Äã\n\nWe can call declarative operations like bind_tools, with_structured_output, with_configurable, etc. on a configurable model and chain a configurable model in the same way that we would a regularly instantiated chat model object.\n\n```\nfrom pydantic import BaseModel, Fieldclass GetWeather(BaseModel):    \"\"\"Get the current weather in a given location\"\"\"    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")class GetPopulation(BaseModel):    \"\"\"Get the current population in a given location\"\"\"    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")llm = init_chat_model(temperature=0)llm_with_tools = llm.bind_tools([GetWeather, GetPopulation])llm_with_tools.invoke(    \"what's bigger in 2024 LA or NYC\", config={\"configurable\": {\"model\": \"gpt-4o\"}}).tool_calls\n```\n\n```\n[{'name': 'GetPopulation',  'args': {'location': 'Los Angeles, CA'},  'id': 'call_Ga9m8FAArIyEjItHmztPYA22',  'type': 'tool_call'}, {'name': 'GetPopulation',  'args': {'location': 'New York, NY'},  'id': 'call_jh2dEvBaAHRaw5JUDthOs7rt',  'type': 'tool_call'}]\n```\n\n```\nllm_with_tools.invoke(    \"what's bigger in 2024 LA or NYC\",    config={\"configurable\": {\"model\": \"claude-3-5-sonnet-latest\"}},).tool_calls\n```\n\n```\n[{'name': 'GetPopulation',  'args': {'location': 'Los Angeles, CA'},  'id': 'toolu_01JMufPf4F4t2zLj7miFeqXp',  'type': 'tool_call'}, {'name': 'GetPopulation',  'args': {'location': 'New York City, NY'},  'id': 'toolu_01RQBHcE8kEEbYTuuS8WqY1u',  'type': 'tool_call'}]\n```\n\n- Configurable model with default values\n- Using a configurable model declaratively", "url_final": "https://python.langchain.com/docs/how_to/chat_models_universal_init/", "url_citable": "https://python.langchain.com/docs/how_to/chat_models_universal_init/#using-a-configurable-model-declaratively", "title": "How to init any model in one line | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Using a configurable model declaratively‚Äã", "section_anchor": "using-a-configurable-model-declaratively", "fetched_at": "2025-10-06T21:55:06.045587", "has_code": true}
{"id": "8f864fcf4c1212b6d794b8cbc553e1831e22c60f", "text": "Setup‚Äã\n\nFirst, get required packages and set environment variables:\n\n```\n%pip install --upgrade --quiet  langchain langchain-community langchain-openai\n```\n\n```\n# Uncomment the below to use LangSmith. Not required.# import os# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n```\n\nThe below example will use a SQLite connection with Chinook database. Follow these installation steps to create Chinook.db in the same directory as this notebook:\n\n- Save this file as Chinook_Sqlite.sql\n- Run sqlite3 Chinook.db\n- Run .read Chinook_Sqlite.sql\n- Test SELECT * FROM Artist LIMIT 10;\n\nNow, Chinook.db is in our directory and we can interface with it using the SQLAlchemy-driven SQLDatabase class:\n\n```\nfrom langchain_community.utilities import SQLDatabasedb = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")print(db.dialect)print(db.get_usable_table_names())print(db.run(\"SELECT * FROM Artist LIMIT 10;\"))\n```\n\n```\nsqlite['Album', 'Artist', 'Customer', 'Employee', 'Genre', 'Invoice', 'InvoiceLine', 'MediaType', 'Playlist', 'PlaylistTrack', 'Track'][(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains'), (6, 'Ant√¥nio Carlos Jobim'), (7, 'Apocalyptica'), (8, 'Audioslave'), (9, 'BackBeat'), (10, 'Billy Cobham')]\n```", "url_final": "https://python.langchain.com/docs/how_to/sql_query_checking/", "url_citable": "https://python.langchain.com/docs/how_to/sql_query_checking/#setup", "title": "How to do query validation as part of SQL question-answering | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Setup‚Äã", "section_anchor": "setup", "fetched_at": "2025-10-06T21:56:27.479582", "has_code": true}
{"id": "a1c2b983214fbf5d38854e577264ad38840aed72", "text": "Query checker‚Äã\n\nPerhaps the simplest strategy is to ask the model itself to check the original query for common mistakes. Suppose we have the following SQL query chain:\n\n```\npip install -qU \"langchain[google-genai]\"\n```\n\n```\nimport getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n```\n\n```\nfrom langchain.chains import create_sql_query_chainchain = create_sql_query_chain(llm, db)\n```\n\nAnd we want to validate its outputs. We can do so by extending the chain with a second prompt and model call:\n\n```\nfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatesystem = \"\"\"Double check the user's {dialect} query for common mistakes, including:- Using NOT IN with NULL values- Using UNION when UNION ALL should have been used- Using BETWEEN for exclusive ranges- Data type mismatch in predicates- Properly quoting identifiers- Using the correct number of arguments for functions- Casting to the correct data type- Using the proper columns for joinsIf there are any of the above mistakes, rewrite the query.If there are no mistakes, just reproduce the original query with no further commentary.Output the final SQL query only.\"\"\"prompt = ChatPromptTemplate.from_messages(    [(\"system\", system), (\"human\", \"{query}\")]).partial(dialect=db.dialect)validation_chain = prompt | llm | StrOutputParser()full_chain = {\"query\": chain} | validation_chain\n```\n\n```\nquery = full_chain.invoke(    {        \"question\": \"What's the average Invoice from an American customer whose Fax is missing since 2003 but before 2010\"    })print(query)\n```\n\n```\nSELECT AVG(i.Total) AS AverageInvoiceFROM Invoice iJOIN Customer c ON i.CustomerId = c.CustomerIdWHERE c.Country = 'USA'AND c.Fax IS NULLAND i.InvoiceDate >= '2003-01-01' AND i.InvoiceDate < '2010-01-01'\n```\n\nNote how we can see both steps of the chain in the Langsmith trace.\n\n```\ndb.run(query)\n```\n\n```\n'[(6.632999999999998,)]'\n```\n\nThe obvious downside of this approach is that we need to make two model calls instead of one to generate our query. To get around this we can try to perform the query generation and query check in a single model invocation:\n\n```\nsystem = \"\"\"You are a {dialect} expert. Given an input question, create a syntactically correct {dialect} query to run.Unless the user specifies in the question a specific number of examples to obtain, query for at most {top_k} results using the LIMIT clause as per {dialect}. You can order the results to return the most informative data in the database.Never query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.Pay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.Pay attention to use date('now') function to get the current date, if the question involves \"today\".Only use the following tables:{table_info}Write an initial draft of the query. Then double check the {dialect} query for common mistakes, including:- Using NOT IN with NULL values- Using UNION when UNION ALL should have been used- Using BETWEEN for exclusive ranges- Data type mismatch in predicates- Properly quoting identifiers- Using the correct number of arguments for functions- Casting to the correct data type- Using the proper columns for joinsUse format:First draft: <<FIRST_DRAFT_QUERY>>Final answer: <<FINAL_ANSWER_QUERY>>\"\"\"prompt = ChatPromptTemplate.from_messages(    [(\"system\", system), (\"human\", \"{input}\")]).partial(dialect=db.dialect)def parse_final_answer(output: str) -> str:    return output.split(\"Final answer: \")[1]chain = create_sql_query_chain(llm, db, prompt=prompt) | parse_final_answerprompt.pretty_print()\n```\n\n```\n================================\u001b[1m System Message \u001b[0m================================You are a \u001b[33;1m\u001b[1;3m{dialect}\u001b[0m expert. Given an input question, create a syntactically correct \u001b[33;1m\u001b[1;3m{dialect}\u001b[0m query to run.Unless the user specifies in the question a specific number of examples to obtain, query for at most \u001b[33;1m\u001b[1;3m{top_k}\u001b[0m results using the LIMIT clause as per \u001b[33;1m\u001b[1;3m{dialect}\u001b[0m. You can order the results to return the most informative data in the database.Never query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.Pay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.Pay attention to use date('now') function to get the current date, if the question involves \"today\".Only use the following tables:\u001b[33;1m\u001b[1;3m{table_info}\u001b[0mWrite an initial draft of the query. Then double check the \u001b[33;1m\u001b[1;3m{dialect}\u001b[0m query for common mistakes, including:- Using NOT IN with NULL values- Using UNION when UNION ALL should have been used- Using BETWEEN for exclusive ranges- Data type mismatch in predicates- Properly quoting identifiers- Using the correct number of arguments for functions- Casting to the correct data type- Using the proper columns for joinsUse format:First draft: <<FIRST_DRAFT_QUERY>>Final answer: <<FINAL_ANSWER_QUERY>>================================\u001b[1m Human Message \u001b[0m=================================\u001b[33;1m\u001b[1;3m{input}\u001b[0m\n```\n\n```\nquery = chain.invoke(    {        \"question\": \"What's the average Invoice from an American customer whose Fax is missing since 2003 but before 2010\"    })print(query)\n```\n\n```\nSELECT AVG(i.\"Total\") AS \"AverageInvoice\"FROM \"Invoice\" iJOIN \"Customer\" c ON i.\"CustomerId\" = c.\"CustomerId\"WHERE c.\"Country\" = 'USA'AND c.\"Fax\" IS NULLAND i.\"InvoiceDate\" BETWEEN '2003-01-01' AND '2010-01-01';\n```\n\n```\ndb.run(query)\n```\n\n```\n'[(6.632999999999998,)]'\n```", "url_final": "https://python.langchain.com/docs/how_to/sql_query_checking/", "url_citable": "https://python.langchain.com/docs/how_to/sql_query_checking/#query-checker", "title": "How to do query validation as part of SQL question-answering | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Query checker‚Äã", "section_anchor": "query-checker", "fetched_at": "2025-10-06T21:56:27.479582", "has_code": true}
{"id": "2d692de87d74fae4869175f74e01447043e4c0da", "text": "Human-in-the-loop‚Äã\n\nIn some cases our data is sensitive enough that we never want to execute a SQL query without a human approving it first. Head to the Tool use: Human-in-the-loop page to learn how to add a human-in-the-loop to any tool, chain or agent.", "url_final": "https://python.langchain.com/docs/how_to/sql_query_checking/", "url_citable": "https://python.langchain.com/docs/how_to/sql_query_checking/#human-in-the-loop", "title": "How to do query validation as part of SQL question-answering | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Human-in-the-loop‚Äã", "section_anchor": "human-in-the-loop", "fetched_at": "2025-10-06T21:56:27.479582", "has_code": false}
{"id": "2d0e44b92550e62c48416fc9eee930d65ea9c588", "text": "Error handling‚Äã\n\nAt some point, the model will make a mistake and craft an invalid SQL query. Or an issue will arise with our database. Or the model API will go down. We'll want to add some error handling behavior to our chains and agents so that we fail gracefully in these situations, and perhaps even automatically recover. To learn about error handling with tools, head to the Tool use: Error handling page.\n\n- Setup\n- Query checker\n- Human-in-the-loop\n- Error handling", "url_final": "https://python.langchain.com/docs/how_to/sql_query_checking/", "url_citable": "https://python.langchain.com/docs/how_to/sql_query_checking/#error-handling", "title": "How to do query validation as part of SQL question-answering | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Error handling‚Äã", "section_anchor": "error-handling", "fetched_at": "2025-10-06T21:56:27.479582", "has_code": false}
{"id": "b9bd1926b0fe6bb7eb2c0b63fea655ee9eb62fc2", "text": "Loading DOCX, XLSX, PPTX with AzureAIDocumentIntelligenceLoader‚Äã\n\nAzure AI Document Intelligence (formerly known as Azure Form Recognizer) is machine-learning based service that extracts texts (including handwriting), tables, document structures (e.g., titles, section headings, etc.) and key-value-pairs from digital or scanned PDFs, images, Office and HTML files. Document Intelligence supports PDF, JPEG/JPG, PNG, BMP, TIFF, HEIF, DOCX, XLSX, PPTX and HTML.\n\nThis current implementation of a loader using Document Intelligence can incorporate content page-wise and turn it into LangChain documents. The default output format is markdown, which can be easily chained with MarkdownHeaderTextSplitter for semantic document chunking. You can also use mode=\"single\" or mode=\"page\" to return pure texts in a single page or document split by page.\n\nPrerequisite‚Äã\n\nAn Azure AI Document Intelligence resource in one of the 3 preview regions: East US, West US2, West Europe - follow this document to create one if you don't have. You will be passing <endpoint> and <key> as parameters to the loader.\n\n```\n%pip install --upgrade --quiet  langchain langchain-community azure-ai-documentintelligencefrom langchain_community.document_loaders import AzureAIDocumentIntelligenceLoaderfile_path = \"<filepath>\"endpoint = \"<endpoint>\"key = \"<key>\"loader = AzureAIDocumentIntelligenceLoader(    api_endpoint=endpoint, api_key=key, file_path=file_path, api_model=\"prebuilt-layout\")documents = loader.load()\n```\n\n- Prerequisite", "url_final": "https://python.langchain.com/docs/how_to/document_loader_office_file/", "url_citable": "https://python.langchain.com/docs/how_to/document_loader_office_file/#loading-docx-xlsx-pptx-with-azureaidocumentintelligenceloader", "title": "How to load Microsoft Office files | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Loading DOCX, XLSX, PPTX with AzureAIDocumentIntelligenceLoader‚Äã", "section_anchor": "loading-docx-xlsx-pptx-with-azureaidocumentintelligenceloader", "fetched_at": "2025-10-06T21:55:25.852013", "has_code": true}
{"id": "b91c01f12b85bd6e55e5494784b2a117c1bf8b6c", "text": "Tools‚Äã\n\nLangChain has a large collection of 3rd party tools. Please visit Tool Integrations for a list of the available tools.\n\nWhen using 3rd party tools, make sure that you understand how the tool works, what permissions it has. Read over its documentation and check if anything is required from you from a security point of view. Please see our security guidelines for more information.\n\nLet's try out the Wikipedia integration.\n\n```\n!pip install -qU langchain-community wikipedia\n```\n\n```\nfrom langchain_community.tools import WikipediaQueryRunfrom langchain_community.utilities import WikipediaAPIWrapperapi_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=100)tool = WikipediaQueryRun(api_wrapper=api_wrapper)print(tool.invoke({\"query\": \"langchain\"}))\n```\n\n```\nPage: LangChainSummary: LangChain is a framework designed to simplify the creation of applications\n```\n\nThe tool has the following defaults associated with it:\n\n```\nprint(f\"Name: {tool.name}\")print(f\"Description: {tool.description}\")print(f\"args schema: {tool.args}\")print(f\"returns directly?: {tool.return_direct}\")\n```\n\n```\nName: wikipediaDescription: A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.args schema: {'query': {'description': 'query to look up on wikipedia', 'title': 'Query', 'type': 'string'}}returns directly?: False\n```", "url_final": "https://python.langchain.com/docs/how_to/tools_builtin/", "url_citable": "https://python.langchain.com/docs/how_to/tools_builtin/#tools", "title": "How to use built-in tools and toolkits | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Tools‚Äã", "section_anchor": "tools", "fetched_at": "2025-10-06T21:54:45.630807", "has_code": true}
{"id": "c7cf69ee29f84386d99a7c544a7cd1b030659f8e", "text": "Customizing Default Tools‚Äã\n\nWe can also modify the built in name, description, and JSON schema of the arguments.\n\nWhen defining the JSON schema of the arguments, it is important that the inputs remain the same as the function, so you shouldn't change that. But you can define custom descriptions for each input easily.\n\n```\nfrom langchain_community.tools import WikipediaQueryRunfrom langchain_community.utilities import WikipediaAPIWrapperfrom pydantic import BaseModel, Fieldclass WikiInputs(BaseModel):    \"\"\"Inputs to the wikipedia tool.\"\"\"    query: str = Field(        description=\"query to look up in Wikipedia, should be 3 or less words\"    )tool = WikipediaQueryRun(    name=\"wiki-tool\",    description=\"look up things in wikipedia\",    args_schema=WikiInputs,    api_wrapper=api_wrapper,    return_direct=True,)print(tool.run(\"langchain\"))\n```\n\n```\nPage: LangChainSummary: LangChain is a framework designed to simplify the creation of applications\n```\n\n```\nprint(f\"Name: {tool.name}\")print(f\"Description: {tool.description}\")print(f\"args schema: {tool.args}\")print(f\"returns directly?: {tool.return_direct}\")\n```\n\n```\nName: wiki-toolDescription: look up things in wikipediaargs schema: {'query': {'description': 'query to look up in Wikipedia, should be 3 or less words', 'title': 'Query', 'type': 'string'}}returns directly?: True\n```", "url_final": "https://python.langchain.com/docs/how_to/tools_builtin/", "url_citable": "https://python.langchain.com/docs/how_to/tools_builtin/#customizing-default-tools", "title": "How to use built-in tools and toolkits | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Customizing Default Tools‚Äã", "section_anchor": "customizing-default-tools", "fetched_at": "2025-10-06T21:54:45.630807", "has_code": true}
{"id": "fe729c982a427975deec6b425e10f564e53e790c", "text": "How to use built-in toolkits‚Äã\n\nToolkits are collections of tools that are designed to be used together for specific tasks. They have convenient loading methods.\n\nAll Toolkits expose a get_tools method which returns a list of tools.\n\nYou're usually meant to use them this way:\n\n```\n# Initialize a toolkittoolkit = ExampleTookit(...)# Get list of toolstools = toolkit.get_tools()\n```\n\n- Tools\n- Customizing Default Tools\n- How to use built-in toolkits", "url_final": "https://python.langchain.com/docs/how_to/tools_builtin/", "url_citable": "https://python.langchain.com/docs/how_to/tools_builtin/#how-to-use-built-in-toolkits", "title": "How to use built-in tools and toolkits | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "How to use built-in toolkits‚Äã", "section_anchor": "how-to-use-built-in-toolkits", "fetched_at": "2025-10-06T21:54:45.630807", "has_code": true}
{"id": "ecdd41171a3905753a8407e0591b4825ecf82d54", "text": "How to select examples by length | ü¶úÔ∏èüîó LangChain\n\nThis example selector selects which examples to use based on length. This is useful when you are worried about constructing a prompt that will go over the length of the context window. For longer inputs, it will select fewer examples to include, while for shorter inputs it will select more.\n\n```\nfrom langchain_core.example_selectors import LengthBasedExampleSelectorfrom langchain_core.prompts import FewShotPromptTemplate, PromptTemplate# Examples of a pretend task of creating antonyms.examples = [    {\"input\": \"happy\", \"output\": \"sad\"},    {\"input\": \"tall\", \"output\": \"short\"},    {\"input\": \"energetic\", \"output\": \"lethargic\"},    {\"input\": \"sunny\", \"output\": \"gloomy\"},    {\"input\": \"windy\", \"output\": \"calm\"},]example_prompt = PromptTemplate(    input_variables=[\"input\", \"output\"],    template=\"Input: {input}\\nOutput: {output}\",)example_selector = LengthBasedExampleSelector(    # The examples it has available to choose from.    examples=examples,    # The PromptTemplate being used to format the examples.    example_prompt=example_prompt,    # The maximum length that the formatted examples should be.    # Length is measured by the get_text_length function below.    max_length=25,    # The function used to get the length of a string, which is used    # to determine which examples to include. It is commented out because    # it is provided as a default value if none is specified.    # get_text_length: Callable[[str], int] = lambda x: len(re.split(\"\\n| \", x)))dynamic_prompt = FewShotPromptTemplate(    # We provide an ExampleSelector instead of examples.    example_selector=example_selector,    example_prompt=example_prompt,    prefix=\"Give the antonym of every input\",    suffix=\"Input: {adjective}\\nOutput:\",    input_variables=[\"adjective\"],)\n```\n\n```\n# An example with small input, so it selects all examples.print(dynamic_prompt.format(adjective=\"big\"))\n```\n\n```\nGive the antonym of every inputInput: happyOutput: sadInput: tallOutput: shortInput: energeticOutput: lethargicInput: sunnyOutput: gloomyInput: windyOutput: calmInput: bigOutput:\n```\n\n```\n# An example with long input, so it selects only one example.long_string = \"big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else\"print(dynamic_prompt.format(adjective=long_string))\n```\n\n```\nGive the antonym of every inputInput: happyOutput: sadInput: big and huge and massive and large and gigantic and tall and much much much much much bigger than everything elseOutput:\n```\n\n```\n# You can add an example to an example selector as well.new_example = {\"input\": \"big\", \"output\": \"small\"}dynamic_prompt.example_selector.add_example(new_example)print(dynamic_prompt.format(adjective=\"enthusiastic\"))\n```\n\n```\nGive the antonym of every inputInput: happyOutput: sadInput: tallOutput: shortInput: energeticOutput: lethargicInput: sunnyOutput: gloomyInput: windyOutput: calmInput: bigOutput: smallInput: enthusiasticOutput:\n```", "url_final": "https://python.langchain.com/docs/how_to/example_selectors_length_based/", "url_citable": "https://python.langchain.com/docs/how_to/example_selectors_length_based/", "title": "How to select examples by length | ü¶úÔ∏èüîó LangChain", "section_level": 1, "section_heading": "How to select examples by length | ü¶úÔ∏èüîó LangChain", "section_anchor": null, "fetched_at": "2025-10-06T21:55:33.488548", "has_code": true}
{"id": "80f64309682e3291ba888551ab559122108f15c9", "text": "Related‚Äã\n\n- LangGraph quickstart\n- Few shot prompting with tools\n- Stream tool calls\n- Pass runtime values to tools\n- Getting structured outputs from models\n\n- Related", "url_final": "https://python.langchain.com/docs/how_to/tool_results_pass_to_model/", "url_citable": "https://python.langchain.com/docs/how_to/tool_results_pass_to_model/#related", "title": "How to pass tool outputs to chat models | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Related‚Äã", "section_anchor": "related", "fetched_at": "2025-10-06T21:56:37.654849", "has_code": false}
{"id": "ce6bcd13c5ca95df4cce7d741501165c9b698241", "text": "How to select examples by n-gram overlap | ü¶úÔ∏èüîó LangChain\n\nThe NGramOverlapExampleSelector selects and orders examples based on which examples are most similar to the input, according to an ngram overlap score. The ngram overlap score is a float between 0.0 and 1.0, inclusive.\n\nThe selector allows for a threshold score to be set. Examples with an ngram overlap score less than or equal to the threshold are excluded. The threshold is set to -1.0, by default, so will not exclude any examples, only reorder them. Setting the threshold to 0.0 will exclude examples that have no ngram overlaps with the input.\n\n```\nfrom langchain_community.example_selectors import NGramOverlapExampleSelectorfrom langchain_core.prompts import FewShotPromptTemplate, PromptTemplateexample_prompt = PromptTemplate(    input_variables=[\"input\", \"output\"],    template=\"Input: {input}\\nOutput: {output}\",)# Examples of a fictional translation task.examples = [    {\"input\": \"See Spot run.\", \"output\": \"Ver correr a Spot.\"},    {\"input\": \"My dog barks.\", \"output\": \"Mi perro ladra.\"},    {\"input\": \"Spot can run.\", \"output\": \"Spot puede correr.\"},]\n```\n\n```\nexample_selector = NGramOverlapExampleSelector(    # The examples it has available to choose from.    examples=examples,    # The PromptTemplate being used to format the examples.    example_prompt=example_prompt,    # The threshold, at which selector stops.    # It is set to -1.0 by default.    threshold=-1.0,    # For negative threshold:    # Selector sorts examples by ngram overlap score, and excludes none.    # For threshold greater than 1.0:    # Selector excludes all examples, and returns an empty list.    # For threshold equal to 0.0:    # Selector sorts examples by ngram overlap score,    # and excludes those with no ngram overlap with input.)dynamic_prompt = FewShotPromptTemplate(    # We provide an ExampleSelector instead of examples.    example_selector=example_selector,    example_prompt=example_prompt,    prefix=\"Give the Spanish translation of every input\",    suffix=\"Input: {sentence}\\nOutput:\",    input_variables=[\"sentence\"],)\n```\n\n```\n# An example input with large ngram overlap with \"Spot can run.\"# and no overlap with \"My dog barks.\"print(dynamic_prompt.format(sentence=\"Spot can run fast.\"))\n```\n\n```\nGive the Spanish translation of every inputInput: Spot can run.Output: Spot puede correr.Input: See Spot run.Output: Ver correr a Spot.Input: My dog barks.Output: Mi perro ladra.Input: Spot can run fast.Output:\n```\n\n```\n# You can add examples to NGramOverlapExampleSelector as well.new_example = {\"input\": \"Spot plays fetch.\", \"output\": \"Spot juega a buscar.\"}example_selector.add_example(new_example)print(dynamic_prompt.format(sentence=\"Spot can run fast.\"))\n```\n\n```\nGive the Spanish translation of every inputInput: Spot can run.Output: Spot puede correr.Input: See Spot run.Output: Ver correr a Spot.Input: Spot plays fetch.Output: Spot juega a buscar.Input: My dog barks.Output: Mi perro ladra.Input: Spot can run fast.Output:\n```\n\n```\n# You can set a threshold at which examples are excluded.# For example, setting threshold equal to 0.0# excludes examples with no ngram overlaps with input.# Since \"My dog barks.\" has no ngram overlaps with \"Spot can run fast.\"# it is excluded.example_selector.threshold = 0.0print(dynamic_prompt.format(sentence=\"Spot can run fast.\"))\n```\n\n```\nGive the Spanish translation of every inputInput: Spot can run.Output: Spot puede correr.Input: See Spot run.Output: Ver correr a Spot.Input: Spot plays fetch.Output: Spot juega a buscar.Input: Spot can run fast.Output:\n```\n\n```\n# Setting small nonzero thresholdexample_selector.threshold = 0.09print(dynamic_prompt.format(sentence=\"Spot can play fetch.\"))\n```\n\n```\nGive the Spanish translation of every inputInput: Spot can run.Output: Spot puede correr.Input: Spot plays fetch.Output: Spot juega a buscar.Input: Spot can play fetch.Output:\n```\n\n```\n# Setting threshold greater than 1.0example_selector.threshold = 1.0 + 1e-9print(dynamic_prompt.format(sentence=\"Spot can play fetch.\"))\n```\n\n```\nGive the Spanish translation of every inputInput: Spot can play fetch.Output:\n```", "url_final": "https://python.langchain.com/docs/how_to/example_selectors_ngram/", "url_citable": "https://python.langchain.com/docs/how_to/example_selectors_ngram/", "title": "How to select examples by n-gram overlap | ü¶úÔ∏èüîó LangChain", "section_level": 1, "section_heading": "How to select examples by n-gram overlap | ü¶úÔ∏èüîó LangChain", "section_anchor": null, "fetched_at": "2025-10-06T21:55:35.510585", "has_code": true}
{"id": "a8c0fcae48b73ff401b3bce040520ece77138d5f", "text": "Low decay rate‚Äã\n\nA low decay rate (in this, to be extreme, we will set it close to 0) means memories will be \"remembered\" for longer. A decay rate of 0 means memories never be forgotten, making this retriever equivalent to the vector lookup.\n\n```\n# Define your embedding modelembeddings_model = OpenAIEmbeddings()# Initialize the vectorstore as emptyembedding_size = 1536index = faiss.IndexFlatL2(embedding_size)vectorstore = FAISS(embeddings_model, index, InMemoryDocstore({}), {})retriever = TimeWeightedVectorStoreRetriever(    vectorstore=vectorstore, decay_rate=0.0000000000000000000000001, k=1)\n```\n\n```\nyesterday = datetime.now() - timedelta(days=1)retriever.add_documents(    [Document(page_content=\"hello world\", metadata={\"last_accessed_at\": yesterday})])retriever.add_documents([Document(page_content=\"hello foo\")])\n```\n\n```\n['73679bc9-d425-49c2-9d74-de6356c73489']\n```\n\n```\n# \"Hello World\" is returned first because it is most salient, and the decay rate is close to 0., meaning it's still recent enoughretriever.invoke(\"hello world\")\n```\n\n```\n[Document(metadata={'last_accessed_at': datetime.datetime(2024, 10, 22, 16, 37, 40, 818583), 'created_at': datetime.datetime(2024, 10, 22, 16, 37, 37, 975074), 'buffer_idx': 0}, page_content='hello world')]\n```", "url_final": "https://python.langchain.com/docs/how_to/time_weighted_vectorstore/", "url_citable": "https://python.langchain.com/docs/how_to/time_weighted_vectorstore/#low-decay-rate", "title": "How to use a time-weighted vector store retriever | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Low decay rate‚Äã", "section_anchor": "low-decay-rate", "fetched_at": "2025-10-06T21:56:31.097448", "has_code": true}
{"id": "c149714acfcad1edc10f5c1aaf98b84ca0b78d93", "text": "High decay rate‚Äã\n\nWith a high decay rate (e.g., several 9's), the recency score quickly goes to 0! If you set this all the way to 1, recency is 0 for all objects, once again making this equivalent to a vector lookup.\n\n```\n# Define your embedding modelembeddings_model = OpenAIEmbeddings()# Initialize the vectorstore as emptyembedding_size = 1536index = faiss.IndexFlatL2(embedding_size)vectorstore = FAISS(embeddings_model, index, InMemoryDocstore({}), {})retriever = TimeWeightedVectorStoreRetriever(    vectorstore=vectorstore, decay_rate=0.999, k=1)\n```\n\n```\nyesterday = datetime.now() - timedelta(days=1)retriever.add_documents(    [Document(page_content=\"hello world\", metadata={\"last_accessed_at\": yesterday})])retriever.add_documents([Document(page_content=\"hello foo\")])\n```\n\n```\n['379631f0-42c2-4773-8cc2-d36201e1e610']\n```\n\n```\n# \"Hello Foo\" is returned first because \"hello world\" is mostly forgottenretriever.invoke(\"hello world\")\n```\n\n```\n[Document(metadata={'last_accessed_at': datetime.datetime(2024, 10, 22, 16, 37, 46, 553633), 'created_at': datetime.datetime(2024, 10, 22, 16, 37, 43, 927429), 'buffer_idx': 1}, page_content='hello foo')]\n```", "url_final": "https://python.langchain.com/docs/how_to/time_weighted_vectorstore/", "url_citable": "https://python.langchain.com/docs/how_to/time_weighted_vectorstore/#high-decay-rate", "title": "How to use a time-weighted vector store retriever | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "High decay rate‚Äã", "section_anchor": "high-decay-rate", "fetched_at": "2025-10-06T21:56:31.097448", "has_code": true}
{"id": "70cec12ecf817d70ff5546a618a0e9f20172dc60", "text": "Virtual time‚Äã\n\nUsing some utils in LangChain, you can mock out the time component.\n\n```\nfrom langchain_core.utils import mock_now\n```\n\n```\n# Notice the last access time is that date timetomorrow = datetime.now() + timedelta(days=1)with mock_now(tomorrow):    print(retriever.invoke(\"hello world\"))\n```\n\n```\n[Document(metadata={'last_accessed_at': MockDateTime(2024, 10, 23, 16, 38, 19, 66711), 'created_at': datetime.datetime(2024, 10, 22, 16, 37, 43, 599877), 'buffer_idx': 0}, page_content='hello world')]\n```\n\n- Low decay rate\n- High decay rate\n- Virtual time", "url_final": "https://python.langchain.com/docs/how_to/time_weighted_vectorstore/", "url_citable": "https://python.langchain.com/docs/how_to/time_weighted_vectorstore/#virtual-time", "title": "How to use a time-weighted vector store retriever | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Virtual time‚Äã", "section_anchor": "virtual-time", "fetched_at": "2025-10-06T21:56:31.097448", "has_code": true}
{"id": "ad236980f30e403305137e1cc0f448e25d09dd53", "text": "Code Example‚Äã\n\nLet's see a concrete example of what this looks like in code. We will use Pinecone for this example.\n\nTo configure Pinecone, set the following environment variable:\n\n- PINECONE_API_KEY: Your Pinecone API key\n\n```\nfrom langchain_openai import OpenAIEmbeddingsfrom langchain_pinecone import PineconeVectorStoreembeddings = OpenAIEmbeddings()vectorstore = PineconeVectorStore(index_name=\"test-example\", embedding=embeddings)vectorstore.add_texts([\"I worked at Kensho\"], namespace=\"harrison\")vectorstore.add_texts([\"I worked at Facebook\"], namespace=\"ankush\")\n```\n\n```\n['f907aab7-77c7-4347-acc2-6859f8142f92']\n```\n\nThe pinecone kwarg for namespace can be used to separate documents\n\n```\n# This will only get documents for Ankushvectorstore.as_retriever(search_kwargs={\"namespace\": \"ankush\"}).invoke(    \"where did i work?\")\n```\n\n```\n[Document(id='f907aab7-77c7-4347-acc2-6859f8142f92', metadata={}, page_content='I worked at Facebook')]\n```\n\n```\n# This will only get documents for Harrisonvectorstore.as_retriever(search_kwargs={\"namespace\": \"harrison\"}).invoke(    \"where did i work?\")\n```\n\n```\n[Document(id='16061fc5-c6fc-4f45-a3b3-23469d7996af', metadata={}, page_content='I worked at Kensho')]\n```\n\nWe can now create the chain that we will use to do question-answering over.\n\nLet's first select a LLM.\n\n```\npip install -qU \"langchain[google-genai]\"\n```\n\n```\nimport getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n```\n\nThis will follow the basic implementation from the RAG tutorial, but we will allow the retrieval step to be configurable.\n\n```\nfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import ConfigurableFieldtemplate = \"\"\"Answer the question based only on the following context:{context}Question: {question}\"\"\"prompt = ChatPromptTemplate.from_template(template)retriever = vectorstore.as_retriever()\n```\n\nHere we mark the retriever as having a configurable field. All vectorstore retrievers have search_kwargs as a field. This is just a dictionary, with vectorstore specific fields.\n\nThis will let us pass in a value for search_kwargs when invoking the chain.\n\n```\nconfigurable_retriever = retriever.configurable_fields(    search_kwargs=ConfigurableField(        id=\"search_kwargs\",        name=\"Search Kwargs\",        description=\"The search kwargs to use\",    ))\n```\n\nWe can now create the chain using our configurable retriever.\n\n```\nfrom langchain_core.documents import Documentfrom langchain_core.runnables import RunnableConfigfrom langgraph.graph import START, StateGraphfrom typing_extensions import List, TypedDictclass State(TypedDict):    question: str    context: List[Document]    answer: strdef retrieve(state: State, config: RunnableConfig):    retrieved_docs = configurable_retriever.invoke(state[\"question\"], config)    return {\"context\": retrieved_docs}def generate(state: State):    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})    response = llm.invoke(messages)    return {\"answer\": response.content}graph_builder = StateGraph(State).add_sequence([retrieve, generate])graph_builder.add_edge(START, \"retrieve\")graph = graph_builder.compile()\n```\n\n```\nfrom IPython.display import Image, displaydisplay(Image(graph.get_graph().draw_mermaid_png()))\n```\n\nWe can now invoke the chain with configurable options. search_kwargs is the id of the configurable field. The value is the search kwargs to use for Pinecone.\n\n```\nresult = graph.invoke(    {\"question\": \"Where did the user work?\"},    config={\"configurable\": {\"search_kwargs\": {\"namespace\": \"harrison\"}}},)result\n```\n\n```\n{'question': 'Where did the user work?', 'context': [Document(id='16061fc5-c6fc-4f45-a3b3-23469d7996af', metadata={}, page_content='I worked at Kensho')], 'answer': 'The user worked at Kensho.'}\n```\n\n```\nresult = graph.invoke(    {\"question\": \"Where did the user work?\"},    config={\"configurable\": {\"search_kwargs\": {\"namespace\": \"ankush\"}}},)result\n```\n\n```\n{'question': 'Where did the user work?', 'context': [Document(id='f907aab7-77c7-4347-acc2-6859f8142f92', metadata={}, page_content='I worked at Facebook')], 'answer': 'The user worked at Facebook.'}\n```\n\nFor details operating your specific vector store, see the integration pages.\n\n- Code Example", "url_final": "https://python.langchain.com/docs/how_to/qa_per_user/", "url_citable": "https://python.langchain.com/docs/how_to/qa_per_user/#code-example", "title": "How to do per-user retrieval | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Code Example‚Äã", "section_anchor": "code-example", "fetched_at": "2025-10-06T21:56:10.894554", "has_code": true}
{"id": "ca0807d4584d1a219c5f5a75c08ead4ed675c5a4", "text": "How to select examples by similarity | ü¶úÔ∏èüîó LangChain\n\nThis object selects examples based on similarity to the inputs. It does this by finding the examples with the embeddings that have the greatest cosine similarity with the inputs.\n\n```\nfrom langchain_chroma import Chromafrom langchain_core.example_selectors import SemanticSimilarityExampleSelectorfrom langchain_core.prompts import FewShotPromptTemplate, PromptTemplatefrom langchain_openai import OpenAIEmbeddingsexample_prompt = PromptTemplate(    input_variables=[\"input\", \"output\"],    template=\"Input: {input}\\nOutput: {output}\",)# Examples of a pretend task of creating antonyms.examples = [    {\"input\": \"happy\", \"output\": \"sad\"},    {\"input\": \"tall\", \"output\": \"short\"},    {\"input\": \"energetic\", \"output\": \"lethargic\"},    {\"input\": \"sunny\", \"output\": \"gloomy\"},    {\"input\": \"windy\", \"output\": \"calm\"},]\n```\n\n```\nexample_selector = SemanticSimilarityExampleSelector.from_examples(    # The list of examples available to select from.    examples,    # The embedding class used to produce embeddings which are used to measure semantic similarity.    OpenAIEmbeddings(),    # The VectorStore class that is used to store the embeddings and do a similarity search over.    Chroma,    # The number of examples to produce.    k=1,)similar_prompt = FewShotPromptTemplate(    # We provide an ExampleSelector instead of examples.    example_selector=example_selector,    example_prompt=example_prompt,    prefix=\"Give the antonym of every input\",    suffix=\"Input: {adjective}\\nOutput:\",    input_variables=[\"adjective\"],)\n```\n\n```\n# Input is a feeling, so should select the happy/sad exampleprint(similar_prompt.format(adjective=\"worried\"))\n```\n\n```\nGive the antonym of every inputInput: happyOutput: sadInput: worriedOutput:\n```\n\n```\n# Input is a measurement, so should select the tall/short exampleprint(similar_prompt.format(adjective=\"large\"))\n```\n\n```\nGive the antonym of every inputInput: tallOutput: shortInput: largeOutput:\n```\n\n```\n# You can add new examples to the SemanticSimilarityExampleSelector as wellsimilar_prompt.example_selector.add_example(    {\"input\": \"enthusiastic\", \"output\": \"apathetic\"})print(similar_prompt.format(adjective=\"passionate\"))\n```\n\n```\nGive the antonym of every inputInput: enthusiasticOutput: apatheticInput: passionateOutput:\n```", "url_final": "https://python.langchain.com/docs/how_to/example_selectors_similarity/", "url_citable": "https://python.langchain.com/docs/how_to/example_selectors_similarity/", "title": "How to select examples by similarity | ü¶úÔ∏èüîó LangChain", "section_level": 1, "section_heading": "How to select examples by similarity | ü¶úÔ∏èüîó LangChain", "section_anchor": null, "fetched_at": "2025-10-06T21:55:36.600817", "has_code": true}
{"id": "fe10e4159c7004fa45aba320ac56129b23ff319a", "text": "Setup‚Äã\n\n```\n%pip install -qU langchain langchain-community langchain-openai langchain-chroma\n```\n\n```\nNote: you may need to restart the kernel to use updated packages.\n```\n\nWe'll use OpenAI in this example:\n\n```\nimport getpassimport osif \"OPENAI_API_KEY\" not in os.environ:    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()# Optional, uncomment to trace runs with LangSmith. Sign up here: https://smith.langchain.com.# os.environ[\"LANGSMITH_TRACING\"] = \"true\"# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n```\n\nCreate Index‚Äã\n\nWe will create a vectorstore over fake information.\n\n```\nfrom langchain_chroma import Chromafrom langchain_openai import OpenAIEmbeddingsfrom langchain_text_splitters import RecursiveCharacterTextSplittertexts = [\"Harrison worked at Kensho\"]embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")vectorstore = Chroma.from_texts(    texts,    embeddings,)retriever = vectorstore.as_retriever()\n```", "url_final": "https://python.langchain.com/docs/how_to/query_no_queries/", "url_citable": "https://python.langchain.com/docs/how_to/query_no_queries/#setup", "title": "How to handle cases where no queries are generated | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Setup‚Äã", "section_anchor": "setup", "fetched_at": "2025-10-06T21:54:33.266692", "has_code": true}
{"id": "e97f48da97a46ff2596926df6f3f4dc985004133", "text": "Query analysis‚Äã\n\nWe will use function calling to structure the output. However, we will configure the LLM such that is doesn't NEED to call the function representing a search query (should it decide not to). We will also then use a prompt to do query analysis that explicitly lays when it should and shouldn't make a search.\n\n```\nfrom typing import Optionalfrom pydantic import BaseModel, Fieldclass Search(BaseModel):    \"\"\"Search over a database of job records.\"\"\"    query: str = Field(        ...,        description=\"Similarity search query applied to job record.\",    )\n```\n\n```\nfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnablePassthroughfrom langchain_openai import ChatOpenAIsystem = \"\"\"You have the ability to issue search queries to get information to help answer user information.You do not NEED to look things up. If you don't need to, then just respond normally.\"\"\"prompt = ChatPromptTemplate.from_messages(    [        (\"system\", system),        (\"human\", \"{question}\"),    ])llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)structured_llm = llm.bind_tools([Search])query_analyzer = {\"question\": RunnablePassthrough()} | prompt | structured_llm\n```\n\nWe can see that by invoking this we get an message that sometimes - but not always - returns a tool call.\n\n```\nquery_analyzer.invoke(\"where did Harrison Work\")\n```\n\n```\nAIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_korLZrh08PTRL94f4L7rFqdj', 'function': {'arguments': '{\"query\":\"Harrison\"}', 'name': 'Search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 95, 'total_tokens': 109}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_483d39d857', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-ea94d376-37bf-4f80-abe6-e3b42b767ea0-0', tool_calls=[{'name': 'Search', 'args': {'query': 'Harrison'}, 'id': 'call_korLZrh08PTRL94f4L7rFqdj', 'type': 'tool_call'}], usage_metadata={'input_tokens': 95, 'output_tokens': 14, 'total_tokens': 109})\n```\n\n```\nquery_analyzer.invoke(\"hi!\")\n```\n\n```\nAIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 93, 'total_tokens': 103}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_483d39d857', 'finish_reason': 'stop', 'logprobs': None}, id='run-ebdfc44a-455a-4ca6-be85-84559886b1e1-0', usage_metadata={'input_tokens': 93, 'output_tokens': 10, 'total_tokens': 103})\n```", "url_final": "https://python.langchain.com/docs/how_to/query_no_queries/", "url_citable": "https://python.langchain.com/docs/how_to/query_no_queries/#query-analysis", "title": "How to handle cases where no queries are generated | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Query analysis‚Äã", "section_anchor": "query-analysis", "fetched_at": "2025-10-06T21:54:33.266692", "has_code": true}
{"id": "9cc07e4126600814ce292f0201651adb58ccd70b", "text": "Retrieval with query analysis‚Äã\n\nSo how would we include this in a chain? Let's look at an example below.\n\n```\nfrom langchain_core.output_parsers.openai_tools import PydanticToolsParserfrom langchain_core.runnables import chainoutput_parser = PydanticToolsParser(tools=[Search])\n```\n\n```\n@chaindef custom_chain(question):    response = query_analyzer.invoke(question)    if \"tool_calls\" in response.additional_kwargs:        query = output_parser.invoke(response)        docs = retriever.invoke(query[0].query)        # Could add more logic - like another LLM call - here        return docs    else:        return response\n```\n\n```\ncustom_chain.invoke(\"where did Harrison Work\")\n```\n\n```\nNumber of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n```\n\n```\n[Document(page_content='Harrison worked at Kensho')]\n```\n\n```\ncustom_chain.invoke(\"hi!\")\n```\n\n```\nAIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 93, 'total_tokens': 103}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_483d39d857', 'finish_reason': 'stop', 'logprobs': None}, id='run-e87f058d-30c0-4075-8a89-a01b982d557e-0', usage_metadata={'input_tokens': 93, 'output_tokens': 10, 'total_tokens': 103})\n```\n\n- Create Index", "url_final": "https://python.langchain.com/docs/how_to/query_no_queries/", "url_citable": "https://python.langchain.com/docs/how_to/query_no_queries/#retrieval-with-query-analysis", "title": "How to handle cases where no queries are generated | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Retrieval with query analysis‚Äã", "section_anchor": "retrieval-with-query-analysis", "fetched_at": "2025-10-06T21:54:33.266692", "has_code": true}
{"id": "2c90c0fc1dbfba00607fba8c416ae5c26dfcf766", "text": "End-to-end agent‚Äã\n\nThe code snippet below represents a fully functional agent that uses an LLM to decide which tools to use. It is equipped with a generic search tool. It has conversational memory - meaning that it can be used as a multi-turn chatbot.\n\nIn the rest of the guide, we will walk through the individual components and what each part does - but if you want to just grab some code and get started, feel free to use this!\n\n```\n# Import relevant functionalityfrom langchain.chat_models import init_chat_modelfrom langchain_tavily import TavilySearchfrom langgraph.checkpoint.memory import MemorySaverfrom langgraph.prebuilt import create_react_agent# Create the agentmemory = MemorySaver()model = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\")search = TavilySearch(max_results=2)tools = [search]agent_executor = create_react_agent(model, tools, checkpointer=memory)\n```\n\n```\n# Use the agentconfig = {\"configurable\": {\"thread_id\": \"abc123\"}}input_message = {    \"role\": \"user\",    \"content\": \"Hi, I'm Bob and I live in SF.\",}for step in agent_executor.stream(    {\"messages\": [input_message]}, config, stream_mode=\"values\"):    step[\"messages\"][-1].pretty_print()\n```\n\n```\n================================\u001b[1m Human Message \u001b[0m=================================Hi, I'm Bob and I live in SF.==================================\u001b[1m Ai Message \u001b[0m==================================Hello Bob! I notice you've introduced yourself and mentioned you live in SF (San Francisco), but you haven't asked a specific question or made a request that requires the use of any tools. Is there something specific you'd like to know about San Francisco or any other topic? I'd be happy to help you find information using the available search tools.\n```\n\n```\ninput_message = {    \"role\": \"user\",    \"content\": \"What's the weather where I live?\",}for step in agent_executor.stream(    {\"messages\": [input_message]}, config, stream_mode=\"values\"):    step[\"messages\"][-1].pretty_print()\n```\n\n```\n================================\u001b[1m Human Message \u001b[0m=================================What's the weather where I live?==================================\u001b[1m Ai Message \u001b[0m==================================[{'text': 'Let me search for current weather information in San Francisco.', 'type': 'text'}, {'id': 'toolu_011kSdheoJp8THURoLmeLtZo', 'input': {'query': 'current weather San Francisco CA'}, 'name': 'tavily_search', 'type': 'tool_use'}]Tool Calls:  tavily_search (toolu_011kSdheoJp8THURoLmeLtZo) Call ID: toolu_011kSdheoJp8THURoLmeLtZo  Args:    query: current weather San Francisco CA=================================\u001b[1m Tool Message \u001b[0m=================================Name: tavily_search{\"query\": \"current weather San Francisco CA\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"Weather in San Francisco, CA\", \"url\": \"https://www.weatherapi.com/\", \"content\": \"{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.775, 'lon': -122.4183, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1750168606, 'localtime': '2025-06-17 06:56'}, 'current': {'last_updated_epoch': 1750167900, 'last_updated': '2025-06-17 06:45', 'temp_c': 11.7, 'temp_f': 53.1, 'is_day': 1, 'condition': {'text': 'Fog', 'icon': '//cdn.weatherapi.com/weather/64x64/day/248.png', 'code': 1135}, 'wind_mph': 4.0, 'wind_kph': 6.5, 'wind_degree': 215, 'wind_dir': 'SW', 'pressure_mb': 1017.0, 'pressure_in': 30.02, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 86, 'cloud': 0, 'feelslike_c': 11.3, 'feelslike_f': 52.4, 'windchill_c': 8.7, 'windchill_f': 47.7, 'heatindex_c': 9.8, 'heatindex_f': 49.7, 'dewpoint_c': 9.6, 'dewpoint_f': 49.2, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 0.0, 'gust_mph': 6.3, 'gust_kph': 10.2}}\", \"score\": 0.944705, \"raw_content\": null}, {\"title\": \"Weather in San Francisco in June 2025\", \"url\": \"https://world-weather.info/forecast/usa/san_francisco/june-2025/\", \"content\": \"Detailed ‚ö° San Francisco Weather Forecast for June 2025 - day/night üå°Ô∏è temperatures, precipitations - World-Weather.info. Add the current city. Search. Weather; Archive; Weather Widget ¬∞F. World; United States; California; Weather in San Francisco; ... 17 +64¬∞ +54¬∞ 18 +61¬∞ +54¬∞ 19\", \"score\": 0.86441374, \"raw_content\": null}], \"response_time\": 2.34}==================================\u001b[1m Ai Message \u001b[0m==================================Based on the search results, here's the current weather in San Francisco:- Temperature: 53.1¬∞F (11.7¬∞C)- Condition: Foggy- Wind: 4.0 mph from the Southwest- Humidity: 86%- Visibility: 9 milesThis is quite typical weather for San Francisco, with the characteristic fog that the city is known for. Would you like to know anything else about the weather or San Francisco in general?\n```", "url_final": "https://python.langchain.com/docs/tutorials/agents/", "url_citable": "https://python.langchain.com/docs/tutorials/agents/#end-to-end-agent", "title": "Build an Agent | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "End-to-end agent‚Äã", "section_anchor": "end-to-end-agent", "fetched_at": "2025-10-06T21:54:09.434085", "has_code": true}
{"id": "98c4ea73be73569ac8fe418a93f1640e71f417d2", "text": "Setup‚Äã\n\nJupyter Notebook‚Äã\n\nThis guide (and most of the other guides in the documentation) uses Jupyter notebooks and assumes the reader is as well. Jupyter notebooks are perfect interactive environments for learning how to work with LLM systems because oftentimes things can go wrong (unexpected output, API down, etc), and observing these cases is a great way to better understand building with LLMs.\n\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebook. See here for instructions on how to install.\n\nInstallation‚Äã\n\nTo install LangChain run:\n\n```\n%pip install -U langgraph langchain-tavily langgraph-checkpoint-sqlite\n```\n\nFor more details, see our Installation guide.\n\nLangSmith‚Äã\n\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\n\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\n\n```\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"\n```\n\nOr, if in a notebook, you can set them with:\n\n```\nimport getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n```\n\nTavily‚Äã\n\nWe will be using Tavily (a search engine) as a tool. In order to use it, you will need to get and set an API key:\n\n```\nexport TAVILY_API_KEY=\"...\"\n```\n\nOr, if in a notebook, you can set it with:\n\n```\nimport getpassimport osos.environ[\"TAVILY_API_KEY\"] = getpass.getpass()\n```", "url_final": "https://python.langchain.com/docs/tutorials/agents/", "url_citable": "https://python.langchain.com/docs/tutorials/agents/#setup", "title": "Build an Agent | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Setup‚Äã", "section_anchor": "setup", "fetched_at": "2025-10-06T21:54:09.434085", "has_code": true}
{"id": "adf6e48b658b00386bc396b787443da44f29a389", "text": "Define tools‚Äã\n\nWe first need to create the tools we want to use. Our main tool of choice will be Tavily - a search engine. We can use the dedicated langchain-tavily integration package to easily use Tavily search engine as tool with LangChain.\n\n```\nfrom langchain_tavily import TavilySearchsearch = TavilySearch(max_results=2)search_results = search.invoke(\"What is the weather in SF\")print(search_results)# If we want, we can create other tools.# Once we have all the tools we want, we can put them in a list that we will reference later.tools = [search]\n```\n\n```\n{'query': 'What is the weather in SF', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'Weather in San Francisco, CA', 'url': 'https://www.weatherapi.com/', 'content': \"{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.775, 'lon': -122.4183, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1750168606, 'localtime': '2025-06-17 06:56'}, 'current': {'last_updated_epoch': 1750167900, 'last_updated': '2025-06-17 06:45', 'temp_c': 11.7, 'temp_f': 53.1, 'is_day': 1, 'condition': {'text': 'Fog', 'icon': '//cdn.weatherapi.com/weather/64x64/day/248.png', 'code': 1135}, 'wind_mph': 4.0, 'wind_kph': 6.5, 'wind_degree': 215, 'wind_dir': 'SW', 'pressure_mb': 1017.0, 'pressure_in': 30.02, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 86, 'cloud': 0, 'feelslike_c': 11.3, 'feelslike_f': 52.4, 'windchill_c': 8.7, 'windchill_f': 47.7, 'heatindex_c': 9.8, 'heatindex_f': 49.7, 'dewpoint_c': 9.6, 'dewpoint_f': 49.2, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 0.0, 'gust_mph': 6.3, 'gust_kph': 10.2}}\", 'score': 0.9185379, 'raw_content': None}, {'title': 'Weather in San Francisco in June 2025', 'url': 'https://world-weather.info/forecast/usa/san_francisco/june-2025/', 'content': \"Weather in San Francisco in June 2025 (California) - Detailed Weather Forecast for a Month *   Weather in San Francisco Weather in San Francisco in June 2025 *   1 +63¬∞ +55¬∞ *   2 +66¬∞ +54¬∞ *   3 +66¬∞ +55¬∞ *   4 +66¬∞ +54¬∞ *   5 +66¬∞ +55¬∞ *   6 +66¬∞ +57¬∞ *   7 +64¬∞ +55¬∞ *   8 +63¬∞ +55¬∞ *   9 +63¬∞ +54¬∞ *   10 +59¬∞ +54¬∞ *   11 +59¬∞ +54¬∞ *   12 +61¬∞ +54¬∞ Weather in Washington, D.C.**+68¬∞** Sacramento**+81¬∞** Pleasanton**+72¬∞** Redwood City**+68¬∞** San Leandro**+61¬∞** San Mateo**+64¬∞** San Rafael**+70¬∞** San Ramon**+64¬∞** South San Francisco**+61¬∞** Daly City**+59¬∞** Wilder**+66¬∞** Woodacre**+70¬∞** world's temperature today Colchani day+50¬∞F night+16¬∞F Az Zubayr day+124¬∞F night+93¬∞F Weather forecast on your site Install _San Francisco_ +61¬∞ Temperature units\", 'score': 0.7978881, 'raw_content': None}], 'response_time': 2.62}\n```\n\nIn many applications, you may want to define custom tools. LangChain supports custom tool creation via Python functions and other means. Refer to the How to create tools guide for details.", "url_final": "https://python.langchain.com/docs/tutorials/agents/", "url_citable": "https://python.langchain.com/docs/tutorials/agents/#define-tools", "title": "Build an Agent | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Define tools‚Äã", "section_anchor": "define-tools", "fetched_at": "2025-10-06T21:54:09.434085", "has_code": true}
{"id": "b8bab28d69075f879a872cbfb83565e5d194e8b7", "text": "Using Language Models‚Äã\n\nNext, let's learn how to use a language model to call tools. LangChain supports many different language models that you can use interchangably - select the one you want to use below!\n\n```\npip install -qU \"langchain[google-genai]\"\n```\n\n```\nimport getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelmodel = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n```\n\nYou can call the language model by passing in a list of messages. By default, the response is a content string.\n\n```\nquery = \"Hi!\"response = model.invoke([{\"role\": \"user\", \"content\": query}])response.text()\n```\n\n```\n'Hello! How can I help you today?'\n```\n\nWe can now see what it is like to enable this model to do tool calling. In order to enable that we use .bind_tools to give the language model knowledge of these tools\n\n```\nmodel_with_tools = model.bind_tools(tools)\n```\n\nWe can now call the model. Let's first call it with a normal message, and see how it responds. We can look at both the content field as well as the tool_calls field.\n\n```\nquery = \"Hi!\"response = model_with_tools.invoke([{\"role\": \"user\", \"content\": query}])print(f\"Message content: {response.text()}\\n\")print(f\"Tool calls: {response.tool_calls}\")\n```\n\n```\nMessage content: Hello! I'm here to help you. I have access to a powerful search tool that can help answer questions and find information about various topics. What would you like to know about?Feel free to ask any question or request information, and I'll do my best to assist you using the available tools.Tool calls: []\n```\n\nNow, let's try calling it with some input that would expect a tool to be called.\n\n```\nquery = \"Search for the weather in SF\"response = model_with_tools.invoke([{\"role\": \"user\", \"content\": query}])print(f\"Message content: {response.text()}\\n\")print(f\"Tool calls: {response.tool_calls}\")\n```\n\n```\nMessage content: I'll help you search for information about the weather in San Francisco.Tool calls: [{'name': 'tavily_search', 'args': {'query': 'current weather San Francisco'}, 'id': 'toolu_015gdPn1jbB2Z21DmN2RAnti', 'type': 'tool_call'}]\n```\n\nWe can see that there's now no text content, but there is a tool call! It wants us to call the Tavily Search tool.\n\nThis isn't calling that tool yet - it's just telling us to. In order to actually call it, we'll want to create our agent.", "url_final": "https://python.langchain.com/docs/tutorials/agents/", "url_citable": "https://python.langchain.com/docs/tutorials/agents/#using-language-models", "title": "Build an Agent | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Using Language Models‚Äã", "section_anchor": "using-language-models", "fetched_at": "2025-10-06T21:54:09.434085", "has_code": true}
{"id": "e2063ebe9beb347a62aeddf7fd9f5ced24c7d407", "text": "Create the agent‚Äã\n\nNow that we have defined the tools and the LLM, we can create the agent. We will be using LangGraph to construct the agent. Currently, we are using a high level interface to construct the agent, but the nice thing about LangGraph is that this high-level interface is backed by a low-level, highly controllable API in case you want to modify the agent logic.\n\nNow, we can initialize the agent with the LLM and the tools.\n\nNote that we are passing in the model, not model_with_tools. That is because create_react_agent will call .bind_tools for us under the hood.\n\n```\nfrom langgraph.prebuilt import create_react_agentagent_executor = create_react_agent(model, tools)\n```", "url_final": "https://python.langchain.com/docs/tutorials/agents/", "url_citable": "https://python.langchain.com/docs/tutorials/agents/#create-the-agent", "title": "Build an Agent | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Create the agent‚Äã", "section_anchor": "create-the-agent", "fetched_at": "2025-10-06T21:54:09.434085", "has_code": true}
{"id": "b9d6ff2eec0a78d85fb7f732503fc8e4ba40e0f0", "text": "Run the agent‚Äã\n\nWe can now run the agent with a few queries! Note that for now, these are all stateless queries (it won't remember previous interactions). Note that the agent will return the final state at the end of the interaction (which includes any inputs, we will see later on how to get only the outputs).\n\nFirst up, let's see how it responds when there's no need to call a tool:\n\n```\ninput_message = {\"role\": \"user\", \"content\": \"Hi!\"}response = agent_executor.invoke({\"messages\": [input_message]})for message in response[\"messages\"]:    message.pretty_print()\n```\n\n```\n================================\u001b[1m Human Message \u001b[0m=================================Hi!==================================\u001b[1m Ai Message \u001b[0m==================================Hello! I'm here to help you with your questions using the available search tools. Please feel free to ask any question, and I'll do my best to find relevant and accurate information for you.\n```\n\nIn order to see exactly what is happening under the hood (and to make sure it's not calling a tool) we can take a look at the LangSmith trace\n\nLet's now try it out on an example where it should be invoking the tool\n\n```\ninput_message = {\"role\": \"user\", \"content\": \"Search for the weather in SF\"}response = agent_executor.invoke({\"messages\": [input_message]})for message in response[\"messages\"]:    message.pretty_print()\n```\n\n```\n================================\u001b[1m Human Message \u001b[0m=================================Search for the weather in SF==================================\u001b[1m Ai Message \u001b[0m==================================[{'text': \"I'll help you search for weather information in San Francisco. Let me use the search engine to find current weather conditions.\", 'type': 'text'}, {'id': 'toolu_01WWcXGnArosybujpKzdmARZ', 'input': {'query': 'current weather San Francisco SF'}, 'name': 'tavily_search', 'type': 'tool_use'}]Tool Calls:  tavily_search (toolu_01WWcXGnArosybujpKzdmARZ) Call ID: toolu_01WWcXGnArosybujpKzdmARZ  Args:    query: current weather San Francisco SF=================================\u001b[1m Tool Message \u001b[0m=================================Name: tavily_search{\"query\": \"current weather San Francisco SF\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"Weather in San Francisco, CA\", \"url\": \"https://www.weatherapi.com/\", \"content\": \"{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.775, 'lon': -122.4183, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1750168606, 'localtime': '2025-06-17 06:56'}, 'current': {'last_updated_epoch': 1750167900, 'last_updated': '2025-06-17 06:45', 'temp_c': 11.7, 'temp_f': 53.1, 'is_day': 1, 'condition': {'text': 'Fog', 'icon': '//cdn.weatherapi.com/weather/64x64/day/248.png', 'code': 1135}, 'wind_mph': 4.0, 'wind_kph': 6.5, 'wind_degree': 215, 'wind_dir': 'SW', 'pressure_mb': 1017.0, 'pressure_in': 30.02, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 86, 'cloud': 0, 'feelslike_c': 11.3, 'feelslike_f': 52.4, 'windchill_c': 8.7, 'windchill_f': 47.7, 'heatindex_c': 9.8, 'heatindex_f': 49.7, 'dewpoint_c': 9.6, 'dewpoint_f': 49.2, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 0.0, 'gust_mph': 6.3, 'gust_kph': 10.2}}\", \"score\": 0.885373, \"raw_content\": null}, {\"title\": \"Weather in San Francisco in June 2025\", \"url\": \"https://world-weather.info/forecast/usa/san_francisco/june-2025/\", \"content\": \"Detailed ‚ö° San Francisco Weather Forecast for June 2025 - day/night üå°Ô∏è temperatures, precipitations - World-Weather.info. Add the current city. Search. Weather; Archive; Weather Widget ¬∞F. World; United States; California; Weather in San Francisco; ... 17 +64¬∞ +54¬∞ 18 +61¬∞ +54¬∞ 19\", \"score\": 0.8830044, \"raw_content\": null}], \"response_time\": 2.6}==================================\u001b[1m Ai Message \u001b[0m==================================Based on the search results, here's the current weather in San Francisco:- Temperature: 53.1¬∞F (11.7¬∞C)- Conditions: Foggy- Wind: 4.0 mph from the SW- Humidity: 86%- Visibility: 9.0 milesThe weather appears to be typical for San Francisco, with morning fog and mild temperatures. The \"feels like\" temperature is 52.4¬∞F (11.3¬∞C).\n```\n\nWe can check out the LangSmith trace to make sure it's calling the search tool effectively.", "url_final": "https://python.langchain.com/docs/tutorials/agents/", "url_citable": "https://python.langchain.com/docs/tutorials/agents/#run-the-agent", "title": "Build an Agent | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Run the agent‚Äã", "section_anchor": "run-the-agent", "fetched_at": "2025-10-06T21:54:09.434085", "has_code": true}
{"id": "501dbc4ea739c252ad3466e4ed3ff34e823e2d38", "text": "Streaming Messages‚Äã\n\nWe've seen how the agent can be called with .invoke to get a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\n\n```\nfor step in agent_executor.stream({\"messages\": [input_message]}, stream_mode=\"values\"):    step[\"messages\"][-1].pretty_print()\n```\n\n```\n================================\u001b[1m Human Message \u001b[0m=================================Search for the weather in SF==================================\u001b[1m Ai Message \u001b[0m==================================[{'text': \"I'll help you search for information about the weather in San Francisco.\", 'type': 'text'}, {'id': 'toolu_01DCPnJES53Fcr7YWnZ47kDG', 'input': {'query': 'current weather San Francisco'}, 'name': 'tavily_search', 'type': 'tool_use'}]Tool Calls:  tavily_search (toolu_01DCPnJES53Fcr7YWnZ47kDG) Call ID: toolu_01DCPnJES53Fcr7YWnZ47kDG  Args:    query: current weather San Francisco=================================\u001b[1m Tool Message \u001b[0m=================================Name: tavily_search{\"query\": \"current weather San Francisco\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"Weather in San Francisco\", \"url\": \"https://www.weatherapi.com/\", \"content\": \"{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.775, 'lon': -122.4183, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1750168506, 'localtime': '2025-06-17 06:55'}, 'current': {'last_updated_epoch': 1750167900, 'last_updated': '2025-06-17 06:45', 'temp_c': 11.7, 'temp_f': 53.1, 'is_day': 1, 'condition': {'text': 'Fog', 'icon': '//cdn.weatherapi.com/weather/64x64/day/248.png', 'code': 1135}, 'wind_mph': 4.0, 'wind_kph': 6.5, 'wind_degree': 215, 'wind_dir': 'SW', 'pressure_mb': 1017.0, 'pressure_in': 30.02, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 86, 'cloud': 0, 'feelslike_c': 11.3, 'feelslike_f': 52.4, 'windchill_c': 8.7, 'windchill_f': 47.7, 'heatindex_c': 9.8, 'heatindex_f': 49.7, 'dewpoint_c': 9.6, 'dewpoint_f': 49.2, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 0.0, 'gust_mph': 6.3, 'gust_kph': 10.2}}\", \"score\": 0.9542825, \"raw_content\": null}, {\"title\": \"Weather in San Francisco in June 2025\", \"url\": \"https://world-weather.info/forecast/usa/san_francisco/june-2025/\", \"content\": \"Detailed ‚ö° San Francisco Weather Forecast for June 2025 - day/night üå°Ô∏è temperatures, precipitations - World-Weather.info. Add the current city. Search. Weather; Archive; Weather Widget ¬∞F. World; United States; California; Weather in San Francisco; ... 17 +64¬∞ +54¬∞ 18 +61¬∞ +54¬∞ 19\", \"score\": 0.8638634, \"raw_content\": null}], \"response_time\": 2.57}==================================\u001b[1m Ai Message \u001b[0m==================================Based on the search results, here's the current weather in San Francisco:- Temperature: 53.1¬∞F (11.7¬∞C)- Condition: Foggy- Wind: 4.0 mph from the Southwest- Humidity: 86%- Visibility: 9.0 miles- Feels like: 52.4¬∞F (11.3¬∞C)This is quite typical weather for San Francisco, which is known for its fog, especially during the morning hours. The city's proximity to the ocean and unique geographical features often result in mild temperatures and foggy conditions.\n```", "url_final": "https://python.langchain.com/docs/tutorials/agents/", "url_citable": "https://python.langchain.com/docs/tutorials/agents/#streaming-messages", "title": "Build an Agent | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Streaming Messages‚Äã", "section_anchor": "streaming-messages", "fetched_at": "2025-10-06T21:54:09.434085", "has_code": true}
{"id": "725c209b74727e3199038f8a7ad445b2c7b1caaa", "text": "Streaming tokens‚Äã\n\nIn addition to streaming back messages, it is also useful to stream back tokens. We can do this by specifying stream_mode=\"messages\".\n\n::: note\n\nBelow we use message.text(), which requires langchain-core>=0.3.37.\n\n:::\n\n```\nfor step, metadata in agent_executor.stream(    {\"messages\": [input_message]}, config, stream_mode=\"messages\"):    if metadata[\"langgraph_node\"] == \"agent\" and (text := step.text()):        print(text, end=\"|\")\n```\n\n```\nI|'ll help you search for information| about the weather in San Francisco.|Base|d on the search results, here|'s the current weather in| San Francisco:-| Temperature: 53.1¬∞F (|11.7¬∞C)-| Condition: Foggy- Wind:| 4.0 mph from| the Southwest- Humidity|: 86%|- Visibility: 9|.0 miles- Pressure: |30.02 in|HgThe weather| is characteristic of San Francisco, with| foggy conditions and mild temperatures|. The \"feels like\" temperature is slightly| lower at 52.4|¬∞F (11.|3¬∞C)| due to the wind chill effect|.|\n```", "url_final": "https://python.langchain.com/docs/tutorials/agents/", "url_citable": "https://python.langchain.com/docs/tutorials/agents/#streaming-tokens", "title": "Build an Agent | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Streaming tokens‚Äã", "section_anchor": "streaming-tokens", "fetched_at": "2025-10-06T21:54:09.434085", "has_code": true}
{"id": "1c380946a922bdc24c3adaa7cdc793f54d834339", "text": "Adding in memory‚Äã\n\nAs mentioned earlier, this agent is stateless. This means it does not remember previous interactions. To give it memory we need to pass in a checkpointer. When passing in a checkpointer, we also have to pass in a thread_id when invoking the agent (so it knows which thread/conversation to resume from).\n\n```\nfrom langgraph.checkpoint.memory import MemorySavermemory = MemorySaver()\n```\n\n```\nagent_executor = create_react_agent(model, tools, checkpointer=memory)config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n```\n\n```\nfor step in agent_executor.stream(    {\"messages\": [(\"user\", \"Hi, I'm Bob!\")]}, config, stream_mode=\"values\"):    step[\"messages\"][-1].pretty_print()\n```\n\n```\n================================\u001b[1m Human Message \u001b[0m=================================Hi, I'm Bob!==================================\u001b[1m Ai Message \u001b[0m==================================Hello Bob! I'm an AI assistant who can help you search for information using specialized search tools. Is there anything specific you'd like to know about or search for? I'm happy to help you find accurate and up-to-date information on various topics.\n```\n\n```\nfor step in agent_executor.stream(    {\"messages\": [(\"user\", \"What is my name?\")]}, config, stream_mode=\"values\"):    step[\"messages\"][-1].pretty_print()\n```\n\n```\n================================\u001b[1m Human Message \u001b[0m=================================What's my name?==================================\u001b[1m Ai Message \u001b[0m==================================Your name is Bob, as you introduced yourself earlier. I can remember information shared within our conversation without needing to search for it.\n```\n\nExample LangSmith trace\n\nIf you want to start a new conversation, all you have to do is change the thread_id used\n\n```\nconfig = {\"configurable\": {\"thread_id\": \"xyz123\"}}for step in agent_executor.stream(    {\"messages\": [(\"user\", \"What is my name?\")]}, config, stream_mode=\"values\"):    step[\"messages\"][-1].pretty_print()\n```\n\n```\n================================\u001b[1m Human Message \u001b[0m=================================What's my name?==================================\u001b[1m Ai Message \u001b[0m==================================I apologize, but I don't have access to any tools that would tell me your name. I can only assist you with searching for publicly available information using the tavily_search function. I don't have access to personal information about users. If you'd like to tell me your name, I'll be happy to address you by it.\n```", "url_final": "https://python.langchain.com/docs/tutorials/agents/", "url_citable": "https://python.langchain.com/docs/tutorials/agents/#adding-in-memory", "title": "Build an Agent | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Adding in memory‚Äã", "section_anchor": "adding-in-memory", "fetched_at": "2025-10-06T21:54:09.434085", "has_code": true}
{"id": "d3a9cdd9653d73a5174225bab4d4077b02815d5e", "text": "Conclusion‚Äã\n\nThat's a wrap! In this quick start we covered how to create a simple agent. We've then shown how to stream back a response - not only with the intermediate steps, but also tokens! We've also added in memory so you can have a conversation with them. Agents are a complex topic with lots to learn!\n\nFor more information on Agents, please check out the LangGraph documentation. This has it's own set of concepts, tutorials, and how-to guides.\n\n- Jupyter Notebook\n- Installation\n- LangSmith\n- Tavily", "url_final": "https://python.langchain.com/docs/tutorials/agents/", "url_citable": "https://python.langchain.com/docs/tutorials/agents/#conclusion", "title": "Build an Agent | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Conclusion‚Äã", "section_anchor": "conclusion", "fetched_at": "2025-10-06T21:54:09.434085", "has_code": false}
{"id": "24d4b4e123c35fb8f6ea0e0a29483988a0685fda", "text": "Concepts‚Äã\n\nConcepts we will cover are:\n\n- Using language models, in particular their tool calling ability\n- Creating a Retriever to expose specific information to our agent\n- Using a Search Tool to look up things online\n- Chat History, which allows a chatbot to \"remember\" past interactions and take them into account when responding to follow-up questions.\n- Debugging and tracing your application using LangSmith", "url_final": "https://python.langchain.com/docs/how_to/agent_executor/", "url_citable": "https://python.langchain.com/docs/how_to/agent_executor/#concepts", "title": "Build an Agent with AgentExecutor (Legacy) | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Concepts‚Äã", "section_anchor": "concepts", "fetched_at": "2025-10-06T21:54:41.471556", "has_code": false}
{"id": "aa37cdfea29425f9040e9a9ca0ce051bfb7f6b6a", "text": "Setup‚Äã\n\nJupyter Notebook‚Äã\n\nThis guide (and most of the other guides in the documentation) uses Jupyter notebooks and assumes the reader is as well. Jupyter notebooks are perfect for learning how to work with LLM systems because oftentimes things can go wrong (unexpected output, API down, etc) and going through guides in an interactive environment is a great way to better understand them.\n\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebook. See here for instructions on how to install.\n\nInstallation‚Äã\n\nTo install LangChain run:\n\n- Pip\n- Conda\n\n```\npip install langchain\n```\n\n```\nconda install langchain -c conda-forge\n```\n\nFor more details, see our Installation guide.\n\nLangSmith‚Äã\n\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\n\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\n\n```\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"\n```\n\nOr, if in a notebook, you can set them with:\n\n```\nimport getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n```", "url_final": "https://python.langchain.com/docs/how_to/agent_executor/", "url_citable": "https://python.langchain.com/docs/how_to/agent_executor/#setup", "title": "Build an Agent with AgentExecutor (Legacy) | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Setup‚Äã", "section_anchor": "setup", "fetched_at": "2025-10-06T21:54:41.471556", "has_code": true}
{"id": "9b61460a6f423a45ecb8f36c18b51d64034d6d85", "text": "Define tools‚Äã\n\nWe first need to create the tools we want to use. We will use two tools: Tavily (to search online) and then a retriever over a local index we will create\n\nTavily‚Äã\n\nWe have a built-in tool in LangChain to easily use Tavily search engine as tool. Note that this requires an API key - they have a free tier, but if you don't have one or don't want to create one, you can always ignore this step.\n\nOnce you create your API key, you will need to export that as:\n\n```\nexport TAVILY_API_KEY=\"...\"\n```\n\n```\nfrom langchain_community.tools.tavily_search import TavilySearchResults\n```\n\n```\nsearch = TavilySearchResults(max_results=2)\n```\n\n```\nsearch.invoke(\"what is the weather in SF\")\n```\n\n```\n[{'url': 'https://www.weatherapi.com/',  'content': \"{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.78, 'lon': -122.42, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1714000492, 'localtime': '2024-04-24 16:14'}, 'current': {'last_updated_epoch': 1713999600, 'last_updated': '2024-04-24 16:00', 'temp_c': 15.6, 'temp_f': 60.1, 'is_day': 1, 'condition': {'text': 'Overcast', 'icon': '//cdn.weatherapi.com/weather/64x64/day/122.png', 'code': 1009}, 'wind_mph': 10.5, 'wind_kph': 16.9, 'wind_degree': 330, 'wind_dir': 'NNW', 'pressure_mb': 1018.0, 'pressure_in': 30.06, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 72, 'cloud': 100, 'feelslike_c': 15.6, 'feelslike_f': 60.1, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 5.0, 'gust_mph': 14.8, 'gust_kph': 23.8}}\"}, {'url': 'https://www.weathertab.com/en/c/e/04/united-states/california/san-francisco/',  'content': 'San Francisco Weather Forecast for Apr 2024 - Risk of Rain Graph. Rain Risk Graph: Monthly Overview. Bar heights indicate rain risk percentages. Yellow bars mark low-risk days, while black and grey bars signal higher risks. Grey-yellow bars act as buffers, advising to keep at least one day clear from the riskier grey and black days, guiding ...'}]\n```", "url_final": "https://python.langchain.com/docs/how_to/agent_executor/", "url_citable": "https://python.langchain.com/docs/how_to/agent_executor/#define-tools", "title": "Build an Agent with AgentExecutor (Legacy) | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Define tools‚Äã", "section_anchor": "define-tools", "fetched_at": "2025-10-06T21:54:41.471556", "has_code": true}
{"id": "5360e30180588f370db8bf7f4f6dc44c78756fbe", "text": "Retriever‚Äã\n\nWe will also create a retriever over some data of our own. For a deeper explanation of each step here, see this tutorial.\n\n```\nfrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_community.vectorstores import FAISSfrom langchain_openai import OpenAIEmbeddingsfrom langchain_text_splitters import RecursiveCharacterTextSplitterloader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")docs = loader.load()documents = RecursiveCharacterTextSplitter(    chunk_size=1000, chunk_overlap=200).split_documents(docs)vector = FAISS.from_documents(documents, OpenAIEmbeddings())retriever = vector.as_retriever()\n```\n\n```\nretriever.invoke(\"how to upload a dataset\")[0]\n```\n\n```\nDocument(page_content='# The data to predict and grade over    evaluators=[exact_match], # The evaluators to score the results    experiment_prefix=\"sample-experiment\", # The name of the experiment    metadata={      \"version\": \"1.0.0\",      \"revision_id\": \"beta\"    },)import { Client, Run, Example } from \\'langsmith\\';import { runOnDataset } from \\'langchain/smith\\';import { EvaluationResult } from \\'langsmith/evaluation\\';const client = new Client();// Define dataset: these are your test casesconst datasetName = \"Sample Dataset\";const dataset = await client.createDataset(datasetName, {    description: \"A sample dataset in LangSmith.\"});await client.createExamples({    inputs: [        { postfix: \"to LangSmith\" },        { postfix: \"to Evaluations in LangSmith\" },    ],    outputs: [        { output: \"Welcome to LangSmith\" },        { output: \"Welcome to Evaluations in LangSmith\" },    ],    datasetId: dataset.id,});// Define your evaluatorconst exactMatch = async ({ run, example }: { run: Run; example?:', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'Getting started with LangSmith | \\uf8ff√º¬∂√∫√î‚àè√®\\uf8ff√º√µ‚Ä†√î‚àè√® LangSmith', 'description': 'Introduction', 'language': 'en'})\n```\n\nNow that we have populated our index that we will do doing retrieval over, we can easily turn it into a tool (the format needed for an agent to properly use it)\n\n```\nfrom langchain.tools.retriever import create_retriever_tool\n```\n\n```\nretriever_tool = create_retriever_tool(    retriever,    \"langsmith_search\",    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",)\n```", "url_final": "https://python.langchain.com/docs/how_to/agent_executor/", "url_citable": "https://python.langchain.com/docs/how_to/agent_executor/#retriever", "title": "Build an Agent with AgentExecutor (Legacy) | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Retriever‚Äã", "section_anchor": "retriever", "fetched_at": "2025-10-06T21:54:41.471556", "has_code": true}
{"id": "70d990ac584a897e59f334e927af1bd083741071", "text": "Tools‚Äã\n\nNow that we have created both, we can create a list of tools that we will use downstream.\n\n```\ntools = [search, retriever_tool]\n```", "url_final": "https://python.langchain.com/docs/how_to/agent_executor/", "url_citable": "https://python.langchain.com/docs/how_to/agent_executor/#tools", "title": "Build an Agent with AgentExecutor (Legacy) | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Tools‚Äã", "section_anchor": "tools", "fetched_at": "2025-10-06T21:54:41.471556", "has_code": true}
{"id": "d9f15e50e08d58eda2357f6c6fe75f8520b8903a", "text": "Using Language Models‚Äã\n\nNext, let's learn how to use a language model by to call tools. LangChain supports many different language models that you can use interchangably - select the one you want to use below!\n\n```\npip install -qU \"langchain[google-genai]\"\n```\n\n```\nimport getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelmodel = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n```\n\nYou can call the language model by passing in a list of messages. By default, the response is a content string.\n\n```\nfrom langchain_core.messages import HumanMessageresponse = model.invoke([HumanMessage(content=\"hi!\")])response.content\n```\n\n```\n'Hello! How can I assist you today?'\n```\n\nWe can now see what it is like to enable this model to do tool calling. In order to enable that we use .bind_tools to give the language model knowledge of these tools\n\n```\nmodel_with_tools = model.bind_tools(tools)\n```\n\nWe can now call the model. Let's first call it with a normal message, and see how it responds. We can look at both the content field as well as the tool_calls field.\n\n```\nresponse = model_with_tools.invoke([HumanMessage(content=\"Hi!\")])print(f\"ContentString: {response.content}\")print(f\"ToolCalls: {response.tool_calls}\")\n```\n\n```\nContentString: Hello! How can I assist you today?ToolCalls: []\n```\n\nNow, let's try calling it with some input that would expect a tool to be called.\n\n```\nresponse = model_with_tools.invoke([HumanMessage(content=\"What's the weather in SF?\")])print(f\"ContentString: {response.content}\")print(f\"ToolCalls: {response.tool_calls}\")\n```\n\n```\nContentString: ToolCalls: [{'name': 'tavily_search_results_json', 'args': {'query': 'current weather in San Francisco'}, 'id': 'call_4HteVahXkRAkWjp6dGXryKZX'}]\n```\n\nWe can see that there's now no content, but there is a tool call! It wants us to call the Tavily Search tool.\n\nThis isn't calling that tool yet - it's just telling us to. In order to actually calll it, we'll want to create our agent.", "url_final": "https://python.langchain.com/docs/how_to/agent_executor/", "url_citable": "https://python.langchain.com/docs/how_to/agent_executor/#using-language-models", "title": "Build an Agent with AgentExecutor (Legacy) | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Using Language Models‚Äã", "section_anchor": "using-language-models", "fetched_at": "2025-10-06T21:54:41.471556", "has_code": true}
{"id": "a9e5ccb725d98cfe78e97fd259e4d64e9462d02e", "text": "Create the agent‚Äã\n\nNow that we have defined the tools and the LLM, we can create the agent. We will be using a tool calling agent - for more information on this type of agent, as well as other options, see this guide.\n\nWe can first choose the prompt we want to use to guide the agent.\n\nIf you want to see the contents of this prompt and have access to LangSmith, you can go to:\n\nhttps://smith.langchain.com/hub/hwchase17/openai-functions-agent\n\n```\nfrom langchain import hub# Get the prompt to use - you can modify this!prompt = hub.pull(\"hwchase17/openai-functions-agent\")prompt.messages\n```\n\n```\n[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')]\n```\n\nNow, we can initialize the agent with the LLM, the prompt, and the tools. The agent is responsible for taking in input and deciding what actions to take. Crucially, the Agent does not execute those actions - that is done by the AgentExecutor (next step). For more information about how to think about these components, see our conceptual guide.\n\nNote that we are passing in the model, not model_with_tools. That is because create_tool_calling_agent will call .bind_tools for us under the hood.\n\n```\nfrom langchain.agents import create_tool_calling_agentagent = create_tool_calling_agent(model, tools, prompt)\n```\n\nFinally, we combine the agent (the brains) with the tools inside the AgentExecutor (which will repeatedly call the agent and execute tools).\n\n```\nfrom langchain.agents import AgentExecutoragent_executor = AgentExecutor(agent=agent, tools=tools)\n```", "url_final": "https://python.langchain.com/docs/how_to/agent_executor/", "url_citable": "https://python.langchain.com/docs/how_to/agent_executor/#create-the-agent", "title": "Build an Agent with AgentExecutor (Legacy) | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Create the agent‚Äã", "section_anchor": "create-the-agent", "fetched_at": "2025-10-06T21:54:41.471556", "has_code": true}
{"id": "8f76293a4a08585ec2b9b0b9b153139c13131351", "text": "Run the agent‚Äã\n\nWe can now run the agent on a few queries! Note that for now, these are all stateless queries (it won't remember previous interactions).\n\nFirst up, let's how it responds when there's no need to call a tool:\n\n```\nagent_executor.invoke({\"input\": \"hi!\"})\n```\n\n```\n{'input': 'hi!', 'output': 'Hello! How can I assist you today?'}\n```\n\nIn order to see exactly what is happening under the hood (and to make sure it's not calling a tool) we can take a look at the LangSmith trace\n\nLet's now try it out on an example where it should be invoking the retriever\n\n```\nagent_executor.invoke({\"input\": \"how can langsmith help with testing?\"})\n```\n\n```\n{'input': 'how can langsmith help with testing?', 'output': 'LangSmith is a platform that aids in building production-grade Language Learning Model (LLM) applications. It can assist with testing in several ways:\\n\\n1. **Monitoring and Evaluation**: LangSmith allows close monitoring and evaluation of your application. This helps you to ensure the quality of your application and deploy it with confidence.\\n\\n2. **Tracing**: LangSmith has tracing capabilities that can be beneficial for debugging and understanding the behavior of your application.\\n\\n3. **Evaluation Capabilities**: LangSmith has built-in tools for evaluating the performance of your LLM. \\n\\n4. **Prompt Hub**: This is a prompt management tool built into LangSmith that can help in testing different prompts and their responses.\\n\\nPlease note that to use LangSmith, you would need to install it and create an API key. The platform offers Python and Typescript SDKs for utilization. It works independently and does not require the use of LangChain.'}\n```\n\nLet's take a look at the LangSmith trace to make sure it's actually calling that.\n\nNow let's try one where it needs to call the search tool:\n\n```\nagent_executor.invoke({\"input\": \"whats the weather in sf?\"})\n```\n\n```\n{'input': 'whats the weather in sf?', 'output': 'The current weather in San Francisco is partly cloudy with a temperature of 16.1¬∞C (61.0¬∞F). The wind is coming from the WNW at a speed of 10.5 mph. The humidity is at 67%. [source](https://www.weatherapi.com/)'}\n```\n\nWe can check out the LangSmith trace to make sure it's calling the search tool effectively.", "url_final": "https://python.langchain.com/docs/how_to/agent_executor/", "url_citable": "https://python.langchain.com/docs/how_to/agent_executor/#run-the-agent", "title": "Build an Agent with AgentExecutor (Legacy) | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Run the agent‚Äã", "section_anchor": "run-the-agent", "fetched_at": "2025-10-06T21:54:41.471556", "has_code": true}
{"id": "6dbb11f8e7f4b408b9feac32e401ed2a80761cfd", "text": "Adding in memory‚Äã\n\nAs mentioned earlier, this agent is stateless. This means it does not remember previous interactions. To give it memory we need to pass in previous chat_history. Note: it needs to be called chat_history because of the prompt we are using. If we use a different prompt, we could change the variable name\n\n```\n# Here we pass in an empty list of messages for chat_history because it is the first message in the chatagent_executor.invoke({\"input\": \"hi! my name is bob\", \"chat_history\": []})\n```\n\n```\n{'input': 'hi! my name is bob', 'chat_history': [], 'output': 'Hello Bob! How can I assist you today?'}\n```\n\n```\nfrom langchain_core.messages import AIMessage, HumanMessage\n```\n\n```\nagent_executor.invoke(    {        \"chat_history\": [            HumanMessage(content=\"hi! my name is bob\"),            AIMessage(content=\"Hello Bob! How can I assist you today?\"),        ],        \"input\": \"what's my name?\",    })\n```\n\n```\n{'chat_history': [HumanMessage(content='hi! my name is bob'),  AIMessage(content='Hello Bob! How can I assist you today?')], 'input': \"what's my name?\", 'output': 'Your name is Bob. How can I assist you further?'}\n```\n\nIf we want to keep track of these messages automatically, we can wrap this in a RunnableWithMessageHistory. For more information on how to use this, see this guide.\n\n```\nfrom langchain_community.chat_message_histories import ChatMessageHistoryfrom langchain_core.chat_history import BaseChatMessageHistoryfrom langchain_core.runnables.history import RunnableWithMessageHistorystore = {}def get_session_history(session_id: str) -> BaseChatMessageHistory:    if session_id not in store:        store[session_id] = ChatMessageHistory()    return store[session_id]\n```\n\nBecause we have multiple inputs, we need to specify two things:\n\n- input_messages_key: The input key to use to add to the conversation history.\n- history_messages_key: The key to add the loaded messages into.\n\n```\nagent_with_chat_history = RunnableWithMessageHistory(    agent_executor,    get_session_history,    input_messages_key=\"input\",    history_messages_key=\"chat_history\",)\n```\n\n```\nagent_with_chat_history.invoke(    {\"input\": \"hi! I'm bob\"},    config={\"configurable\": {\"session_id\": \"<foo>\"}},)\n```\n\n```\n{'input': \"hi! I'm bob\", 'chat_history': [], 'output': 'Hello Bob! How can I assist you today?'}\n```\n\n```\nagent_with_chat_history.invoke(    {\"input\": \"what's my name?\"},    config={\"configurable\": {\"session_id\": \"<foo>\"}},)\n```\n\n```\n{'input': \"what's my name?\", 'chat_history': [HumanMessage(content=\"hi! I'm bob\"),  AIMessage(content='Hello Bob! How can I assist you today?')], 'output': 'Your name is Bob.'}\n```\n\nExample LangSmith trace: https://smith.langchain.com/public/98c8d162-60ae-4493-aa9f-992d87bd0429/r", "url_final": "https://python.langchain.com/docs/how_to/agent_executor/", "url_citable": "https://python.langchain.com/docs/how_to/agent_executor/#adding-in-memory", "title": "Build an Agent with AgentExecutor (Legacy) | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Adding in memory‚Äã", "section_anchor": "adding-in-memory", "fetched_at": "2025-10-06T21:54:41.471556", "has_code": true}
{"id": "597510b20a7f6ad6c6471e3755016fe5a17b21c0", "text": "Conclusion‚Äã\n\nThat's a wrap! In this quick start we covered how to create a simple agent. Agents are a complex topic, and there's lot to learn!\n\nThis section covered building with LangChain Agents. They are fine for getting started, but past a certain point you will likely want flexibility and control which they do not offer. To develop more advanced agents, we recommend checking out LangGraph\n\nIf you want to continue using LangChain agents, some good advanced guides are:\n\n- How to use LangGraph's built-in versions of AgentExecutor\n- How to create a custom agent\n- How to stream responses from an agent\n- How to return structured output from an agent\n\n- Jupyter Notebook\n- Installation\n- LangSmith\n\n- Tavily\n- Retriever\n- Tools", "url_final": "https://python.langchain.com/docs/how_to/agent_executor/", "url_citable": "https://python.langchain.com/docs/how_to/agent_executor/#conclusion", "title": "Build an Agent with AgentExecutor (Legacy) | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Conclusion‚Äã", "section_anchor": "conclusion", "fetched_at": "2025-10-06T21:54:41.471556", "has_code": false}
{"id": "cf91a0f73cbc55dd5cf939ac9395693f4d185605", "text": "Motivation‚Äã\n\nMany chat or Q+A applications involve chunking input documents prior to embedding and vector storage.\n\nThese notes from Pinecone provide some useful tips:\n\n```\nWhen a full paragraph or document is embedded, the embedding process considers both the overall context and the relationships between the sentences and phrases within the text. This can result in a more comprehensive vector representation that captures the broader meaning and themes of the text.\n```\n\nAs mentioned, chunking often aims to keep text with common context together. With this in mind, we might want to specifically honor the structure of the document itself. For example, a markdown file is organized by headers. Creating chunks within specific header groups is an intuitive idea. To address this challenge, we can use MarkdownHeaderTextSplitter. This will split a markdown file by a specified set of headers.\n\nFor example, if we want to split this markdown:\n\n```\nmd = '# Foo\\n\\n ## Bar\\n\\nHi this is Jim  \\nHi this is Joe\\n\\n ## Baz\\n\\n Hi this is Molly' \n```\n\nWe can specify the headers to split on:\n\n```\n[(\"#\", \"Header 1\"),(\"##\", \"Header 2\")]\n```\n\nAnd content is grouped or split by common headers:\n\n```\n{'content': 'Hi this is Jim  \\nHi this is Joe', 'metadata': {'Header 1': 'Foo', 'Header 2': 'Bar'}}{'content': 'Hi this is Molly', 'metadata': {'Header 1': 'Foo', 'Header 2': 'Baz'}}\n```\n\nLet's have a look at some examples below.\n\nBasic usage:‚Äã\n\n```\n%pip install -qU langchain-text-splitters\n```\n\n```\nfrom langchain_text_splitters import MarkdownHeaderTextSplitter\n```\n\n```\nmarkdown_document = \"# Foo\\n\\n    ## Bar\\n\\nHi this is Jim\\n\\nHi this is Joe\\n\\n ### Boo \\n\\n Hi this is Lance \\n\\n ## Baz\\n\\n Hi this is Molly\"headers_to_split_on = [    (\"#\", \"Header 1\"),    (\"##\", \"Header 2\"),    (\"###\", \"Header 3\"),]markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on)md_header_splits = markdown_splitter.split_text(markdown_document)md_header_splits\n```\n\n```\n[Document(page_content='Hi this is Jim  \\nHi this is Joe', metadata={'Header 1': 'Foo', 'Header 2': 'Bar'}), Document(page_content='Hi this is Lance', metadata={'Header 1': 'Foo', 'Header 2': 'Bar', 'Header 3': 'Boo'}), Document(page_content='Hi this is Molly', metadata={'Header 1': 'Foo', 'Header 2': 'Baz'})]\n```\n\n```\ntype(md_header_splits[0])\n```\n\n```\nlangchain_core.documents.base.Document\n```\n\nBy default, MarkdownHeaderTextSplitter strips headers being split on from the output chunk's content. This can be disabled by setting strip_headers = False.\n\n```\nmarkdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on, strip_headers=False)md_header_splits = markdown_splitter.split_text(markdown_document)md_header_splits\n```\n\n```\n[Document(page_content='# Foo  \\n## Bar  \\nHi this is Jim  \\nHi this is Joe', metadata={'Header 1': 'Foo', 'Header 2': 'Bar'}), Document(page_content='### Boo  \\nHi this is Lance', metadata={'Header 1': 'Foo', 'Header 2': 'Bar', 'Header 3': 'Boo'}), Document(page_content='## Baz  \\nHi this is Molly', metadata={'Header 1': 'Foo', 'Header 2': 'Baz'})]\n```\n\nThe default MarkdownHeaderTextSplitter strips white spaces and new lines. To preserve the original formatting of your Markdown documents, check out ExperimentalMarkdownSyntaxTextSplitter.", "url_final": "https://python.langchain.com/docs/how_to/markdown_header_metadata_splitter/", "url_citable": "https://python.langchain.com/docs/how_to/markdown_header_metadata_splitter/", "title": "How to split Markdown by Headers | ü¶úÔ∏èüîó LangChain", "section_level": 1, "section_heading": null, "section_anchor": null, "fetched_at": "2025-10-06T21:55:52.156550", "has_code": true}
{"id": "365b05771378c4bc8541cfb10334d6037e842561", "text": "How to return Markdown lines as separate documents‚Äã\n\nBy default, MarkdownHeaderTextSplitter aggregates lines based on the headers specified in headers_to_split_on. We can disable this by specifying return_each_line:\n\n```\nmarkdown_splitter = MarkdownHeaderTextSplitter(    headers_to_split_on,    return_each_line=True,)md_header_splits = markdown_splitter.split_text(markdown_document)md_header_splits\n```\n\n```\n[Document(page_content='Hi this is Jim', metadata={'Header 1': 'Foo', 'Header 2': 'Bar'}), Document(page_content='Hi this is Joe', metadata={'Header 1': 'Foo', 'Header 2': 'Bar'}), Document(page_content='Hi this is Lance', metadata={'Header 1': 'Foo', 'Header 2': 'Bar', 'Header 3': 'Boo'}), Document(page_content='Hi this is Molly', metadata={'Header 1': 'Foo', 'Header 2': 'Baz'})]\n```\n\nNote that here header information is retained in the metadata for each document.", "url_final": "https://python.langchain.com/docs/how_to/markdown_header_metadata_splitter/", "url_citable": "https://python.langchain.com/docs/how_to/markdown_header_metadata_splitter/#how-to-return-markdown-lines-as-separate-documents", "title": "How to split Markdown by Headers | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "How to return Markdown lines as separate documents‚Äã", "section_anchor": "how-to-return-markdown-lines-as-separate-documents", "fetched_at": "2025-10-06T21:55:52.156550", "has_code": true}
{"id": "c3188c6e3f34b0fb9f77eec9a623bbc5a670b3fc", "text": "How to constrain chunk size:‚Äã\n\nWithin each markdown group we can then apply any text splitter we want, such as RecursiveCharacterTextSplitter, which allows for further control of the chunk size.\n\n```\nmarkdown_document = \"# Intro \\n\\n    ## History \\n\\n Markdown[9] is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9] \\n\\n Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files. \\n\\n ## Rise and divergence \\n\\n As Markdown popularity grew rapidly, many Markdown implementations appeared, driven mostly by the need for \\n\\n additional features such as tables, footnotes, definition lists,[note 1] and Markdown inside HTML blocks. \\n\\n #### Standardization \\n\\n From 2012, a group of people, including Jeff Atwood and John MacFarlane, launched what Atwood characterised as a standardisation effort. \\n\\n ## Implementations \\n\\n Implementations of Markdown are available for over a dozen programming languages.\"headers_to_split_on = [    (\"#\", \"Header 1\"),    (\"##\", \"Header 2\"),]# MD splitsmarkdown_splitter = MarkdownHeaderTextSplitter(    headers_to_split_on=headers_to_split_on, strip_headers=False)md_header_splits = markdown_splitter.split_text(markdown_document)# Char-level splitsfrom langchain_text_splitters import RecursiveCharacterTextSplitterchunk_size = 250chunk_overlap = 30text_splitter = RecursiveCharacterTextSplitter(    chunk_size=chunk_size, chunk_overlap=chunk_overlap)# Splitsplits = text_splitter.split_documents(md_header_splits)splits\n```\n\n```\n[Document(page_content='# Intro  \\n## History  \\nMarkdown[9] is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9]', metadata={'Header 1': 'Intro', 'Header 2': 'History'}), Document(page_content='Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files.', metadata={'Header 1': 'Intro', 'Header 2': 'History'}), Document(page_content='## Rise and divergence  \\nAs Markdown popularity grew rapidly, many Markdown implementations appeared, driven mostly by the need for  \\nadditional features such as tables, footnotes, definition lists,[note 1] and Markdown inside HTML blocks.', metadata={'Header 1': 'Intro', 'Header 2': 'Rise and divergence'}), Document(page_content='#### Standardization  \\nFrom 2012, a group of people, including Jeff Atwood and John MacFarlane, launched what Atwood characterised as a standardisation effort.', metadata={'Header 1': 'Intro', 'Header 2': 'Rise and divergence'}), Document(page_content='## Implementations  \\nImplementations of Markdown are available for over a dozen programming languages.', metadata={'Header 1': 'Intro', 'Header 2': 'Implementations'})]\n```\n\n- Motivation\n- Basic usage:\n- How to return Markdown lines as separate documents\n- How to constrain chunk size:", "url_final": "https://python.langchain.com/docs/how_to/markdown_header_metadata_splitter/", "url_citable": "https://python.langchain.com/docs/how_to/markdown_header_metadata_splitter/", "title": "How to split Markdown by Headers | ü¶úÔ∏èüîó LangChain", "section_level": 1, "section_heading": null, "section_anchor": null, "fetched_at": "2025-10-06T21:55:52.156550", "has_code": true}
{"id": "8fff67772f64cceed9644d5f7a87ee367e2c8fb1", "text": "Loading HTML with Unstructured‚Äã\n\n```\n%pip install unstructured\n```\n\n```\nfrom langchain_community.document_loaders import UnstructuredHTMLLoaderfile_path = \"../../docs/integrations/document_loaders/example_data/fake-content.html\"loader = UnstructuredHTMLLoader(file_path)data = loader.load()print(data)\n```\n\n```\n[Document(page_content='My First Heading\\n\\nMy first paragraph.', metadata={'source': '../../docs/integrations/document_loaders/example_data/fake-content.html'})]\n```", "url_final": "https://python.langchain.com/docs/how_to/document_loader_html/", "url_citable": "https://python.langchain.com/docs/how_to/document_loader_html/#loading-html-with-unstructured", "title": "How to load HTML | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Loading HTML with Unstructured‚Äã", "section_anchor": "loading-html-with-unstructured", "fetched_at": "2025-10-06T21:55:22.016532", "has_code": true}
{"id": "0d68619ed2f3999658f7e23c1cb64d2e04444040", "text": "Loading HTML with BeautifulSoup4‚Äã\n\nWe can also use BeautifulSoup4 to load HTML documents using the BSHTMLLoader. This will extract the text from the HTML into page_content, and the page title as title into metadata.\n\n```\n%pip install bs4\n```\n\n```\nfrom langchain_community.document_loaders import BSHTMLLoaderloader = BSHTMLLoader(file_path)data = loader.load()print(data)\n```\n\n```\n[Document(page_content='\\nTest Title\\n\\n\\nMy First Heading\\nMy first paragraph.\\n\\n\\n', metadata={'source': '../../docs/integrations/document_loaders/example_data/fake-content.html', 'title': 'Test Title'})]\n```\n\n- Loading HTML with Unstructured\n- Loading HTML with BeautifulSoup4", "url_final": "https://python.langchain.com/docs/how_to/document_loader_html/", "url_citable": "https://python.langchain.com/docs/how_to/document_loader_html/#loading-html-with-beautifulsoup4", "title": "How to load HTML | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Loading HTML with BeautifulSoup4‚Äã", "section_anchor": "loading-html-with-beautifulsoup4", "fetched_at": "2025-10-06T21:55:22.016532", "has_code": true}
{"id": "7d46a504efef1c0790d2af5e9efda1c3357be1fa", "text": "Show a progress bar‚Äã\n\nBy default a progress bar will not be shown. To show a progress bar, install the tqdm library (e.g. pip install tqdm), and set the show_progress parameter to True.\n\n```\nloader = DirectoryLoader(\"../\", glob=\"**/*.md\", show_progress=True)docs = loader.load()\n```\n\n```\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\u0000\u0000‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 54.56it/s]\n```", "url_final": "https://python.langchain.com/docs/how_to/document_loader_directory/", "url_citable": "https://python.langchain.com/docs/how_to/document_loader_directory/#show-a-progress-bar", "title": "How to load documents from a directory | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Show a progress bar‚Äã", "section_anchor": "show-a-progress-bar", "fetched_at": "2025-10-06T21:55:20.840548", "has_code": true}
{"id": "54134397cf7a063cb32875b8e491216917f457b1", "text": "Use multithreading‚Äã\n\nBy default the loading happens in one thread. In order to utilize several threads set the use_multithreading flag to true.\n\n```\nloader = DirectoryLoader(\"../\", glob=\"**/*.md\", use_multithreading=True)docs = loader.load()\n```", "url_final": "https://python.langchain.com/docs/how_to/document_loader_directory/", "url_citable": "https://python.langchain.com/docs/how_to/document_loader_directory/#use-multithreading", "title": "How to load documents from a directory | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Use multithreading‚Äã", "section_anchor": "use-multithreading", "fetched_at": "2025-10-06T21:55:20.840548", "has_code": true}
{"id": "b028a7cad009a68d9d0a0f744093002b71c5342b", "text": "Change loader class‚Äã\n\nBy default this uses the UnstructuredLoader class. To customize the loader, specify the loader class in the loader_cls kwarg. Below we show an example using TextLoader:\n\n```\nfrom langchain_community.document_loaders import TextLoaderloader = DirectoryLoader(\"../\", glob=\"**/*.md\", loader_cls=TextLoader)docs = loader.load()\n```\n\n```\nprint(docs[0].page_content[:100])\n```\n\n```\n# SecurityLangChain has a large ecosystem of integrations with various external resources like loc\n```\n\nNotice that while the UnstructuredLoader parses Markdown headers, TextLoader does not.\n\nIf you need to load Python source code files, use the PythonLoader:\n\n```\nfrom langchain_community.document_loaders import PythonLoaderloader = DirectoryLoader(\"../../../../../\", glob=\"**/*.py\", loader_cls=PythonLoader)\n```", "url_final": "https://python.langchain.com/docs/how_to/document_loader_directory/", "url_citable": "https://python.langchain.com/docs/how_to/document_loader_directory/#change-loader-class", "title": "How to load documents from a directory | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Change loader class‚Äã", "section_anchor": "change-loader-class", "fetched_at": "2025-10-06T21:55:20.840548", "has_code": true}
{"id": "63070c36c15c102d3b06e2f9190ee26fad3d619d", "text": "Auto-detect file encodings with TextLoader‚Äã\n\nDirectoryLoader can help manage errors due to variations in file encodings. Below we will attempt to load in a collection of files, one of which includes non-UTF8 encodings.\n\n```\npath = \"../../../libs/langchain/tests/unit_tests/examples/\"loader = DirectoryLoader(path, glob=\"**/*.txt\", loader_cls=TextLoader)\n```\n\nA. Default Behavior‚Äã\n\nBy default we raise an error:\n\n```\nloader.load()\n```\n\n```\nError loading file ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\n```\n---------------------------------------------------------------------------``````outputUnicodeDecodeError                        Traceback (most recent call last)``````outputFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:43, in TextLoader.lazy_load(self)     42     with open(self.file_path, encoding=self.encoding) as f:---> 43         text = f.read()     44 except UnicodeDecodeError as e:``````outputFile ~/.pyenv/versions/3.10.4/lib/python3.10/codecs.py:322, in BufferedIncrementalDecoder.decode(self, input, final)    321 data = self.buffer + input--> 322 (result, consumed) = self._buffer_decode(data, self.errors, final)    323 # keep undecoded input until the next call``````outputUnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte``````outputThe above exception was the direct cause of the following exception:``````outputRuntimeError                              Traceback (most recent call last)``````outputCell In[10], line 1----> 1 loader.load()``````outputFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:117, in DirectoryLoader.load(self)    115 def load(self) -> List[Document]:    116     \"\"\"Load documents.\"\"\"--> 117     return list(self.lazy_load())``````outputFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:182, in DirectoryLoader.lazy_load(self)    180 else:    181     for i in items:--> 182         yield from self._lazy_load_file(i, p, pbar)    184 if pbar:    185     pbar.close()``````outputFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:220, in DirectoryLoader._lazy_load_file(self, item, path, pbar)    218     else:    219         logger.error(f\"Error loading file {str(item)}\")--> 220         raise e    221 finally:    222     if pbar:``````outputFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:210, in DirectoryLoader._lazy_load_file(self, item, path, pbar)    208 loader = self.loader_cls(str(item), **self.loader_kwargs)    209 try:--> 210     for subdoc in loader.lazy_load():    211         yield subdoc    212 except NotImplementedError:``````outputFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:56, in TextLoader.lazy_load(self)     54                 continue     55     else:---> 56         raise RuntimeError(f\"Error loading {self.file_path}\") from e     57 except Exception as e:     58     raise RuntimeError(f\"Error loading {self.file_path}\") from e``````outputRuntimeError: Error loading ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\nThe file example-non-utf8.txt uses a different encoding, so the load() function fails with a helpful message indicating which file failed decoding.\n\nWith the default behavior of TextLoader any failure to load any of the documents will fail the whole loading process and no documents are loaded.", "url_final": "https://python.langchain.com/docs/how_to/document_loader_directory/", "url_citable": "https://python.langchain.com/docs/how_to/document_loader_directory/#auto-detect-file-encodings-with-textloader", "title": "How to load documents from a directory | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Auto-detect file encodings with TextLoader‚Äã", "section_anchor": "auto-detect-file-encodings-with-textloader", "fetched_at": "2025-10-06T21:55:20.840548", "has_code": true}
{"id": "4bd14e10beb579d6d030ed0200d5470d909ac3bb", "text": "B. Silent fail‚Äã\n\nWe can pass the parameter silent_errors to the DirectoryLoader to skip the files which could not be loaded and continue the load process.\n\n```\nloader = DirectoryLoader(    path, glob=\"**/*.txt\", loader_cls=TextLoader, silent_errors=True)docs = loader.load()\n```\n\n```\nError loading file ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt: Error loading ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\n```\ndoc_sources = [doc.metadata[\"source\"] for doc in docs]doc_sources\n```\n\n```\n['../../../../libs/langchain/tests/unit_tests/examples/example-utf8.txt']\n```", "url_final": "https://python.langchain.com/docs/how_to/document_loader_directory/", "url_citable": "https://python.langchain.com/docs/how_to/document_loader_directory/#b-silent-fail", "title": "How to load documents from a directory | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "B. Silent fail‚Äã", "section_anchor": "b-silent-fail", "fetched_at": "2025-10-06T21:55:20.840548", "has_code": true}
{"id": "b0d1cd5405fbd1c56b8ce3f46575008b95d0c309", "text": "C. Auto detect encodings‚Äã\n\nWe can also ask TextLoader to auto detect the file encoding before failing, by passing the autodetect_encoding to the loader class.\n\n```\ntext_loader_kwargs = {\"autodetect_encoding\": True}loader = DirectoryLoader(    path, glob=\"**/*.txt\", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)docs = loader.load()\n```\n\n```\ndoc_sources = [doc.metadata[\"source\"] for doc in docs]doc_sources\n```\n\n```\n['../../../../libs/langchain/tests/unit_tests/examples/example-utf8.txt', '../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt']\n```\n\n- A. Default Behavior\n- B. Silent fail\n- C. Auto detect encodings", "url_final": "https://python.langchain.com/docs/how_to/document_loader_directory/", "url_citable": "https://python.langchain.com/docs/how_to/document_loader_directory/#c-auto-detect-encodings", "title": "How to load documents from a directory | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "C. Auto detect encodings‚Äã", "section_anchor": "c-auto-detect-encodings", "fetched_at": "2025-10-06T21:55:20.840548", "has_code": true}
{"id": "fac3b967992ca5fe2b15233ad99af0fe6e632916", "text": "Retrieving full documents‚Äã\n\nIn this mode, we want to retrieve the full documents. Therefore, we only specify a child splitter.\n\n```\n# This text splitter is used to create the child documentschild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)# The vectorstore to use to index the child chunksvectorstore = Chroma(    collection_name=\"full_documents\", embedding_function=OpenAIEmbeddings())# The storage layer for the parent documentsstore = InMemoryStore()retriever = ParentDocumentRetriever(    vectorstore=vectorstore,    docstore=store,    child_splitter=child_splitter,)\n```\n\n```\nretriever.add_documents(docs, ids=None)\n```\n\nThis should yield two keys, because we added two documents.\n\n```\nlist(store.yield_keys())\n```\n\n```\n['9a63376c-58cc-42c9-b0f7-61f0e1a3a688', '40091598-e918-4a18-9be0-f46413a95ae4']\n```\n\nLet's now call the vector store search functionality - we should see that it returns small chunks (since we're storing the small chunks).\n\n```\nsub_docs = vectorstore.similarity_search(\"justice breyer\")\n```\n\n```\nprint(sub_docs[0].page_content)\n```\n\n```\nTonight, I‚Äôd like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer‚Äîan Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.\n```\n\nLet's now retrieve from the overall retriever. This should return large documents - since it returns the documents where the smaller chunks are located.\n\n```\nretrieved_docs = retriever.invoke(\"justice breyer\")\n```\n\n```\nlen(retrieved_docs[0].page_content)\n```\n\n```\n38540\n```", "url_final": "https://python.langchain.com/docs/how_to/parent_document_retriever/", "url_citable": "https://python.langchain.com/docs/how_to/parent_document_retriever/#retrieving-full-documents", "title": "How to use the Parent Document Retriever | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Retrieving full documents‚Äã", "section_anchor": "retrieving-full-documents", "fetched_at": "2025-10-06T21:56:06.606594", "has_code": true}
{"id": "8d3b599a166453ebad597cc1dbe0090b2428b848", "text": "Retrieving larger chunks‚Äã\n\nSometimes, the full documents can be too big to want to retrieve them as is. In that case, what we really want to do is to first split the raw documents into larger chunks, and then split it into smaller chunks. We then index the smaller chunks, but on retrieval we retrieve the larger chunks (but still not the full documents).\n\n```\n# This text splitter is used to create the parent documentsparent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)# This text splitter is used to create the child documents# It should create documents smaller than the parentchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)# The vectorstore to use to index the child chunksvectorstore = Chroma(    collection_name=\"split_parents\", embedding_function=OpenAIEmbeddings())# The storage layer for the parent documentsstore = InMemoryStore()\n```\n\n```\nretriever = ParentDocumentRetriever(    vectorstore=vectorstore,    docstore=store,    child_splitter=child_splitter,    parent_splitter=parent_splitter,)\n```\n\n```\nretriever.add_documents(docs)\n```\n\nWe can see that there are much more than two documents now - these are the larger chunks.\n\n```\nlen(list(store.yield_keys()))\n```\n\n```\n66\n```\n\nLet's make sure the underlying vector store still retrieves the small chunks.\n\n```\nsub_docs = vectorstore.similarity_search(\"justice breyer\")\n```\n\n```\nprint(sub_docs[0].page_content)\n```\n\n```\nTonight, I‚Äôd like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer‚Äîan Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.\n```\n\n```\nretrieved_docs = retriever.invoke(\"justice breyer\")\n```\n\n```\nlen(retrieved_docs[0].page_content)\n```\n\n```\n1849\n```\n\n```\nprint(retrieved_docs[0].page_content)\n```\n\n```\nIn state after state, new laws have been passed, not only to suppress the vote, but to subvert entire elections. We cannot let this happen. Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you‚Äôre at it, pass the Disclose Act so Americans can know who is funding our elections. Tonight, I‚Äôd like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer‚Äîan Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation‚Äôs top legal minds, who will continue Justice Breyer‚Äôs legacy of excellence. A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she‚Äôs been nominated, she‚Äôs received a broad range of support‚Äîfrom the Fraternal Order of Police to former judges appointed by Democrats and Republicans. And if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. We can do both. At our border, we‚Äôve installed new technology like cutting-edge scanners to better detect drug smuggling.  We‚Äôve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  We‚Äôre putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. We‚Äôre securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.\n```\n\n- Retrieving full documents\n- Retrieving larger chunks", "url_final": "https://python.langchain.com/docs/how_to/parent_document_retriever/", "url_citable": "https://python.langchain.com/docs/how_to/parent_document_retriever/#retrieving-larger-chunks", "title": "How to use the Parent Document Retriever | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Retrieving larger chunks‚Äã", "section_anchor": "retrieving-larger-chunks", "fetched_at": "2025-10-06T21:56:06.606594", "has_code": true}
{"id": "d4af4fc2103c8a7892215153f3f5b1010e19b954", "text": "Next steps‚Äã\n\nYou've now learned how to prompt a model to return YAML. Next, check out the broader guide on obtaining structured output for other related techniques.\n\n- Next steps", "url_final": "https://python.langchain.com/docs/how_to/output_parser_yaml/", "url_citable": "https://python.langchain.com/docs/how_to/output_parser_yaml/#next-steps", "title": "How to parse YAML output | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Next steps‚Äã", "section_anchor": "next-steps", "fetched_at": "2025-10-06T21:56:05.581597", "has_code": false}
{"id": "fe1f76e312889134831cec13c4d1e844c144cb62", "text": "Official release‚Äã\n\nTo install the main langchain package, run:\n\n- Pip\n- Conda\n\n```\npip install langchain\n```\n\n```\nconda install langchain -c conda-forge\n```\n\nWhile this package acts as a sane starting point to using LangChain, much of the value of LangChain comes when integrating it with various model providers, datastores, etc. By default, the dependencies needed to do that are NOT installed. You will need to install the dependencies for specific integrations separately, which we show below.", "url_final": "https://python.langchain.com/docs/how_to/installation/", "url_citable": "https://python.langchain.com/docs/how_to/installation/#official-release", "title": "How to install LangChain packages | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Official release‚Äã", "section_anchor": "official-release", "fetched_at": "2025-10-06T21:54:28.114581", "has_code": true}
{"id": "63b39199210a3cbb14b052eb3a208c00a0339aa5", "text": "Ecosystem packages‚Äã\n\nWith the exception of the langsmith SDK, all packages in the LangChain ecosystem depend on langchain-core, which contains base classes and abstractions that other packages use. The dependency graph below shows how the different packages are related. A directed arrow indicates that the source package depends on the target package:\n\nWhen installing a package, you do not need to explicitly install that package's explicit dependencies (such as langchain-core). However, you may choose to if you are using a feature only available in a certain version of that dependency. If you do, you should make sure that the installed or pinned version is compatible with any other integration packages you use.\n\nLangChain core‚Äã\n\nThe langchain-core package contains base abstractions that the rest of the LangChain ecosystem uses, along with the LangChain Expression Language. It is automatically installed by langchain, but can also be used separately. Install with:\n\n```\npip install langchain-core\n```\n\nIntegration packages‚Äã\n\nCertain integrations like OpenAI and Anthropic have their own packages. Any integrations that require their own package will be documented as such in the Integration docs. You can see a list of all integration packages in the API reference under the \"Partner libs\" dropdown. To install one of these run:\n\n```\npip install langchain-openai\n```\n\nAny integrations that haven't been split out into their own packages will live in the langchain-community package. Install with:\n\n```\npip install langchain-community\n```\n\nLangChain experimental‚Äã\n\nThe langchain-experimental package holds experimental LangChain code, intended for research and experimental uses. Install with:\n\n```\npip install langchain-experimental\n```\n\nLangGraph‚Äã\n\nlanggraph is a library for building stateful, multi-actor applications with LLMs. It integrates smoothly with LangChain, but can be used without it. Install with:\n\n```\npip install langgraph\n```\n\nLangServe‚Äã\n\nLangServe helps developers deploy LangChain runnables and chains as a REST API. LangServe is automatically installed by LangChain CLI. If not using LangChain CLI, install with:\n\n```\npip install \"langserve[all]\"\n```\n\nfor both client and server dependencies. Or pip install \"langserve[client]\" for client code, and pip install \"langserve[server]\" for server code.\n\nLangChain CLI‚Äã\n\nThe LangChain CLI is useful for working with LangChain templates and other LangServe projects. Install with:\n\n```\npip install langchain-cli\n```\n\nLangSmith SDK‚Äã\n\nThe LangSmith SDK is automatically installed by LangChain. However, it does not depend on langchain-core, and can be installed and used independently if desired. If you are not using LangChain, you can install it with:\n\n```\npip install langsmith\n```\n\nFrom source‚Äã\n\nIf you want to install a package from source, you can do so by cloning the main LangChain repo, enter the directory of the package you want to install PATH/TO/REPO/langchain/libs/{package}, and run:\n\n```\npip install -e .\n```\n\nLangGraph, LangSmith SDK, and certain integration packages live outside the main LangChain repo. You can see all repos here.\n\n- LangChain core\n- Integration packages\n- LangChain experimental\n- LangGraph\n- LangServe\n- LangChain CLI\n- LangSmith SDK\n- From source", "url_final": "https://python.langchain.com/docs/how_to/installation/", "url_citable": "https://python.langchain.com/docs/how_to/installation/#ecosystem-packages", "title": "How to install LangChain packages | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Ecosystem packages‚Äã", "section_anchor": "ecosystem-packages", "fetched_at": "2025-10-06T21:54:28.114581", "has_code": true}
{"id": "09b88b80f6c240c7fead020b6f6cb8f8b8c9861c", "text": "Sync streaming‚Äã\n\nBelow we use a | to help visualize the delimiter between tokens.\n\n```\nfrom langchain_anthropic.chat_models import ChatAnthropicchat = ChatAnthropic(model=\"claude-3-haiku-20240307\")for chunk in chat.stream(\"Write me a 1 verse song about goldfish on the moon\"):    print(chunk.content, end=\"|\", flush=True)\n```\n\n```\nHere| is| a| |1| |verse| song| about| gol|dfish| on| the| moon|:|Floating| up| in| the| star|ry| night|,|Fins| a|-|gl|im|mer| in| the| pale| moon|light|.|Gol|dfish| swimming|,| peaceful| an|d free|,|Se|ren|ely| |drif|ting| across| the| lunar| sea|.|\n```", "url_final": "https://python.langchain.com/docs/how_to/chat_streaming/", "url_citable": "https://python.langchain.com/docs/how_to/chat_streaming/#sync-streaming", "title": "How to stream chat model responses | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Sync streaming‚Äã", "section_anchor": "sync-streaming", "fetched_at": "2025-10-06T21:54:22.849635", "has_code": true}
{"id": "9e1bf9db48f24d63ad06be5c992a09a2b3f054c0", "text": "Async Streaming‚Äã\n\n```\nfrom langchain_anthropic.chat_models import ChatAnthropicchat = ChatAnthropic(model=\"claude-3-haiku-20240307\")async for chunk in chat.astream(\"Write me a 1 verse song about goldfish on the moon\"):    print(chunk.content, end=\"|\", flush=True)\n```\n\n```\nHere| is| a| |1| |verse| song| about| gol|dfish| on| the| moon|:|Floating| up| above| the| Earth|,|Gol|dfish| swim| in| alien| m|irth|.|In| their| bowl| of| lunar| dust|,|Gl|it|tering| scales| reflect| the| trust|Of| swimming| free| in| this| new| worl|d,|Where| their| aqu|atic| dream|'s| unf|ur|le|d.|\n```", "url_final": "https://python.langchain.com/docs/how_to/chat_streaming/", "url_citable": "https://python.langchain.com/docs/how_to/chat_streaming/#async-streaming", "title": "How to stream chat model responses | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Async Streaming‚Äã", "section_anchor": "async-streaming", "fetched_at": "2025-10-06T21:54:22.849635", "has_code": true}
{"id": "8f87fd97e95e12040aea2292ccca1e701a1ccdeb", "text": "Astream events‚Äã\n\nChat models also support the standard astream events method.\n\nThis method is useful if you're streaming output from a larger LLM application that contains multiple steps (e.g., an LLM chain composed of a prompt, llm and parser).\n\n```\nfrom langchain_anthropic.chat_models import ChatAnthropicchat = ChatAnthropic(model=\"claude-3-haiku-20240307\")idx = 0async for event in chat.astream_events(    \"Write me a 1 verse song about goldfish on the moon\"):    idx += 1    if idx >= 5:  # Truncate the output        print(\"...Truncated\")        break    print(event)\n```\n\n```\n{'event': 'on_chat_model_start', 'data': {'input': 'Write me a 1 verse song about goldfish on the moon'}, 'name': 'ChatAnthropic', 'tags': [], 'run_id': '1d430164-52b1-4d00-8c00-b16460f7737e', 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-haiku-20240307', 'ls_model_type': 'chat', 'ls_temperature': None, 'ls_max_tokens': 1024}, 'parent_ids': []}{'event': 'on_chat_model_stream', 'run_id': '1d430164-52b1-4d00-8c00-b16460f7737e', 'name': 'ChatAnthropic', 'tags': [], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-haiku-20240307', 'ls_model_type': 'chat', 'ls_temperature': None, 'ls_max_tokens': 1024}, 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run-1d430164-52b1-4d00-8c00-b16460f7737e', usage_metadata={'input_tokens': 21, 'output_tokens': 2, 'total_tokens': 23, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})}, 'parent_ids': []}{'event': 'on_chat_model_stream', 'run_id': '1d430164-52b1-4d00-8c00-b16460f7737e', 'name': 'ChatAnthropic', 'tags': [], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-haiku-20240307', 'ls_model_type': 'chat', 'ls_temperature': None, 'ls_max_tokens': 1024}, 'data': {'chunk': AIMessageChunk(content=\"Here's\", additional_kwargs={}, response_metadata={}, id='run-1d430164-52b1-4d00-8c00-b16460f7737e')}, 'parent_ids': []}{'event': 'on_chat_model_stream', 'run_id': '1d430164-52b1-4d00-8c00-b16460f7737e', 'name': 'ChatAnthropic', 'tags': [], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-haiku-20240307', 'ls_model_type': 'chat', 'ls_temperature': None, 'ls_max_tokens': 1024}, 'data': {'chunk': AIMessageChunk(content=' a short one-verse song', additional_kwargs={}, response_metadata={}, id='run-1d430164-52b1-4d00-8c00-b16460f7737e')}, 'parent_ids': []}...Truncated\n```\n\n- Sync streaming\n- Async Streaming\n- Astream events", "url_final": "https://python.langchain.com/docs/how_to/chat_streaming/", "url_citable": "https://python.langchain.com/docs/how_to/chat_streaming/#astream-events", "title": "How to stream chat model responses | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Astream events‚Äã", "section_anchor": "astream-events", "fetched_at": "2025-10-06T21:54:22.849635", "has_code": true}
{"id": "1b9abfe2fa355008bc3bf0e489c06b69fb39d6cf", "text": "Streaming‚Äã\n\nOne convenient feature of this method is that it allows values to pass through as soon as they are available. To show this off, we'll use RunnablePassthrough.assign() to immediately return source docs in a retrieval chain:\n\n```\nfrom langchain_community.vectorstores import FAISSfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnablePassthroughfrom langchain_openai import ChatOpenAI, OpenAIEmbeddingsvectorstore = FAISS.from_texts(    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())retriever = vectorstore.as_retriever()template = \"\"\"Answer the question based only on the following context:{context}Question: {question}\"\"\"prompt = ChatPromptTemplate.from_template(template)model = ChatOpenAI()generation_chain = prompt | model | StrOutputParser()retrieval_chain = {    \"context\": retriever,    \"question\": RunnablePassthrough(),} | RunnablePassthrough.assign(output=generation_chain)stream = retrieval_chain.stream(\"where did harrison work?\")for chunk in stream:    print(chunk)\n```\n\n```\n{'question': 'where did harrison work?'}{'context': [Document(page_content='harrison worked at kensho')]}{'output': ''}{'output': 'H'}{'output': 'arrison'}{'output': ' worked'}{'output': ' at'}{'output': ' Kens'}{'output': 'ho'}{'output': '.'}{'output': ''}\n```\n\nWe can see that the first chunk contains the original \"question\" since that is immediately available. The second chunk contains \"context\" since the retriever finishes second. Finally, the output from the generation_chain streams in chunks as soon as it is available.", "url_final": "https://python.langchain.com/docs/how_to/assign/", "url_citable": "https://python.langchain.com/docs/how_to/assign/#streaming", "title": "How to add values to a chain's state | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Streaming‚Äã", "section_anchor": "streaming", "fetched_at": "2025-10-06T21:54:49.715556", "has_code": true}
{"id": "6ecbc8aa5552bdf5b834960a891cba2e766d0a5d", "text": "Next steps‚Äã\n\nNow you've learned how to pass data through your chains to help format the data flowing through your chains.\n\nTo learn more, see the other how-to guides on runnables in this section.\n\n- Streaming\n- Next steps", "url_final": "https://python.langchain.com/docs/how_to/assign/", "url_citable": "https://python.langchain.com/docs/how_to/assign/#next-steps", "title": "How to add values to a chain's state | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Next steps‚Äã", "section_anchor": "next-steps", "fetched_at": "2025-10-06T21:54:49.715556", "has_code": false}
{"id": "de06af44bb798410e37fd8b4a03414bcc7fefafe", "text": "Next steps‚Äã\n\nYou've now learned how to prompt a model to return XML. Next, check out the broader guide on obtaining structured output for other related techniques.\n\n- Next steps", "url_final": "https://python.langchain.com/docs/how_to/output_parser_xml/", "url_citable": "https://python.langchain.com/docs/how_to/output_parser_xml/#next-steps", "title": "How to parse XML output | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Next steps‚Äã", "section_anchor": "next-steps", "fetched_at": "2025-10-06T21:56:04.438672", "has_code": false}
{"id": "4ade791d4e8f2a03ee19a9b5706270807647768a", "text": "tiktoken‚Äã\n\ntiktoken is a fast BPE tokenizer created by OpenAI.\n\nWe can use tiktoken to estimate tokens used. It will probably be more accurate for the OpenAI models.\n\n- How the text is split: by character passed in.\n- How the chunk size is measured: by tiktoken tokenizer.\n\nCharacterTextSplitter, RecursiveCharacterTextSplitter, and TokenTextSplitter can be used with tiktoken directly.\n\n```\n%pip install --upgrade --quiet langchain-text-splitters tiktoken\n```\n\n```\nfrom langchain_text_splitters import CharacterTextSplitter# This is a long document we can split up.with open(\"state_of_the_union.txt\") as f:    state_of_the_union = f.read()\n```\n\nTo split with a CharacterTextSplitter and then merge chunks with tiktoken, use its .from_tiktoken_encoder() method. Note that splits from this method can be larger than the chunk size measured by the tiktoken tokenizer.\n\nThe .from_tiktoken_encoder() method takes either encoding_name as an argument (e.g. cl100k_base), or the model_name (e.g. gpt-4). All additional arguments like chunk_size, chunk_overlap, and separators are used to instantiate CharacterTextSplitter:\n\n```\ntext_splitter = CharacterTextSplitter.from_tiktoken_encoder(    encoding_name=\"cl100k_base\", chunk_size=100, chunk_overlap=0)texts = text_splitter.split_text(state_of_the_union)\n```\n\n```\nprint(texts[0])\n```\n\n```\nMadam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  Last year COVID-19 kept us apart. This year we are finally together again. Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. With a duty to one another to the American people to the Constitution.\n```\n\nTo implement a hard constraint on the chunk size, we can use RecursiveCharacterTextSplitter.from_tiktoken_encoder, where each split will be recursively split if it has a larger size:\n\n```\nfrom langchain_text_splitters import RecursiveCharacterTextSplittertext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(    model_name=\"gpt-4\",    chunk_size=100,    chunk_overlap=0,)\n```\n\nWe can also load a TokenTextSplitter splitter, which works with tiktoken directly and will ensure each split is smaller than chunk size.\n\n```\nfrom langchain_text_splitters import TokenTextSplittertext_splitter = TokenTextSplitter(chunk_size=10, chunk_overlap=0)texts = text_splitter.split_text(state_of_the_union)print(texts[0])\n```\n\n```\nMadam Speaker, Madam Vice President, our\n```\n\nSome written languages (e.g. Chinese and Japanese) have characters which encode to 2 or more tokens. Using the TokenTextSplitter directly can split the tokens for a character between two chunks causing malformed Unicode characters. Use RecursiveCharacterTextSplitter.from_tiktoken_encoder or CharacterTextSplitter.from_tiktoken_encoder to ensure chunks contain valid Unicode strings.", "url_final": "https://python.langchain.com/docs/how_to/split_by_token/", "url_citable": "https://python.langchain.com/docs/how_to/split_by_token/#tiktoken", "title": "How to split text by tokens | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "tiktoken‚Äã", "section_anchor": "tiktoken", "fetched_at": "2025-10-06T21:56:22.335479", "has_code": true}
{"id": "4608ea6f39d34cde10659d4ac20584e5510eac4b", "text": "spaCy‚Äã\n\nspaCy is an open-source software library for advanced natural language processing, written in the programming languages Python and Cython.\n\nLangChain implements splitters based on the spaCy tokenizer.\n\n- How the text is split: by spaCy tokenizer.\n- How the chunk size is measured: by number of characters.\n\n```\n%pip install --upgrade --quiet  spacy\n```\n\n```\n# This is a long document we can split up.with open(\"state_of_the_union.txt\") as f:    state_of_the_union = f.read()\n```\n\n```\nfrom langchain_text_splitters import SpacyTextSplittertext_splitter = SpacyTextSplitter(chunk_size=1000)texts = text_splitter.split_text(state_of_the_union)print(texts[0])\n```\n\n```\nMadam Speaker, Madam Vice President, our First Lady and Second Gentleman.Members of Congress and the Cabinet.Justices of the Supreme Court.My fellow Americans.  Last year COVID-19 kept us apart.This year we are finally together again. Tonight, we meet as Democrats Republicans and Independents.But most importantly as Americans. With a duty to one another to the American people to the Constitution. And with an unwavering resolve that freedom will always triumph over tyranny. Six days ago, Russia‚Äôs Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways.But he badly miscalculated. He thought he could roll into Ukraine and the world would roll over.Instead he met a wall of strength he never imagined. He met the Ukrainian people. From President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world.\n```", "url_final": "https://python.langchain.com/docs/how_to/split_by_token/", "url_citable": "https://python.langchain.com/docs/how_to/split_by_token/#spacy", "title": "How to split text by tokens | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "spaCy‚Äã", "section_anchor": "spacy", "fetched_at": "2025-10-06T21:56:22.335479", "has_code": true}
{"id": "8a0dfa7a5d19bf145a270fc43623ae77a3878c9e", "text": "SentenceTransformers‚Äã\n\nThe SentenceTransformersTokenTextSplitter is a specialized text splitter for use with the sentence-transformer models. The default behaviour is to split the text into chunks that fit the token window of the sentence transformer model that you would like to use.\n\nTo split text and constrain token counts according to the sentence-transformers tokenizer, instantiate a SentenceTransformersTokenTextSplitter. You can optionally specify:\n\n- chunk_overlap: integer count of token overlap;\n- model_name: sentence-transformer model name, defaulting to \"sentence-transformers/all-mpnet-base-v2\";\n- tokens_per_chunk: desired token count per chunk.\n\n```\nfrom langchain_text_splitters import SentenceTransformersTokenTextSplittersplitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0)text = \"Lorem \"count_start_and_stop_tokens = 2text_token_count = splitter.count_tokens(text=text) - count_start_and_stop_tokensprint(text_token_count)\n```\n\n```\n2\n```\n\n```\ntoken_multiplier = splitter.maximum_tokens_per_chunk // text_token_count + 1# `text_to_split` does not fit in a single chunktext_to_split = text * token_multiplierprint(f\"tokens in text to split: {splitter.count_tokens(text=text_to_split)}\")\n```\n\n```\ntokens in text to split: 514\n```\n\n```\ntext_chunks = splitter.split_text(text=text_to_split)print(text_chunks[1])\n```\n\n```\nlorem\n```", "url_final": "https://python.langchain.com/docs/how_to/split_by_token/", "url_citable": "https://python.langchain.com/docs/how_to/split_by_token/#sentencetransformers", "title": "How to split text by tokens | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "SentenceTransformers‚Äã", "section_anchor": "sentencetransformers", "fetched_at": "2025-10-06T21:56:22.335479", "has_code": true}
{"id": "a4ea53e43664d204a219301639145a8d812372f0", "text": "NLTK‚Äã\n\nThe Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing (NLP) for English written in the Python programming language.\n\nRather than just splitting on \"\\n\\n\", we can use NLTK to split based on NLTK tokenizers.\n\n- How the text is split: by NLTK tokenizer.\n- How the chunk size is measured: by number of characters.\n\n```\n# pip install nltk\n```\n\n```\n# This is a long document we can split up.with open(\"state_of_the_union.txt\") as f:    state_of_the_union = f.read()\n```\n\n```\nfrom langchain_text_splitters import NLTKTextSplittertext_splitter = NLTKTextSplitter(chunk_size=1000)\n```\n\n```\ntexts = text_splitter.split_text(state_of_the_union)print(texts[0])\n```\n\n```\nMadam Speaker, Madam Vice President, our First Lady and Second Gentleman.Members of Congress and the Cabinet.Justices of the Supreme Court.My fellow Americans.Last year COVID-19 kept us apart.This year we are finally together again.Tonight, we meet as Democrats Republicans and Independents.But most importantly as Americans.With a duty to one another to the American people to the Constitution.And with an unwavering resolve that freedom will always triumph over tyranny.Six days ago, Russia‚Äôs Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways.But he badly miscalculated.He thought he could roll into Ukraine and the world would roll over.Instead he met a wall of strength he never imagined.He met the Ukrainian people.From President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world.Groups of citizens blocking tanks with their bodies.\n```", "url_final": "https://python.langchain.com/docs/how_to/split_by_token/", "url_citable": "https://python.langchain.com/docs/how_to/split_by_token/#nltk", "title": "How to split text by tokens | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "NLTK‚Äã", "section_anchor": "nltk", "fetched_at": "2025-10-06T21:56:22.335479", "has_code": true}
{"id": "559e1f8eccadb19eddf9f3b3957be318725d077b", "text": "KoNLPY‚Äã\n\nKoNLPy: Korean NLP in Python is is a Python package for natural language processing (NLP) of the Korean language.\n\nToken splitting involves the segmentation of text into smaller, more manageable units called tokens. These tokens are often words, phrases, symbols, or other meaningful elements crucial for further processing and analysis. In languages like English, token splitting typically involves separating words by spaces and punctuation marks. The effectiveness of token splitting largely depends on the tokenizer's understanding of the language structure, ensuring the generation of meaningful tokens. Since tokenizers designed for the English language are not equipped to understand the unique semantic structures of other languages, such as Korean, they cannot be effectively used for Korean language processing.\n\nToken splitting for Korean with KoNLPy's Kkma Analyzer‚Äã\n\nIn case of Korean text, KoNLPY includes at morphological analyzer called Kkma (Korean Knowledge Morpheme Analyzer). Kkma provides detailed morphological analysis of Korean text. It breaks down sentences into words and words into their respective morphemes, identifying parts of speech for each token. It can segment a block of text into individual sentences, which is particularly useful for processing long texts.\n\nUsage Considerations‚Äã\n\nWhile Kkma is renowned for its detailed analysis, it is important to note that this precision may impact processing speed. Thus, Kkma is best suited for applications where analytical depth is prioritized over rapid text processing.\n\n```\n# pip install konlpy\n```\n\n```\n# This is a long Korean document that we want to split up into its component sentences.with open(\"./your_korean_doc.txt\") as f:    korean_document = f.read()\n```\n\n```\nfrom langchain_text_splitters import KonlpyTextSplittertext_splitter = KonlpyTextSplitter()\n```\n\n```\ntexts = text_splitter.split_text(korean_document)# The sentences are split with \"\\n\\n\" characters.print(texts[0])\n```\n\n```\nÏ∂òÌñ•Ï†Ñ ÏòõÎÇ†Ïóê ÎÇ®ÏõêÏóê Ïù¥ ÎèÑÎ†πÏù¥ÎùºÎäî Î≤ºÏä¨ÏïÑÏπò ÏïÑÎì§Ïù¥ ÏûàÏóàÎã§.Í∑∏Ïùò Ïô∏Î™®Îäî ÎπõÎÇòÎäî Îã¨Ï≤òÎüº ÏûòÏÉùÍ≤ºÍ≥†, Í∑∏Ïùò ÌïôÏãùÍ≥º Í∏∞ÏòàÎäî ÎÇ®Î≥¥Îã§ Îõ∞Ïñ¥ÎÇ¨Îã§.ÌïúÌé∏, Ïù¥ ÎßàÏùÑÏóêÎäî Ï∂òÌñ•Ïù¥ÎùºÎäî Ï†àÏÑ∏ Í∞ÄÏù∏Ïù¥ ÏÇ¥Í≥† ÏûàÏóàÎã§.Ï∂ò Ìñ•Ïùò ÏïÑÎ¶ÑÎã§ÏõÄÏùÄ ÍΩÉÍ≥º Í∞ôÏïÑ ÎßàÏùÑ ÏÇ¨ÎûåÎì§ Î°úÎ∂ÄÌÑ∞ ÎßéÏùÄ ÏÇ¨ÎûëÏùÑ Î∞õÏïòÎã§.Ïñ¥Îäê Î¥ÑÎÇ†, ÎèÑÎ†πÏùÄ ÏπúÍµ¨Îì§Í≥º ÎÜÄÎü¨ ÎÇòÍ∞îÎã§Í∞Ä Ï∂ò Ìñ•ÏùÑ Îßå ÎÇò Ï≤´ ÎààÏóê Î∞òÌïòÍ≥† ÎßêÏïòÎã§.Îëê ÏÇ¨ÎûåÏùÄ ÏÑúÎ°ú ÏÇ¨ÎûëÌïòÍ≤å ÎêòÏóàÍ≥†, Ïù¥ÎÇ¥ ÎπÑÎ∞ÄÏä§Îü¨Ïö¥ ÏÇ¨ÎûëÏùò ÎßπÏÑ∏Î•º ÎÇòÎàÑÏóàÎã§.ÌïòÏßÄÎßå Ï¢ãÏùÄ ÎÇ†Îì§ÏùÄ Ïò§ÎûòÍ∞ÄÏßÄ ÏïäÏïòÎã§.ÎèÑÎ†πÏùò ÏïÑÎ≤ÑÏßÄÍ∞Ä Îã§Î•∏ Í≥≥ÏúºÎ°ú Ï†ÑÍ∑ºÏùÑ Í∞ÄÍ≤å ÎêòÏñ¥ ÎèÑÎ†πÎèÑ Îñ†ÎÇò ÏïºÎßå ÌñàÎã§.Ïù¥Î≥ÑÏùò ÏïÑÌîî ÏÜçÏóêÏÑúÎèÑ, Îëê ÏÇ¨ÎûåÏùÄ Ïû¨ÌöåÎ•º Í∏∞ÏïΩÌïòÎ©∞ ÏÑúÎ°úÎ•º ÎØøÍ≥† Í∏∞Îã§Î¶¨Í∏∞Î°ú ÌñàÎã§.Í∑∏Îü¨ÎÇò ÏÉàÎ°ú Î∂ÄÏûÑÌïú Í¥ÄÏïÑÏùò ÏÇ¨ÎòêÍ∞Ä Ï∂ò Ìñ•Ïùò ÏïÑÎ¶ÑÎã§ÏõÄÏóê ÏöïÏã¨ÏùÑ ÎÇ¥ Ïñ¥ Í∑∏ÎÖÄÏóêÍ≤å Í∞ïÏöîÎ•º ÏãúÏûëÌñàÎã§.Ï∂ò Ìñ• ÏùÄ ÎèÑÎ†πÏóê ÎåÄÌïú ÏûêÏã†Ïùò ÏÇ¨ÎûëÏùÑ ÏßÄÌÇ§Í∏∞ ÏúÑÌï¥, ÏÇ¨ÎòêÏùò ÏöîÍµ¨Î•º Îã®Ìò∏Ìûà Í±∞Ï†àÌñàÎã§.Ïù¥Ïóê Î∂ÑÎÖ∏Ìïú ÏÇ¨ÎòêÎäî Ï∂ò Ìñ•ÏùÑ Í∞êÏò•Ïóê Í∞ÄÎëêÍ≥† ÌòπÎèÖÌïú ÌòïÎ≤åÏùÑ ÎÇ¥Î†∏Îã§.Ïù¥ÏïºÍ∏∞Îäî Ïù¥ ÎèÑÎ†πÏù¥ Í≥†ÏúÑ Í¥ÄÏßÅÏóê Ïò§Î•∏ ÌõÑ, Ï∂ò Ìñ•ÏùÑ Íµ¨Ìï¥ ÎÇ¥Îäî Í≤ÉÏúºÎ°ú ÎÅùÎÇúÎã§.Îëê ÏÇ¨ÎûåÏùÄ Ïò§Îûú ÏãúÎ†® ÎÅùÏóê Îã§Ïãú ÎßåÎÇòÍ≤å ÎêòÍ≥†, Í∑∏Îì§Ïùò ÏÇ¨ÎûëÏùÄ Ïò® ÏÑ∏ÏÉÅÏóê Ï†ÑÌï¥ ÏßÄÎ©∞ ÌõÑÏÑ∏ÏóêÍπåÏßÄ Ïù¥Ïñ¥ÏßÑÎã§.- Ï∂òÌñ•Ï†Ñ (The Tale of Chunhyang)\n```", "url_final": "https://python.langchain.com/docs/how_to/split_by_token/", "url_citable": "https://python.langchain.com/docs/how_to/split_by_token/#konlpy", "title": "How to split text by tokens | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "KoNLPY‚Äã", "section_anchor": "konlpy", "fetched_at": "2025-10-06T21:56:22.335479", "has_code": true}
{"id": "c08492819bfbe852d042a35b7aaf4e844fb84730", "text": "Hugging Face tokenizer‚Äã\n\nHugging Face has many tokenizers.\n\nWe use Hugging Face tokenizer, the GPT2TokenizerFast to count the text length in tokens.\n\n- How the text is split: by character passed in.\n- How the chunk size is measured: by number of tokens calculated by the Hugging Face tokenizer.\n\n```\nfrom transformers import GPT2TokenizerFasttokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n```\n\n```\n# This is a long document we can split up.with open(\"state_of_the_union.txt\") as f:    state_of_the_union = f.read()from langchain_text_splitters import CharacterTextSplitter\n```\n\n```\ntext_splitter = CharacterTextSplitter.from_huggingface_tokenizer(    tokenizer, chunk_size=100, chunk_overlap=0)texts = text_splitter.split_text(state_of_the_union)\n```\n\n```\nprint(texts[0])\n```\n\n```\nMadam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  Last year COVID-19 kept us apart. This year we are finally together again. Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. With a duty to one another to the American people to the Constitution.\n```\n\n- Token splitting for Korean with KoNLPy's Kkma Analyzer\n- Usage Considerations", "url_final": "https://python.langchain.com/docs/how_to/split_by_token/", "url_citable": "https://python.langchain.com/docs/how_to/split_by_token/#hugging-face-tokenizer", "title": "How to split text by tokens | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Hugging Face tokenizer‚Äã", "section_anchor": "hugging-face-tokenizer", "fetched_at": "2025-10-06T21:56:22.335479", "has_code": true}
{"id": "5b5de161d478523820b64be91f543524d28c5def", "text": "Setup‚Äã\n\nLet's initialize a chat model:\n\n```\npip install -qU \"langchain[google-genai]\"\n```\n\n```\nimport getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n```", "url_final": "https://python.langchain.com/docs/how_to/message_history/", "url_citable": "https://python.langchain.com/docs/how_to/message_history/#setup", "title": "How to add message history | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Setup‚Äã", "section_anchor": "setup", "fetched_at": "2025-10-06T21:55:54.203575", "has_code": true}
{"id": "e5df4ab04bffa07cd4ee7f27db683cc8a8c2a8f7", "text": "Example: message inputs‚Äã\n\nAdding memory to a chat model provides a simple example. Chat models accept a list of messages as input and output a message. LangGraph includes a built-in MessagesState that we can use for this purpose.\n\nBelow, we:\n\n- Define the graph state to be a list of messages;\n- Add a single node to the graph that calls a chat model;\n- Compile the graph with an in-memory checkpointer to store messages between runs.\n\nThe output of a LangGraph application is its state. This can be any Python type, but in this context it will typically be a TypedDict that matches the schema of your runnable.\n\n```\nfrom langchain_core.messages import HumanMessagefrom langgraph.checkpoint.memory import MemorySaverfrom langgraph.graph import START, MessagesState, StateGraph# Define a new graphworkflow = StateGraph(state_schema=MessagesState)# Define the function that calls the modeldef call_model(state: MessagesState):    response = llm.invoke(state[\"messages\"])    # Update message history with response:    return {\"messages\": response}# Define the (single) node in the graphworkflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)# Add memorymemory = MemorySaver()app = workflow.compile(checkpointer=memory)\n```\n\nWhen we run the application, we pass in a configuration dict that specifies a thread_id. This ID is used to distinguish conversational threads (e.g., between different users).\n\n```\nconfig = {\"configurable\": {\"thread_id\": \"abc123\"}}\n```\n\nWe can then invoke the application:\n\n```\nquery = \"Hi! I'm Bob.\"input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()  # output contains all messages in state\n```\n\n```\n==================================\u001b[1m Ai Message \u001b[0m==================================It's nice to meet you, Bob! I'm Claude, an AI assistant created by Anthropic. How can I help you today?\n```\n\n```\nquery = \"What's my name?\"input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()\n```\n\n```\n==================================\u001b[1m Ai Message \u001b[0m==================================Your name is Bob, as you introduced yourself at the beginning of our conversation.\n```\n\nNote that states are separated for different threads. If we issue the same query to a thread with a new thread_id, the model indicates that it does not know the answer:\n\n```\nquery = \"What's my name?\"config = {\"configurable\": {\"thread_id\": \"abc234\"}}input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()\n```\n\n```\n==================================\u001b[1m Ai Message \u001b[0m==================================I'm afraid I don't actually know your name. As an AI assistant, I don't have personal information about you unless you provide it to me directly.\n```", "url_final": "https://python.langchain.com/docs/how_to/message_history/", "url_citable": "https://python.langchain.com/docs/how_to/message_history/#example-message-inputs", "title": "How to add message history | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Example: message inputs‚Äã", "section_anchor": "example-message-inputs", "fetched_at": "2025-10-06T21:55:54.203575", "has_code": true}
{"id": "a54da35b1b95cc11578546ceee5fda0e91ddb50a", "text": "Example: dictionary inputs‚Äã\n\nLangChain runnables often accept multiple inputs via separate keys in a single dict argument. A common example is a prompt template with multiple parameters.\n\nWhereas before our runnable was a chat model, here we chain together a prompt template and chat model.\n\n```\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholderprompt = ChatPromptTemplate.from_messages(    [        (\"system\", \"Answer in {language}.\"),        MessagesPlaceholder(variable_name=\"messages\"),    ])runnable = prompt | llm\n```\n\nFor this scenario, we define the graph state to include these parameters (in addition to the message history). We then define a single-node graph in the same way as before.\n\nNote that in the below state:\n\n- Updates to the messages list will append messages;\n- Updates to the language string will overwrite the string.\n\n```\nfrom typing import Sequencefrom langchain_core.messages import BaseMessagefrom langgraph.graph.message import add_messagesfrom typing_extensions import Annotated, TypedDictclass State(TypedDict):    messages: Annotated[Sequence[BaseMessage], add_messages]    language: strworkflow = StateGraph(state_schema=State)def call_model(state: State):    response = runnable.invoke(state)    # Update message history with response:    return {\"messages\": [response]}workflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)memory = MemorySaver()app = workflow.compile(checkpointer=memory)\n```\n\n```\nconfig = {\"configurable\": {\"thread_id\": \"abc345\"}}input_dict = {    \"messages\": [HumanMessage(\"Hi, I'm Bob.\")],    \"language\": \"Spanish\",}output = app.invoke(input_dict, config)output[\"messages\"][-1].pretty_print()\n```\n\n```\n==================================\u001b[1m Ai Message \u001b[0m==================================¬°Hola, Bob! Es un placer conocerte.\n```", "url_final": "https://python.langchain.com/docs/how_to/message_history/", "url_citable": "https://python.langchain.com/docs/how_to/message_history/#example-dictionary-inputs", "title": "How to add message history | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Example: dictionary inputs‚Äã", "section_anchor": "example-dictionary-inputs", "fetched_at": "2025-10-06T21:55:54.203575", "has_code": true}
{"id": "5f941692103f3aad850ce32e1b4efa9d0bdd275d", "text": "Managing message history‚Äã\n\nThe message history (and other elements of the application state) can be accessed via .get_state:\n\n```\nstate = app.get_state(config).valuesprint(f\"Language: {state['language']}\")for message in state[\"messages\"]:    message.pretty_print()\n```\n\n```\nLanguage: Spanish================================\u001b[1m Human Message \u001b[0m=================================Hi, I'm Bob.==================================\u001b[1m Ai Message \u001b[0m==================================¬°Hola, Bob! Es un placer conocerte.\n```\n\nWe can also update the state via .update_state. For example, we can manually append a new message:\n\n```\nfrom langchain_core.messages import HumanMessage_ = app.update_state(config, {\"messages\": [HumanMessage(\"Test\")]})\n```\n\n```\nstate = app.get_state(config).valuesprint(f\"Language: {state['language']}\")for message in state[\"messages\"]:    message.pretty_print()\n```\n\n```\nLanguage: Spanish================================\u001b[1m Human Message \u001b[0m=================================Hi, I'm Bob.==================================\u001b[1m Ai Message \u001b[0m==================================¬°Hola, Bob! Es un placer conocerte.================================\u001b[1m Human Message \u001b[0m=================================Test\n```\n\nFor details on managing state, including deleting messages, see the LangGraph documentation:\n\n- How to delete messages\n- How to view and update past graph state\n\n- Setup\n- Example: message inputs\n- Example: dictionary inputs\n- Managing message history", "url_final": "https://python.langchain.com/docs/how_to/message_history/", "url_citable": "https://python.langchain.com/docs/how_to/message_history/#managing-message-history", "title": "How to add message history | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Managing message history‚Äã", "section_anchor": "managing-message-history", "fetched_at": "2025-10-06T21:55:54.203575", "has_code": true}
{"id": "204b1e7665437f91ccc2df8ae4163924e5c996d7", "text": "How to construct filters for query analysis | ü¶úÔ∏èüîó LangChain\n\nWe may want to do query analysis to extract filters to pass into retrievers. One way we ask the LLM to represent these filters is as a Pydantic model. There is then the issue of converting that Pydantic model into a filter that can be passed into a retriever.\n\nThis can be done manually, but LangChain also provides some \"Translators\" that are able to translate from a common syntax into filters specific to each retriever. Here, we will cover how to use those translators.\n\n```\nfrom typing import Optionalfrom langchain.chains.query_constructor.ir import (    Comparator,    Comparison,    Operation,    Operator,    StructuredQuery,)from langchain_community.query_constructors.chroma import ChromaTranslatorfrom langchain_community.query_constructors.elasticsearch import ElasticsearchTranslatorfrom pydantic import BaseModel\n```\n\nIn this example, year and author are both attributes to filter on.\n\n```\nclass Search(BaseModel):    query: str    start_year: Optional[int]    author: Optional[str]\n```\n\n```\nsearch_query = Search(query=\"RAG\", start_year=2022, author=\"LangChain\")\n```\n\n```\ndef construct_comparisons(query: Search):    comparisons = []    if query.start_year is not None:        comparisons.append(            Comparison(                comparator=Comparator.GT,                attribute=\"start_year\",                value=query.start_year,            )        )    if query.author is not None:        comparisons.append(            Comparison(                comparator=Comparator.EQ,                attribute=\"author\",                value=query.author,            )        )    return comparisons\n```\n\n```\ncomparisons = construct_comparisons(search_query)\n```\n\n```\n_filter = Operation(operator=Operator.AND, arguments=comparisons)\n```\n\n```\nElasticsearchTranslator().visit_operation(_filter)\n```\n\n```\n{'bool': {'must': [{'range': {'metadata.start_year': {'gt': 2022}}},   {'term': {'metadata.author.keyword': 'LangChain'}}]}}\n```\n\n```\nChromaTranslator().visit_operation(_filter)\n```\n\n```\n{'$and': [{'start_year': {'$gt': 2022}}, {'author': {'$eq': 'LangChain'}}]}\n```", "url_final": "https://python.langchain.com/docs/how_to/query_constructing_filters/", "url_citable": "https://python.langchain.com/docs/how_to/query_constructing_filters/", "title": "How to construct filters for query analysis | ü¶úÔ∏èüîó LangChain", "section_level": 1, "section_heading": "How to construct filters for query analysis | ü¶úÔ∏èüîó LangChain", "section_anchor": null, "fetched_at": "2025-10-06T21:54:50.720785", "has_code": true}
{"id": "271db3ce89ff158d23dfd6896754490ffd4cee98", "text": "Overview‚Äã\n\nMany LLM applications involve retrieving information from external data sources using a Retriever.\n\nA retriever is responsible for retrieving a list of relevant Documents to a given user query.\n\nThe retrieved documents are often formatted into prompts that are fed into an LLM, allowing the LLM to use the information in the to generate an appropriate response (e.g., answering a user question based on a knowledge base).", "url_final": "https://python.langchain.com/docs/how_to/custom_retriever/", "url_citable": "https://python.langchain.com/docs/how_to/custom_retriever/#overview", "title": "Custom Retriever | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Overview‚Äã", "section_anchor": "overview", "fetched_at": "2025-10-06T21:55:16.342185", "has_code": false}
{"id": "f996ee60e51ea91e9e8a1573085b6686b6e772a2", "text": "Interface‚Äã\n\nTo create your own retriever, you need to extend the BaseRetriever class and implement the following methods:\n\nThe logic inside of _get_relevant_documents can involve arbitrary calls to a database or to the web using requests.\n\nBy inherting from BaseRetriever, your retriever automatically becomes a LangChain Runnable and will gain the standard Runnable functionality out of the box!\n\nYou can use a RunnableLambda or RunnableGenerator to implement a retriever.\n\nThe main benefit of implementing a retriever as a BaseRetriever vs. a RunnableLambda (a custom runnable function) is that a BaseRetriever is a well known LangChain entity so some tooling for monitoring may implement specialized behavior for retrievers. Another difference is that a BaseRetriever will behave slightly differently from RunnableLambda in some APIs; e.g., the start event in astream_events API will be on_retriever_start instead of on_chain_start.", "url_final": "https://python.langchain.com/docs/how_to/custom_retriever/", "url_citable": "https://python.langchain.com/docs/how_to/custom_retriever/#interface", "title": "Custom Retriever | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Interface‚Äã", "section_anchor": "interface", "fetched_at": "2025-10-06T21:55:16.342185", "has_code": false}
{"id": "e799b190a866a7556107f82ccc9fa4c33d017f02", "text": "Example‚Äã\n\nLet's implement a toy retriever that returns all documents whose text contains the text in the user query.\n\n```\nfrom typing import Listfrom langchain_core.callbacks import CallbackManagerForRetrieverRunfrom langchain_core.documents import Documentfrom langchain_core.retrievers import BaseRetrieverclass ToyRetriever(BaseRetriever):    \"\"\"A toy retriever that contains the top k documents that contain the user query.    This retriever only implements the sync method _get_relevant_documents.    If the retriever were to involve file access or network access, it could benefit    from a native async implementation of `_aget_relevant_documents`.    As usual, with Runnables, there's a default async implementation that's provided    that delegates to the sync implementation running on another thread.    \"\"\"    documents: List[Document]    \"\"\"List of documents to retrieve from.\"\"\"    k: int    \"\"\"Number of top results to return\"\"\"    def _get_relevant_documents(        self, query: str, *, run_manager: CallbackManagerForRetrieverRun    ) -> List[Document]:        \"\"\"Sync implementations for retriever.\"\"\"        matching_documents = []        for document in self.documents:            if len(matching_documents) > self.k:                return matching_documents            if query.lower() in document.page_content.lower():                matching_documents.append(document)        return matching_documents    # Optional: Provide a more efficient native implementation by overriding    # _aget_relevant_documents    # async def _aget_relevant_documents(    #     self, query: str, *, run_manager: AsyncCallbackManagerForRetrieverRun    # ) -> List[Document]:    #     \"\"\"Asynchronously get documents relevant to a query.    #     Args:    #         query: String to find relevant documents for    #         run_manager: The callbacks handler to use    #     Returns:    #         List of relevant documents    #     \"\"\"\n```", "url_final": "https://python.langchain.com/docs/how_to/custom_retriever/", "url_citable": "https://python.langchain.com/docs/how_to/custom_retriever/#example", "title": "Custom Retriever | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Example‚Äã", "section_anchor": "example", "fetched_at": "2025-10-06T21:55:16.342185", "has_code": true}
{"id": "a203594e810cb16453656ea8940495772c1fbaaf", "text": "Test it üß™‚Äã\n\n```\ndocuments = [    Document(        page_content=\"Dogs are great companions, known for their loyalty and friendliness.\",        metadata={\"type\": \"dog\", \"trait\": \"loyalty\"},    ),    Document(        page_content=\"Cats are independent pets that often enjoy their own space.\",        metadata={\"type\": \"cat\", \"trait\": \"independence\"},    ),    Document(        page_content=\"Goldfish are popular pets for beginners, requiring relatively simple care.\",        metadata={\"type\": \"fish\", \"trait\": \"low maintenance\"},    ),    Document(        page_content=\"Parrots are intelligent birds capable of mimicking human speech.\",        metadata={\"type\": \"bird\", \"trait\": \"intelligence\"},    ),    Document(        page_content=\"Rabbits are social animals that need plenty of space to hop around.\",        metadata={\"type\": \"rabbit\", \"trait\": \"social\"},    ),]retriever = ToyRetriever(documents=documents, k=3)\n```\n\n```\nretriever.invoke(\"that\")\n```\n\n```\n[Document(page_content='Cats are independent pets that often enjoy their own space.', metadata={'type': 'cat', 'trait': 'independence'}), Document(page_content='Rabbits are social animals that need plenty of space to hop around.', metadata={'type': 'rabbit', 'trait': 'social'})]\n```\n\nIt's a runnable so it'll benefit from the standard Runnable Interface! ü§©\n\n```\nawait retriever.ainvoke(\"that\")\n```\n\n```\n[Document(page_content='Cats are independent pets that often enjoy their own space.', metadata={'type': 'cat', 'trait': 'independence'}), Document(page_content='Rabbits are social animals that need plenty of space to hop around.', metadata={'type': 'rabbit', 'trait': 'social'})]\n```\n\n```\nretriever.batch([\"dog\", \"cat\"])\n```\n\n```\n[[Document(page_content='Dogs are great companions, known for their loyalty and friendliness.', metadata={'type': 'dog', 'trait': 'loyalty'})], [Document(page_content='Cats are independent pets that often enjoy their own space.', metadata={'type': 'cat', 'trait': 'independence'})]]\n```\n\n```\nasync for event in retriever.astream_events(\"bar\", version=\"v1\"):    print(event)\n```\n\n```\n{'event': 'on_retriever_start', 'run_id': 'f96f268d-8383-4921-b175-ca583924d9ff', 'name': 'ToyRetriever', 'tags': [], 'metadata': {}, 'data': {'input': 'bar'}}{'event': 'on_retriever_stream', 'run_id': 'f96f268d-8383-4921-b175-ca583924d9ff', 'tags': [], 'metadata': {}, 'name': 'ToyRetriever', 'data': {'chunk': []}}{'event': 'on_retriever_end', 'name': 'ToyRetriever', 'run_id': 'f96f268d-8383-4921-b175-ca583924d9ff', 'tags': [], 'metadata': {}, 'data': {'output': []}}\n```", "url_final": "https://python.langchain.com/docs/how_to/custom_retriever/", "url_citable": "https://python.langchain.com/docs/how_to/custom_retriever/#test-it-", "title": "Custom Retriever | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Test it üß™‚Äã", "section_anchor": "test-it-", "fetched_at": "2025-10-06T21:55:16.342185", "has_code": true}
{"id": "bd4ad9fa6503878ab233087a09a2ad09dff3042a", "text": "Contributing‚Äã\n\nWe appreciate contributions of interesting retrievers!\n\nHere's a checklist to help make sure your contribution gets added to LangChain:\n\nDocumentation:\n\n- The retriever contains doc-strings for all initialization arguments, as these will be surfaced in the API Reference.\n- The class doc-string for the model contains a link to any relevant APIs used for the retriever (e.g., if the retriever is retrieving from wikipedia, it'll be good to link to the wikipedia API!)\n\nTests:\n\n- Add unit or integration tests to verify that invoke and ainvoke work.\n\nOptimizations:\n\nIf the retriever is connecting to external data sources (e.g., an API or a file), it'll almost certainly benefit from an async native optimization!\n\n- Provide a native async implementation of _aget_relevant_documents (used by ainvoke)\n\n- Overview\n- Interface\n- Example\n- Test it üß™\n- Contributing", "url_final": "https://python.langchain.com/docs/how_to/custom_retriever/", "url_citable": "https://python.langchain.com/docs/how_to/custom_retriever/#contributing", "title": "Custom Retriever | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Contributing‚Äã", "section_anchor": "contributing", "fetched_at": "2025-10-06T21:55:16.342185", "has_code": false}
{"id": "bf54bd31f973ff0e312c9d336df97db678e49f18", "text": "How to disable parallel tool calling | ü¶úÔ∏èüîó LangChain\n\nThis API is currently only supported by OpenAI and Anthropic.\n\nOpenAI tool calling performs tool calling in parallel by default. That means that if we ask a question like \"What is the weather in Tokyo, New York, and Chicago?\" and we have a tool for getting the weather, it will call the tool 3 times in parallel. We can force it to call only a single tool once by using the parallel_tool_call parameter.\n\nFirst let's set up our tools and model:\n\n```\nfrom langchain_core.tools import tool@tooldef add(a: int, b: int) -> int:    \"\"\"Adds a and b.\"\"\"    return a + b@tooldef multiply(a: int, b: int) -> int:    \"\"\"Multiplies a and b.\"\"\"    return a * btools = [add, multiply]\n```\n\n```\nimport osfrom getpass import getpassfrom langchain.chat_models import init_chat_modelif \"OPENAI_API_KEY\" not in os.environ:    os.environ[\"OPENAI_API_KEY\"] = getpass()llm = init_chat_model(\"openai:gpt-4.1-mini\")\n```\n\nNow let's show a quick example of how disabling parallel tool calls work:\n\n```\nllm_with_tools = llm.bind_tools(tools, parallel_tool_calls=False)llm_with_tools.invoke(\"Please call the first tool two times\").tool_calls\n```\n\n```\n[{'name': 'add',  'args': {'a': 2, 'b': 2},  'id': 'call_Hh4JOTCDM85Sm9Pr84VKrWu5'}]\n```\n\nAs we can see, even though we explicitly told the model to call a tool twice, by disabling parallel tool calls the model was constrained to only calling one.", "url_final": "https://python.langchain.com/docs/how_to/tool_calling_parallel/", "url_citable": "https://python.langchain.com/docs/how_to/tool_calling_parallel/", "title": "How to disable parallel tool calling | ü¶úÔ∏èüîó LangChain", "section_level": 1, "section_heading": "How to disable parallel tool calling | ü¶úÔ∏èüîó LangChain", "section_anchor": null, "fetched_at": "2025-10-06T21:56:34.538424", "has_code": true}
{"id": "1f0687f76f837d947c3b3a4e1de8a26bb6a5e3ae", "text": "Initialize a rate limiter‚Äã\n\nLangchain comes with a built-in in memory rate limiter. This rate limiter is thread safe and can be shared by multiple threads in the same process.\n\nThe provided rate limiter can only limit the number of requests per unit time. It will not help if you need to also limit based on the size of the requests.\n\n```\nfrom langchain_core.rate_limiters import InMemoryRateLimiterrate_limiter = InMemoryRateLimiter(    requests_per_second=0.1,  # <-- Super slow! We can only make a request once every 10 seconds!!    check_every_n_seconds=0.1,  # Wake up every 100 ms to check whether allowed to make a request,    max_bucket_size=10,  # Controls the maximum burst size.)\n```", "url_final": "https://python.langchain.com/docs/how_to/chat_model_rate_limiting/", "url_citable": "https://python.langchain.com/docs/how_to/chat_model_rate_limiting/#initialize-a-rate-limiter", "title": "How to handle rate limits | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Initialize a rate limiter‚Äã", "section_anchor": "initialize-a-rate-limiter", "fetched_at": "2025-10-06T21:55:04.991558", "has_code": true}
{"id": "ee450c6c384fe0fb7f510f712e56cefa0d084f88", "text": "Choose a model‚Äã\n\nChoose any model and pass to it the rate_limiter via the rate_limiter attribute.\n\n```\nimport osimport timefrom getpass import getpassif \"ANTHROPIC_API_KEY\" not in os.environ:    os.environ[\"ANTHROPIC_API_KEY\"] = getpass()from langchain_anthropic import ChatAnthropicmodel = ChatAnthropic(model_name=\"claude-3-opus-20240229\", rate_limiter=rate_limiter)\n```\n\nLet's confirm that the rate limiter works. We should only be able to invoke the model once per 10 seconds.\n\n```\nfor _ in range(5):    tic = time.time()    model.invoke(\"hello\")    toc = time.time()    print(toc - tic)\n```\n\n```\n11.59907364845275910.750212192535410.2442579269409188.8308875560760511.645203590393066\n```\n\n- Initialize a rate limiter\n- Choose a model", "url_final": "https://python.langchain.com/docs/how_to/chat_model_rate_limiting/", "url_citable": "https://python.langchain.com/docs/how_to/chat_model_rate_limiting/#choose-a-model", "title": "How to handle rate limits | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Choose a model‚Äã", "section_anchor": "choose-a-model", "fetched_at": "2025-10-06T21:55:04.991558", "has_code": true}
{"id": "ec804e96cbb1a5eee8747465ff01d5b1fd62763b", "text": "Setup‚Äã\n\nFirst we'll need to install some dependencies:\n\n```\n%pip install -qU langchain-community wikipedia\n```\n\nLet's first select a LLM:\n\n```\npip install -qU \"langchain[google-genai]\"\n```\n\n```\nimport getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n```\n\nWe can now load a retriever and construct our prompt:\n\n```\nfrom langchain_community.retrievers import WikipediaRetrieverfrom langchain_core.prompts import ChatPromptTemplatesystem_prompt = (    \"You're a helpful AI assistant. Given a user question \"    \"and some Wikipedia article snippets, answer the user \"    \"question. If none of the articles answer the question, \"    \"just say you don't know.\"    \"\\n\\nHere are the Wikipedia articles: \"    \"{context}\")retriever = WikipediaRetriever(top_k_results=6, doc_content_chars_max=2000)prompt = ChatPromptTemplate.from_messages(    [        (\"system\", system_prompt),        (\"human\", \"{question}\"),    ])prompt.pretty_print()\n```\n\n```\n================================\u001b[1m System Message \u001b[0m================================You're a helpful AI assistant. Given a user question and some Wikipedia article snippets, answer the user question. If none of the articles answer the question, just say you don't know.Here are the Wikipedia articles: \u001b[33;1m\u001b[1;3m{context}\u001b[0m================================\u001b[1m Human Message \u001b[0m=================================\u001b[33;1m\u001b[1;3m{question}\u001b[0m\n```\n\nNow that we've got a model, retriever and prompt, let's chain them all together. Following the how-to guide on adding citations to a RAG application, we'll make it so our chain returns both the answer and the retrieved Documents. This uses the same LangGraph implementation as in the RAG Tutorial.\n\n```\nfrom langchain_core.documents import Documentfrom langgraph.graph import START, StateGraphfrom typing_extensions import List, TypedDict# Define state for applicationclass State(TypedDict):    question: str    context: List[Document]    answer: str# Define application stepsdef retrieve(state: State):    retrieved_docs = retriever.invoke(state[\"question\"])    return {\"context\": retrieved_docs}def generate(state: State):    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})    response = llm.invoke(messages)    return {\"answer\": response.content}# Compile application and testgraph_builder = StateGraph(State).add_sequence([retrieve, generate])graph_builder.add_edge(START, \"retrieve\")graph = graph_builder.compile()\n```\n\n```\nfrom IPython.display import Image, displaydisplay(Image(graph.get_graph().draw_mermaid_png()))\n```\n\n```\nresult = graph.invoke({\"question\": \"How fast are cheetahs?\"})sources = [doc.metadata[\"source\"] for doc in result[\"context\"]]print(f\"Sources: {sources}\\n\\n\")print(f\"Answer: {result['answer']}\")\n```\n\n```\nSources: ['https://en.wikipedia.org/wiki/Cheetah', 'https://en.wikipedia.org/wiki/Southeast_African_cheetah', 'https://en.wikipedia.org/wiki/Footspeed', 'https://en.wikipedia.org/wiki/Fastest_animals', 'https://en.wikipedia.org/wiki/Pursuit_predation', 'https://en.wikipedia.org/wiki/Gepard-class_fast_attack_craft']Answer: Cheetahs are capable of running at speeds between 93 to 104 km/h (58 to 65 mph).\n```\n\nCheck out the LangSmith trace.", "url_final": "https://python.langchain.com/docs/how_to/qa_citations/", "url_citable": "https://python.langchain.com/docs/how_to/qa_citations/#setup", "title": "How to get a RAG application to add citations | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Setup‚Äã", "section_anchor": "setup", "fetched_at": "2025-10-06T21:56:09.888549", "has_code": true}
{"id": "517ae95d21b4a70e81c2750a915afc90fb500847", "text": "Tool-calling‚Äã\n\nIf your LLM of choice implements a tool-calling feature, you can use it to make the model specify which of the provided documents it's referencing when generating its answer. LangChain tool-calling models implement a .with_structured_output method which will force generation adhering to a desired schema (see details here).", "url_final": "https://python.langchain.com/docs/how_to/qa_citations/", "url_citable": "https://python.langchain.com/docs/how_to/qa_citations/#tool-calling", "title": "How to get a RAG application to add citations | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Tool-calling‚Äã", "section_anchor": "tool-calling", "fetched_at": "2025-10-06T21:56:09.888549", "has_code": false}
{"id": "96628a0cbb1deee31bcb0e913c82b02cd2845deb", "text": "Cite documents‚Äã\n\nTo cite documents using an identifier, we format the identifiers into the prompt, then use .with_structured_output to coerce the LLM to reference these identifiers in its output.\n\nFirst we define a schema for the output. The .with_structured_output supports multiple formats, including JSON schema and Pydantic. Here we will use Pydantic:\n\n```\nfrom pydantic import BaseModel, Fieldclass CitedAnswer(BaseModel):    \"\"\"Answer the user question based only on the given sources, and cite the sources used.\"\"\"    answer: str = Field(        ...,        description=\"The answer to the user question, which is based only on the given sources.\",    )    citations: List[int] = Field(        ...,        description=\"The integer IDs of the SPECIFIC sources which justify the answer.\",    )\n```\n\nLet's see what the model output is like when we pass in our functions and a user input:\n\n```\nstructured_llm = llm.with_structured_output(CitedAnswer)example_q = \"\"\"What Brian's height?Source: 1Information: Suzy is 6'2\"Source: 2Information: Jeremiah is blondeSource: 3Information: Brian is 3 inches shorter than Suzy\"\"\"result = structured_llm.invoke(example_q)result\n```\n\n```\nCitedAnswer(answer='Brian is 5\\'11\".', citations=[1, 3])\n```\n\nOr as a dict:\n\n```\nresult.dict()\n```\n\n```\n{'answer': 'Brian is 5\\'11\".', 'citations': [1, 3]}\n```\n\nNow we structure the source identifiers into the prompt to replicate with our chain. We will make three changes:\n\n- Update the prompt to include source identifiers;\n- Use the structured_llm (i.e., llm.with_structured_output(CitedAnswer));\n- Return the Pydantic object in the output.\n\n```\ndef format_docs_with_id(docs: List[Document]) -> str:    formatted = [        f\"Source ID: {i}\\nArticle Title: {doc.metadata['title']}\\nArticle Snippet: {doc.page_content}\"        for i, doc in enumerate(docs)    ]    return \"\\n\\n\" + \"\\n\\n\".join(formatted)class State(TypedDict):    question: str    context: List[Document]    answer: CitedAnswerdef generate(state: State):    formatted_docs = format_docs_with_id(state[\"context\"])    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": formatted_docs})    structured_llm = llm.with_structured_output(CitedAnswer)    response = structured_llm.invoke(messages)    return {\"answer\": response}graph_builder = StateGraph(State).add_sequence([retrieve, generate])graph_builder.add_edge(START, \"retrieve\")graph = graph_builder.compile()\n```\n\n```\nresult = graph.invoke({\"question\": \"How fast are cheetahs?\"})result[\"answer\"]\n```\n\n```\nCitedAnswer(answer='Cheetahs are capable of running at speeds between 93 to 104 km/h (58 to 65 mph).', citations=[0, 3])\n```\n\nWe can inspect the document at index 0, which the model cited:\n\n```\nprint(result[\"context\"][0])\n```\n\n```\npage_content='The cheetah (Acinonyx jubatus) is a large cat and the fastest land animal. It has a tawny to creamy white or pale buff fur that is marked with evenly spaced, solid black spots. The head is small and rounded, with a short snout and black tear-like facial streaks. It reaches 67‚Äì94 cm (26‚Äì37 in) at the shoulder, and the head-and-body length is between 1.1 and 1.5 m (3 ft 7 in and 4 ft 11 in). Adults weigh between 21 and 72 kg (46 and 159 lb). The cheetah is capable of running at 93 to 104 km/h (58 to 65 mph); it has evolved specialized adaptations for speed, including a light build, long thin legs and a long tail.The cheetah was first described in the late 18th century. Four subspecies are recognised today that are native to Africa and central Iran. An African subspecies was introduced to India in 2022. It is now distributed mainly in small, fragmented populations in northwestern, eastern and southern Africa and central Iran. It lives in a variety of habitats such as savannahs in the Serengeti, arid mountain ranges in the Sahara, and hilly desert terrain.The cheetah lives in three main social groups: females and their cubs, male \"coalitions\", and solitary males. While females lead a nomadic life searching for prey in large home ranges, males are more sedentary and instead establish much smaller territories in areas with plentiful prey and access to females. The cheetah is active during the day, with peaks during dawn and dusk. It feeds on small- to medium-sized prey, mostly weighing under 40 kg (88 lb), and prefers medium-sized ungulates such as impala, springbok and Thomson's gazelles. The cheetah typically stalks its prey within 60‚Äì100 m (200‚Äì330 ft) before charging towards it, trips it during the chase and bites its throat to suffocate it to death. It breeds throughout the year. After a gestation of nearly three months, females give birth to a litter of three or four cubs. Cheetah cubs are highly vulnerable to predation by other large carnivores. They are weaned a' metadata={'title': 'Cheetah', 'summary': 'The cheetah (Acinonyx jubatus) is a large cat and the fastest land animal. It has a tawny to creamy white or pale buff fur that is marked with evenly spaced, solid black spots. The head is small and rounded, with a short snout and black tear-like facial streaks. It reaches 67‚Äì94 cm (26‚Äì37 in) at the shoulder, and the head-and-body length is between 1.1 and 1.5 m (3 ft 7 in and 4 ft 11 in). Adults weigh between 21 and 72 kg (46 and 159 lb). The cheetah is capable of running at 93 to 104 km/h (58 to 65 mph); it has evolved specialized adaptations for speed, including a light build, long thin legs and a long tail.\\nThe cheetah was first described in the late 18th century. Four subspecies are recognised today that are native to Africa and central Iran. An African subspecies was introduced to India in 2022. It is now distributed mainly in small, fragmented populations in northwestern, eastern and southern Africa and central Iran. It lives in a variety of habitats such as savannahs in the Serengeti, arid mountain ranges in the Sahara, and hilly desert terrain.\\nThe cheetah lives in three main social groups: females and their cubs, male \"coalitions\", and solitary males. While females lead a nomadic life searching for prey in large home ranges, males are more sedentary and instead establish much smaller territories in areas with plentiful prey and access to females. The cheetah is active during the day, with peaks during dawn and dusk. It feeds on small- to medium-sized prey, mostly weighing under 40 kg (88 lb), and prefers medium-sized ungulates such as impala, springbok and Thomson\\'s gazelles. The cheetah typically stalks its prey within 60‚Äì100 m (200‚Äì330 ft) before charging towards it, trips it during the chase and bites its throat to suffocate it to death. It breeds throughout the year. After a gestation of nearly three months, females give birth to a litter of three or four cubs. Cheetah cubs are highly vulnerable to predation by other large carnivores. They are weaned at around four months and are independent by around 20 months of age.\\nThe cheetah is threatened by habitat loss, conflict with humans, poaching and high susceptibility to diseases. The global cheetah population was estimated in 2021 at 6,517; it is listed as Vulnerable on the IUCN Red List. It has been widely depicted in art, literature, advertising, and animation. It was tamed in ancient Egypt and trained for hunting ungulates in the Arabian Peninsula and India. It has been kept in zoos since the early 19th century.', 'source': 'https://en.wikipedia.org/wiki/Cheetah'}\n```\n\nLangSmith trace: https://smith.langchain.com/public/6f34d136-451d-4625-90c8-2d8decebc21a/r", "url_final": "https://python.langchain.com/docs/how_to/qa_citations/", "url_citable": "https://python.langchain.com/docs/how_to/qa_citations/#cite-documents", "title": "How to get a RAG application to add citations | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Cite documents‚Äã", "section_anchor": "cite-documents", "fetched_at": "2025-10-06T21:56:09.888549", "has_code": true}
{"id": "0199557a7a2aa413748d1bff8cf5314c5f243d59", "text": "Cite snippets‚Äã\n\nTo return text spans (perhaps in addition to source identifiers), we can use the same approach. The only change will be to build a more complex output schema, here using Pydantic, that includes a \"quote\" alongside a source identifier.\n\nAside: Note that if we break up our documents so that we have many documents with only a sentence or two instead of a few long documents, citing documents becomes roughly equivalent to citing snippets, and may be easier for the model because the model just needs to return an identifier for each snippet instead of the actual text. Probably worth trying both approaches and evaluating.\n\n```\nclass Citation(BaseModel):    source_id: int = Field(        ...,        description=\"The integer ID of a SPECIFIC source which justifies the answer.\",    )    quote: str = Field(        ...,        description=\"The VERBATIM quote from the specified source that justifies the answer.\",    )class QuotedAnswer(BaseModel):    \"\"\"Answer the user question based only on the given sources, and cite the sources used.\"\"\"    answer: str = Field(        ...,        description=\"The answer to the user question, which is based only on the given sources.\",    )    citations: List[Citation] = Field(        ..., description=\"Citations from the given sources that justify the answer.\"    )\n```\n\n```\nclass State(TypedDict):    question: str    context: List[Document]    answer: QuotedAnswerdef generate(state: State):    formatted_docs = format_docs_with_id(state[\"context\"])    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": formatted_docs})    structured_llm = llm.with_structured_output(QuotedAnswer)    response = structured_llm.invoke(messages)    return {\"answer\": response}graph_builder = StateGraph(State).add_sequence([retrieve, generate])graph_builder.add_edge(START, \"retrieve\")graph = graph_builder.compile()\n```\n\nHere we see that the model has extracted a relevant snippet of text from source 0:\n\n```\nresult = graph.invoke({\"question\": \"How fast are cheetahs?\"})result[\"answer\"]\n```\n\n```\nQuotedAnswer(answer='Cheetahs are capable of running at speeds of 93 to 104 km/h (58 to 65 mph).', citations=[Citation(source_id=0, quote='The cheetah is capable of running at 93 to 104 km/h (58 to 65 mph); it has evolved specialized adaptations for speed.')])\n```\n\nLangSmith trace: https://smith.langchain.com/public/e16dc72f-4261-4f25-a9a7-906238737283/r", "url_final": "https://python.langchain.com/docs/how_to/qa_citations/", "url_citable": "https://python.langchain.com/docs/how_to/qa_citations/#cite-snippets", "title": "How to get a RAG application to add citations | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Cite snippets‚Äã", "section_anchor": "cite-snippets", "fetched_at": "2025-10-06T21:56:09.888549", "has_code": true}
{"id": "fa219ccc892dd70c8f7fb4073bf5b726c5b65f86", "text": "Direct prompting‚Äã\n\nSome models don't support function-calling. We can achieve similar results with direct prompting. Let's try instructing a model to generate structured XML for its output:\n\n```\nxml_system = \"\"\"You're a helpful AI assistant. Given a user question and some Wikipedia article snippets, \\answer the user question and provide citations. If none of the articles answer the question, just say you don't know.Remember, you must return both an answer and citations. A citation consists of a VERBATIM quote that \\justifies the answer and the ID of the quote article. Return a citation for every quote across all articles \\that justify the answer. Use the following format for your final output:<cited_answer>    <answer></answer>    <citations>        <citation><source_id></source_id><quote></quote></citation>        <citation><source_id></source_id><quote></quote></citation>        ...    </citations></cited_answer>Here are the Wikipedia articles:{context}\"\"\"xml_prompt = ChatPromptTemplate.from_messages(    [(\"system\", xml_system), (\"human\", \"{question}\")])\n```\n\nWe now make similar small updates to our chain:\n\n- We update the formatting function to wrap the retrieved context in XML tags;\n- We do not use .with_structured_output (e.g., because it does not exist for a model);\n- We use XMLOutputParser to parse the answer into a dict.\n\n```\nfrom langchain_core.output_parsers import XMLOutputParserdef format_docs_xml(docs: List[Document]) -> str:    formatted = []    for i, doc in enumerate(docs):        doc_str = f\"\"\"\\    <source id=\\\"{i}\\\">        <title>{doc.metadata[\"title\"]}</title>        <article_snippet>{doc.page_content}</article_snippet>    </source>\"\"\"        formatted.append(doc_str)    return \"\\n\\n<sources>\" + \"\\n\".join(formatted) + \"</sources>\"class State(TypedDict):    question: str    context: List[Document]    answer: dictdef generate(state: State):    formatted_docs = format_docs_xml(state[\"context\"])    messages = xml_prompt.invoke(        {\"question\": state[\"question\"], \"context\": formatted_docs}    )    response = llm.invoke(messages)    parsed_response = XMLOutputParser().invoke(response)    return {\"answer\": parsed_response}graph_builder = StateGraph(State).add_sequence([retrieve, generate])graph_builder.add_edge(START, \"retrieve\")graph = graph_builder.compile()\n```\n\nNote that citations are again structured into the answer:\n\n```\nresult = graph.invoke({\"question\": \"How fast are cheetahs?\"})result[\"answer\"]\n```\n\n```\n{'cited_answer': [{'answer': 'Cheetahs can run at speeds of 93 to 104 km/h (58 to 65 mph).'},  {'citations': [{'citation': [{'source_id': '0'},      {'quote': 'The cheetah is capable of running at 93 to 104 km/h (58 to 65 mph);'}]},    {'citation': [{'source_id': '3'},      {'quote': 'The fastest land animal is the cheetah.'}]}]}]}\n```\n\nLangSmith trace: https://smith.langchain.com/public/0c45f847-c640-4b9a-a5fa-63559e413527/r", "url_final": "https://python.langchain.com/docs/how_to/qa_citations/", "url_citable": "https://python.langchain.com/docs/how_to/qa_citations/#direct-prompting", "title": "How to get a RAG application to add citations | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Direct prompting‚Äã", "section_anchor": "direct-prompting", "fetched_at": "2025-10-06T21:56:09.888549", "has_code": true}
{"id": "f32f309275214a7b9c36a1b631b4f6e8fe05c79b", "text": "Retrieval post-processing‚Äã\n\nAnother approach is to post-process our retrieved documents to compress the content, so that the source content is already minimal enough that we don't need the model to cite specific sources or spans. For example, we could break up each document into a sentence or two, embed those and keep only the most relevant ones. LangChain has some built-in components for this. Here we'll use a RecursiveCharacterTextSplitter, which creates chunks of a specified size by splitting on separator substrings, and an EmbeddingsFilter, which keeps only the texts with the most relevant embeddings.\n\nThis approach effectively updates our retrieve step to compress the documents. Let's first select an embedding model:\n\n```\npip install -qU langchain-openai\n```\n\n```\nimport getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"):  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")from langchain_openai import OpenAIEmbeddingsembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n```\n\nWe can now rewrite the retrieve step:\n\n```\nfrom langchain.retrievers.document_compressors import EmbeddingsFilterfrom langchain_core.runnables import RunnableParallelfrom langchain_text_splitters import RecursiveCharacterTextSplittersplitter = RecursiveCharacterTextSplitter(    chunk_size=400,    chunk_overlap=0,    separators=[\"\\n\\n\", \"\\n\", \".\", \" \"],    keep_separator=False,)compressor = EmbeddingsFilter(embeddings=embeddings, k=10)class State(TypedDict):    question: str    context: List[Document]    answer: strdef retrieve(state: State):    retrieved_docs = retriever.invoke(state[\"question\"])    split_docs = splitter.split_documents(retrieved_docs)    stateful_docs = compressor.compress_documents(split_docs, state[\"question\"])    return {\"context\": stateful_docs}\n```\n\nLet's test this out:\n\n```\nretrieval_result = retrieve({\"question\": \"How fast are cheetahs?\"})for doc in retrieval_result[\"context\"]:    print(f\"{doc.page_content}\\n\\n\")\n```\n\n```\nAdults weigh between 21 and 72 kg (46 and 159 lb). The cheetah is capable of running at 93 to 104 km/h (58 to 65 mph); it has evolved specialized adaptations for speed, including a light build, long thin legs and a long tailThe cheetah (Acinonyx jubatus) is a large cat and the fastest land animal. It has a tawny to creamy white or pale buff fur that is marked with evenly spaced, solid black spots. The head is small and rounded, with a short snout and black tear-like facial streaks. It reaches 67‚Äì94 cm (26‚Äì37 in) at the shoulder, and the head-and-body length is between 1.1 and 1.5 m (3 ft 7 in and 4 ft 11 in)2 mph), or 171 body lengths per second. The cheetah, the fastest land mammal, scores at only 16 body lengths per secondIt feeds on small- to medium-sized prey, mostly weighing under 40 kg (88 lb), and prefers medium-sized ungulates such as impala, springbok and Thomson's gazelles. The cheetah typically stalks its prey within 60‚Äì100 m (200‚Äì330 ft) before charging towards it, trips it during the chase and bites its throat to suffocate it to death. It breeds throughout the yearThe cheetah was first described in the late 18th century. Four subspecies are recognised today that are native to Africa and central Iran. An African subspecies was introduced to India in 2022. It is now distributed mainly in small, fragmented populations in northwestern, eastern and southern Africa and central IranThe cheetah lives in three main social groups: females and their cubs, male \"coalitions\", and solitary males. While females lead a nomadic life searching for prey in large home ranges, males are more sedentary and instead establish much smaller territories in areas with plentiful prey and access to females. The cheetah is active during the day, with peaks during dawn and duskThe Southeast African cheetah (Acinonyx jubatus jubatus) is the nominate cheetah subspecies native to East and Southern Africa. The Southern African cheetah lives mainly in the lowland areas and deserts of the Kalahari, the savannahs of Okavango Delta, and the grasslands of the Transvaal region in South Africa. In Namibia, cheetahs are mostly found in farmlandsSubpopulations have been called \"South African cheetah\" and \"Namibian cheetah.\"In India, four cheetahs of the subspecies are living in Kuno National Park in Madhya Pradesh after having been introduced thereAcinonyx jubatus velox proposed in 1913 by Edmund Heller on basis of a cheetah that was shot by Kermit Roosevelt in June 1909 in the Kenyan highlands.Acinonyx rex proposed in 1927 by Reginald Innes Pocock on basis of a specimen from the Umvukwe Range in Rhodesia.\n```\n\nNext, we assemble it into our chain as before:\n\n```\n# This step is unchanged from our original RAG implementationdef generate(state: State):    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})    response = llm.invoke(messages)    return {\"answer\": response.content}graph_builder = StateGraph(State).add_sequence([retrieve, generate])graph_builder.add_edge(START, \"retrieve\")graph = graph_builder.compile()\n```\n\n```\nresult = graph.invoke({\"question\": \"How fast are cheetahs?\"})print(result[\"answer\"])\n```\n\n```\nCheetahs are capable of running at speeds between 93 to 104 km/h (58 to 65 mph). They are known as the fastest land animals.\n```\n\nNote that the document content is now compressed, although the document objects retain the original content in a \"summary\" key in their metadata. These summaries are not passed to the model; only the condensed content is.\n\n```\nresult[\"context\"][0].page_content  # passed to model\n```\n\n```\n'Adults weigh between 21 and 72 kg (46 and 159 lb). The cheetah is capable of running at 93 to 104 km/h (58 to 65 mph); it has evolved specialized adaptations for speed, including a light build, long thin legs and a long tail'\n```\n\n```\nresult[\"context\"][0].metadata[\"summary\"]  # original document  # original document\n```\n\n```\n'The cheetah (Acinonyx jubatus) is a large cat and the fastest land animal. It has a tawny to creamy white or pale buff fur that is marked with evenly spaced, solid black spots. The head is small and rounded, with a short snout and black tear-like facial streaks. It reaches 67‚Äì94 cm (26‚Äì37 in) at the shoulder, and the head-and-body length is between 1.1 and 1.5 m (3 ft 7 in and 4 ft 11 in). Adults weigh between 21 and 72 kg (46 and 159 lb). The cheetah is capable of running at 93 to 104 km/h (58 to 65 mph); it has evolved specialized adaptations for speed, including a light build, long thin legs and a long tail.\\nThe cheetah was first described in the late 18th century. Four subspecies are recognised today that are native to Africa and central Iran. An African subspecies was introduced to India in 2022. It is now distributed mainly in small, fragmented populations in northwestern, eastern and southern Africa and central Iran. It lives in a variety of habitats such as savannahs in the Serengeti, arid mountain ranges in the Sahara, and hilly desert terrain.\\nThe cheetah lives in three main social groups: females and their cubs, male \"coalitions\", and solitary males. While females lead a nomadic life searching for prey in large home ranges, males are more sedentary and instead establish much smaller territories in areas with plentiful prey and access to females. The cheetah is active during the day, with peaks during dawn and dusk. It feeds on small- to medium-sized prey, mostly weighing under 40 kg (88 lb), and prefers medium-sized ungulates such as impala, springbok and Thomson\\'s gazelles. The cheetah typically stalks its prey within 60‚Äì100 m (200‚Äì330 ft) before charging towards it, trips it during the chase and bites its throat to suffocate it to death. It breeds throughout the year. After a gestation of nearly three months, females give birth to a litter of three or four cubs. Cheetah cubs are highly vulnerable to predation by other large carnivores. They are weaned at around four months and are independent by around 20 months of age.\\nThe cheetah is threatened by habitat loss, conflict with humans, poaching and high susceptibility to diseases. The global cheetah population was estimated in 2021 at 6,517; it is listed as Vulnerable on the IUCN Red List. It has been widely depicted in art, literature, advertising, and animation. It was tamed in ancient Egypt and trained for hunting ungulates in the Arabian Peninsula and India. It has been kept in zoos since the early 19th century.'\n```\n\nLangSmith trace: https://smith.langchain.com/public/21b0dc15-d70a-4293-9402-9c70f9178e66/r", "url_final": "https://python.langchain.com/docs/how_to/qa_citations/", "url_citable": "https://python.langchain.com/docs/how_to/qa_citations/#retrieval-post-processing", "title": "How to get a RAG application to add citations | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Retrieval post-processing‚Äã", "section_anchor": "retrieval-post-processing", "fetched_at": "2025-10-06T21:56:09.888549", "has_code": true}
{"id": "b7e37146402b16dc7d78ed66f43d6b57ac7306c3", "text": "Generation post-processing‚Äã\n\nAnother approach is to post-process our model generation. In this example we'll first generate just an answer, and then we'll ask the model to annotate it's own answer with citations. The downside of this approach is of course that it is slower and more expensive, because two model calls need to be made.\n\nLet's apply this to our initial chain. If desired, we can implement this via a third step in our application.\n\n```\nclass Citation(BaseModel):    source_id: int = Field(        ...,        description=\"The integer ID of a SPECIFIC source which justifies the answer.\",    )    quote: str = Field(        ...,        description=\"The VERBATIM quote from the specified source that justifies the answer.\",    )class AnnotatedAnswer(BaseModel):    \"\"\"Annotate the answer to the user question with quote citations that justify the answer.\"\"\"    citations: List[Citation] = Field(        ..., description=\"Citations from the given sources that justify the answer.\"    )structured_llm = llm.with_structured_output(AnnotatedAnswer)\n```\n\n```\nclass State(TypedDict):    question: str    context: List[Document]    answer: str    annotations: AnnotatedAnswerdef retrieve(state: State):    retrieved_docs = retriever.invoke(state[\"question\"])    return {\"context\": retrieved_docs}def generate(state: State):    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})    response = llm.invoke(messages)    return {\"answer\": response.content}def annotate(state: State):    formatted_docs = format_docs_with_id(state[\"context\"])    messages = [        (\"system\", system_prompt.format(context=formatted_docs)),        (\"human\", state[\"question\"]),        (\"ai\", state[\"answer\"]),        (\"human\", \"Annotate your answer with citations.\"),    ]    response = structured_llm.invoke(messages)    return {\"annotations\": response}graph_builder = StateGraph(State).add_sequence([retrieve, generate, annotate])graph_builder.add_edge(START, \"retrieve\")graph = graph_builder.compile()\n```\n\n```\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n```\n\n```\nresult = graph.invoke({\"question\": \"How fast are cheetahs?\"})print(result[\"answer\"])\n```\n\n```\nCheetahs are capable of running at speeds between 93 to 104 km/h (58 to 65 mph).\n```\n\n```\nresult[\"annotations\"]\n```\n\n```\nAnnotatedAnswer(citations=[Citation(source_id=0, quote='The cheetah is capable of running at 93 to 104 km/h (58 to 65 mph)')])\n```\n\nLangSmith trace: https://smith.langchain.com/public/b8257417-573b-47c4-a750-74e542035f19/r\n\n- Cite documents\n- Cite snippets", "url_final": "https://python.langchain.com/docs/how_to/qa_citations/", "url_citable": "https://python.langchain.com/docs/how_to/qa_citations/#generation-post-processing", "title": "How to get a RAG application to add citations | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Generation post-processing‚Äã", "section_anchor": "generation-post-processing", "fetched_at": "2025-10-06T21:56:09.888549", "has_code": true}
{"id": "17e3f478bea13f49373f0fab2725f4188a470ac3", "text": "Overview‚Äã\n\nWe'll go over an example of how to design and implement an LLM-powered chatbot. This chatbot will be able to have a conversation and remember previous interactions with a chat model.\n\nNote that this chatbot that we build will only use the language model to have a conversation. There are several other related concepts that you may be looking for:\n\n- Conversational RAG: Enable a chatbot experience over an external source of data\n- Agents: Build a chatbot that can take actions\n\nThis tutorial will cover the basics which will be helpful for those two more advanced topics, but feel free to skip directly to there should you choose.", "url_final": "https://python.langchain.com/docs/tutorials/chatbot/", "url_citable": "https://python.langchain.com/docs/tutorials/chatbot/#overview", "title": "Build a Chatbot | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Overview‚Äã", "section_anchor": "overview", "fetched_at": "2025-10-06T21:54:06.424785", "has_code": false}
{"id": "c17a4864e724c3abddfce1bedf00815676b49e22", "text": "Setup‚Äã\n\nJupyter Notebook‚Äã\n\nThis guide (and most of the other guides in the documentation) uses Jupyter notebooks and assumes the reader is as well. Jupyter notebooks are perfect for learning how to work with LLM systems because oftentimes things can go wrong (unexpected output, API down, etc) and going through guides in an interactive environment is a great way to better understand them.\n\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebook. See here for instructions on how to install.\n\nInstallation‚Äã\n\nFor this tutorial we will need langchain-core and langgraph. This guide requires langgraph >= 0.2.28.\n\n- Pip\n- Conda\n\n```\npip install langchain-core langgraph>0.2.27\n```\n\n```\nconda install langchain-core langgraph>0.2.27 -c conda-forge\n```\n\nFor more details, see our Installation guide.\n\nLangSmith‚Äã\n\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\n\nAfter you sign up at the link above, (you'll need to create an API key from the Settings -> API Keys page on the LangSmith website), make sure to set your environment variables to start logging traces:\n\n```\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"\n```\n\nOr, if in a notebook, you can set them with:\n\n```\nimport getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n```", "url_final": "https://python.langchain.com/docs/tutorials/chatbot/", "url_citable": "https://python.langchain.com/docs/tutorials/chatbot/#setup", "title": "Build a Chatbot | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Setup‚Äã", "section_anchor": "setup", "fetched_at": "2025-10-06T21:54:06.424785", "has_code": true}
{"id": "96e1912afefb3f3afb05f1657f7d2762bbc1a52e", "text": "Quickstart‚Äã\n\nFirst up, let's learn how to use a language model by itself. LangChain supports many different language models that you can use interchangeably - select the one you want to use below!\n\n```\npip install -qU \"langchain[google-genai]\"\n```\n\n```\nimport getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelmodel = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n```\n\nLet's first use the model directly. ChatModels are instances of LangChain \"Runnables\", which means they expose a standard interface for interacting with them. To just simply call the model, we can pass in a list of messages to the .invoke method.\n\n```\nfrom langchain_core.messages import HumanMessagemodel.invoke([HumanMessage(content=\"Hi! I'm Bob\")])\n```\n\n```\nAIMessage(content='Hi Bob! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 11, 'total_tokens': 21, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-5211544f-da9f-4325-8b8e-b3d92b2fc71a-0', usage_metadata={'input_tokens': 11, 'output_tokens': 10, 'total_tokens': 21, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n```\n\nThe model on its own does not have any concept of state. For example, if you ask a followup question:\n\n```\nmodel.invoke([HumanMessage(content=\"What's my name?\")])\n```\n\n```\nAIMessage(content=\"I'm sorry, but I don't have access to personal information about users unless it has been shared with me in the course of our conversation. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 11, 'total_tokens': 45, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-a2d13a18-7022-4784-b54f-f85c097d1075-0', usage_metadata={'input_tokens': 11, 'output_tokens': 34, 'total_tokens': 45, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n```\n\nLet's take a look at the example LangSmith trace\n\nWe can see that it doesn't take the previous conversation turn into context, and cannot answer the question. This makes for a terrible chatbot experience!\n\nTo get around this, we need to pass the entire conversation history into the model. Let's see what happens when we do that:\n\n```\nfrom langchain_core.messages import AIMessagemodel.invoke(    [        HumanMessage(content=\"Hi! I'm Bob\"),        AIMessage(content=\"Hello Bob! How can I assist you today?\"),        HumanMessage(content=\"What's my name?\"),    ])\n```\n\n```\nAIMessage(content='Your name is Bob! How can I help you today, Bob?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 33, 'total_tokens': 47, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-34bcccb3-446e-42f2-b1de-52c09936c02c-0', usage_metadata={'input_tokens': 33, 'output_tokens': 14, 'total_tokens': 47, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n```\n\nAnd now we can see that we get a good response!\n\nThis is the basic idea underpinning a chatbot's ability to interact conversationally. So how do we best implement this?", "url_final": "https://python.langchain.com/docs/tutorials/chatbot/", "url_citable": "https://python.langchain.com/docs/tutorials/chatbot/#quickstart", "title": "Build a Chatbot | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Quickstart‚Äã", "section_anchor": "quickstart", "fetched_at": "2025-10-06T21:54:06.424785", "has_code": true}
{"id": "5c924ba6e92e208a6eaa2dca30ce4fe20c3959a0", "text": "Message persistence‚Äã\n\nLangGraph implements a built-in persistence layer, making it ideal for chat applications that support multiple conversational turns.\n\nWrapping our chat model in a minimal LangGraph application allows us to automatically persist the message history, simplifying the development of multi-turn applications.\n\nLangGraph comes with a simple in-memory checkpointer, which we use below. See its documentation for more detail, including how to use different persistence backends (e.g., SQLite or Postgres).\n\n```\nfrom langgraph.checkpoint.memory import MemorySaverfrom langgraph.graph import START, MessagesState, StateGraph# Define a new graphworkflow = StateGraph(state_schema=MessagesState)# Define the function that calls the modeldef call_model(state: MessagesState):    response = model.invoke(state[\"messages\"])    return {\"messages\": response}# Define the (single) node in the graphworkflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)# Add memorymemory = MemorySaver()app = workflow.compile(checkpointer=memory)\n```\n\nWe now need to create a config that we pass into the runnable every time. This config contains information that is not part of the input directly, but is still useful. In this case, we want to include a thread_id. This should look like:\n\n```\nconfig = {\"configurable\": {\"thread_id\": \"abc123\"}}\n```\n\nThis enables us to support multiple conversation threads with a single application, a common requirement when your application has multiple users.\n\nWe can then invoke the application:\n\n```\nquery = \"Hi! I'm Bob.\"input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()  # output contains all messages in state\n```\n\n```\n==================================\u001b[1m Ai Message \u001b[0m==================================Hi Bob! How can I assist you today?\n```\n\n```\nquery = \"What's my name?\"input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()\n```\n\n```\n==================================\u001b[1m Ai Message \u001b[0m==================================Your name is Bob! How can I help you today, Bob?\n```\n\nGreat! Our chatbot now remembers things about us. If we change the config to reference a different thread_id, we can see that it starts the conversation fresh.\n\n```\nconfig = {\"configurable\": {\"thread_id\": \"abc234\"}}input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()\n```\n\n```\n==================================\u001b[1m Ai Message \u001b[0m==================================I'm sorry, but I don't have access to personal information about you unless you've shared it in this conversation. How can I assist you today?\n```\n\nHowever, we can always go back to the original conversation (since we are persisting it in a database)\n\n```\nconfig = {\"configurable\": {\"thread_id\": \"abc123\"}}input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()\n```\n\n```\n==================================\u001b[1m Ai Message \u001b[0m==================================Your name is Bob. What would you like to discuss today?\n```\n\nThis is how we can support a chatbot having conversations with many users!\n\nFor async support, update the call_model node to be an async function and use .ainvoke when invoking the application:\n\n```\n# Async function for node:async def call_model(state: MessagesState):    response = await model.ainvoke(state[\"messages\"])    return {\"messages\": response}# Define graph as before:workflow = StateGraph(state_schema=MessagesState)workflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)app = workflow.compile(checkpointer=MemorySaver())# Async invocation:output = await app.ainvoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()\n```\n\nRight now, all we've done is add a simple persistence layer around the model. We can start to make the chatbot more complicated and personalized by adding in a prompt template.", "url_final": "https://python.langchain.com/docs/tutorials/chatbot/", "url_citable": "https://python.langchain.com/docs/tutorials/chatbot/#message-persistence", "title": "Build a Chatbot | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Message persistence‚Äã", "section_anchor": "message-persistence", "fetched_at": "2025-10-06T21:54:06.424785", "has_code": true}
{"id": "e6dd2bd148f15bd7f069abda24eb832b95b8a0ff", "text": "Prompt templates‚Äã\n\nPrompt Templates help to turn raw user information into a format that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM. Let's now make that a bit more complicated. First, let's add in a system message with some custom instructions (but still taking messages as input). Next, we'll add in more input besides just the messages.\n\nTo add in a system message, we will create a ChatPromptTemplate. We will utilize MessagesPlaceholder to pass all the messages in.\n\n```\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholderprompt_template = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"You talk like a pirate. Answer all questions to the best of your ability.\",        ),        MessagesPlaceholder(variable_name=\"messages\"),    ])\n```\n\nWe can now update our application to incorporate this template:\n\n```\nworkflow = StateGraph(state_schema=MessagesState)def call_model(state: MessagesState):    prompt = prompt_template.invoke(state)    response = model.invoke(prompt)    return {\"messages\": response}workflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)memory = MemorySaver()app = workflow.compile(checkpointer=memory)\n```\n\nWe invoke the application in the same way:\n\n```\nconfig = {\"configurable\": {\"thread_id\": \"abc345\"}}query = \"Hi! I'm Jim.\"input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()\n```\n\n```\n==================================\u001b[1m Ai Message \u001b[0m==================================Ahoy there, Jim! What brings ye to these waters today? Be ye seekin' treasure, knowledge, or perhaps a good tale from the high seas? Arrr!\n```\n\n```\nquery = \"What is my name?\"input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()\n```\n\n```\n==================================\u001b[1m Ai Message \u001b[0m==================================Ye be called Jim, matey! A fine name fer a swashbuckler such as yerself! What else can I do fer ye? Arrr!\n```\n\nAwesome! Let's now make our prompt a little bit more complicated. Let's assume that the prompt template now looks something like this:\n\n```\nprompt_template = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",        ),        MessagesPlaceholder(variable_name=\"messages\"),    ])\n```\n\nNote that we have added a new language input to the prompt. Our application now has two parameters-- the input messages and language. We should update our application's state to reflect this:\n\n```\nfrom typing import Sequencefrom langchain_core.messages import BaseMessagefrom langgraph.graph.message import add_messagesfrom typing_extensions import Annotated, TypedDictclass State(TypedDict):    messages: Annotated[Sequence[BaseMessage], add_messages]    language: strworkflow = StateGraph(state_schema=State)def call_model(state: State):    prompt = prompt_template.invoke(state)    response = model.invoke(prompt)    return {\"messages\": [response]}workflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)memory = MemorySaver()app = workflow.compile(checkpointer=memory)\n```\n\n```\nconfig = {\"configurable\": {\"thread_id\": \"abc456\"}}query = \"Hi! I'm Bob.\"language = \"Spanish\"input_messages = [HumanMessage(query)]output = app.invoke(    {\"messages\": input_messages, \"language\": language},    config,)output[\"messages\"][-1].pretty_print()\n```\n\n```\n==================================\u001b[1m Ai Message \u001b[0m==================================¬°Hola, Bob! ¬øC√≥mo puedo ayudarte hoy?\n```\n\nNote that the entire state is persisted, so we can omit parameters like language if no changes are desired:\n\n```\nquery = \"What is my name?\"input_messages = [HumanMessage(query)]output = app.invoke(    {\"messages\": input_messages},    config,)output[\"messages\"][-1].pretty_print()\n```\n\n```\n==================================\u001b[1m Ai Message \u001b[0m==================================Tu nombre es Bob. ¬øHay algo m√°s en lo que pueda ayudarte?\n```\n\nTo help you understand what's happening internally, check out this LangSmith trace.", "url_final": "https://python.langchain.com/docs/tutorials/chatbot/", "url_citable": "https://python.langchain.com/docs/tutorials/chatbot/#prompt-templates", "title": "Build a Chatbot | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Prompt templates‚Äã", "section_anchor": "prompt-templates", "fetched_at": "2025-10-06T21:54:06.424785", "has_code": true}
{"id": "9f37cf9c7a5c752aa64e250b7c343192aa1271cd", "text": "Managing Conversation History‚Äã\n\nOne important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages you are passing in.\n\nImportantly, you will want to do this BEFORE the prompt template but AFTER you load previous messages from Message History.\n\nWe can do this by adding a simple step in front of the prompt that modifies the messages key appropriately, and then wrap that new chain in the Message History class.\n\nLangChain comes with a few built-in helpers for managing a list of messages. In this case we'll use the trim_messages helper to reduce how many messages we're sending to the model. The trimmer allows us to specify how many tokens we want to keep, along with other parameters like if we want to always keep the system message and whether to allow partial messages:\n\n```\nfrom langchain_core.messages import SystemMessage, trim_messagestrimmer = trim_messages(    max_tokens=65,    strategy=\"last\",    token_counter=model,    include_system=True,    allow_partial=False,    start_on=\"human\",)messages = [    SystemMessage(content=\"you're a good assistant\"),    HumanMessage(content=\"hi! I'm bob\"),    AIMessage(content=\"hi!\"),    HumanMessage(content=\"I like vanilla ice cream\"),    AIMessage(content=\"nice\"),    HumanMessage(content=\"whats 2 + 2\"),    AIMessage(content=\"4\"),    HumanMessage(content=\"thanks\"),    AIMessage(content=\"no problem!\"),    HumanMessage(content=\"having fun?\"),    AIMessage(content=\"yes!\"),]trimmer.invoke(messages)\n```\n\n```\n[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}), HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}), AIMessage(content='4', additional_kwargs={}, response_metadata={}), HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}), AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}), HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}), AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]\n```\n\nTo use it in our chain, we just need to run the trimmer before we pass the messages input to our prompt.\n\n```\nworkflow = StateGraph(state_schema=State)def call_model(state: State):    print(f\"Messages before trimming: {len(state['messages'])}\")    trimmed_messages = trimmer.invoke(state[\"messages\"])    print(f\"Messages after trimming: {len(trimmed_messages)}\")    print(\"Remaining messages:\")    for msg in trimmed_messages:        print(f\"  {type(msg).__name__}: {msg.content}\")    prompt = prompt_template.invoke(        {\"messages\": trimmed_messages, \"language\": state[\"language\"]}    )    response = model.invoke(prompt)    return {\"messages\": [response]}workflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)memory = MemorySaver()app = workflow.compile(checkpointer=memory)\n```\n\nNow if we try asking the model our name, it won't know it since we trimmed that part of the chat history. (By defining our trim stragegy as 'last', we are only keeping the most recent messages that fit within the max_tokens.)\n\n```\nconfig = {\"configurable\": {\"thread_id\": \"abc567\"}}query = \"What is my name?\"language = \"English\"input_messages = messages + [HumanMessage(query)]output = app.invoke(    {\"messages\": input_messages, \"language\": language},    config,)output[\"messages\"][-1].pretty_print()\n```\n\n```\nMessages before trimming: 12Messages after trimming: 8Remaining messages:  SystemMessage: you're a good assistant  HumanMessage: whats 2 + 2  AIMessage: 4  HumanMessage: thanks  AIMessage: no problem!  HumanMessage: having fun?  AIMessage: yes!  HumanMessage: What is my name?==================================\u001b[1m Ai Message \u001b[0m==================================I don't know your name. If you'd like to share it, feel free!\n```\n\nBut if we ask about information that is within the last few messages, it remembers:\n\n```\nconfig = {\"configurable\": {\"thread_id\": \"abc678\"}}query = \"What math problem was asked?\"language = \"English\"input_messages = messages + [HumanMessage(query)]output = app.invoke(    {\"messages\": input_messages, \"language\": language},    config,)output[\"messages\"][-1].pretty_print()\n```\n\n```\nMessages before trimming: 12Messages after trimming: 8Remaining messages:  SystemMessage: you're a good assistant  HumanMessage: whats 2 + 2  AIMessage: 4  HumanMessage: thanks  AIMessage: no problem!  HumanMessage: having fun?  AIMessage: yes!  HumanMessage: What math problem was asked?==================================\u001b[1m Ai Message \u001b[0m==================================The math problem that was asked was \"what's 2 + 2.\"\n```\n\nIf you take a look at LangSmith, you can see exactly what is happening under the hood in the LangSmith trace.", "url_final": "https://python.langchain.com/docs/tutorials/chatbot/", "url_citable": "https://python.langchain.com/docs/tutorials/chatbot/#managing-conversation-history", "title": "Build a Chatbot | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Managing Conversation History‚Äã", "section_anchor": "managing-conversation-history", "fetched_at": "2025-10-06T21:54:06.424785", "has_code": true}
{"id": "d22fc32fef82e8e157a4889851a3f93cd3cdfdb4", "text": "Streaming‚Äã\n\nNow we've got a functioning chatbot. However, one really important UX consideration for chatbot applications is streaming. LLMs can sometimes take a while to respond, and so in order to improve the user experience one thing that most applications do is stream back each token as it is generated. This allows the user to see progress.\n\nIt's actually super easy to do this!\n\nBy default, .stream in our LangGraph application streams application steps-- in this case, the single step of the model response. Setting stream_mode=\"messages\" allows us to stream output tokens instead:\n\n```\nconfig = {\"configurable\": {\"thread_id\": \"abc789\"}}query = \"Hi I'm Todd, please tell me a joke.\"language = \"English\"input_messages = [HumanMessage(query)]for chunk, metadata in app.stream(    {\"messages\": input_messages, \"language\": language},    config,    stream_mode=\"messages\",):    if isinstance(chunk, AIMessage):  # Filter to just model responses        print(chunk.content, end=\"|\")\n```\n\n```\n|Hi| Todd|!| Here|‚Äôs| a| joke| for| you|:|Why| don't| scientists| trust| atoms|?|Because| they| make| up| everything|!||\n```", "url_final": "https://python.langchain.com/docs/tutorials/chatbot/", "url_citable": "https://python.langchain.com/docs/tutorials/chatbot/#streaming", "title": "Build a Chatbot | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Streaming‚Äã", "section_anchor": "streaming", "fetched_at": "2025-10-06T21:54:06.424785", "has_code": true}
{"id": "1937edcd41257c9613b60d6bfef11891cb367f8e", "text": "Next Steps‚Äã\n\nNow that you understand the basics of how to create a chatbot in LangChain, some more advanced tutorials you may be interested in are:\n\n- Conversational RAG: Enable a chatbot experience over an external source of data\n- Agents: Build a chatbot that can take actions\n\nIf you want to dive deeper on specifics, some things worth checking out are:\n\n- Streaming: streaming is crucial for chat applications\n- How to add message history: for a deeper dive into all things related to message history\n- How to manage large message history: more techniques for managing a large chat history\n- LangGraph main docs: for more detail on building with LangGraph\n\n- Jupyter Notebook\n- Installation\n- LangSmith", "url_final": "https://python.langchain.com/docs/tutorials/chatbot/", "url_citable": "https://python.langchain.com/docs/tutorials/chatbot/#next-steps", "title": "Build a Chatbot | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Next Steps‚Äã", "section_anchor": "next-steps", "fetched_at": "2025-10-06T21:54:06.424785", "has_code": false}
{"id": "acc7542b30fede357c656bdabe0957f7a6782612", "text": "How to parse text from message objects | ü¶úÔ∏èüîó LangChain\n\nThis guide assumes familiarity with the following concepts:\n\n- Chat models\n- Messages\n- Output parsers\n- LangChain Expression Language (LCEL)\n\nLangChain message objects support content in a variety of formats, including text, multimodal data, and a list of content block dicts.\n\nThe format of Chat model response content may depend on the provider. For example, the chat model for Anthropic will return string content for typical string input:\n\n```\nfrom langchain_anthropic import ChatAnthropicllm = ChatAnthropic(model=\"claude-3-5-haiku-latest\")response = llm.invoke(\"Hello\")response.content\n```\n\n```\n'Hi there! How are you doing today? Is there anything I can help you with?'\n```\n\nBut when tool calls are generated, the response content is structured into content blocks that convey the model's reasoning process:\n\n```\nfrom langchain_core.tools import tool@tooldef get_weather(location: str) -> str:    \"\"\"Get the weather from a location.\"\"\"    return \"Sunny.\"llm_with_tools = llm.bind_tools([get_weather])response = llm_with_tools.invoke(\"What's the weather in San Francisco, CA?\")response.content\n```\n\n```\n[{'text': \"I'll help you get the current weather for San Francisco, California. Let me check that for you right away.\",  'type': 'text'}, {'id': 'toolu_015PwwcKxWYctKfY3pruHFyy',  'input': {'location': 'San Francisco, CA'},  'name': 'get_weather',  'type': 'tool_use'}]\n```\n\nTo automatically parse text from message objects irrespective of the format of the underlying content, we can use StrOutputParser. We can compose it with a chat model as follows:\n\n```\nfrom langchain_core.output_parsers import StrOutputParserchain = llm_with_tools | StrOutputParser()\n```\n\nStrOutputParser simplifies the extraction of text from message objects:\n\n```\nresponse = chain.invoke(\"What's the weather in San Francisco, CA?\")print(response)\n```\n\n```\nI'll help you check the weather in San Francisco, CA right away.\n```\n\nThis is particularly useful in streaming contexts:\n\n```\nfor chunk in chain.stream(\"What's the weather in San Francisco, CA?\"):    print(chunk, end=\"|\")\n```\n\n```\n|I'll| help| you get| the current| weather for| San Francisco, California|. Let| me retrieve| that| information for you.||||||||||\n```\n\nSee the API Reference for more information.", "url_final": "https://python.langchain.com/docs/how_to/output_parser_string/", "url_citable": "https://python.langchain.com/docs/how_to/output_parser_string/", "title": "How to parse text from message objects | ü¶úÔ∏èüîó LangChain", "section_level": 1, "section_heading": "How to parse text from message objects | ü¶úÔ∏èüîó LangChain", "section_anchor": null, "fetched_at": "2025-10-06T21:56:03.420843", "has_code": true}
{"id": "0ff8bd4af4b336aa959bcff55e6fc5ba4c6a75ca", "text": "Setup‚Äã\n\nBefore getting started make sure you've created a LangSmith account and set your credentials:\n\n```\nimport getpassimport osif not os.environ.get(\"LANGSMITH_API_KEY\"):    os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Set LangSmith API key:\\n\\n\")os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n```\n\n```\nSet LangSmith API key:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n```\n\nWe'll need to install the langsmith SDK. In this example we'll also make use of langchain, langchain-openai, and langchain-benchmarks:\n\n```\n%pip install -qU \"langsmith>=0.1.101\" \"langchain-core>=0.2.34\" langchain langchain-openai langchain-benchmarks\n```\n\nNow we'll clone a public dataset and turn on indexing for the dataset. We can also turn on indexing via the LangSmith UI.\n\nWe'll clone the Multiverse math few shot example dataset.\n\nThis enables searching over the dataset and will make sure that anytime we update/add examples they are also indexed.\n\n```\nfrom langsmith import Client as LangSmithls_client = LangSmith()dataset_name = \"multiverse-math-few-shot-examples-v2\"dataset_public_url = (    \"https://smith.langchain.com/public/620596ee-570b-4d2b-8c8f-f828adbe5242/d\")ls_client.clone_public_dataset(dataset_public_url)dataset_id = ls_client.read_dataset(dataset_name=dataset_name).idls_client.index_dataset(dataset_id=dataset_id)\n```", "url_final": "https://python.langchain.com/docs/how_to/example_selectors_langsmith/", "url_citable": "https://python.langchain.com/docs/how_to/example_selectors_langsmith/#setup", "title": "How to select examples from a LangSmith dataset | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Setup‚Äã", "section_anchor": "setup", "fetched_at": "2025-10-06T21:55:32.045656", "has_code": true}
{"id": "279432e4460ae50ae58e9343170e27b3b8a10224", "text": "Querying dataset‚Äã\n\nIndexing can take a few seconds. Once the dataset is indexed, we can search for similar examples. Note that the input to the similar_examples method must have the same schema as the examples inputs. In this case our example inputs are a dictionary with a \"question\" key:\n\n```\nexamples = ls_client.similar_examples(    {\"question\": \"whats the negation of the negation of the negation of 3\"},    limit=3,    dataset_id=dataset_id,)len(examples)\n```\n\n```\n3\n```\n\n```\nexamples[0].inputs[\"question\"]\n```\n\n```\n'evaluate the negation of -100'\n```\n\nFor this dataset, the outputs are the conversation that followed the question in OpenAI message format:\n\n```\nexamples[0].outputs[\"conversation\"]\n```\n\n```\n[{'role': 'assistant',  'content': None,  'tool_calls': [{'id': 'toolu_01HTpq4cYNUac6F7omUc2Wz3',    'type': 'function',    'function': {'name': 'negate', 'arguments': '{\"a\": -100}'}}]}, {'role': 'tool',  'content': '-100.0',  'tool_call_id': 'toolu_01HTpq4cYNUac6F7omUc2Wz3'}, {'role': 'assistant', 'content': 'So the answer is 100.'}, {'role': 'user',  'content': '100 is incorrect. Please refer to the output of your tool call.'}, {'role': 'assistant',  'content': [{'text': \"You're right, my previous answer was incorrect. Let me re-evaluate using the tool output:\",    'type': 'text'}],  'tool_calls': [{'id': 'toolu_01XsJQboYghGDygQpPjJkeRq',    'type': 'function',    'function': {'name': 'negate', 'arguments': '{\"a\": -100}'}}]}, {'role': 'tool',  'content': '-100.0',  'tool_call_id': 'toolu_01XsJQboYghGDygQpPjJkeRq'}, {'role': 'assistant', 'content': 'The answer is -100.0'}, {'role': 'user',  'content': 'You have the correct numerical answer but are returning additional text. Please only respond with the numerical answer.'}, {'role': 'assistant', 'content': '-100.0'}]\n```", "url_final": "https://python.langchain.com/docs/how_to/example_selectors_langsmith/", "url_citable": "https://python.langchain.com/docs/how_to/example_selectors_langsmith/#querying-dataset", "title": "How to select examples from a LangSmith dataset | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Querying dataset‚Äã", "section_anchor": "querying-dataset", "fetched_at": "2025-10-06T21:55:32.045656", "has_code": true}
{"id": "72f3c81942a58560005ed70e34817efa5f3228d6", "text": "Creating dynamic few-shot prompts‚Äã\n\nThe search returns the examples whose inputs are most similar to the query input. We can use this for few-shot prompting a model like so:\n\n```\nfrom langchain.chat_models import init_chat_modelfrom langchain_benchmarks.tool_usage.tasks.multiverse_math import (    add,    cos,    divide,    log,    multiply,    negate,    pi,    power,    sin,    subtract,)from langchain_core.runnables import RunnableLambdafrom langsmith import AsyncClient as AsyncLangSmithasync_ls_client = AsyncLangSmith()def similar_examples(input_: dict) -> dict:    examples = ls_client.similar_examples(input_, limit=5, dataset_id=dataset_id)    return {**input_, \"examples\": examples}async def asimilar_examples(input_: dict) -> dict:    examples = await async_ls_client.similar_examples(        input_, limit=5, dataset_id=dataset_id    )    return {**input_, \"examples\": examples}def construct_prompt(input_: dict) -> list:    instructions = \"\"\"You are great at using mathematical tools.\"\"\"    examples = []    for ex in input_[\"examples\"]:        examples.append({\"role\": \"user\", \"content\": ex.inputs[\"question\"]})        for msg in ex.outputs[\"conversation\"]:            if msg[\"role\"] == \"assistant\":                msg[\"name\"] = \"example_assistant\"            if msg[\"role\"] == \"user\":                msg[\"name\"] = \"example_user\"            examples.append(msg)    return [        {\"role\": \"system\", \"content\": instructions},        *examples,        {\"role\": \"user\", \"content\": input_[\"question\"]},    ]tools = [add, cos, divide, log, multiply, negate, pi, power, sin, subtract]llm = init_chat_model(\"gpt-4o-2024-08-06\")llm_with_tools = llm.bind_tools(tools)example_selector = RunnableLambda(func=similar_examples, afunc=asimilar_examples)chain = example_selector | construct_prompt | llm_with_tools\n```\n\n```\nai_msg = await chain.ainvoke({\"question\": \"whats the negation of the negation of 3\"})ai_msg.tool_calls\n```\n\n```\n[{'name': 'negate',  'args': {'a': 3},  'id': 'call_uMSdoTl6ehfHh5a6JQUb2NoZ',  'type': 'tool_call'}]\n```\n\nLooking at the LangSmith trace, we can see that relevant examples were pulled in in the similar_examples step and passed as messages to ChatOpenAI: https://smith.langchain.com/public/9585e30f-765a-4ed9-b964-2211420cd2f8/r/fdea98d6-e90f-49d4-ac22-dfd012e9e0d9.\n\n- Setup\n- Querying dataset\n- Creating dynamic few-shot prompts", "url_final": "https://python.langchain.com/docs/how_to/example_selectors_langsmith/", "url_citable": "https://python.langchain.com/docs/how_to/example_selectors_langsmith/#creating-dynamic-few-shot-prompts", "title": "How to select examples from a LangSmith dataset | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Creating dynamic few-shot prompts‚Äã", "section_anchor": "creating-dynamic-few-shot-prompts", "fetched_at": "2025-10-06T21:55:32.045656", "has_code": true}
{"id": "ba2b08b03b71a2d973186375376805458b86ea67", "text": "Next steps‚Äã\n\nYou've now learned how to pass callbacks at runtime.\n\nNext, check out the other how-to guides in this section, such as how to pass callbacks into a module constructor.\n\n- Next steps", "url_final": "https://python.langchain.com/docs/how_to/callbacks_runtime/", "url_citable": "https://python.langchain.com/docs/how_to/callbacks_runtime/#next-steps", "title": "How to pass callbacks in at runtime | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Next steps‚Äã", "section_anchor": "next-steps", "fetched_at": "2025-10-06T21:55:01.972333", "has_code": false}
{"id": "8fb73f6ff5e47e181e3b4646e79f28a1310bea57", "text": "‚ö†Ô∏è Security note ‚ö†Ô∏è‚Äã\n\nBoth approaches mentioned above carry significant risks. Using SQL requires executing model-generated SQL queries. Using a library like Pandas requires letting the model execute Python code. Since it is easier to tightly scope SQL connection permissions and sanitize SQL queries than it is to sandbox Python environments, we HIGHLY recommend interacting with CSV data via SQL. For more on general security best practices, see here.", "url_final": "https://python.langchain.com/docs/how_to/sql_csv/", "url_citable": "https://python.langchain.com/docs/how_to/sql_csv/#Ô∏è-security-note-Ô∏è", "title": "How to do question answering over CSVs | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "‚ö†Ô∏è Security note ‚ö†Ô∏è‚Äã", "section_anchor": "Ô∏è-security-note-Ô∏è", "fetched_at": "2025-10-06T21:56:24.369270", "has_code": false}
{"id": "481d231ca2ec05d095eab25741f99b120d20320a", "text": "Setup‚Äã\n\nDependencies for this guide:\n\n```\n%pip install -qU langchain langchain-openai langchain-community langchain-experimental pandas\n```\n\nSet required environment variables:\n\n```\n# Using LangSmith is recommended but not required. Uncomment below lines to use.# import os# os.environ[\"LANGSMITH_TRACING\"] = \"true\"# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n```\n\nDownload the Titanic dataset if you don't already have it:\n\n```\n!wget https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv -O titanic.csv\n```\n\n```\nimport pandas as pddf = pd.read_csv(\"titanic.csv\")print(df.shape)print(df.columns.tolist())\n```\n\n```\n(887, 8)['Survived', 'Pclass', 'Name', 'Sex', 'Age', 'Siblings/Spouses Aboard', 'Parents/Children Aboard', 'Fare']\n```", "url_final": "https://python.langchain.com/docs/how_to/sql_csv/", "url_citable": "https://python.langchain.com/docs/how_to/sql_csv/#setup", "title": "How to do question answering over CSVs | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Setup‚Äã", "section_anchor": "setup", "fetched_at": "2025-10-06T21:56:24.369270", "has_code": true}
{"id": "363a58fcbe597f3a974f2498a0423a4d71054391", "text": "SQL‚Äã\n\nUsing SQL to interact with CSV data is the recommended approach because it is easier to limit permissions and sanitize queries than with arbitrary Python.\n\nMost SQL databases make it easy to load a CSV file in as a table (DuckDB, SQLite, etc.). Once you've done this you can use all of the chain and agent-creating techniques outlined in the SQL tutorial. Here's a quick example of how we might do this with SQLite:\n\n```\nfrom langchain_community.utilities import SQLDatabasefrom sqlalchemy import create_engineengine = create_engine(\"sqlite:///titanic.db\")df.to_sql(\"titanic\", engine, index=False)\n```\n\n```\n887\n```\n\n```\ndb = SQLDatabase(engine=engine)print(db.dialect)print(db.get_usable_table_names())print(db.run(\"SELECT * FROM titanic WHERE Age < 2;\"))\n```\n\n```\nsqlite['titanic'][(1, 2, 'Master. Alden Gates Caldwell', 'male', 0.83, 0, 2, 29.0), (0, 3, 'Master. Eino Viljami Panula', 'male', 1.0, 4, 1, 39.6875), (1, 3, 'Miss. Eleanor Ileen Johnson', 'female', 1.0, 1, 1, 11.1333), (1, 2, 'Master. Richard F Becker', 'male', 1.0, 2, 1, 39.0), (1, 1, 'Master. Hudson Trevor Allison', 'male', 0.92, 1, 2, 151.55), (1, 3, 'Miss. Maria Nakid', 'female', 1.0, 0, 2, 15.7417), (0, 3, 'Master. Sidney Leonard Goodwin', 'male', 1.0, 5, 2, 46.9), (1, 3, 'Miss. Helene Barbara Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 3, 'Miss. Eugenie Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 2, 'Master. Viljo Hamalainen', 'male', 0.67, 1, 1, 14.5), (1, 3, 'Master. Bertram Vere Dean', 'male', 1.0, 1, 2, 20.575), (1, 3, 'Master. Assad Alexander Thomas', 'male', 0.42, 0, 1, 8.5167), (1, 2, 'Master. Andre Mallet', 'male', 1.0, 0, 2, 37.0042), (1, 2, 'Master. George Sibley Richards', 'male', 0.83, 1, 1, 18.75)]\n```\n\nAnd create a SQL agent to interact with it:\n\n```\npip install -qU \"langchain[google-genai]\"\n```\n\n```\nimport getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n```\n\n```\nfrom langchain_community.agent_toolkits import create_sql_agentagent_executor = create_sql_agent(llm, db=db, agent_type=\"openai-tools\", verbose=True)\n```\n\n```\nagent_executor.invoke({\"input\": \"what's the average age of survivors\"})\n```\n\n```\n\u001b[1m> Entering new SQL Agent Executor chain...\u001b[0m\u001b[32;1m\u001b[1;3mInvoking: `sql_db_list_tables` with `{}`\u001b[0m\u001b[38;5;200m\u001b[1;3mtitanic\u001b[0m\u001b[32;1m\u001b[1;3mInvoking: `sql_db_schema` with `{'table_names': 'titanic'}`\u001b[0m\u001b[33;1m\u001b[1;3mCREATE TABLE titanic (\t\"Survived\" BIGINT, \t\"Pclass\" BIGINT, \t\"Name\" TEXT, \t\"Sex\" TEXT, \t\"Age\" FLOAT, \t\"Siblings/Spouses Aboard\" BIGINT, \t\"Parents/Children Aboard\" BIGINT, \t\"Fare\" FLOAT)/*3 rows from titanic table:Survived\tPclass\tName\tSex\tAge\tSiblings/Spouses Aboard\tParents/Children Aboard\tFare0\t3\tMr. Owen Harris Braund\tmale\t22.0\t1\t0\t7.251\t1\tMrs. John Bradley (Florence Briggs Thayer) Cumings\tfemale\t38.0\t1\t0\t71.28331\t3\tMiss. Laina Heikkinen\tfemale\t26.0\t0\t0\t7.925*/\u001b[0m\u001b[32;1m\u001b[1;3mInvoking: `sql_db_query` with `{'query': 'SELECT AVG(Age) AS Average_Age FROM titanic WHERE Survived = 1'}`\u001b[0m\u001b[36;1m\u001b[1;3m[(28.408391812865496,)]\u001b[0m\u001b[32;1m\u001b[1;3mThe average age of survivors in the Titanic dataset is approximately 28.41 years.\u001b[0m\u001b[1m> Finished chain.\u001b[0m\n```\n\n```\n{'input': \"what's the average age of survivors\", 'output': 'The average age of survivors in the Titanic dataset is approximately 28.41 years.'}\n```\n\nThis approach easily generalizes to multiple CSVs, since we can just load each of them into our database as its own table. See the Multiple CSVs section below.", "url_final": "https://python.langchain.com/docs/how_to/sql_csv/", "url_citable": "https://python.langchain.com/docs/how_to/sql_csv/#sql", "title": "How to do question answering over CSVs | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "SQL‚Äã", "section_anchor": "sql", "fetched_at": "2025-10-06T21:56:24.369270", "has_code": true}
{"id": "ffda00756c0b33e222319ef5d766663f8224b007", "text": "Pandas‚Äã\n\nInstead of SQL we can also use data analysis libraries like pandas and the code generating abilities of LLMs to interact with CSV data. Again, this approach is not fit for production use cases unless you have extensive safeguards in place. For this reason, our code-execution utilities and constructors live in the langchain-experimental package.", "url_final": "https://python.langchain.com/docs/how_to/sql_csv/", "url_citable": "https://python.langchain.com/docs/how_to/sql_csv/#pandas", "title": "How to do question answering over CSVs | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Pandas‚Äã", "section_anchor": "pandas", "fetched_at": "2025-10-06T21:56:24.369270", "has_code": false}
{"id": "e98436f414d048baaf62d2b1ff513a6731560470", "text": "Chain‚Äã\n\nMost LLMs have been trained on enough pandas Python code that they can generate it just by being asked to:\n\n```\nai_msg = llm.invoke(    \"I have a pandas DataFrame 'df' with columns 'Age' and 'Fare'. Write code to compute the correlation between the two columns. Return Markdown for a Python code snippet and nothing else.\")print(ai_msg.content)\n```\n\n```\n\\`\\`\\`pythoncorrelation = df['Age'].corr(df['Fare'])correlation\\`\\`\\`\n```\n\nWe can combine this ability with a Python-executing tool to create a simple data analysis chain. We'll first want to load our CSV table as a dataframe, and give the tool access to this dataframe:\n\n```\nimport pandas as pdfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_experimental.tools import PythonAstREPLTooldf = pd.read_csv(\"titanic.csv\")tool = PythonAstREPLTool(locals={\"df\": df})tool.invoke(\"df['Fare'].mean()\")\n```\n\n```\n32.30542018038331\n```\n\nTo help enforce proper use of our Python tool, we'll using tool calling:\n\n```\nllm_with_tools = llm.bind_tools([tool], tool_choice=tool.name)response = llm_with_tools.invoke(    \"I have a dataframe 'df' and want to know the correlation between the 'Age' and 'Fare' columns\")response\n```\n\n```\nAIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_SBrK246yUbdnJemXFC8Iod05', 'function': {'arguments': '{\"query\":\"df.corr()[\\'Age\\'][\\'Fare\\']\"}', 'name': 'python_repl_ast'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 125, 'total_tokens': 138}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3b956da36b', 'finish_reason': 'stop', 'logprobs': None}, id='run-1fd332ba-fa72-4351-8182-d464e7368311-0', tool_calls=[{'name': 'python_repl_ast', 'args': {'query': \"df.corr()['Age']['Fare']\"}, 'id': 'call_SBrK246yUbdnJemXFC8Iod05'}])\n```\n\n```\nresponse.tool_calls\n```\n\n```\n[{'name': 'python_repl_ast',  'args': {'query': \"df.corr()['Age']['Fare']\"},  'id': 'call_SBrK246yUbdnJemXFC8Iod05'}]\n```\n\nWe'll add a tools output parser to extract the function call as a dict:\n\n```\nfrom langchain_core.output_parsers.openai_tools import JsonOutputKeyToolsParserparser = JsonOutputKeyToolsParser(key_name=tool.name, first_tool_only=True)(llm_with_tools | parser).invoke(    \"I have a dataframe 'df' and want to know the correlation between the 'Age' and 'Fare' columns\")\n```\n\n```\n{'query': \"df[['Age', 'Fare']].corr()\"}\n```\n\nAnd combine with a prompt so that we can just specify a question without needing to specify the dataframe info every invocation:\n\n```\nsystem = f\"\"\"You have access to a pandas dataframe `df`. \\Here is the output of `df.head().to_markdown()`:\\`\\`\\`{df.head().to_markdown()}\\`\\`\\`Given a user question, write the Python code to answer it. \\Return ONLY the valid Python code and nothing else. \\Don't assume you have access to any libraries other than built-in Python ones and pandas.\"\"\"prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", \"{question}\")])code_chain = prompt | llm_with_tools | parsercode_chain.invoke({\"question\": \"What's the correlation between age and fare\"})\n```\n\n```\n{'query': \"df[['Age', 'Fare']].corr()\"}\n```\n\nAnd lastly we'll add our Python tool so that the generated code is actually executed:\n\n```\nchain = prompt | llm_with_tools | parser | toolchain.invoke({\"question\": \"What's the correlation between age and fare\"})\n```\n\n```\n0.11232863699941621\n```\n\nAnd just like that we have a simple data analysis chain. We can take a peak at the intermediate steps by looking at the LangSmith trace: https://smith.langchain.com/public/b1309290-7212-49b7-bde2-75b39a32b49a/r\n\nWe could add an additional LLM call at the end to generate a conversational response, so that we're not just responding with the tool output. For this we'll want to add a chat history MessagesPlaceholder to our prompt:\n\n```\nfrom operator import itemgetterfrom langchain_core.messages import ToolMessagefrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import MessagesPlaceholderfrom langchain_core.runnables import RunnablePassthroughsystem = f\"\"\"You have access to a pandas dataframe `df`. \\Here is the output of `df.head().to_markdown()`:\\`\\`\\`{df.head().to_markdown()}\\`\\`\\`Given a user question, write the Python code to answer it. \\Don't assume you have access to any libraries other than built-in Python ones and pandas.Respond directly to the question once you have enough information to answer it.\"\"\"prompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            system,        ),        (\"human\", \"{question}\"),        # This MessagesPlaceholder allows us to optionally append an arbitrary number of messages        # at the end of the prompt using the 'chat_history' arg.        MessagesPlaceholder(\"chat_history\", optional=True),    ])def _get_chat_history(x: dict) -> list:    \"\"\"Parse the chain output up to this point into a list of chat history messages to insert in the prompt.\"\"\"    ai_msg = x[\"ai_msg\"]    tool_call_id = x[\"ai_msg\"].additional_kwargs[\"tool_calls\"][0][\"id\"]    tool_msg = ToolMessage(tool_call_id=tool_call_id, content=str(x[\"tool_output\"]))    return [ai_msg, tool_msg]chain = (    RunnablePassthrough.assign(ai_msg=prompt | llm_with_tools)    .assign(tool_output=itemgetter(\"ai_msg\") | parser | tool)    .assign(chat_history=_get_chat_history)    .assign(response=prompt | llm | StrOutputParser())    .pick([\"tool_output\", \"response\"]))\n```\n\n```\nchain.invoke({\"question\": \"What's the correlation between age and fare\"})\n```\n\n```\n{'tool_output': 0.11232863699941616, 'response': 'The correlation between age and fare is approximately 0.1123.'}\n```\n\nHere's the LangSmith trace for this run: https://smith.langchain.com/public/14e38d70-45b1-4b81-8477-9fd2b7c07ea6/r", "url_final": "https://python.langchain.com/docs/how_to/sql_csv/", "url_citable": "https://python.langchain.com/docs/how_to/sql_csv/#chain", "title": "How to do question answering over CSVs | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Chain‚Äã", "section_anchor": "chain", "fetched_at": "2025-10-06T21:56:24.369270", "has_code": true}
{"id": "d32a8df32d5b845b7064fb1be45afa6f9847e432", "text": "Agent‚Äã\n\nFor complex questions it can be helpful for an LLM to be able to iteratively execute code while maintaining the inputs and outputs of its previous executions. This is where Agents come into play. They allow an LLM to decide how many times a tool needs to be invoked and keep track of the executions it's made so far. The create_pandas_dataframe_agent is a built-in agent that makes it easy to work with dataframes:\n\n```\nfrom langchain_experimental.agents import create_pandas_dataframe_agentagent = create_pandas_dataframe_agent(    llm, df, agent_type=\"openai-tools\", verbose=True, allow_dangerous_code=True)agent.invoke(    {        \"input\": \"What's the correlation between age and fare? is that greater than the correlation between fare and survival?\"    })\n```\n\n```\n\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\u001b[32;1m\u001b[1;3mInvoking: `python_repl_ast` with `{'query': \"df[['Age', 'Fare']].corr().iloc[0,1]\"}`\u001b[0m\u001b[36;1m\u001b[1;3m0.11232863699941621\u001b[0m\u001b[32;1m\u001b[1;3mInvoking: `python_repl_ast` with `{'query': \"df[['Fare', 'Survived']].corr().iloc[0,1]\"}`\u001b[0m\u001b[36;1m\u001b[1;3m0.2561785496289603\u001b[0m\u001b[32;1m\u001b[1;3mThe correlation between Age and Fare is approximately 0.112, and the correlation between Fare and Survival is approximately 0.256.Therefore, the correlation between Fare and Survival (0.256) is greater than the correlation between Age and Fare (0.112).\u001b[0m\u001b[1m> Finished chain.\u001b[0m\n```\n\n```\n{'input': \"What's the correlation between age and fare? is that greater than the correlation between fare and survival?\", 'output': 'The correlation between Age and Fare is approximately 0.112, and the correlation between Fare and Survival is approximately 0.256.\\n\\nTherefore, the correlation between Fare and Survival (0.256) is greater than the correlation between Age and Fare (0.112).'}\n```\n\nHere's the LangSmith trace for this run: https://smith.langchain.com/public/6a86aee2-4f22-474a-9264-bd4c7283e665/r", "url_final": "https://python.langchain.com/docs/how_to/sql_csv/", "url_citable": "https://python.langchain.com/docs/how_to/sql_csv/#agent", "title": "How to do question answering over CSVs | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Agent‚Äã", "section_anchor": "agent", "fetched_at": "2025-10-06T21:56:24.369270", "has_code": true}
{"id": "88ff307571dcce6e2dc44f2e53217d089d1cad1f", "text": "Multiple CSVs‚Äã\n\nTo handle multiple CSVs (or dataframes) we just need to pass multiple dataframes to our Python tool. Our create_pandas_dataframe_agent constructor can do this out of the box, we can pass in a list of dataframes instead of just one. If we're constructing a chain ourselves, we can do something like:\n\n```\ndf_1 = df[[\"Age\", \"Fare\"]]df_2 = df[[\"Fare\", \"Survived\"]]tool = PythonAstREPLTool(locals={\"df_1\": df_1, \"df_2\": df_2})llm_with_tool = llm.bind_tools(tools=[tool], tool_choice=tool.name)df_template = \"\"\"\\`\\`\\`python{df_name}.head().to_markdown()>>> {df_head}\\`\\`\\`\"\"\"df_context = \"\\n\\n\".join(    df_template.format(df_head=_df.head().to_markdown(), df_name=df_name)    for _df, df_name in [(df_1, \"df_1\"), (df_2, \"df_2\")])system = f\"\"\"You have access to a number of pandas dataframes. \\Here is a sample of rows from each dataframe and the python code that was used to generate the sample:{df_context}Given a user question about the dataframes, write the Python code to answer it. \\Don't assume you have access to any libraries other than built-in Python ones and pandas. \\Make sure to refer only to the variables mentioned above.\"\"\"prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", \"{question}\")])chain = prompt | llm_with_tool | parser | toolchain.invoke(    {        \"question\": \"return the difference in the correlation between age and fare and the correlation between fare and survival\"    })\n```\n\n```\n0.14384991262954416\n```\n\nHere's the LangSmith trace for this run: https://smith.langchain.com/public/cc2a7d7f-7c5a-4e77-a10c-7b5420fcd07f/r", "url_final": "https://python.langchain.com/docs/how_to/sql_csv/", "url_citable": "https://python.langchain.com/docs/how_to/sql_csv/#multiple-csvs", "title": "How to do question answering over CSVs | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Multiple CSVs‚Äã", "section_anchor": "multiple-csvs", "fetched_at": "2025-10-06T21:56:24.369270", "has_code": true}
{"id": "43fe27baf45f8e74aafef360450922684d325141", "text": "Sandboxed code execution‚Äã\n\nThere are a number of tools like E2B and Bearly that provide sandboxed environments for Python code execution, to allow for safer code-executing chains and agents.", "url_final": "https://python.langchain.com/docs/how_to/sql_csv/", "url_citable": "https://python.langchain.com/docs/how_to/sql_csv/#sandboxed-code-execution", "title": "How to do question answering over CSVs | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Sandboxed code execution‚Äã", "section_anchor": "sandboxed-code-execution", "fetched_at": "2025-10-06T21:56:24.369270", "has_code": false}
{"id": "dda79caa9c8c8fe44f1d21f76a111713676b7afb", "text": "Next steps‚Äã\n\nFor more advanced data analysis applications we recommend checking out:\n\n- SQL tutorial: Many of the challenges of working with SQL db's and CSV's are generic to any structured data type, so it's useful to read the SQL techniques even if you're using Pandas for CSV data analysis.\n- Tool use: Guides on general best practices when working with chains and agents that invoke tools\n- Agents: Understand the fundamentals of building LLM agents.\n- Integrations: Sandboxed envs like E2B and Bearly, utilities like SQLDatabase, related agents like Spark DataFrame agent.\n\n- Chain\n- Agent\n- Multiple CSVs\n- Sandboxed code execution", "url_final": "https://python.langchain.com/docs/how_to/sql_csv/", "url_citable": "https://python.langchain.com/docs/how_to/sql_csv/#next-steps", "title": "How to do question answering over CSVs | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Next steps‚Äã", "section_anchor": "next-steps", "fetched_at": "2025-10-06T21:56:24.369270", "has_code": false}
{"id": "66ae1b6dff4b01e8ef52382a3c1d31864b937ca3", "text": "Setup‚Äã\n\nFirst we'll install the dependencies needed for this guide:\n\n```\n%pip install -qU langchain-community lxml faiss-cpu langchain-openai\n```\n\n```\nNote: you may need to restart the kernel to use updated packages.\n```\n\nNow we need some example data! Let's download an article about cars from wikipedia and load it as a LangChain Document.\n\n```\nimport reimport requestsfrom langchain_community.document_loaders import BSHTMLLoader# Download the contentresponse = requests.get(\"https://en.wikipedia.org/wiki/Car\")# Write it to a filewith open(\"car.html\", \"w\", encoding=\"utf-8\") as f:    f.write(response.text)# Load it with an HTML parserloader = BSHTMLLoader(\"car.html\")document = loader.load()[0]# Clean up code# Replace consecutive new lines with a single new linedocument.page_content = re.sub(\"\\n\\n+\", \"\\n\", document.page_content)\n```\n\n```\nprint(len(document.page_content))\n```\n\n```\n78865\n```", "url_final": "https://python.langchain.com/docs/how_to/extraction_long_text/", "url_citable": "https://python.langchain.com/docs/how_to/extraction_long_text/#setup", "title": "How to handle long text when doing extraction | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Setup‚Äã", "section_anchor": "setup", "fetched_at": "2025-10-06T21:55:38.662993", "has_code": true}
{"id": "f564c8ef73e84fb7afb66425722db99de9447a86", "text": "Define the schema‚Äã\n\nFollowing the extraction tutorial, we will use Pydantic to define the schema of information we wish to extract. In this case, we will extract a list of \"key developments\" (e.g., important historical events) that include a year and description.\n\nNote that we also include an evidence key and instruct the model to provide in verbatim the relevant sentences of text from the article. This allows us to compare the extraction results to (the model's reconstruction of) text from the original document.\n\n```\nfrom typing import List, Optionalfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholderfrom pydantic import BaseModel, Fieldclass KeyDevelopment(BaseModel):    \"\"\"Information about a development in the history of cars.\"\"\"    year: int = Field(        ..., description=\"The year when there was an important historic development.\"    )    description: str = Field(        ..., description=\"What happened in this year? What was the development?\"    )    evidence: str = Field(        ...,        description=\"Repeat in verbatim the sentence(s) from which the year and description information were extracted\",    )class ExtractionData(BaseModel):    \"\"\"Extracted information about key developments in the history of cars.\"\"\"    key_developments: List[KeyDevelopment]# Define a custom prompt to provide instructions and any additional context.# 1) You can add examples into the prompt template to improve extraction quality# 2) Introduce additional parameters to take context into account (e.g., include metadata#    about the document from which the text was extracted.)prompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"You are an expert at identifying key historic development in text. \"            \"Only extract important historic developments. Extract nothing if no important information can be found in the text.\",        ),        (\"human\", \"{text}\"),    ])\n```", "url_final": "https://python.langchain.com/docs/how_to/extraction_long_text/", "url_citable": "https://python.langchain.com/docs/how_to/extraction_long_text/#define-the-schema", "title": "How to handle long text when doing extraction | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Define the schema‚Äã", "section_anchor": "define-the-schema", "fetched_at": "2025-10-06T21:55:38.662993", "has_code": true}
{"id": "652672ee1e4b442903171e0b436fae1bb7b407a9", "text": "Create an extractor‚Äã\n\nLet's select an LLM. Because we are using tool-calling, we will need a model that supports a tool-calling feature. See this table for available LLMs.\n\n```\npip install -qU \"langchain[google-genai]\"\n```\n\n```\nimport getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n```\n\n```\nextractor = prompt | llm.with_structured_output(    schema=ExtractionData,    include_raw=False,)\n```", "url_final": "https://python.langchain.com/docs/how_to/extraction_long_text/", "url_citable": "https://python.langchain.com/docs/how_to/extraction_long_text/#create-an-extractor", "title": "How to handle long text when doing extraction | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Create an extractor‚Äã", "section_anchor": "create-an-extractor", "fetched_at": "2025-10-06T21:55:38.662993", "has_code": true}
{"id": "1eb513bc846e075bd335a09f8ac75fc0a0c824bc", "text": "Brute force approach‚Äã\n\nSplit the documents into chunks such that each chunk fits into the context window of the LLMs.\n\n```\nfrom langchain_text_splitters import TokenTextSplittertext_splitter = TokenTextSplitter(    # Controls the size of each chunk    chunk_size=2000,    # Controls overlap between chunks    chunk_overlap=20,)texts = text_splitter.split_text(document.page_content)\n```\n\nUse batch functionality to run the extraction in parallel across each chunk!\n\nYou can often use .batch() to parallelize the extractions! .batch uses a threadpool under the hood to help you parallelize workloads.\n\nIf your model is exposed via an API, this will likely speed up your extraction flow!\n\n```\n# Limit just to the first 3 chunks# so the code can be re-run quicklyfirst_few = texts[:3]extractions = extractor.batch(    [{\"text\": text} for text in first_few],    {\"max_concurrency\": 5},  # limit the concurrency by passing max concurrency!)\n```", "url_final": "https://python.langchain.com/docs/how_to/extraction_long_text/", "url_citable": "https://python.langchain.com/docs/how_to/extraction_long_text/#brute-force-approach", "title": "How to handle long text when doing extraction | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Brute force approach‚Äã", "section_anchor": "brute-force-approach", "fetched_at": "2025-10-06T21:55:38.662993", "has_code": true}
{"id": "15b93b8b1413636a0c23e7b4acc89e3d0a4b87c9", "text": "Merge results‚Äã\n\nAfter extracting data from across the chunks, we'll want to merge the extractions together.\n\n```\nkey_developments = []for extraction in extractions:    key_developments.extend(extraction.key_developments)key_developments[:10]\n```\n\n```\n[KeyDevelopment(year=1769, description='Nicolas-Joseph Cugnot built the first steam-powered road vehicle.', evidence='The French inventor Nicolas-Joseph Cugnot built the first steam-powered road vehicle in 1769, while the Swiss inventor Fran√ßois Isaac de Rivaz designed and constructed the first internal combustion-powered automobile in 1808.'), KeyDevelopment(year=1808, description='Fran√ßois Isaac de Rivaz designed and constructed the first internal combustion-powered automobile.', evidence='The French inventor Nicolas-Joseph Cugnot built the first steam-powered road vehicle in 1769, while the Swiss inventor Fran√ßois Isaac de Rivaz designed and constructed the first internal combustion-powered automobile in 1808.'), KeyDevelopment(year=1886, description='Carl Benz invented the modern car, a practical, marketable automobile for everyday use, and patented his Benz Patent-Motorwagen.', evidence='The modern car‚Äîa practical, marketable automobile for everyday use‚Äîwas invented in 1886, when the German inventor Carl Benz patented his Benz Patent-Motorwagen.'), KeyDevelopment(year=1901, description='The Oldsmobile Curved Dash became the first mass-produced car.', evidence='The 1901 Oldsmobile Curved Dash and the 1908 Ford Model T, both American cars, are widely considered the first mass-produced[3][4] and mass-affordable[5][6][7] cars, respectively.'), KeyDevelopment(year=1908, description='The Ford Model T became the first mass-affordable car.', evidence='The 1901 Oldsmobile Curved Dash and the 1908 Ford Model T, both American cars, are widely considered the first mass-produced[3][4] and mass-affordable[5][6][7] cars, respectively.'), KeyDevelopment(year=1885, description='Carl Benz built the original Benz Patent-Motorwagen, the first modern car.', evidence='The original Benz Patent-Motorwagen, the first modern car, built in 1885 and awarded the patent for the concept'), KeyDevelopment(year=1881, description='Gustave Trouv√© demonstrated a three-wheeled car powered by electricity.', evidence='In November 1881, French inventor Gustave Trouv√© demonstrated a three-wheeled car powered by electricity at the International Exposition of Electricity.'), KeyDevelopment(year=1888, description=\"Bertha Benz undertook the first road trip by car to prove the road-worthiness of her husband's invention.\", evidence=\"In August 1888, Bertha Benz, the wife and business partner of Carl Benz, undertook the first road trip by car, to prove the road-worthiness of her husband's invention.\"), KeyDevelopment(year=1896, description='Benz designed and patented the first internal-combustion flat engine, called boxermotor.', evidence='In 1896, Benz designed and patented the first internal-combustion flat engine, called boxermotor.'), KeyDevelopment(year=1897, description='The first motor car in central Europe and one of the first factory-made cars in the world was produced by Czech company Nesselsdorfer Wagenbau (later renamed to Tatra), the Pr√§sident automobil.', evidence='The first motor car in central Europe and one of the first factory-made cars in the world, was produced by Czech company Nesselsdorfer Wagenbau (later renamed to Tatra) in 1897, the Pr√§sident automobil.')]\n```", "url_final": "https://python.langchain.com/docs/how_to/extraction_long_text/", "url_citable": "https://python.langchain.com/docs/how_to/extraction_long_text/#merge-results", "title": "How to handle long text when doing extraction | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Merge results‚Äã", "section_anchor": "merge-results", "fetched_at": "2025-10-06T21:55:38.662993", "has_code": true}
{"id": "f70d1004ad1ee5a93c0cc71f9d0f6c66610180b1", "text": "RAG based approach‚Äã\n\nAnother simple idea is to chunk up the text, but instead of extracting information from every chunk, just focus on the most relevant chunks.\n\nIt can be difficult to identify which chunks are relevant.\n\nFor example, in the car article we're using here, most of the article contains key development information. So by using RAG, we'll likely be throwing out a lot of relevant information.\n\nWe suggest experimenting with your use case and determining whether this approach works or not.\n\nTo implement the RAG based approach:\n\n- Chunk up your document(s) and index them (e.g., in a vectorstore);\n- Prepend the extractor chain with a retrieval step using the vectorstore.\n\nHere's a simple example that relies on the FAISS vectorstore.\n\n```\nfrom langchain_community.vectorstores import FAISSfrom langchain_core.documents import Documentfrom langchain_core.runnables import RunnableLambdafrom langchain_openai import OpenAIEmbeddingsfrom langchain_text_splitters import CharacterTextSplittertexts = text_splitter.split_text(document.page_content)vectorstore = FAISS.from_texts(texts, embedding=OpenAIEmbeddings())retriever = vectorstore.as_retriever(    search_kwargs={\"k\": 1})  # Only extract from first document\n```\n\nIn this case the RAG extractor is only looking at the top document.\n\n```\nrag_extractor = {    \"text\": retriever | (lambda docs: docs[0].page_content)  # fetch content of top doc} | extractor\n```\n\n```\nresults = rag_extractor.invoke(\"Key developments associated with cars\")\n```\n\n```\nfor key_development in results.key_developments:    print(key_development)\n```\n\n```\nyear=2006 description='Car-sharing services in the US experienced double-digit growth in revenue and membership.' evidence='in the US, some car-sharing services have experienced double-digit growth in revenue and membership growth between 2006 and 2007.'year=2020 description='56 million cars were manufactured worldwide, with China producing the most.' evidence='In 2020, there were 56 million cars manufactured worldwide, down from 67 million the previous year. The automotive industry in China produces by far the most (20 million in 2020).'\n```", "url_final": "https://python.langchain.com/docs/how_to/extraction_long_text/", "url_citable": "https://python.langchain.com/docs/how_to/extraction_long_text/#rag-based-approach", "title": "How to handle long text when doing extraction | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "RAG based approach‚Äã", "section_anchor": "rag-based-approach", "fetched_at": "2025-10-06T21:55:38.662993", "has_code": true}
{"id": "357b92340c2b9a6c9cdaa21d5fefd576b8ef2c7d", "text": "Common issues‚Äã\n\nDifferent methods have their own pros and cons related to cost, speed, and accuracy.\n\nWatch out for these issues:\n\n- Chunking content means that the LLM can fail to extract information if the information is spread across multiple chunks.\n- Large chunk overlap may cause the same information to be extracted twice, so be prepared to de-duplicate!\n- LLMs can make up data. If looking for a single fact across a large text and using a brute force approach, you may end up getting more made up data.\n\n- Merge results", "url_final": "https://python.langchain.com/docs/how_to/extraction_long_text/", "url_citable": "https://python.langchain.com/docs/how_to/extraction_long_text/#common-issues", "title": "How to handle long text when doing extraction | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Common issues‚Äã", "section_anchor": "common-issues", "fetched_at": "2025-10-06T21:55:38.662993", "has_code": false}
{"id": "3a2770c0315ed13c1acf10176d05758a41e74b11", "text": "Overview‚Äã\n\nTagging has a few components:\n\n- function: Like extraction, tagging uses functions to specify how the model should tag a document\n- schema: defines how we want to tag the document", "url_final": "https://python.langchain.com/docs/tutorials/classification/", "url_citable": "https://python.langchain.com/docs/tutorials/classification/#overview", "title": "Tagging | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Overview‚Äã", "section_anchor": "overview", "fetched_at": "2025-10-06T21:54:10.432485", "has_code": false}
{"id": "5034c5499a00df1f84eb6736240f990c155af3d7", "text": "Quickstart‚Äã\n\nLet's see a very straightforward example of how we can use OpenAI tool calling for tagging in LangChain. We'll use the with_structured_output method supported by OpenAI models.\n\n```\npip install -U langchain-core\n```\n\nWe'll need to load a chat model:\n\n```\npip install -qU \"langchain[google-genai]\"\n```\n\n```\nimport getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n```\n\nLet's specify a Pydantic model with a few properties and their expected type in our schema.\n\n```\nfrom langchain_core.prompts import ChatPromptTemplatefrom pydantic import BaseModel, Fieldtagging_prompt = ChatPromptTemplate.from_template(    \"\"\"Extract the desired information from the following passage.Only extract the properties mentioned in the 'Classification' function.Passage:{input}\"\"\")class Classification(BaseModel):    sentiment: str = Field(description=\"The sentiment of the text\")    aggressiveness: int = Field(        description=\"How aggressive the text is on a scale from 1 to 10\"    )    language: str = Field(description=\"The language the text is written in\")# Structured LLMstructured_llm = llm.with_structured_output(Classification)\n```\n\n```\ninp = \"Estoy increiblemente contento de haberte conocido! Creo que seremos muy buenos amigos!\"prompt = tagging_prompt.invoke({\"input\": inp})response = structured_llm.invoke(prompt)response\n```\n\n```\nClassification(sentiment='positive', aggressiveness=1, language='Spanish')\n```\n\nIf we want dictionary output, we can just call .model_dump()\n\n```\ninp = \"Estoy muy enojado con vos! Te voy a dar tu merecido!\"prompt = tagging_prompt.invoke({\"input\": inp})response = structured_llm.invoke(prompt)response.model_dump()\n```\n\n```\n{'sentiment': 'angry', 'aggressiveness': 8, 'language': 'Spanish'}\n```\n\nAs we can see in the examples, it correctly interprets what we want.\n\nThe results vary so that we may get, for example, sentiments in different languages ('positive', 'enojado' etc.).\n\nWe will see how to control these results in the next section.", "url_final": "https://python.langchain.com/docs/tutorials/classification/", "url_citable": "https://python.langchain.com/docs/tutorials/classification/#quickstart", "title": "Tagging | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Quickstart‚Äã", "section_anchor": "quickstart", "fetched_at": "2025-10-06T21:54:10.432485", "has_code": true}
{"id": "2dd45f8157f84ea3ce773d8070f5d04d2261780c", "text": "Finer control‚Äã\n\nCareful schema definition gives us more control over the model's output.\n\nSpecifically, we can define:\n\n- Possible values for each property\n- Description to make sure that the model understands the property\n- Required properties to be returned\n\nLet's redeclare our Pydantic model to control for each of the previously mentioned aspects using enums:\n\n```\nclass Classification(BaseModel):    sentiment: str = Field(..., enum=[\"happy\", \"neutral\", \"sad\"])    aggressiveness: int = Field(        ...,        description=\"describes how aggressive the statement is, the higher the number the more aggressive\",        enum=[1, 2, 3, 4, 5],    )    language: str = Field(        ..., enum=[\"spanish\", \"english\", \"french\", \"german\", \"italian\"]    )\n```\n\n```\ntagging_prompt = ChatPromptTemplate.from_template(    \"\"\"Extract the desired information from the following passage.Only extract the properties mentioned in the 'Classification' function.Passage:{input}\"\"\")structured_llm = llm.with_structured_output(Classification)\n```\n\nNow the answers will be restricted in a way we expect!\n\n```\ninp = \"Estoy increiblemente contento de haberte conocido! Creo que seremos muy buenos amigos!\"prompt = tagging_prompt.invoke({\"input\": inp})structured_llm.invoke(prompt)\n```\n\n```\nClassification(sentiment='happy', aggressiveness=1, language='spanish')\n```\n\n```\ninp = \"Estoy muy enojado con vos! Te voy a dar tu merecido!\"prompt = tagging_prompt.invoke({\"input\": inp})structured_llm.invoke(prompt)\n```\n\n```\nClassification(sentiment='sad', aggressiveness=4, language='spanish')\n```\n\n```\ninp = \"Weather is ok here, I can go outside without much more than a coat\"prompt = tagging_prompt.invoke({\"input\": inp})structured_llm.invoke(prompt)\n```\n\n```\nClassification(sentiment='happy', aggressiveness=1, language='english')\n```\n\nThe LangSmith trace lets us peek under the hood:\n\nGoing deeper‚Äã\n\n- You can use the metadata tagger document transformer to extract metadata from a LangChain Document.\n- This covers the same basic functionality as the tagging chain, only applied to a LangChain Document.\n\n- Going deeper", "url_final": "https://python.langchain.com/docs/tutorials/classification/", "url_citable": "https://python.langchain.com/docs/tutorials/classification/#finer-control", "title": "Tagging | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Finer control‚Äã", "section_anchor": "finer-control", "fetched_at": "2025-10-06T21:54:10.432485", "has_code": true}
{"id": "f8f6b6fc2c91190a9aaf5df6014233ba6dadef96", "text": "Setup‚Äã\n\nFirst, get required packages and set environment variables:\n\n```\n%pip install --upgrade --quiet  langchain langchain-neo4j langchain-openai\n```\n\nWe default to OpenAI models in this guide, but you can swap them out for the model provider of your choice.\n\n```\nimport getpassimport osos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()# Uncomment the below to use LangSmith. Not required.# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n```\n\n```\n ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n```\n\nNext, we need to define Neo4j credentials. Follow these installation steps to set up a Neo4j database.\n\n```\nos.environ[\"NEO4J_URI\"] = \"bolt://localhost:7687\"os.environ[\"NEO4J_USERNAME\"] = \"neo4j\"os.environ[\"NEO4J_PASSWORD\"] = \"password\"\n```\n\nThe below example will create a connection with a Neo4j database and will populate it with example data about movies and their actors.\n\n```\nfrom langchain_neo4j import Neo4jGraphgraph = Neo4jGraph(refresh_schema=False)# Import movie informationmovies_query = \"\"\"LOAD CSV WITH HEADERS FROM 'https://raw.githubusercontent.com/tomasonjo/blog-datasets/main/movies/movies_small.csv'AS rowMERGE (m:Movie {id:row.movieId})SET m.released = date(row.released),    m.title = row.title,    m.imdbRating = toFloat(row.imdbRating)FOREACH (director in split(row.director, '|') |     MERGE (p:Person {name:trim(director)})    MERGE (p)-[:DIRECTED]->(m))FOREACH (actor in split(row.actors, '|') |     MERGE (p:Person {name:trim(actor)})    MERGE (p)-[:ACTED_IN]->(m))FOREACH (genre in split(row.genres, '|') |     MERGE (g:Genre {name:trim(genre)})    MERGE (m)-[:IN_GENRE]->(g))\"\"\"graph.query(movies_query)\n```\n\n```\n[]\n```", "url_final": "https://python.langchain.com/docs/how_to/graph_semantic/", "url_citable": "https://python.langchain.com/docs/how_to/graph_semantic/#setup", "title": "How to add a semantic layer over graph database | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Setup‚Äã", "section_anchor": "setup", "fetched_at": "2025-10-06T21:54:20.756347", "has_code": true}
{"id": "aa98b2716f87ed9f3c6be518303454ad8a25a1a9", "text": "Custom tools with Cypher templates‚Äã\n\nA semantic layer consists of various tools exposed to an LLM that it can use to interact with a knowledge graph. They can be of various complexity. You can think of each tool in a semantic layer as a function.\n\nThe function we will implement is to retrieve information about movies or their cast.\n\n```\ndescription_query = \"\"\"MATCH (m:Movie|Person)WHERE m.title CONTAINS $candidate OR m.name CONTAINS $candidateMATCH (m)-[r:ACTED_IN|IN_GENRE]-(t)WITH m, type(r) as type, collect(coalesce(t.name, t.title)) as namesWITH m, type+\": \"+reduce(s=\"\", n IN names | s + n + \", \") as typesWITH m, collect(types) as contextsWITH m, \"type:\" + labels(m)[0] + \"\\ntitle: \"+ coalesce(m.title, m.name)        + \"\\nyear: \"+coalesce(m.released,\"\") +\"\\n\" +       reduce(s=\"\", c in contexts | s + substring(c, 0, size(c)-2) +\"\\n\") as contextRETURN context LIMIT 1\"\"\"def get_information(entity: str) -> str:    try:        data = graph.query(description_query, params={\"candidate\": entity})        return data[0][\"context\"]    except IndexError:        return \"No information was found\"\n```\n\nYou can observe that we have defined the Cypher statement used to retrieve information. Therefore, we can avoid generating Cypher statements and use the LLM agent to only populate the input parameters. To provide additional information to an LLM agent about when to use the tool and their input parameters, we wrap the function as a tool.\n\n```\nfrom typing import Optional, Typefrom langchain_core.tools import BaseToolfrom pydantic import BaseModel, Fieldclass InformationInput(BaseModel):    entity: str = Field(description=\"movie or a person mentioned in the question\")class InformationTool(BaseTool):    name: str = \"Information\"    description: str = (        \"useful for when you need to answer questions about various actors or movies\"    )    args_schema: Type[BaseModel] = InformationInput    def _run(        self,        entity: str,    ) -> str:        \"\"\"Use the tool.\"\"\"        return get_information(entity)    async def _arun(        self,        entity: str,    ) -> str:        \"\"\"Use the tool asynchronously.\"\"\"        return get_information(entity)\n```", "url_final": "https://python.langchain.com/docs/how_to/graph_semantic/", "url_citable": "https://python.langchain.com/docs/how_to/graph_semantic/#custom-tools-with-cypher-templates", "title": "How to add a semantic layer over graph database | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Custom tools with Cypher templates‚Äã", "section_anchor": "custom-tools-with-cypher-templates", "fetched_at": "2025-10-06T21:54:20.756347", "has_code": true}
{"id": "060595424a7d906df5f5c6a696a445646fa44056", "text": "LangGraph Agent‚Äã\n\nWe will implement a straightforward ReAct agent using LangGraph.\n\nThe agent consists of an LLM and tools step. As we interact with the agent, we will first call the LLM to decide if we should use tools. Then we will run a loop:\n\nIf the agent said to take an action (i.e. call tool), we‚Äôll run the tools and pass the results back to the agent. If the agent did not ask to run tools, we will finish (respond to the user).\n\nThe code implementation is as straightforward as it gets. First we bind the tools to the LLM and define the assistant step.\n\n```\nfrom langchain_core.messages import HumanMessage, SystemMessagefrom langchain_openai import ChatOpenAIfrom langgraph.graph import MessagesStatellm = ChatOpenAI(model=\"gpt-4o\")tools = [InformationTool()]llm_with_tools = llm.bind_tools(tools)# System messagesys_msg = SystemMessage(    content=\"You are a helpful assistant tasked with finding and explaining relevant information about movies.\")# Nodedef assistant(state: MessagesState):    return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}\n```\n\nNext we define the LangGraph flow.\n\n```\nfrom IPython.display import Image, displayfrom langgraph.graph import END, START, StateGraphfrom langgraph.prebuilt import ToolNode, tools_condition# Graphbuilder = StateGraph(MessagesState)# Define nodes: these do the workbuilder.add_node(\"assistant\", assistant)builder.add_node(\"tools\", ToolNode(tools))# Define edges: these determine how the control flow movesbuilder.add_edge(START, \"assistant\")builder.add_conditional_edges(    \"assistant\",    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END    tools_condition,)builder.add_edge(\"tools\", \"assistant\")react_graph = builder.compile()# Showdisplay(Image(react_graph.get_graph(xray=True).draw_mermaid_png()))\n```\n\nLet's test the workflow now with an example question.\n\n```\ninput_messages = [HumanMessage(content=\"Who played in the Casino?\")]messages = react_graph.invoke({\"messages\": input_messages})for m in messages[\"messages\"]:    m.pretty_print()\n```\n\n```\n================================\u001b[1m Human Message \u001b[0m=================================Who played in the Casino?==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls:  Information (call_j4usgFStGtBM16fuguRaeoGc) Call ID: call_j4usgFStGtBM16fuguRaeoGc  Args:    entity: Casino=================================\u001b[1m Tool Message \u001b[0m=================================Name: Informationtype:Movietitle: Casinoyear: 1995-11-22ACTED_IN: Robert De Niro, Joe Pesci, Sharon Stone, James WoodsIN_GENRE: Drama, Crime==================================\u001b[1m Ai Message \u001b[0m==================================The movie \"Casino,\" released in 1995, features the following actors:- Robert De Niro- Joe Pesci- Sharon Stone- James WoodsThe film is in the Drama and Crime genres.\n```\n\n- Setup\n- Custom tools with Cypher templates\n- LangGraph Agent", "url_final": "https://python.langchain.com/docs/how_to/graph_semantic/", "url_citable": "https://python.langchain.com/docs/how_to/graph_semantic/#langgraph-agent", "title": "How to add a semantic layer over graph database | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "LangGraph Agent‚Äã", "section_anchor": "langgraph-agent", "fetched_at": "2025-10-06T21:54:20.756347", "has_code": true}
{"id": "e158d16cd495ef28f34f93a455c85f7d6b979f0c", "text": "Sync stream‚Äã\n\nBelow we use a | to help visualize the delimiter between tokens.\n\n```\nfrom langchain_openai import OpenAIllm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0, max_tokens=512)for chunk in llm.stream(\"Write me a 1 verse song about sparkling water.\"):    print(chunk, end=\"|\", flush=True)\n```\n\n```\n|Spark|ling| water|,| oh| so clear||Bubbles dancing|,| without| fear||Refreshing| taste|,| a| pure| delight||Spark|ling| water|,| my| thirst|'s| delight||\n```", "url_final": "https://python.langchain.com/docs/how_to/streaming_llm/", "url_citable": "https://python.langchain.com/docs/how_to/streaming_llm/#sync-stream", "title": "How to stream responses from an LLM | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Sync stream‚Äã", "section_anchor": "sync-stream", "fetched_at": "2025-10-06T21:56:29.669571", "has_code": true}
{"id": "3e2fbfe70da4330ef07c1351d6f05d51b4d1fd1c", "text": "Async streaming‚Äã\n\nLet's see how to stream in an async setting using astream.\n\n```\nfrom langchain_openai import OpenAIllm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0, max_tokens=512)async for chunk in llm.astream(\"Write me a 1 verse song about sparkling water.\"):    print(chunk, end=\"|\", flush=True)\n```\n\n```\n|Spark|ling| water|,| oh| so clear||Bubbles dancing|,| without| fear||Refreshing| taste|,| a| pure| delight||Spark|ling| water|,| my| thirst|'s| delight||\n```", "url_final": "https://python.langchain.com/docs/how_to/streaming_llm/", "url_citable": "https://python.langchain.com/docs/how_to/streaming_llm/#async-streaming", "title": "How to stream responses from an LLM | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Async streaming‚Äã", "section_anchor": "async-streaming", "fetched_at": "2025-10-06T21:56:29.669571", "has_code": true}
{"id": "fcef6c1ad3045c26bcc758f6bc262c4b37aaf17c", "text": "Async event streaming‚Äã\n\nLLMs also support the standard astream events method.\n\nastream_events is most useful when implementing streaming in a larger LLM application that contains multiple steps (e.g., an application that involves an agent).\n\n```\nfrom langchain_openai import OpenAIllm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0, max_tokens=512)idx = 0async for event in llm.astream_events(    \"Write me a 1 verse song about goldfish on the moon\", version=\"v1\"):    idx += 1    if idx >= 5:  # Truncate the output        print(\"...Truncated\")        break    print(event)\n```\n\n- Sync stream\n- Async streaming\n- Async event streaming", "url_final": "https://python.langchain.com/docs/how_to/streaming_llm/", "url_citable": "https://python.langchain.com/docs/how_to/streaming_llm/#async-event-streaming", "title": "How to stream responses from an LLM | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Async event streaming‚Äã", "section_anchor": "async-event-streaming", "fetched_at": "2025-10-06T21:56:29.669571", "has_code": true}
{"id": "9ee7cd975b0956a1e4ede4b216bd9f87bec5ab85", "text": "How to split by character | ü¶úÔ∏èüîó LangChain\n\nThis is the simplest method. This splits based on a given character sequence, which defaults to \"\\n\\n\". Chunk length is measured by number of characters.\n\n- How the text is split: by single character separator.\n- How the chunk size is measured: by number of characters.\n\nTo obtain the string content directly, use .split_text.\n\nTo create LangChain Document objects (e.g., for use in downstream tasks), use .create_documents.\n\n```\n%pip install -qU langchain-text-splitters\n```\n\n```\nfrom langchain_text_splitters import CharacterTextSplitter# Load an example documentwith open(\"state_of_the_union.txt\") as f:    state_of_the_union = f.read()text_splitter = CharacterTextSplitter(    separator=\"\\n\\n\",    chunk_size=1000,    chunk_overlap=200,    length_function=len,    is_separator_regex=False,)texts = text_splitter.create_documents([state_of_the_union])print(texts[0])\n```\n\n```\npage_content='Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \\n\\nLast year COVID-19 kept us apart. This year we are finally together again. \\n\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\n\\nWith a duty to one another to the American people to the Constitution. \\n\\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \\n\\nSix days ago, Russia‚Äôs Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \\n\\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \\n\\nHe met the Ukrainian people. \\n\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world.'\n```\n\nUse .create_documents to propagate metadata associated with each document to the output chunks:\n\n```\nmetadatas = [{\"document\": 1}, {\"document\": 2}]documents = text_splitter.create_documents(    [state_of_the_union, state_of_the_union], metadatas=metadatas)print(documents[0])\n```\n\n```\npage_content='Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \\n\\nLast year COVID-19 kept us apart. This year we are finally together again. \\n\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\n\\nWith a duty to one another to the American people to the Constitution. \\n\\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \\n\\nSix days ago, Russia‚Äôs Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \\n\\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \\n\\nHe met the Ukrainian people. \\n\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world.' metadata={'document': 1}\n```\n\nUse .split_text to obtain the string content directly:\n\n```\ntext_splitter.split_text(state_of_the_union)[0]\n```\n\n```\n'Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \\n\\nLast year COVID-19 kept us apart. This year we are finally together again. \\n\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\n\\nWith a duty to one another to the American people to the Constitution. \\n\\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \\n\\nSix days ago, Russia‚Äôs Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \\n\\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \\n\\nHe met the Ukrainian people. \\n\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world.'\n```", "url_final": "https://python.langchain.com/docs/how_to/character_text_splitter/", "url_citable": "https://python.langchain.com/docs/how_to/character_text_splitter/", "title": "How to split by character | ü¶úÔ∏èüîó LangChain", "section_level": 1, "section_heading": "How to split by character | ü¶úÔ∏èüîó LangChain", "section_anchor": null, "fetched_at": "2025-10-06T21:55:02.989886", "has_code": true}
{"id": "d78bb660e8f1255cae0bfebb9d7a2a95f7735446", "text": "Setup‚Äã\n\nComponents‚Äã\n\nWe will need to select three components from LangChain's suite of integrations.\n\n```\npip install -qU \"langchain[google-genai]\"\n```\n\n```\nimport getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n```\n\n```\npip install -qU langchain-openai\n```\n\n```\nimport getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"):  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")from langchain_openai import OpenAIEmbeddingsembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n```\n\n```\npip install -qU langchain-core\n```\n\n```\nfrom langchain_core.vectorstores import InMemoryVectorStorevector_store = InMemoryVectorStore(embeddings)\n```\n\nDependencies‚Äã\n\nIn addition, we'll use the following packages:\n\n```\n%%capture --no-stderr%pip install --upgrade --quiet langgraph langchain-community beautifulsoup4\n```\n\nLangSmith‚Äã\n\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\n\nNote that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:\n\n```\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"if not os.environ.get(\"LANGSMITH_API_KEY\"):    os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n```", "url_final": "https://python.langchain.com/docs/tutorials/qa_chat_history/", "url_citable": "https://python.langchain.com/docs/tutorials/qa_chat_history/#setup", "title": "Build a Retrieval Augmented Generation (RAG) App: Part 2 | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Setup‚Äã", "section_anchor": "setup", "fetched_at": "2025-10-06T21:54:07.431871", "has_code": true}
{"id": "ce9522b1eae8d5de21b2f79bbe7bfa4fa38c2cfc", "text": "Chains‚Äã\n\nLet's first revisit the vector store we built in Part 1, which indexes an LLM Powered Autonomous Agents blog post by Lilian Weng.\n\n```\nimport bs4from langchain import hubfrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_core.documents import Documentfrom langchain_text_splitters import RecursiveCharacterTextSplitterfrom typing_extensions import List, TypedDict# Load and chunk contents of the blogloader = WebBaseLoader(    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),    bs_kwargs=dict(        parse_only=bs4.SoupStrainer(            class_=(\"post-content\", \"post-title\", \"post-header\")        )    ),)docs = loader.load()text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)all_splits = text_splitter.split_documents(docs)\n```\n\n```\n# Index chunks_ = vector_store.add_documents(documents=all_splits)\n```\n\nIn the Part 1 of the RAG tutorial, we represented the user input, retrieved context, and generated answer as separate keys in the state. Conversational experiences can be naturally represented using a sequence of messages. In addition to messages from the user and assistant, retrieved documents and other artifacts can be incorporated into a message sequence via tool messages. This motivates us to represent the state of our RAG application using a sequence of messages. Specifically, we will have\n\n- User input as a HumanMessage;\n- Vector store query as an AIMessage with tool calls;\n- Retrieved documents as a ToolMessage;\n- Final response as a AIMessage.\n\nThis model for state is so versatile that LangGraph offers a built-in version for convenience:\n\n```\nfrom langgraph.graph import MessagesState, StateGraphgraph_builder = StateGraph(MessagesState)\n```\n\nLeveraging tool-calling to interact with a retrieval step has another benefit, which is that the query for the retrieval is generated by our model. This is especially important in a conversational setting, where user queries may require contextualization based on the chat history. For instance, consider the following exchange:\n\nHuman: \"What is Task Decomposition?\"\n\nAI: \"Task decomposition involves breaking down complex tasks into smaller and simpler steps to make them more manageable for an agent or model.\"\n\nHuman: \"What are common ways of doing it?\"\n\nIn this scenario, a model could generate a query such as \"common approaches to task decomposition\". Tool-calling facilitates this naturally. As in the query analysis section of the RAG tutorial, this allows a model to rewrite user queries into more effective search queries. It also provides support for direct responses that do not involve a retrieval step (e.g., in response to a generic greeting from the user).\n\nLet's turn our retrieval step into a tool:\n\n```\nfrom langchain_core.tools import tool@tool(response_format=\"content_and_artifact\")def retrieve(query: str):    \"\"\"Retrieve information related to a query.\"\"\"    retrieved_docs = vector_store.similarity_search(query, k=2)    serialized = \"\\n\\n\".join(        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")        for doc in retrieved_docs    )    return serialized, retrieved_docs\n```\n\nSee this guide for more detail on creating tools.\n\nOur graph will consist of three nodes:\n\n- A node that fields the user input, either generating a query for the retriever or responding directly;\n- A node for the retriever tool that executes the retrieval step;\n- A node that generates the final response using the retrieved context.\n\nWe build them below. Note that we leverage another pre-built LangGraph component, ToolNode, that executes the tool and adds the result as a ToolMessage to the state.\n\n```\nfrom langchain_core.messages import SystemMessagefrom langgraph.prebuilt import ToolNode# Step 1: Generate an AIMessage that may include a tool-call to be sent.def query_or_respond(state: MessagesState):    \"\"\"Generate tool call for retrieval or respond.\"\"\"    llm_with_tools = llm.bind_tools([retrieve])    response = llm_with_tools.invoke(state[\"messages\"])    # MessagesState appends messages to state instead of overwriting    return {\"messages\": [response]}# Step 2: Execute the retrieval.tools = ToolNode([retrieve])# Step 3: Generate a response using the retrieved content.def generate(state: MessagesState):    \"\"\"Generate answer.\"\"\"    # Get generated ToolMessages    recent_tool_messages = []    for message in reversed(state[\"messages\"]):        if message.type == \"tool\":            recent_tool_messages.append(message)        else:            break    tool_messages = recent_tool_messages[::-1]    # Format into prompt    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)    system_message_content = (        \"You are an assistant for question-answering tasks. \"        \"Use the following pieces of retrieved context to answer \"        \"the question. If you don't know the answer, say that you \"        \"don't know. Use three sentences maximum and keep the \"        \"answer concise.\"        \"\\n\\n\"        f\"{docs_content}\"    )    conversation_messages = [        message        for message in state[\"messages\"]        if message.type in (\"human\", \"system\")        or (message.type == \"ai\" and not message.tool_calls)    ]    prompt = [SystemMessage(system_message_content)] + conversation_messages    # Run    response = llm.invoke(prompt)    return {\"messages\": [response]}\n```\n\nFinally, we compile our application into a single graph object. In this case, we are just connecting the steps into a sequence. We also allow the first query_or_respond step to \"short-circuit\" and respond directly to the user if it does not generate a tool call. This allows our application to support conversational experiences-- e.g., responding to generic greetings that may not require a retrieval step\n\n```\nfrom langgraph.graph import ENDfrom langgraph.prebuilt import ToolNode, tools_conditiongraph_builder.add_node(query_or_respond)graph_builder.add_node(tools)graph_builder.add_node(generate)graph_builder.set_entry_point(\"query_or_respond\")graph_builder.add_conditional_edges(    \"query_or_respond\",    tools_condition,    {END: END, \"tools\": \"tools\"},)graph_builder.add_edge(\"tools\", \"generate\")graph_builder.add_edge(\"generate\", END)graph = graph_builder.compile()\n```\n\n```\nfrom IPython.display import Image, displaydisplay(Image(graph.get_graph().draw_mermaid_png()))\n```\n\nLet's test our application.\n\nNote that it responds appropriately to messages that do not require an additional retrieval step:\n\n```\ninput_message = \"Hello\"for step in graph.stream(    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},    stream_mode=\"values\",):    step[\"messages\"][-1].pretty_print()\n```\n\n```\n================================\u001b[1m Human Message \u001b[0m=================================Hello==================================\u001b[1m Ai Message \u001b[0m==================================Hello! How can I assist you today?\n```\n\nAnd when executing a search, we can stream the steps to observe the query generation, retrieval, and answer generation:\n\n```\ninput_message = \"What is Task Decomposition?\"for step in graph.stream(    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},    stream_mode=\"values\",):    step[\"messages\"][-1].pretty_print()\n```\n\n```\n================================\u001b[1m Human Message \u001b[0m=================================What is Task Decomposition?==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls:  retrieve (call_dLjB3rkMoxZZxwUGXi33UBeh) Call ID: call_dLjB3rkMoxZZxwUGXi33UBeh  Args:    query: Task Decomposition=================================\u001b[1m Tool Message \u001b[0m=================================Name: retrieveSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}Content: Fig. 1. Overview of a LLM-powered autonomous agent system.Component One: Planning#A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.Task Decomposition#Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.==================================\u001b[1m Ai Message \u001b[0m==================================Task Decomposition is the process of breaking down a complicated task into smaller, manageable steps. It often involves techniques like Chain of Thought (CoT), which encourages models to think step by step, enhancing performance on complex tasks. This approach allows for a clearer understanding of the task and aids in structuring the problem-solving process.\n```\n\nCheck out the LangSmith trace here.", "url_final": "https://python.langchain.com/docs/tutorials/qa_chat_history/", "url_citable": "https://python.langchain.com/docs/tutorials/qa_chat_history/#chains", "title": "Build a Retrieval Augmented Generation (RAG) App: Part 2 | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Chains‚Äã", "section_anchor": "chains", "fetched_at": "2025-10-06T21:54:07.431871", "has_code": true}
{"id": "cb377bbf88779b8819a49af467219b6c6de0b4c6", "text": "Stateful management of chat history‚Äã\n\nThis section of the tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.\n\nAs of the v0.3 release of LangChain, we recommend that LangChain users take advantage of LangGraph persistence to incorporate memory into new LangChain applications.\n\nIf your code is already relying on RunnableWithMessageHistory or BaseChatMessageHistory, you do not need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses RunnableWithMessageHistory will continue to work as expected.\n\nPlease see How to migrate to LangGraph Memory for more details.\n\nIn production, the Q&A application will usually persist the chat history into a database, and be able to read and update it appropriately.\n\nLangGraph implements a built-in persistence layer, making it ideal for chat applications that support multiple conversational turns.\n\nTo manage multiple conversational turns and threads, all we have to do is specify a checkpointer when compiling our application. Because the nodes in our graph are appending messages to the state, we will retain a consistent chat history across invocations.\n\nLangGraph comes with a simple in-memory checkpointer, which we use below. See its documentation for more detail, including how to use different persistence backends (e.g., SQLite or Postgres).\n\nFor a detailed walkthrough of how to manage message history, head to the How to add message history (memory) guide.\n\n```\nfrom langgraph.checkpoint.memory import MemorySavermemory = MemorySaver()graph = graph_builder.compile(checkpointer=memory)# Specify an ID for the threadconfig = {\"configurable\": {\"thread_id\": \"abc123\"}}\n```\n\nWe can now invoke similar to before:\n\n```\ninput_message = \"What is Task Decomposition?\"for step in graph.stream(    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},    stream_mode=\"values\",    config=config,):    step[\"messages\"][-1].pretty_print()\n```\n\n```\n================================\u001b[1m Human Message \u001b[0m=================================What is Task Decomposition?==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls:  retrieve (call_JZb6GLD812bW2mQsJ5EJQDnN) Call ID: call_JZb6GLD812bW2mQsJ5EJQDnN  Args:    query: Task Decomposition=================================\u001b[1m Tool Message \u001b[0m=================================Name: retrieveSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}Content: Fig. 1. Overview of a LLM-powered autonomous agent system.Component One: Planning#A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.Task Decomposition#Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.==================================\u001b[1m Ai Message \u001b[0m==================================Task Decomposition is a technique used to break down complicated tasks into smaller, manageable steps. It involves using methods like Chain of Thought (CoT) prompting, which encourages the model to think step by step, enhancing performance on complex tasks. This process helps to clarify the model's reasoning and makes it easier to tackle difficult problems.\n```\n\n```\ninput_message = \"Can you look up some common ways of doing it?\"for step in graph.stream(    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},    stream_mode=\"values\",    config=config,):    step[\"messages\"][-1].pretty_print()\n```\n\n```\n================================\u001b[1m Human Message \u001b[0m=================================Can you look up some common ways of doing it?==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls:  retrieve (call_kjRI4Y5cJOiB73yvd7dmb6ux) Call ID: call_kjRI4Y5cJOiB73yvd7dmb6ux  Args:    query: common methods of task decomposition=================================\u001b[1m Tool Message \u001b[0m=================================Name: retrieveSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}Content: Fig. 1. Overview of a LLM-powered autonomous agent system.Component One: Planning#A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.Task Decomposition#Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.==================================\u001b[1m Ai Message \u001b[0m==================================Common ways of performing Task Decomposition include: (1) using Large Language Models (LLMs) with simple prompts like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\", (2) employing task-specific instructions such as \"Write a story outline\" for specific tasks, and (3) incorporating human inputs to guide the decomposition process.\n```\n\nNote that the query generated by the model in the second question incorporates the conversational context.\n\nThe LangSmith trace is particularly informative here, as we can see exactly what messages are visible to our chat model at each step.", "url_final": "https://python.langchain.com/docs/tutorials/qa_chat_history/", "url_citable": "https://python.langchain.com/docs/tutorials/qa_chat_history/#stateful-management-of-chat-history", "title": "Build a Retrieval Augmented Generation (RAG) App: Part 2 | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Stateful management of chat history‚Äã", "section_anchor": "stateful-management-of-chat-history", "fetched_at": "2025-10-06T21:54:07.431871", "has_code": true}
{"id": "b6845bc1e77e8463444e35102dd2496a2acab4e4", "text": "Agents‚Äã\n\nAgents leverage the reasoning capabilities of LLMs to make decisions during execution. Using agents allows you to offload additional discretion over the retrieval process. Although their behavior is less predictable than the above \"chain\", they are able to execute multiple retrieval steps in service of a query, or iterate on a single search.\n\nBelow we assemble a minimal RAG agent. Using LangGraph's pre-built ReAct agent constructor, we can do this in one line.\n\nCheck out LangGraph's Agentic RAG tutorial for more advanced formulations.\n\n```\nfrom langgraph.prebuilt import create_react_agentagent_executor = create_react_agent(llm, [retrieve], checkpointer=memory)\n```\n\nLet's inspect the graph:\n\n```\ndisplay(Image(agent_executor.get_graph().draw_mermaid_png()))\n```\n\nThe key difference from our earlier implementation is that instead of a final generation step that ends the run, here the tool invocation loops back to the original LLM call. The model can then either answer the question using the retrieved context, or generate another tool call to obtain more information.\n\nLet's test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\n\n```\nconfig = {\"configurable\": {\"thread_id\": \"def234\"}}input_message = (    \"What is the standard method for Task Decomposition?\\n\\n\"    \"Once you get the answer, look up common extensions of that method.\")for event in agent_executor.stream(    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},    stream_mode=\"values\",    config=config,):    event[\"messages\"][-1].pretty_print()\n```\n\n```\n================================\u001b[1m Human Message \u001b[0m=================================What is the standard method for Task Decomposition?Once you get the answer, look up common extensions of that method.==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls:  retrieve (call_Y3YaIzL71B83Cjqa8d2G0O8N) Call ID: call_Y3YaIzL71B83Cjqa8d2G0O8N  Args:    query: standard method for Task Decomposition=================================\u001b[1m Tool Message \u001b[0m=================================Name: retrieveSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}Content: Fig. 1. Overview of a LLM-powered autonomous agent system.Component One: Planning#A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.Task Decomposition#Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls:  retrieve (call_2JntP1x4XQMWwgVpYurE12ff) Call ID: call_2JntP1x4XQMWwgVpYurE12ff  Args:    query: common extensions of Task Decomposition methods=================================\u001b[1m Tool Message \u001b[0m=================================Name: retrieveSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}Content: Fig. 1. Overview of a LLM-powered autonomous agent system.Component One: Planning#A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.Task Decomposition#Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.==================================\u001b[1m Ai Message \u001b[0m==================================The standard method for task decomposition involves using techniques such as Chain of Thought (CoT), where a model is instructed to \"think step by step\" to break down complex tasks into smaller, more manageable components. This approach enhances model performance by allowing for more thorough reasoning and planning. Task decomposition can be accomplished through various means, including:1. Simple prompting (e.g., asking for steps to achieve a goal).2. Task-specific instructions (e.g., asking for a story outline).3. Human inputs to guide the decomposition process.### Common Extensions of Task Decomposition Methods:1. **Tree of Thoughts**: This extension builds on CoT by not only decomposing the problem into thought steps but also generating multiple thoughts at each step, creating a tree structure. The search process can employ breadth-first search (BFS) or depth-first search (DFS), with each state evaluated by a classifier or through majority voting.These extensions aim to enhance reasoning capabilities and improve the effectiveness of task decomposition in various contexts.\n```\n\nNote that the agent:\n\n- Generates a query to search for a standard method for task decomposition;\n- Receiving the answer, generates a second query to search for common extensions of it;\n- Having received all necessary context, answers the question.\n\nWe can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.", "url_final": "https://python.langchain.com/docs/tutorials/qa_chat_history/", "url_citable": "https://python.langchain.com/docs/tutorials/qa_chat_history/#agents", "title": "Build a Retrieval Augmented Generation (RAG) App: Part 2 | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Agents‚Äã", "section_anchor": "agents", "fetched_at": "2025-10-06T21:54:07.431871", "has_code": true}
{"id": "e016470ae0582c71dd7bc5e708c25393cc782dd0", "text": "Next steps‚Äã\n\nWe've covered the steps to build a basic conversational Q&A application:\n\n- We used chains to build a predictable application that generates at most one query per user input;\n- We used agents to build an application that can iterate on a sequence of queries.\n\nTo explore different types of retrievers and retrieval strategies, visit the retrievers section of the how-to guides.\n\nFor a detailed walkthrough of LangChain's conversation memory abstractions, visit the How to add message history (memory) guide.\n\nTo learn more about agents, check out the conceptual guide and LangGraph agent architectures page.\n\n- Components\n- Dependencies\n- LangSmith\n\n- Stateful management of chat history", "url_final": "https://python.langchain.com/docs/tutorials/qa_chat_history/", "url_citable": "https://python.langchain.com/docs/tutorials/qa_chat_history/#next-steps", "title": "Build a Retrieval Augmented Generation (RAG) App: Part 2 | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Next steps‚Äã", "section_anchor": "next-steps", "fetched_at": "2025-10-06T21:54:07.431871", "has_code": false}
{"id": "67a08dbb3345f25af6a4fd48b493b6889442eea6", "text": "Partial with strings‚Äã\n\nOne common use case for wanting to partial a prompt template is if you get access to some of the variables in a prompt before others. For example, suppose you have a prompt template that requires two variables, foo and baz. If you get the foo value early on in your chain, but the baz value later, it can be inconvenient to pass both variables all the way through the chain. Instead, you can partial the prompt template with the foo value, and then pass the partialed prompt template along and just use that. Below is an example of doing this:\n\n```\nfrom langchain_core.prompts import PromptTemplateprompt = PromptTemplate.from_template(\"{foo}{bar}\")partial_prompt = prompt.partial(foo=\"foo\")print(partial_prompt.format(bar=\"baz\"))\n```\n\n```\nfoobaz\n```\n\nYou can also just initialize the prompt with the partialed variables.\n\n```\nprompt = PromptTemplate(    template=\"{foo}{bar}\", input_variables=[\"bar\"], partial_variables={\"foo\": \"foo\"})print(prompt.format(bar=\"baz\"))\n```\n\n```\nfoobaz\n```", "url_final": "https://python.langchain.com/docs/how_to/prompts_partial/", "url_citable": "https://python.langchain.com/docs/how_to/prompts_partial/#partial-with-strings", "title": "How to partially format prompt templates | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Partial with strings‚Äã", "section_anchor": "partial-with-strings", "fetched_at": "2025-10-06T21:54:43.582588", "has_code": true}
{"id": "82de44c57d35605f57346d4992d654509db5f5d8", "text": "Partial with functions‚Äã\n\nThe other common use is to partial with a function. The use case for this is when you have a variable you know that you always want to fetch in a common way. A prime example of this is with date or time. Imagine you have a prompt which you always want to have the current date. You can't hard code it in the prompt, and passing it along with the other input variables is inconvenient. In this case, it's handy to be able to partial the prompt with a function that always returns the current date.\n\n```\nfrom datetime import datetimedef _get_datetime():    now = datetime.now()    return now.strftime(\"%m/%d/%Y, %H:%M:%S\")prompt = PromptTemplate(    template=\"Tell me a {adjective} joke about the day {date}\",    input_variables=[\"adjective\", \"date\"],)partial_prompt = prompt.partial(date=_get_datetime)print(partial_prompt.format(adjective=\"funny\"))\n```\n\n```\nTell me a funny joke about the day 04/21/2024, 19:43:57\n```\n\nYou can also just initialize the prompt with the partialed variables, which often makes more sense in this workflow.\n\n```\nprompt = PromptTemplate(    template=\"Tell me a {adjective} joke about the day {date}\",    input_variables=[\"adjective\"],    partial_variables={\"date\": _get_datetime},)print(prompt.format(adjective=\"funny\"))\n```\n\n```\nTell me a funny joke about the day 04/21/2024, 19:43:57\n```", "url_final": "https://python.langchain.com/docs/how_to/prompts_partial/", "url_citable": "https://python.langchain.com/docs/how_to/prompts_partial/#partial-with-functions", "title": "How to partially format prompt templates | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Partial with functions‚Äã", "section_anchor": "partial-with-functions", "fetched_at": "2025-10-06T21:54:43.582588", "has_code": true}
{"id": "92f1635b111cc5e239799a384c42518644f969f8", "text": "Next steps‚Äã\n\nYou've now learned how to partially apply variables to your prompt templates.\n\nNext, check out the other how-to guides on prompt templates in this section, like adding few-shot examples to your prompt templates.\n\n- Partial with strings\n- Partial with functions\n- Next steps", "url_final": "https://python.langchain.com/docs/how_to/prompts_partial/", "url_citable": "https://python.langchain.com/docs/how_to/prompts_partial/#next-steps", "title": "How to partially format prompt templates | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Next steps‚Äã", "section_anchor": "next-steps", "fetched_at": "2025-10-06T21:54:43.582588", "has_code": false}
{"id": "5453a0d6f958eb0ebd5e6e0ef8eed3c8d914a972", "text": "Setup‚Äã\n\n```\n%pip install -qU langchain langchain-community langchain-openai langchain-chroma\n```\n\n```\nNote: you may need to restart the kernel to use updated packages.\n```\n\nWe'll use OpenAI in this example:\n\n```\nimport getpassimport osif \"OPENAI_API_KEY\" not in os.environ:    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()# Optional, uncomment to trace runs with LangSmith. Sign up here: https://smith.langchain.com.# os.environ[\"LANGSMITH_TRACING\"] = \"true\"# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n```\n\nCreate Index‚Äã\n\nWe will create a vectorstore over fake information.\n\n```\nfrom langchain_chroma import Chromafrom langchain_openai import OpenAIEmbeddingsfrom langchain_text_splitters import RecursiveCharacterTextSplittertexts = [\"Harrison worked at Kensho\"]embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")vectorstore = Chroma.from_texts(texts, embeddings, collection_name=\"harrison\")retriever_harrison = vectorstore.as_retriever(search_kwargs={\"k\": 1})texts = [\"Ankush worked at Facebook\"]embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")vectorstore = Chroma.from_texts(texts, embeddings, collection_name=\"ankush\")retriever_ankush = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n```", "url_final": "https://python.langchain.com/docs/how_to/query_multiple_retrievers/", "url_citable": "https://python.langchain.com/docs/how_to/query_multiple_retrievers/#setup", "title": "How to handle multiple retrievers when doing query analysis | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Setup‚Äã", "section_anchor": "setup", "fetched_at": "2025-10-06T21:54:48.663767", "has_code": true}
{"id": "45b708d17144dc0f78269927389661ab11a6bca9", "text": "Query analysis‚Äã\n\nWe will use function calling to structure the output. We will let it return multiple queries.\n\n```\nfrom typing import List, Optionalfrom pydantic import BaseModel, Fieldclass Search(BaseModel):    \"\"\"Search for information about a person.\"\"\"    query: str = Field(        ...,        description=\"Query to look up\",    )    person: str = Field(        ...,        description=\"Person to look things up for. Should be `HARRISON` or `ANKUSH`.\",    )\n```\n\n```\nfrom langchain_core.output_parsers.openai_tools import PydanticToolsParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnablePassthroughfrom langchain_openai import ChatOpenAIoutput_parser = PydanticToolsParser(tools=[Search])system = \"\"\"You have the ability to issue search queries to get information to help answer user information.\"\"\"prompt = ChatPromptTemplate.from_messages(    [        (\"system\", system),        (\"human\", \"{question}\"),    ])llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)structured_llm = llm.with_structured_output(Search)query_analyzer = {\"question\": RunnablePassthrough()} | prompt | structured_llm\n```\n\nWe can see that this allows for routing between retrievers\n\n```\nquery_analyzer.invoke(\"where did Harrison Work\")\n```\n\n```\nSearch(query='work history', person='HARRISON')\n```\n\n```\nquery_analyzer.invoke(\"where did ankush Work\")\n```\n\n```\nSearch(query='work history', person='ANKUSH')\n```", "url_final": "https://python.langchain.com/docs/how_to/query_multiple_retrievers/", "url_citable": "https://python.langchain.com/docs/how_to/query_multiple_retrievers/#query-analysis", "title": "How to handle multiple retrievers when doing query analysis | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Query analysis‚Äã", "section_anchor": "query-analysis", "fetched_at": "2025-10-06T21:54:48.663767", "has_code": true}
{"id": "c2adb43f8515f47fe825bf5777e522a6a5ddde65", "text": "Retrieval with query analysis‚Äã\n\nSo how would we include this in a chain? We just need some simple logic to select the retriever and pass in the search query\n\n```\nfrom langchain_core.runnables import chain\n```\n\n```\nretrievers = {    \"HARRISON\": retriever_harrison,    \"ANKUSH\": retriever_ankush,}\n```\n\n```\n@chaindef custom_chain(question):    response = query_analyzer.invoke(question)    retriever = retrievers[response.person]    return retriever.invoke(response.query)\n```\n\n```\ncustom_chain.invoke(\"where did Harrison Work\")\n```\n\n```\n[Document(page_content='Harrison worked at Kensho')]\n```\n\n```\ncustom_chain.invoke(\"where did ankush Work\")\n```\n\n```\n[Document(page_content='Ankush worked at Facebook')]\n```\n\n- Create Index", "url_final": "https://python.langchain.com/docs/how_to/query_multiple_retrievers/", "url_citable": "https://python.langchain.com/docs/how_to/query_multiple_retrievers/#retrieval-with-query-analysis", "title": "How to handle multiple retrievers when doing query analysis | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Retrieval with query analysis‚Äã", "section_anchor": "retrieval-with-query-analysis", "fetched_at": "2025-10-06T21:54:48.663767", "has_code": true}
{"id": "cc82abed01983f12e1e3e3fbbd57373e26ef6299", "text": "How to use LangChain with different Pydantic versions | ü¶úÔ∏èüîó LangChain\n\nAs of the 0.3 release, LangChain uses Pydantic 2 internally.\n\nUsers should install Pydantic 2 and are advised to avoid using the pydantic.v1 namespace of Pydantic 2 with LangChain APIs.\n\nIf you're working with prior versions of LangChain, please see the following guide on Pydantic compatibility.", "url_final": "https://python.langchain.com/docs/how_to/pydantic_compatibility/", "url_citable": "https://python.langchain.com/docs/how_to/pydantic_compatibility/", "title": "How to use LangChain with different Pydantic versions | ü¶úÔ∏èüîó LangChain", "section_level": 1, "section_heading": "How to use LangChain with different Pydantic versions | ü¶úÔ∏èüîó LangChain", "section_anchor": null, "fetched_at": "2025-10-06T21:56:07.853553", "has_code": false}
{"id": "a176d6c25dea863b25f27a29fa53cae5cc9c2ae2", "text": "Inputs and outputs‚Äã\n\nFirst, we need to talk about messages, which are the inputs and outputs of chat models.\n\nMessages‚Äã\n\nChat models take messages as inputs and return a message as output.\n\nLangChain has a few built-in message types:\n\nToolMessage and FunctionMessage closely follow OpenAI's function and tool roles.\n\nThis is a rapidly developing field and as more models add function calling capabilities. Expect that there will be additions to this schema.\n\n```\nfrom langchain_core.messages import (    AIMessage,    BaseMessage,    FunctionMessage,    HumanMessage,    SystemMessage,    ToolMessage,)\n```\n\nStreaming Variant‚Äã\n\nAll the chat messages have a streaming variant that contains Chunk in the name.\n\n```\nfrom langchain_core.messages import (    AIMessageChunk,    FunctionMessageChunk,    HumanMessageChunk,    SystemMessageChunk,    ToolMessageChunk,)\n```\n\nThese chunks are used when streaming output from chat models, and they all define an additive property!\n\n```\nAIMessageChunk(content=\"Hello\") + AIMessageChunk(content=\" World!\")\n```\n\n```\nAIMessageChunk(content='Hello World!')\n```", "url_final": "https://python.langchain.com/docs/how_to/custom_chat_model/", "url_citable": "https://python.langchain.com/docs/how_to/custom_chat_model/#inputs-and-outputs", "title": "How to create a custom chat model class | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Inputs and outputs‚Äã", "section_anchor": "inputs-and-outputs", "fetched_at": "2025-10-06T21:55:13.138491", "has_code": true}
{"id": "6edc4bc5306cb832861c18364d64d30a96e549a7", "text": "Base Chat Model‚Äã\n\nLet's implement a chat model that echoes back the first n characters of the last message in the prompt!\n\nTo do so, we will inherit from BaseChatModel and we'll need to implement the following:\n\nThe _astream implementation uses run_in_executor to launch the sync _stream in a separate thread if _stream is implemented, otherwise it fallsback to use _agenerate.\n\nYou can use this trick if you want to reuse the _stream implementation, but if you're able to implement code that's natively async that's a better solution since that code will run with less overhead.", "url_final": "https://python.langchain.com/docs/how_to/custom_chat_model/", "url_citable": "https://python.langchain.com/docs/how_to/custom_chat_model/#base-chat-model", "title": "How to create a custom chat model class | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Base Chat Model‚Äã", "section_anchor": "base-chat-model", "fetched_at": "2025-10-06T21:55:13.138491", "has_code": false}
{"id": "df34c0b879a362439a90b361b0e8c2313c5fce26", "text": "Implementation‚Äã\n\n```\nfrom typing import Any, Dict, Iterator, List, Optionalfrom langchain_core.callbacks import (    CallbackManagerForLLMRun,)from langchain_core.language_models import BaseChatModelfrom langchain_core.messages import (    AIMessage,    AIMessageChunk,    BaseMessage,)from langchain_core.messages.ai import UsageMetadatafrom langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResultfrom pydantic import Fieldclass ChatParrotLink(BaseChatModel):    \"\"\"A custom chat model that echoes the first `parrot_buffer_length` characters    of the input.    When contributing an implementation to LangChain, carefully document    the model including the initialization parameters, include    an example of how to initialize the model and include any relevant    links to the underlying models documentation or API.    Example:        .. code-block:: python            model = ChatParrotLink(parrot_buffer_length=2, model=\"bird-brain-001\")            result = model.invoke([HumanMessage(content=\"hello\")])            result = model.batch([[HumanMessage(content=\"hello\")],                                 [HumanMessage(content=\"world\")]])    \"\"\"    model_name: str = Field(alias=\"model\")    \"\"\"The name of the model\"\"\"    parrot_buffer_length: int    \"\"\"The number of characters from the last message of the prompt to be echoed.\"\"\"    temperature: Optional[float] = None    max_tokens: Optional[int] = None    timeout: Optional[int] = None    stop: Optional[List[str]] = None    max_retries: int = 2    def _generate(        self,        messages: List[BaseMessage],        stop: Optional[List[str]] = None,        run_manager: Optional[CallbackManagerForLLMRun] = None,        **kwargs: Any,    ) -> ChatResult:        \"\"\"Override the _generate method to implement the chat model logic.        This can be a call to an API, a call to a local model, or any other        implementation that generates a response to the input prompt.        Args:            messages: the prompt composed of a list of messages.            stop: a list of strings on which the model should stop generating.                  If generation stops due to a stop token, the stop token itself                  SHOULD BE INCLUDED as part of the output. This is not enforced                  across models right now, but it's a good practice to follow since                  it makes it much easier to parse the output of the model                  downstream and understand why generation stopped.            run_manager: A run manager with callbacks for the LLM.        \"\"\"        # Replace this with actual logic to generate a response from a list        # of messages.        last_message = messages[-1]        tokens = last_message.content[: self.parrot_buffer_length]        ct_input_tokens = sum(len(message.content) for message in messages)        ct_output_tokens = len(tokens)        message = AIMessage(            content=tokens,            additional_kwargs={},  # Used to add additional payload to the message            response_metadata={  # Use for response metadata                \"time_in_seconds\": 3,                \"model_name\": self.model_name,            },            usage_metadata={                \"input_tokens\": ct_input_tokens,                \"output_tokens\": ct_output_tokens,                \"total_tokens\": ct_input_tokens + ct_output_tokens,            },        )        ##        generation = ChatGeneration(message=message)        return ChatResult(generations=[generation])    def _stream(        self,        messages: List[BaseMessage],        stop: Optional[List[str]] = None,        run_manager: Optional[CallbackManagerForLLMRun] = None,        **kwargs: Any,    ) -> Iterator[ChatGenerationChunk]:        \"\"\"Stream the output of the model.        This method should be implemented if the model can generate output        in a streaming fashion. If the model does not support streaming,        do not implement it. In that case streaming requests will be automatically        handled by the _generate method.        Args:            messages: the prompt composed of a list of messages.            stop: a list of strings on which the model should stop generating.                  If generation stops due to a stop token, the stop token itself                  SHOULD BE INCLUDED as part of the output. This is not enforced                  across models right now, but it's a good practice to follow since                  it makes it much easier to parse the output of the model                  downstream and understand why generation stopped.            run_manager: A run manager with callbacks for the LLM.        \"\"\"        last_message = messages[-1]        tokens = str(last_message.content[: self.parrot_buffer_length])        ct_input_tokens = sum(len(message.content) for message in messages)        for token in tokens:            usage_metadata = UsageMetadata(                {                    \"input_tokens\": ct_input_tokens,                    \"output_tokens\": 1,                    \"total_tokens\": ct_input_tokens + 1,                }            )            ct_input_tokens = 0            chunk = ChatGenerationChunk(                message=AIMessageChunk(content=token, usage_metadata=usage_metadata)            )            if run_manager:                # This is optional in newer versions of LangChain                # The on_llm_new_token will be called automatically                run_manager.on_llm_new_token(token, chunk=chunk)            yield chunk        # Let's add some other information (e.g., response metadata)        chunk = ChatGenerationChunk(            message=AIMessageChunk(                content=\"\",                response_metadata={\"time_in_sec\": 3, \"model_name\": self.model_name},            )        )        if run_manager:            # This is optional in newer versions of LangChain            # The on_llm_new_token will be called automatically            run_manager.on_llm_new_token(token, chunk=chunk)        yield chunk    @property    def _llm_type(self) -> str:        \"\"\"Get the type of language model used by this chat model.\"\"\"        return \"echoing-chat-model-advanced\"    @property    def _identifying_params(self) -> Dict[str, Any]:        \"\"\"Return a dictionary of identifying parameters.        This information is used by the LangChain callback system, which        is used for tracing purposes make it possible to monitor LLMs.        \"\"\"        return {            # The model name allows users to specify custom token counting            # rules in LLM monitoring applications (e.g., in LangSmith users            # can provide per token pricing for their model and monitor            # costs for the given LLM.)            \"model_name\": self.model_name,        }\n```", "url_final": "https://python.langchain.com/docs/how_to/custom_chat_model/", "url_citable": "https://python.langchain.com/docs/how_to/custom_chat_model/#implementation", "title": "How to create a custom chat model class | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Implementation‚Äã", "section_anchor": "implementation", "fetched_at": "2025-10-06T21:55:13.138491", "has_code": true}
{"id": "145a473d5476864163769f787ddd5ab75d444aeb", "text": "Let's test it üß™‚Äã\n\nThe chat model will implement the standard Runnable interface of LangChain which many of the LangChain abstractions support!\n\n```\nmodel = ChatParrotLink(parrot_buffer_length=3, model=\"my_custom_model\")model.invoke(    [        HumanMessage(content=\"hello!\"),        AIMessage(content=\"Hi there human!\"),        HumanMessage(content=\"Meow!\"),    ])\n```\n\n```\nAIMessage(content='Meo', additional_kwargs={}, response_metadata={'time_in_seconds': 3}, id='run-cf11aeb6-8ab6-43d7-8c68-c1ef89b6d78e-0', usage_metadata={'input_tokens': 26, 'output_tokens': 3, 'total_tokens': 29})\n```\n\n```\nmodel.invoke(\"hello\")\n```\n\n```\nAIMessage(content='hel', additional_kwargs={}, response_metadata={'time_in_seconds': 3}, id='run-618e5ed4-d611-4083-8cf1-c270726be8d9-0', usage_metadata={'input_tokens': 5, 'output_tokens': 3, 'total_tokens': 8})\n```\n\n```\nmodel.batch([\"hello\", \"goodbye\"])\n```\n\n```\n[AIMessage(content='hel', additional_kwargs={}, response_metadata={'time_in_seconds': 3}, id='run-eea4ed7d-d750-48dc-90c0-7acca1ff388f-0', usage_metadata={'input_tokens': 5, 'output_tokens': 3, 'total_tokens': 8}), AIMessage(content='goo', additional_kwargs={}, response_metadata={'time_in_seconds': 3}, id='run-07cfc5c1-3c62-485f-b1e0-3d46e1547287-0', usage_metadata={'input_tokens': 7, 'output_tokens': 3, 'total_tokens': 10})]\n```\n\n```\nfor chunk in model.stream(\"cat\"):    print(chunk.content, end=\"|\")\n```\n\n```\nc|a|t||\n```\n\nPlease see the implementation of _astream in the model! If you do not implement it, then no output will stream.!\n\n```\nasync for chunk in model.astream(\"cat\"):    print(chunk.content, end=\"|\")\n```\n\n```\nc|a|t||\n```\n\nLet's try to use the astream events API which will also help double check that all the callbacks were implemented!\n\n```\nasync for event in model.astream_events(\"cat\", version=\"v1\"):    print(event)\n```\n\n```\n{'event': 'on_chat_model_start', 'run_id': '3f0b5501-5c78-45b3-92fc-8322a6a5024a', 'name': 'ChatParrotLink', 'tags': [], 'metadata': {}, 'data': {'input': 'cat'}, 'parent_ids': []}{'event': 'on_chat_model_stream', 'run_id': '3f0b5501-5c78-45b3-92fc-8322a6a5024a', 'tags': [], 'metadata': {}, 'name': 'ChatParrotLink', 'data': {'chunk': AIMessageChunk(content='c', additional_kwargs={}, response_metadata={}, id='run-3f0b5501-5c78-45b3-92fc-8322a6a5024a', usage_metadata={'input_tokens': 3, 'output_tokens': 1, 'total_tokens': 4})}, 'parent_ids': []}{'event': 'on_chat_model_stream', 'run_id': '3f0b5501-5c78-45b3-92fc-8322a6a5024a', 'tags': [], 'metadata': {}, 'name': 'ChatParrotLink', 'data': {'chunk': AIMessageChunk(content='a', additional_kwargs={}, response_metadata={}, id='run-3f0b5501-5c78-45b3-92fc-8322a6a5024a', usage_metadata={'input_tokens': 0, 'output_tokens': 1, 'total_tokens': 1})}, 'parent_ids': []}{'event': 'on_chat_model_stream', 'run_id': '3f0b5501-5c78-45b3-92fc-8322a6a5024a', 'tags': [], 'metadata': {}, 'name': 'ChatParrotLink', 'data': {'chunk': AIMessageChunk(content='t', additional_kwargs={}, response_metadata={}, id='run-3f0b5501-5c78-45b3-92fc-8322a6a5024a', usage_metadata={'input_tokens': 0, 'output_tokens': 1, 'total_tokens': 1})}, 'parent_ids': []}{'event': 'on_chat_model_stream', 'run_id': '3f0b5501-5c78-45b3-92fc-8322a6a5024a', 'tags': [], 'metadata': {}, 'name': 'ChatParrotLink', 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'time_in_sec': 3}, id='run-3f0b5501-5c78-45b3-92fc-8322a6a5024a')}, 'parent_ids': []}{'event': 'on_chat_model_end', 'name': 'ChatParrotLink', 'run_id': '3f0b5501-5c78-45b3-92fc-8322a6a5024a', 'tags': [], 'metadata': {}, 'data': {'output': AIMessageChunk(content='cat', additional_kwargs={}, response_metadata={'time_in_sec': 3}, id='run-3f0b5501-5c78-45b3-92fc-8322a6a5024a', usage_metadata={'input_tokens': 3, 'output_tokens': 3, 'total_tokens': 6})}, 'parent_ids': []}\n```", "url_final": "https://python.langchain.com/docs/how_to/custom_chat_model/", "url_citable": "https://python.langchain.com/docs/how_to/custom_chat_model/#lets-test-it-", "title": "How to create a custom chat model class | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Let's test it üß™‚Äã", "section_anchor": "lets-test-it-", "fetched_at": "2025-10-06T21:55:13.138491", "has_code": true}
{"id": "d1aa98953a73abbd2354a91350335a68be5b4a21", "text": "Contributing‚Äã\n\nWe appreciate all chat model integration contributions.\n\nHere's a checklist to help make sure your contribution gets added to LangChain:\n\nDocumentation:\n\n- The model contains doc-strings for all initialization arguments, as these will be surfaced in the API Reference.\n- The class doc-string for the model contains a link to the model API if the model is powered by a service.\n\nTests:\n\n- Add unit or integration tests to the overridden methods. Verify that invoke, ainvoke, batch, stream work if you've over-ridden the corresponding code.\n\nStreaming (if you're implementing it):\n\n- Implement the _stream method to get streaming working\n\nStop Token Behavior:\n\n- Stop token should be respected\n- Stop token should be INCLUDED as part of the response\n\nSecret API Keys:\n\n- If your model connects to an API it will likely accept API keys as part of its initialization. Use Pydantic's SecretStr type for secrets, so they don't get accidentally printed out when folks print the model.\n\nIdentifying Params:\n\n- Include a model_name in identifying params\n\nOptimizations:\n\nConsider providing native async support to reduce the overhead from the model!\n\n- Provided a native async of _agenerate (used by ainvoke)\n- Provided a native async of _astream (used by astream)", "url_final": "https://python.langchain.com/docs/how_to/custom_chat_model/", "url_citable": "https://python.langchain.com/docs/how_to/custom_chat_model/#contributing", "title": "How to create a custom chat model class | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Contributing‚Äã", "section_anchor": "contributing", "fetched_at": "2025-10-06T21:55:13.138491", "has_code": false}
{"id": "52b769951083f61584bd8d1375a2478c9788303c", "text": "Next steps‚Äã\n\nYou've now learned how to create your own custom chat models.\n\nNext, check out the other how-to guides chat models in this section, like how to get a model to return structured output or how to track chat model token usage.\n\n- Messages\n- Streaming Variant\n\n- Implementation\n- Let's test it üß™", "url_final": "https://python.langchain.com/docs/how_to/custom_chat_model/", "url_citable": "https://python.langchain.com/docs/how_to/custom_chat_model/#next-steps", "title": "How to create a custom chat model class | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Next steps‚Äã", "section_anchor": "next-steps", "fetched_at": "2025-10-06T21:55:13.138491", "has_code": false}
{"id": "1f8a0a22197f277b5e29e2c28c77ddad03d6ac7b", "text": "Binding stop sequences‚Äã\n\nSuppose we have a simple prompt + model chain:\n\n```\nfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnablePassthroughfrom langchain_openai import ChatOpenAIprompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"Write out the following equation using algebraic symbols then solve it. Use the format\\n\\nEQUATION:...\\nSOLUTION:...\\n\\n\",        ),        (\"human\", \"{equation_statement}\"),    ])model = ChatOpenAI(temperature=0)runnable = (    {\"equation_statement\": RunnablePassthrough()} | prompt | model | StrOutputParser())print(runnable.invoke(\"x raised to the third plus seven equals 12\"))\n```\n\n```\nEQUATION: x^3 + 7 = 12SOLUTION: Subtract 7 from both sides:x^3 = 5Take the cube root of both sides:x = ‚àõ5\n```\n\nand want to call the model with certain stop words so that we shorten the output as is useful in certain types of prompting techniques. While we can pass some arguments into the constructor, other runtime args use the .bind() method as follows:\n\n```\nrunnable = (    {\"equation_statement\": RunnablePassthrough()}    | prompt    | model.bind(stop=\"SOLUTION\")    | StrOutputParser())print(runnable.invoke(\"x raised to the third plus seven equals 12\"))\n```\n\n```\nEQUATION: x^3 + 7 = 12\n```\n\nWhat you can bind to a Runnable will depend on the extra parameters you can pass when invoking it.", "url_final": "https://python.langchain.com/docs/how_to/binding/", "url_citable": "https://python.langchain.com/docs/how_to/binding/#binding-stop-sequences", "title": "How to add default invocation args to a Runnable | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Binding stop sequences‚Äã", "section_anchor": "binding-stop-sequences", "fetched_at": "2025-10-06T21:54:23.858559", "has_code": true}
{"id": "10bbeb58b6df9437394606d39cdbf76bdb568132", "text": "Attaching OpenAI tools‚Äã\n\nAnother common use-case is tool calling. While you should generally use the .bind_tools() method for tool-calling models, you can also bind provider-specific args directly if you want lower level control:\n\n```\ntools = [    {        \"type\": \"function\",        \"function\": {            \"name\": \"get_current_weather\",            \"description\": \"Get the current weather in a given location\",            \"parameters\": {                \"type\": \"object\",                \"properties\": {                    \"location\": {                        \"type\": \"string\",                        \"description\": \"The city and state, e.g. San Francisco, CA\",                    },                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},                },                \"required\": [\"location\"],            },        },    }]\n```\n\n```\nmodel = ChatOpenAI(model=\"gpt-4o-mini\").bind(tools=tools)model.invoke(\"What's the weather in SF, NYC and LA?\")\n```\n\n```\nAIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_z0OU2CytqENVrRTI6T8DkI3u', 'function': {'arguments': '{\"location\": \"San Francisco, CA\", \"unit\": \"celsius\"}', 'name': 'get_current_weather'}, 'type': 'function'}, {'id': 'call_ft96IJBh0cMKkQWrZjNg4bsw', 'function': {'arguments': '{\"location\": \"New York, NY\", \"unit\": \"celsius\"}', 'name': 'get_current_weather'}, 'type': 'function'}, {'id': 'call_tfbtGgCLmuBuWgZLvpPwvUMH', 'function': {'arguments': '{\"location\": \"Los Angeles, CA\", \"unit\": \"celsius\"}', 'name': 'get_current_weather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 84, 'prompt_tokens': 85, 'total_tokens': 169}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_77a673219d', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-d57ad5fa-b52a-4822-bc3e-74f838697e18-0', tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'San Francisco, CA', 'unit': 'celsius'}, 'id': 'call_z0OU2CytqENVrRTI6T8DkI3u'}, {'name': 'get_current_weather', 'args': {'location': 'New York, NY', 'unit': 'celsius'}, 'id': 'call_ft96IJBh0cMKkQWrZjNg4bsw'}, {'name': 'get_current_weather', 'args': {'location': 'Los Angeles, CA', 'unit': 'celsius'}, 'id': 'call_tfbtGgCLmuBuWgZLvpPwvUMH'}])\n```", "url_final": "https://python.langchain.com/docs/how_to/binding/", "url_citable": "https://python.langchain.com/docs/how_to/binding/#attaching-openai-tools", "title": "How to add default invocation args to a Runnable | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Attaching OpenAI tools‚Äã", "section_anchor": "attaching-openai-tools", "fetched_at": "2025-10-06T21:54:23.858559", "has_code": true}
{"id": "e1f3b85f252efb3227521eabe9bfced5ae459f63", "text": "Next steps‚Äã\n\nYou now know how to bind runtime arguments to a Runnable.\n\nTo learn more, see the other how-to guides on runnables in this section, including:\n\n- Using configurable fields and alternatives to change parameters of a step in a chain, or even swap out entire steps, at runtime\n\n- Binding stop sequences\n- Attaching OpenAI tools\n- Next steps", "url_final": "https://python.langchain.com/docs/how_to/binding/", "url_citable": "https://python.langchain.com/docs/how_to/binding/#next-steps", "title": "How to add default invocation args to a Runnable | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Next steps‚Äã", "section_anchor": "next-steps", "fetched_at": "2025-10-06T21:54:23.858559", "has_code": false}
{"id": "b067b1bb4e163aedf1aae0183391544bdb25facf", "text": "Hiding arguments from the model‚Äã\n\nWe can use the InjectedToolArg annotation to mark certain parameters of our Tool, like user_id as being injected at runtime, meaning they shouldn't be generated by the model\n\n```\nfrom typing import Listfrom langchain_core.tools import InjectedToolArg, toolfrom typing_extensions import Annotateduser_to_pets = {}@tool(parse_docstring=True)def update_favorite_pets(    pets: List[str], user_id: Annotated[str, InjectedToolArg]) -> None:    \"\"\"Add the list of favorite pets.    Args:        pets: List of favorite pets to set.        user_id: User's ID.    \"\"\"    user_to_pets[user_id] = pets@tool(parse_docstring=True)def delete_favorite_pets(user_id: Annotated[str, InjectedToolArg]) -> None:    \"\"\"Delete the list of favorite pets.    Args:        user_id: User's ID.    \"\"\"    if user_id in user_to_pets:        del user_to_pets[user_id]@tool(parse_docstring=True)def list_favorite_pets(user_id: Annotated[str, InjectedToolArg]) -> None:    \"\"\"List favorite pets if any.    Args:        user_id: User's ID.    \"\"\"    return user_to_pets.get(user_id, [])\n```\n\nIf we look at the input schemas for these tools, we'll see that user_id is still listed:\n\n```\nupdate_favorite_pets.get_input_schema().model_json_schema()\n```\n\n```\n{'description': 'Add the list of favorite pets.', 'properties': {'pets': {'description': 'List of favorite pets to set.',   'items': {'type': 'string'},   'title': 'Pets',   'type': 'array'},  'user_id': {'description': \"User's ID.\",   'title': 'User Id',   'type': 'string'}}, 'required': ['pets', 'user_id'], 'title': 'update_favorite_petsSchema', 'type': 'object'}\n```\n\nBut if we look at the tool call schema, which is what is passed to the model for tool-calling, user_id has been removed:\n\n```\nupdate_favorite_pets.tool_call_schema.model_json_schema()\n```\n\n```\n{'description': 'Add the list of favorite pets.', 'properties': {'pets': {'description': 'List of favorite pets to set.',   'items': {'type': 'string'},   'title': 'Pets',   'type': 'array'}}, 'required': ['pets'], 'title': 'update_favorite_pets', 'type': 'object'}\n```\n\nSo when we invoke our tool, we need to pass in user_id:\n\n```\nuser_id = \"123\"update_favorite_pets.invoke({\"pets\": [\"lizard\", \"dog\"], \"user_id\": user_id})print(user_to_pets)print(list_favorite_pets.invoke({\"user_id\": user_id}))\n```\n\n```\n{'123': ['lizard', 'dog']}['lizard', 'dog']\n```\n\nBut when the model calls the tool, no user_id argument will be generated:\n\n```\ntools = [    update_favorite_pets,    delete_favorite_pets,    list_favorite_pets,]llm_with_tools = llm.bind_tools(tools)ai_msg = llm_with_tools.invoke(\"my favorite animals are cats and parrots\")ai_msg.tool_calls\n```\n\n```\n[{'name': 'update_favorite_pets',  'args': {'pets': ['cats', 'parrots']},  'id': 'call_pZ6XVREGh1L0BBSsiGIf1xVm',  'type': 'tool_call'}]\n```", "url_final": "https://python.langchain.com/docs/how_to/tool_runtime/", "url_citable": "https://python.langchain.com/docs/how_to/tool_runtime/#hiding-arguments-from-the-model", "title": "How to pass run time values to tools | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Hiding arguments from the model‚Äã", "section_anchor": "hiding-arguments-from-the-model", "fetched_at": "2025-10-06T21:56:38.711399", "has_code": true}
{"id": "e9758952d82b4bb400f08a4093cb4682679f4fbc", "text": "Injecting arguments at runtime‚Äã\n\nIf we want to actually execute our tools using the model-generated tool call, we'll need to inject the user_id ourselves:\n\n```\nfrom copy import deepcopyfrom langchain_core.runnables import chain@chaindef inject_user_id(ai_msg):    tool_calls = []    for tool_call in ai_msg.tool_calls:        tool_call_copy = deepcopy(tool_call)        tool_call_copy[\"args\"][\"user_id\"] = user_id        tool_calls.append(tool_call_copy)    return tool_callsinject_user_id.invoke(ai_msg)\n```\n\n```\n[{'name': 'update_favorite_pets',  'args': {'pets': ['cats', 'parrots'], 'user_id': '123'},  'id': 'call_pZ6XVREGh1L0BBSsiGIf1xVm',  'type': 'tool_call'}]\n```\n\nAnd now we can chain together our model, injection code, and the actual tools to create a tool-executing chain:\n\n```\ntool_map = {tool.name: tool for tool in tools}@chaindef tool_router(tool_call):    return tool_map[tool_call[\"name\"]]chain = llm_with_tools | inject_user_id | tool_router.map()chain.invoke(\"my favorite animals are cats and parrots\")\n```\n\n```\n[ToolMessage(content='null', name='update_favorite_pets', tool_call_id='call_oYCD0THSedHTbwNAY3NW6uUj')]\n```\n\nLooking at the user_to_pets dict, we can see that it's been updated to include cats and parrots:\n\n```\nuser_to_pets\n```\n\n```\n{'123': ['cats', 'parrots']}\n```", "url_final": "https://python.langchain.com/docs/how_to/tool_runtime/", "url_citable": "https://python.langchain.com/docs/how_to/tool_runtime/#injecting-arguments-at-runtime", "title": "How to pass run time values to tools | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Injecting arguments at runtime‚Äã", "section_anchor": "injecting-arguments-at-runtime", "fetched_at": "2025-10-06T21:56:38.711399", "has_code": true}
{"id": "a000ee40e5b9cdfb6fb882b97b314bfb0211d07d", "text": "Other ways of annotating args‚Äã\n\nHere are a few other ways of annotating our tool args:\n\n```\nfrom langchain_core.tools import BaseToolfrom pydantic import BaseModel, Fieldclass UpdateFavoritePetsSchema(BaseModel):    \"\"\"Update list of favorite pets\"\"\"    pets: List[str] = Field(..., description=\"List of favorite pets to set.\")    user_id: Annotated[str, InjectedToolArg] = Field(..., description=\"User's ID.\")@tool(args_schema=UpdateFavoritePetsSchema)def update_favorite_pets(pets, user_id):    user_to_pets[user_id] = petsupdate_favorite_pets.get_input_schema().model_json_schema()\n```\n\n```\n{'description': 'Update list of favorite pets', 'properties': {'pets': {'description': 'List of favorite pets to set.',   'items': {'type': 'string'},   'title': 'Pets',   'type': 'array'},  'user_id': {'description': \"User's ID.\",   'title': 'User Id',   'type': 'string'}}, 'required': ['pets', 'user_id'], 'title': 'UpdateFavoritePetsSchema', 'type': 'object'}\n```\n\n```\nupdate_favorite_pets.tool_call_schema.model_json_schema()\n```\n\n```\n{'description': 'Update list of favorite pets', 'properties': {'pets': {'description': 'List of favorite pets to set.',   'items': {'type': 'string'},   'title': 'Pets',   'type': 'array'}}, 'required': ['pets'], 'title': 'update_favorite_pets', 'type': 'object'}\n```\n\n```\nfrom typing import Optional, Typeclass UpdateFavoritePets(BaseTool):    name: str = \"update_favorite_pets\"    description: str = \"Update list of favorite pets\"    args_schema: Optional[Type[BaseModel]] = UpdateFavoritePetsSchema    def _run(self, pets, user_id):        user_to_pets[user_id] = petsUpdateFavoritePets().get_input_schema().model_json_schema()\n```\n\n```\n{'description': 'Update list of favorite pets', 'properties': {'pets': {'description': 'List of favorite pets to set.',   'items': {'type': 'string'},   'title': 'Pets',   'type': 'array'},  'user_id': {'description': \"User's ID.\",   'title': 'User Id',   'type': 'string'}}, 'required': ['pets', 'user_id'], 'title': 'UpdateFavoritePetsSchema', 'type': 'object'}\n```\n\n```\nUpdateFavoritePets().tool_call_schema.model_json_schema()\n```\n\n```\n{'description': 'Update list of favorite pets', 'properties': {'pets': {'description': 'List of favorite pets to set.',   'items': {'type': 'string'},   'title': 'Pets',   'type': 'array'}}, 'required': ['pets'], 'title': 'update_favorite_pets', 'type': 'object'}\n```\n\n```\nclass UpdateFavoritePets2(BaseTool):    name: str = \"update_favorite_pets\"    description: str = \"Update list of favorite pets\"    def _run(self, pets: List[str], user_id: Annotated[str, InjectedToolArg]) -> None:        user_to_pets[user_id] = petsUpdateFavoritePets2().get_input_schema().model_json_schema()\n```\n\n```\n{'description': 'Use the tool.\\n\\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\\nto child implementations to enable tracing.', 'properties': {'pets': {'items': {'type': 'string'},   'title': 'Pets',   'type': 'array'},  'user_id': {'title': 'User Id', 'type': 'string'}}, 'required': ['pets', 'user_id'], 'title': 'update_favorite_petsSchema', 'type': 'object'}\n```\n\n```\nUpdateFavoritePets2().tool_call_schema.model_json_schema()\n```\n\n```\n{'description': 'Update list of favorite pets', 'properties': {'pets': {'items': {'type': 'string'},   'title': 'Pets',   'type': 'array'}}, 'required': ['pets'], 'title': 'update_favorite_pets', 'type': 'object'}\n```\n\n- Hiding arguments from the model\n- Injecting arguments at runtime\n- Other ways of annotating args", "url_final": "https://python.langchain.com/docs/how_to/tool_runtime/", "url_citable": "https://python.langchain.com/docs/how_to/tool_runtime/#other-ways-of-annotating-args", "title": "How to pass run time values to tools | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Other ways of annotating args‚Äã", "section_anchor": "other-ways-of-annotating-args", "fetched_at": "2025-10-06T21:56:38.711399", "has_code": true}
{"id": "159b74077b5d8371690e59665af15134ec732985", "text": "How to use the output-fixing parser | ü¶úÔ∏èüîó LangChain\n\nThis output parser wraps another output parser, and in the event that the first one fails it calls out to another LLM to fix any errors.\n\nBut we can do other things besides throw errors. Specifically, we can pass the misformatted output, along with the formatted instructions, to the model and ask it to fix it.\n\nFor this example, we'll use the above Pydantic output parser. Here's what happens if we pass it a result that does not comply with the schema:\n\n```\nfrom typing import Listfrom langchain_core.exceptions import OutputParserExceptionfrom langchain_core.output_parsers import PydanticOutputParserfrom langchain_openai import ChatOpenAIfrom pydantic import BaseModel, Field\n```\n\n```\nclass Actor(BaseModel):    name: str = Field(description=\"name of an actor\")    film_names: List[str] = Field(description=\"list of names of films they starred in\")actor_query = \"Generate the filmography for a random actor.\"parser = PydanticOutputParser(pydantic_object=Actor)\n```\n\n```\nmisformatted = \"{'name': 'Tom Hanks', 'film_names': ['Forrest Gump']}\"\n```\n\n```\ntry:    parser.parse(misformatted)except OutputParserException as e:    print(e)\n```\n\n```\nInvalid json output: {'name': 'Tom Hanks', 'film_names': ['Forrest Gump']}For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE\n```\n\nNow we can construct and use a OutputFixingParser. This output parser takes as an argument another output parser but also an LLM with which to try to correct any formatting mistakes.\n\n```\nfrom langchain.output_parsers import OutputFixingParsernew_parser = OutputFixingParser.from_llm(parser=parser, llm=ChatOpenAI())\n```\n\n```\nnew_parser.parse(misformatted)\n```\n\n```\nActor(name='Tom Hanks', film_names=['Forrest Gump'])\n```\n\nFind out api documentation for OutputFixingParser.", "url_final": "https://python.langchain.com/docs/how_to/output_parser_fixing/", "url_citable": "https://python.langchain.com/docs/how_to/output_parser_fixing/", "title": "How to use the output-fixing parser | ü¶úÔ∏èüîó LangChain", "section_level": 1, "section_heading": "How to use the output-fixing parser | ü¶úÔ∏èüîó LangChain", "section_anchor": null, "fetched_at": "2025-10-06T21:56:00.380588", "has_code": true}
{"id": "cce85cc9aa80d27ee9bd49c0c2460e0ad32675f4", "text": "Get started‚Äã\n\nFamiliarize yourself with LangChain's open-source components by building simple applications.\n\nIf you're looking to get started with chat models, vector stores, or other LangChain components from a specific provider, check out our supported integrations.\n\n- Chat models and prompts: Build a simple LLM application with prompt templates and chat models.\n- Semantic search: Build a semantic search engine over a PDF with document loaders, embedding models, and vector stores.\n- Classification: Classify text into categories or labels using chat models with structured outputs.\n- Extraction: Extract structured data from text and other unstructured media using chat models and few-shot examples.\n\nRefer to the how-to guides for more detail on using all LangChain components.", "url_final": "https://python.langchain.com/docs/tutorials/", "url_citable": "https://python.langchain.com/docs/tutorials/#get-started", "title": "Tutorials | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Get started‚Äã", "section_anchor": "get-started", "fetched_at": "2025-10-06T21:54:03.417987", "has_code": false}
{"id": "acea8a460e4b36dd2e15d590dde370e224a10233", "text": "Orchestration‚Äã\n\nGet started using LangGraph to assemble LangChain components into full-featured applications.\n\n- Chatbots: Build a chatbot that incorporates memory.\n- Agents: Build an agent that interacts with external tools.\n- Retrieval Augmented Generation (RAG) Part 1: Build an application that uses your own documents to inform its responses.\n- Retrieval Augmented Generation (RAG) Part 2: Build a RAG application that incorporates a memory of its user interactions and multi-step retrieval.\n- Question-Answering with SQL: Build a question-answering system that executes SQL queries to inform its responses.\n- Summarization: Generate summaries of (potentially long) texts.\n- Question-Answering with Graph Databases: Build a question-answering system that queries a graph database to inform its responses.", "url_final": "https://python.langchain.com/docs/tutorials/", "url_citable": "https://python.langchain.com/docs/tutorials/#orchestration", "title": "Tutorials | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Orchestration‚Äã", "section_anchor": "orchestration", "fetched_at": "2025-10-06T21:54:03.417987", "has_code": false}
{"id": "0ae8542098079af70438cab41d37ecf9485c535a", "text": "LangSmith‚Äã\n\nLangSmith allows you to closely trace, monitor and evaluate your LLM application. It seamlessly integrates with LangChain, and you can use it to inspect and debug individual steps of your chains as you build.\n\nLangSmith documentation is hosted on a separate site. You can peruse LangSmith tutorials here.\n\nEvaluation‚Äã\n\nLangSmith helps you evaluate the performance of your LLM applications. The tutorial below is a great way to get started:\n\n- Evaluate your LLM application\n\n- Evaluation", "url_final": "https://python.langchain.com/docs/tutorials/", "url_citable": "https://python.langchain.com/docs/tutorials/#langsmith", "title": "Tutorials | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "LangSmith‚Äã", "section_anchor": "langsmith", "fetched_at": "2025-10-06T21:54:03.417987", "has_code": false}
{"id": "f0b3d2f32e1c239e2679eb42391ce8382b7c0aa9", "text": "Create a formatter for the few-shot examples‚Äã\n\nConfigure a formatter that will format the few-shot examples into a string. This formatter should be a PromptTemplate object.\n\n```\nfrom langchain_core.prompts import PromptTemplateexample_prompt = PromptTemplate.from_template(\"Question: {question}\\n{answer}\")\n```", "url_final": "https://python.langchain.com/docs/how_to/few_shot_examples/", "url_citable": "https://python.langchain.com/docs/how_to/few_shot_examples/#create-a-formatter-for-the-few-shot-examples", "title": "How to use few shot examples | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Create a formatter for the few-shot examples‚Äã", "section_anchor": "create-a-formatter-for-the-few-shot-examples", "fetched_at": "2025-10-06T21:54:30.117556", "has_code": true}
{"id": "c202fcc48fe9b6c70abebbff41a8dbbb98116d12", "text": "Creating the example set‚Äã\n\nNext, we'll create a list of few-shot examples. Each example should be a dictionary representing an example input to the formatter prompt we defined above.\n\n```\nexamples = [    {        \"question\": \"Who lived longer, Muhammad Ali or Alan Turing?\",        \"answer\": \"\"\"Are follow up questions needed here: Yes.Follow up: How old was Muhammad Ali when he died?Intermediate answer: Muhammad Ali was 74 years old when he died.Follow up: How old was Alan Turing when he died?Intermediate answer: Alan Turing was 41 years old when he died.So the final answer is: Muhammad Ali\"\"\",    },    {        \"question\": \"When was the founder of craigslist born?\",        \"answer\": \"\"\"Are follow up questions needed here: Yes.Follow up: Who was the founder of craigslist?Intermediate answer: Craigslist was founded by Craig Newmark.Follow up: When was Craig Newmark born?Intermediate answer: Craig Newmark was born on December 6, 1952.So the final answer is: December 6, 1952\"\"\",    },    {        \"question\": \"Who was the maternal grandfather of George Washington?\",        \"answer\": \"\"\"Are follow up questions needed here: Yes.Follow up: Who was the mother of George Washington?Intermediate answer: The mother of George Washington was Mary Ball Washington.Follow up: Who was the father of Mary Ball Washington?Intermediate answer: The father of Mary Ball Washington was Joseph Ball.So the final answer is: Joseph Ball\"\"\",    },    {        \"question\": \"Are both the directors of Jaws and Casino Royale from the same country?\",        \"answer\": \"\"\"Are follow up questions needed here: Yes.Follow up: Who is the director of Jaws?Intermediate Answer: The director of Jaws is Steven Spielberg.Follow up: Where is Steven Spielberg from?Intermediate Answer: The United States.Follow up: Who is the director of Casino Royale?Intermediate Answer: The director of Casino Royale is Martin Campbell.Follow up: Where is Martin Campbell from?Intermediate Answer: New Zealand.So the final answer is: No\"\"\",    },]\n```\n\nLet's test the formatting prompt with one of our examples:\n\n```\nprint(example_prompt.invoke(examples[0]).to_string())\n```\n\n```\nQuestion: Who lived longer, Muhammad Ali or Alan Turing?Are follow up questions needed here: Yes.Follow up: How old was Muhammad Ali when he died?Intermediate answer: Muhammad Ali was 74 years old when he died.Follow up: How old was Alan Turing when he died?Intermediate answer: Alan Turing was 41 years old when he died.So the final answer is: Muhammad Ali\n```", "url_final": "https://python.langchain.com/docs/how_to/few_shot_examples/", "url_citable": "https://python.langchain.com/docs/how_to/few_shot_examples/#creating-the-example-set", "title": "How to use few shot examples | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Creating the example set‚Äã", "section_anchor": "creating-the-example-set", "fetched_at": "2025-10-06T21:54:30.117556", "has_code": true}
{"id": "1c841954d90ff419d11f76332f2b74e1768360a9", "text": "Pass the examples and formatter to FewShotPromptTemplate‚Äã\n\nFinally, create a FewShotPromptTemplate object. This object takes in the few-shot examples and the formatter for the few-shot examples. When this FewShotPromptTemplate is formatted, it formats the passed examples using the example_prompt, then and adds them to the final prompt before suffix:\n\n```\nfrom langchain_core.prompts import FewShotPromptTemplateprompt = FewShotPromptTemplate(    examples=examples,    example_prompt=example_prompt,    suffix=\"Question: {input}\",    input_variables=[\"input\"],)print(    prompt.invoke({\"input\": \"Who was the father of Mary Ball Washington?\"}).to_string())\n```\n\n```\nQuestion: Who lived longer, Muhammad Ali or Alan Turing?Are follow up questions needed here: Yes.Follow up: How old was Muhammad Ali when he died?Intermediate answer: Muhammad Ali was 74 years old when he died.Follow up: How old was Alan Turing when he died?Intermediate answer: Alan Turing was 41 years old when he died.So the final answer is: Muhammad AliQuestion: When was the founder of craigslist born?Are follow up questions needed here: Yes.Follow up: Who was the founder of craigslist?Intermediate answer: Craigslist was founded by Craig Newmark.Follow up: When was Craig Newmark born?Intermediate answer: Craig Newmark was born on December 6, 1952.So the final answer is: December 6, 1952Question: Who was the maternal grandfather of George Washington?Are follow up questions needed here: Yes.Follow up: Who was the mother of George Washington?Intermediate answer: The mother of George Washington was Mary Ball Washington.Follow up: Who was the father of Mary Ball Washington?Intermediate answer: The father of Mary Ball Washington was Joseph Ball.So the final answer is: Joseph BallQuestion: Are both the directors of Jaws and Casino Royale from the same country?Are follow up questions needed here: Yes.Follow up: Who is the director of Jaws?Intermediate Answer: The director of Jaws is Steven Spielberg.Follow up: Where is Steven Spielberg from?Intermediate Answer: The United States.Follow up: Who is the director of Casino Royale?Intermediate Answer: The director of Casino Royale is Martin Campbell.Follow up: Where is Martin Campbell from?Intermediate Answer: New Zealand.So the final answer is: NoQuestion: Who was the father of Mary Ball Washington?\n```\n\nBy providing the model with examples like this, we can guide the model to a better response.", "url_final": "https://python.langchain.com/docs/how_to/few_shot_examples/", "url_citable": "https://python.langchain.com/docs/how_to/few_shot_examples/#pass-the-examples-and-formatter-to-fewshotprompttemplate", "title": "How to use few shot examples | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Pass the examples and formatter to FewShotPromptTemplate‚Äã", "section_anchor": "pass-the-examples-and-formatter-to-fewshotprompttemplate", "fetched_at": "2025-10-06T21:54:30.117556", "has_code": true}
{"id": "4a7cc8956c027269435822ea0b8b23586dffa025", "text": "Using an example selector‚Äã\n\nWe will reuse the example set and the formatter from the previous section. However, instead of feeding the examples directly into the FewShotPromptTemplate object, we will feed them into an implementation of ExampleSelector called SemanticSimilarityExampleSelector instance. This class selects few-shot examples from the initial set based on their similarity to the input. It uses an embedding model to compute the similarity between the input and the few-shot examples, as well as a vector store to perform the nearest neighbor search.\n\nTo show what it looks like, let's initialize an instance and call it in isolation:\n\n```\nfrom langchain_chroma import Chromafrom langchain_core.example_selectors import SemanticSimilarityExampleSelectorfrom langchain_openai import OpenAIEmbeddingsexample_selector = SemanticSimilarityExampleSelector.from_examples(    # This is the list of examples available to select from.    examples,    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.    OpenAIEmbeddings(),    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.    Chroma,    # This is the number of examples to produce.    k=1,)# Select the most similar example to the input.question = \"Who was the father of Mary Ball Washington?\"selected_examples = example_selector.select_examples({\"question\": question})print(f\"Examples most similar to the input: {question}\")for example in selected_examples:    print(\"\\n\")    for k, v in example.items():        print(f\"{k}: {v}\")\n```\n\n```\nExamples most similar to the input: Who was the father of Mary Ball Washington?answer: Are follow up questions needed here: Yes.Follow up: Who was the mother of George Washington?Intermediate answer: The mother of George Washington was Mary Ball Washington.Follow up: Who was the father of Mary Ball Washington?Intermediate answer: The father of Mary Ball Washington was Joseph Ball.So the final answer is: Joseph Ballquestion: Who was the maternal grandfather of George Washington?\n```\n\nNow, let's create a FewShotPromptTemplate object. This object takes in the example selector and the formatter prompt for the few-shot examples.\n\n```\nprompt = FewShotPromptTemplate(    example_selector=example_selector,    example_prompt=example_prompt,    suffix=\"Question: {input}\",    input_variables=[\"input\"],)print(    prompt.invoke({\"input\": \"Who was the father of Mary Ball Washington?\"}).to_string())\n```\n\n```\nQuestion: Who was the maternal grandfather of George Washington?Are follow up questions needed here: Yes.Follow up: Who was the mother of George Washington?Intermediate answer: The mother of George Washington was Mary Ball Washington.Follow up: Who was the father of Mary Ball Washington?Intermediate answer: The father of Mary Ball Washington was Joseph Ball.So the final answer is: Joseph BallQuestion: Who was the father of Mary Ball Washington?\n```", "url_final": "https://python.langchain.com/docs/how_to/few_shot_examples/", "url_citable": "https://python.langchain.com/docs/how_to/few_shot_examples/#using-an-example-selector", "title": "How to use few shot examples | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Using an example selector‚Äã", "section_anchor": "using-an-example-selector", "fetched_at": "2025-10-06T21:54:30.117556", "has_code": true}
{"id": "b29f7ef379db151d036c6f0b5615e6f5c2504e82", "text": "Next steps‚Äã\n\nYou've now learned how to add few-shot examples to your prompts.\n\nNext, check out the other how-to guides on prompt templates in this section, the related how-to guide on few shotting with chat models, or the other example selector how-to guides.\n\n- Pass the examples and formatter to FewShotPromptTemplate", "url_final": "https://python.langchain.com/docs/how_to/few_shot_examples/", "url_citable": "https://python.langchain.com/docs/how_to/few_shot_examples/#next-steps", "title": "How to use few shot examples | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Next steps‚Äã", "section_anchor": "next-steps", "fetched_at": "2025-10-06T21:54:30.117556", "has_code": false}
{"id": "7b09d7556586e3b65f3b800a81555c07ee3e113a", "text": "In Memory Cache‚Äã\n\nThis is an ephemeral cache that stores model calls in memory. It will be wiped when your environment restarts, and is not shared across processes.\n\n```\n%%timefrom langchain_core.caches import InMemoryCacheset_llm_cache(InMemoryCache())# The first time, it is not yet in cache, so it should take longerllm.invoke(\"Tell me a joke\")\n```\n\n```\nCPU times: user 645 ms, sys: 214 ms, total: 859 msWall time: 829 ms\n```\n\n```\nAIMessage(content=\"Why don't scientists trust atoms?\\n\\nBecause they make up everything!\", response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 11, 'total_tokens': 24}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-b6836bdd-8c30-436b-828f-0ac5fc9ab50e-0')\n```\n\n```\n%%time# The second time it is, so it goes fasterllm.invoke(\"Tell me a joke\")\n```\n\n```\nCPU times: user 822 ¬µs, sys: 288 ¬µs, total: 1.11 msWall time: 1.06 ms\n```\n\n```\nAIMessage(content=\"Why don't scientists trust atoms?\\n\\nBecause they make up everything!\", response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 11, 'total_tokens': 24}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-b6836bdd-8c30-436b-828f-0ac5fc9ab50e-0')\n```", "url_final": "https://python.langchain.com/docs/how_to/chat_model_caching/", "url_citable": "https://python.langchain.com/docs/how_to/chat_model_caching/#in-memory-cache", "title": "How to cache chat model responses | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "In Memory Cache‚Äã", "section_anchor": "in-memory-cache", "fetched_at": "2025-10-06T21:55:03.986578", "has_code": true}
{"id": "a0a53a3927d7b78aa4122b25295e8b53075259da", "text": "SQLite Cache‚Äã\n\nThis cache implementation uses a SQLite database to store responses, and will last across process restarts.\n\n```\n!rm .langchain.db\n```\n\n```\n# We can do the same thing with a SQLite cachefrom langchain_community.cache import SQLiteCacheset_llm_cache(SQLiteCache(database_path=\".langchain.db\"))\n```\n\n```\n%%time# The first time, it is not yet in cache, so it should take longerllm.invoke(\"Tell me a joke\")\n```\n\n```\nCPU times: user 9.91 ms, sys: 7.68 ms, total: 17.6 msWall time: 657 ms\n```\n\n```\nAIMessage(content='Why did the scarecrow win an award? Because he was outstanding in his field!', response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 11, 'total_tokens': 28}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-39d9e1e8-7766-4970-b1d8-f50213fd94c5-0')\n```\n\n```\n%%time# The second time it is, so it goes fasterllm.invoke(\"Tell me a joke\")\n```\n\n```\nCPU times: user 52.2 ms, sys: 60.5 ms, total: 113 msWall time: 127 ms\n```\n\n```\nAIMessage(content='Why did the scarecrow win an award? Because he was outstanding in his field!', id='run-39d9e1e8-7766-4970-b1d8-f50213fd94c5-0')\n```", "url_final": "https://python.langchain.com/docs/how_to/chat_model_caching/", "url_citable": "https://python.langchain.com/docs/how_to/chat_model_caching/#sqlite-cache", "title": "How to cache chat model responses | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "SQLite Cache‚Äã", "section_anchor": "sqlite-cache", "fetched_at": "2025-10-06T21:55:03.986578", "has_code": true}
{"id": "f2ae282fbf8c74ec16896f930720cbb805c33a99", "text": "Next steps‚Äã\n\nYou've now learned how to cache model responses to save time and money.\n\nNext, check out the other how-to guides chat models in this section, like how to get a model to return structured output or how to create your own custom chat model.\n\n- In Memory Cache\n- SQLite Cache\n- Next steps", "url_final": "https://python.langchain.com/docs/how_to/chat_model_caching/", "url_citable": "https://python.langchain.com/docs/how_to/chat_model_caching/#next-steps", "title": "How to cache chat model responses | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Next steps‚Äã", "section_anchor": "next-steps", "fetched_at": "2025-10-06T21:55:03.986578", "has_code": false}
{"id": "cc6488807457835d37f6098601e4d6b0fb6dda13", "text": "Defining the tool‚Äã\n\nIf we want our tool to distinguish between message content and other artifacts, we need to specify response_format=\"content_and_artifact\" when defining our tool and make sure that we return a tuple of (content, artifact):\n\n```\n%pip install -qU \"langchain-core>=0.2.19\"\n```\n\n```\nimport randomfrom typing import List, Tuplefrom langchain_core.tools import tool@tool(response_format=\"content_and_artifact\")def generate_random_ints(min: int, max: int, size: int) -> Tuple[str, List[int]]:    \"\"\"Generate size random ints in the range [min, max].\"\"\"    array = [random.randint(min, max) for _ in range(size)]    content = f\"Successfully generated array of {size} random ints in [{min}, {max}].\"    return content, array\n```", "url_final": "https://python.langchain.com/docs/how_to/tool_artifacts/", "url_citable": "https://python.langchain.com/docs/how_to/tool_artifacts/#defining-the-tool", "title": "How to return artifacts from a tool | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Defining the tool‚Äã", "section_anchor": "defining-the-tool", "fetched_at": "2025-10-06T21:56:32.527496", "has_code": true}
{"id": "37152855cb369c8858adf529d600020896dbd5b9", "text": "Invoking the tool with ToolCall‚Äã\n\nIf we directly invoke our tool with just the tool arguments, you'll notice that we only get back the content part of the Tool output:\n\n```\ngenerate_random_ints.invoke({\"min\": 0, \"max\": 9, \"size\": 10})\n```\n\n```\n'Successfully generated array of 10 random ints in [0, 9].'\n```\n\nIn order to get back both the content and the artifact, we need to invoke our model with a ToolCall (which is just a dictionary with \"name\", \"args\", \"id\" and \"type\" keys), which has additional info needed to generate a ToolMessage like the tool call ID:\n\n```\ngenerate_random_ints.invoke(    {        \"name\": \"generate_random_ints\",        \"args\": {\"min\": 0, \"max\": 9, \"size\": 10},        \"id\": \"123\",  # required        \"type\": \"tool_call\",  # required    })\n```\n\n```\nToolMessage(content='Successfully generated array of 10 random ints in [0, 9].', name='generate_random_ints', tool_call_id='123', artifact=[2, 8, 0, 6, 0, 0, 1, 5, 0, 0])\n```", "url_final": "https://python.langchain.com/docs/how_to/tool_artifacts/", "url_citable": "https://python.langchain.com/docs/how_to/tool_artifacts/#invoking-the-tool-with-toolcall", "title": "How to return artifacts from a tool | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Invoking the tool with ToolCall‚Äã", "section_anchor": "invoking-the-tool-with-toolcall", "fetched_at": "2025-10-06T21:56:32.527496", "has_code": true}
{"id": "28442a27171fdc38508a324bca85f9bdb693a857", "text": "Using with a model‚Äã\n\nWith a tool-calling model, we can easily use a model to call our Tool and generate ToolMessages:\n\n```\npip install -qU \"langchain[google-genai]\"\n```\n\n```\nimport getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n```\n\n```\nllm_with_tools = llm.bind_tools([generate_random_ints])ai_msg = llm_with_tools.invoke(\"generate 6 positive ints less than 25\")ai_msg.tool_calls\n```\n\n```\n[{'name': 'generate_random_ints',  'args': {'min': 1, 'max': 24, 'size': 6},  'id': 'toolu_01EtALY3Wz1DVYhv1TLvZGvE',  'type': 'tool_call'}]\n```\n\n```\ngenerate_random_ints.invoke(ai_msg.tool_calls[0])\n```\n\n```\nToolMessage(content='Successfully generated array of 6 random ints in [1, 24].', name='generate_random_ints', tool_call_id='toolu_01EtALY3Wz1DVYhv1TLvZGvE', artifact=[2, 20, 23, 8, 1, 15])\n```\n\nIf we just pass in the tool call args, we'll only get back the content:\n\n```\ngenerate_random_ints.invoke(ai_msg.tool_calls[0][\"args\"])\n```\n\n```\n'Successfully generated array of 6 random ints in [1, 24].'\n```\n\nIf we wanted to declaratively create a chain, we could do this:\n\n```\nfrom operator import attrgetterchain = llm_with_tools | attrgetter(\"tool_calls\") | generate_random_ints.map()chain.invoke(\"give me a random number between 1 and 5\")\n```\n\n```\n[ToolMessage(content='Successfully generated array of 1 random ints in [1, 5].', name='generate_random_ints', tool_call_id='toolu_01FwYhnkwDPJPbKdGq4ng6uD', artifact=[5])]\n```", "url_final": "https://python.langchain.com/docs/how_to/tool_artifacts/", "url_citable": "https://python.langchain.com/docs/how_to/tool_artifacts/#using-with-a-model", "title": "How to return artifacts from a tool | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Using with a model‚Äã", "section_anchor": "using-with-a-model", "fetched_at": "2025-10-06T21:56:32.527496", "has_code": true}
{"id": "b6edda1ffb57fdef24ace1e571e36180aa0c7d77", "text": "Creating from BaseTool class‚Äã\n\nIf you want to create a BaseTool object directly, instead of decorating a function with @tool, you can do so like this:\n\n```\nfrom langchain_core.tools import BaseToolclass GenerateRandomFloats(BaseTool):    name: str = \"generate_random_floats\"    description: str = \"Generate size random floats in the range [min, max].\"    response_format: str = \"content_and_artifact\"    ndigits: int = 2    def _run(self, min: float, max: float, size: int) -> Tuple[str, List[float]]:        range_ = max - min        array = [            round(min + (range_ * random.random()), ndigits=self.ndigits)            for _ in range(size)        ]        content = f\"Generated {size} floats in [{min}, {max}], rounded to {self.ndigits} decimals.\"        return content, array    # Optionally define an equivalent async method    # async def _arun(self, min: float, max: float, size: int) -> Tuple[str, List[float]]:    #     ...\n```\n\n```\nrand_gen = GenerateRandomFloats(ndigits=4)rand_gen.invoke({\"min\": 0.1, \"max\": 3.3333, \"size\": 3})\n```\n\n```\n'Generated 3 floats in [0.1, 3.3333], rounded to 4 decimals.'\n```\n\n```\nrand_gen.invoke(    {        \"name\": \"generate_random_floats\",        \"args\": {\"min\": 0.1, \"max\": 3.3333, \"size\": 3},        \"id\": \"123\",        \"type\": \"tool_call\",    })\n```\n\n```\nToolMessage(content='Generated 3 floats in [0.1, 3.3333], rounded to 4 decimals.', name='generate_random_floats', tool_call_id='123', artifact=[1.5789, 2.464, 2.2719])\n```\n\n- Defining the tool\n- Invoking the tool with ToolCall\n- Using with a model\n- Creating from BaseTool class", "url_final": "https://python.langchain.com/docs/how_to/tool_artifacts/", "url_citable": "https://python.langchain.com/docs/how_to/tool_artifacts/#creating-from-basetool-class", "title": "How to return artifacts from a tool | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Creating from BaseTool class‚Äã", "section_anchor": "creating-from-basetool-class", "fetched_at": "2025-10-06T21:56:32.527496", "has_code": true}
{"id": "4d4202c3e32d9cb9adfaaaf37da00621edf06c49", "text": "Passing tools to LLMs‚Äã\n\nChat models supporting tool calling features implement a .bind_tools method, which receives a list of LangChain tool objects and binds them to the chat model in its expected format. Subsequent invocations of the chat model will include tool schemas in its calls to the LLM.\n\nFor example, we can define the schema for custom tools using the @tool decorator on Python functions:\n\n```\nfrom langchain_core.tools import tool@tooldef add(a: int, b: int) -> int:    \"\"\"Adds a and b.\"\"\"    return a + b@tooldef multiply(a: int, b: int) -> int:    \"\"\"Multiplies a and b.\"\"\"    return a * btools = [add, multiply]\n```\n\nOr below, we define the schema using Pydantic:\n\n```\nfrom pydantic import BaseModel, Field# Note that the docstrings here are crucial, as they will be passed along# to the model along with the class name.class Add(BaseModel):    \"\"\"Add two integers together.\"\"\"    a: int = Field(..., description=\"First integer\")    b: int = Field(..., description=\"Second integer\")class Multiply(BaseModel):    \"\"\"Multiply two integers together.\"\"\"    a: int = Field(..., description=\"First integer\")    b: int = Field(..., description=\"Second integer\")tools = [Add, Multiply]\n```\n\nWe can bind them to chat models as follows:\n\n```\npip install -qU \"langchain[google-genai]\"\n```\n\n```\nimport getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n```\n\nWe can use the bind_tools() method to handle converting Multiply to a \"tool\" and binding it to the model (i.e., passing it in each time the model is invoked).\n\n```\nllm_with_tools = llm.bind_tools(tools)\n```", "url_final": "https://python.langchain.com/docs/how_to/function_calling/", "url_citable": "https://python.langchain.com/docs/how_to/function_calling/#passing-tools-to-llms", "title": "How to do tool/function calling | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Passing tools to LLMs‚Äã", "section_anchor": "passing-tools-to-llms", "fetched_at": "2025-10-06T21:54:27.107285", "has_code": true}
{"id": "c86a383ec954d98b2f20caee68f9038dd8b973c3", "text": "Tool calls‚Äã\n\nIf tool calls are included in a LLM response, they are attached to the corresponding message or message chunk as a list of tool call objects in the .tool_calls attribute. A ToolCall is a typed dict that includes a tool name, dict of argument values, and (optionally) an identifier. Messages with no tool calls default to an empty list for this attribute.\n\nExample:\n\n```\nquery = \"What is 3 * 12? Also, what is 11 + 49?\"llm_with_tools.invoke(query).tool_calls\n```\n\n```\n[{'name': 'Multiply',  'args': {'a': 3, 'b': 12},  'id': 'call_1Tdp5wUXbYQzpkBoagGXqUTo'}, {'name': 'Add',  'args': {'a': 11, 'b': 49},  'id': 'call_k9v09vYioS3X0Qg35zESuUKI'}]\n```\n\nThe .tool_calls attribute should contain valid tool calls. Note that on occasion, model providers may output malformed tool calls (e.g., arguments that are not valid JSON). When parsing fails in these cases, instances of InvalidToolCall are populated in the .invalid_tool_calls attribute. An InvalidToolCall can have a name, string arguments, identifier, and error message.\n\nIf desired, output parsers can further process the output. For example, we can convert back to the original Pydantic class:\n\n```\nfrom langchain_core.output_parsers.openai_tools import PydanticToolsParserchain = llm_with_tools | PydanticToolsParser(tools=[Multiply, Add])chain.invoke(query)\n```\n\n```\n[Multiply(a=3, b=12), Add(a=11, b=49)]\n```", "url_final": "https://python.langchain.com/docs/how_to/function_calling/", "url_citable": "https://python.langchain.com/docs/how_to/function_calling/#tool-calls", "title": "How to do tool/function calling | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Tool calls‚Äã", "section_anchor": "tool-calls", "fetched_at": "2025-10-06T21:54:27.107285", "has_code": true}
{"id": "65c786a540bf2d340ee3740cabacdc5fb3497e39", "text": "Streaming‚Äã\n\nWhen tools are called in a streaming context, message chunks will be populated with tool call chunk objects in a list via the .tool_call_chunks attribute. A ToolCallChunk includes optional string fields for the tool name, args, and id, and includes an optional integer field index that can be used to join chunks together. Fields are optional because portions of a tool call may be streamed across different chunks (e.g., a chunk that includes a substring of the arguments may have null values for the tool name and id).\n\nBecause message chunks inherit from their parent message class, an AIMessageChunk with tool call chunks will also include .tool_calls and .invalid_tool_calls fields. These fields are parsed best-effort from the message's tool call chunks.\n\nNote that not all providers currently support streaming for tool calls.\n\nExample:\n\n```\nasync for chunk in llm_with_tools.astream(query):    print(chunk.tool_call_chunks)\n```\n\n```\n[][{'name': 'Multiply', 'args': '', 'id': 'call_d39MsxKM5cmeGJOoYKdGBgzc', 'index': 0}][{'name': None, 'args': '{\"a\"', 'id': None, 'index': 0}][{'name': None, 'args': ': 3, ', 'id': None, 'index': 0}][{'name': None, 'args': '\"b\": 1', 'id': None, 'index': 0}][{'name': None, 'args': '2}', 'id': None, 'index': 0}][{'name': 'Add', 'args': '', 'id': 'call_QJpdxD9AehKbdXzMHxgDMMhs', 'index': 1}][{'name': None, 'args': '{\"a\"', 'id': None, 'index': 1}][{'name': None, 'args': ': 11,', 'id': None, 'index': 1}][{'name': None, 'args': ' \"b\": ', 'id': None, 'index': 1}][{'name': None, 'args': '49}', 'id': None, 'index': 1}][]\n```\n\nNote that adding message chunks will merge their corresponding tool call chunks. This is the principle by which LangChain's various tool output parsers support streaming.\n\nFor example, below we accumulate tool call chunks:\n\n```\nfirst = Trueasync for chunk in llm_with_tools.astream(query):    if first:        gathered = chunk        first = False    else:        gathered = gathered + chunk    print(gathered.tool_call_chunks)\n```\n\n```\n[][{'name': 'Multiply', 'args': '', 'id': 'call_erKtz8z3e681cmxYKbRof0NS', 'index': 0}][{'name': 'Multiply', 'args': '{\"a\"', 'id': 'call_erKtz8z3e681cmxYKbRof0NS', 'index': 0}][{'name': 'Multiply', 'args': '{\"a\": 3, ', 'id': 'call_erKtz8z3e681cmxYKbRof0NS', 'index': 0}][{'name': 'Multiply', 'args': '{\"a\": 3, \"b\": 1', 'id': 'call_erKtz8z3e681cmxYKbRof0NS', 'index': 0}][{'name': 'Multiply', 'args': '{\"a\": 3, \"b\": 12}', 'id': 'call_erKtz8z3e681cmxYKbRof0NS', 'index': 0}][{'name': 'Multiply', 'args': '{\"a\": 3, \"b\": 12}', 'id': 'call_erKtz8z3e681cmxYKbRof0NS', 'index': 0}, {'name': 'Add', 'args': '', 'id': 'call_tYHYdEV2YBvzDcSCiFCExNvw', 'index': 1}][{'name': 'Multiply', 'args': '{\"a\": 3, \"b\": 12}', 'id': 'call_erKtz8z3e681cmxYKbRof0NS', 'index': 0}, {'name': 'Add', 'args': '{\"a\"', 'id': 'call_tYHYdEV2YBvzDcSCiFCExNvw', 'index': 1}][{'name': 'Multiply', 'args': '{\"a\": 3, \"b\": 12}', 'id': 'call_erKtz8z3e681cmxYKbRof0NS', 'index': 0}, {'name': 'Add', 'args': '{\"a\": 11,', 'id': 'call_tYHYdEV2YBvzDcSCiFCExNvw', 'index': 1}][{'name': 'Multiply', 'args': '{\"a\": 3, \"b\": 12}', 'id': 'call_erKtz8z3e681cmxYKbRof0NS', 'index': 0}, {'name': 'Add', 'args': '{\"a\": 11, \"b\": ', 'id': 'call_tYHYdEV2YBvzDcSCiFCExNvw', 'index': 1}][{'name': 'Multiply', 'args': '{\"a\": 3, \"b\": 12}', 'id': 'call_erKtz8z3e681cmxYKbRof0NS', 'index': 0}, {'name': 'Add', 'args': '{\"a\": 11, \"b\": 49}', 'id': 'call_tYHYdEV2YBvzDcSCiFCExNvw', 'index': 1}][{'name': 'Multiply', 'args': '{\"a\": 3, \"b\": 12}', 'id': 'call_erKtz8z3e681cmxYKbRof0NS', 'index': 0}, {'name': 'Add', 'args': '{\"a\": 11, \"b\": 49}', 'id': 'call_tYHYdEV2YBvzDcSCiFCExNvw', 'index': 1}]\n```\n\n```\nprint(type(gathered.tool_call_chunks[0][\"args\"]))\n```\n\n```\n<class 'str'>\n```\n\nAnd below we accumulate tool calls to demonstrate partial parsing:\n\n```\nfirst = Trueasync for chunk in llm_with_tools.astream(query):    if first:        gathered = chunk        first = False    else:        gathered = gathered + chunk    print(gathered.tool_calls)\n```\n\n```\n[][][{'name': 'Multiply', 'args': {}, 'id': 'call_BXqUtt6jYCwR1DguqpS2ehP0'}][{'name': 'Multiply', 'args': {'a': 3}, 'id': 'call_BXqUtt6jYCwR1DguqpS2ehP0'}][{'name': 'Multiply', 'args': {'a': 3, 'b': 1}, 'id': 'call_BXqUtt6jYCwR1DguqpS2ehP0'}][{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_BXqUtt6jYCwR1DguqpS2ehP0'}][{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_BXqUtt6jYCwR1DguqpS2ehP0'}][{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_BXqUtt6jYCwR1DguqpS2ehP0'}, {'name': 'Add', 'args': {}, 'id': 'call_UjSHJKROSAw2BDc8cp9cSv4i'}][{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_BXqUtt6jYCwR1DguqpS2ehP0'}, {'name': 'Add', 'args': {'a': 11}, 'id': 'call_UjSHJKROSAw2BDc8cp9cSv4i'}][{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_BXqUtt6jYCwR1DguqpS2ehP0'}, {'name': 'Add', 'args': {'a': 11}, 'id': 'call_UjSHJKROSAw2BDc8cp9cSv4i'}][{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_BXqUtt6jYCwR1DguqpS2ehP0'}, {'name': 'Add', 'args': {'a': 11, 'b': 49}, 'id': 'call_UjSHJKROSAw2BDc8cp9cSv4i'}][{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_BXqUtt6jYCwR1DguqpS2ehP0'}, {'name': 'Add', 'args': {'a': 11, 'b': 49}, 'id': 'call_UjSHJKROSAw2BDc8cp9cSv4i'}]\n```\n\n```\nprint(type(gathered.tool_calls[0][\"args\"]))\n```\n\n```\n<class 'dict'>\n```", "url_final": "https://python.langchain.com/docs/how_to/function_calling/", "url_citable": "https://python.langchain.com/docs/how_to/function_calling/#streaming", "title": "How to do tool/function calling | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Streaming‚Äã", "section_anchor": "streaming", "fetched_at": "2025-10-06T21:54:27.107285", "has_code": true}
{"id": "bb9960c42dcab722c2d72c995320e788aeab0471", "text": "Passing tool outputs to model‚Äã\n\nIf we're using the model-generated tool invocations to actually call tools and want to pass the tool results back to the model, we can do so using ToolMessages.\n\n```\nfrom langchain_core.messages import HumanMessage, ToolMessagemessages = [HumanMessage(query)]ai_msg = llm_with_tools.invoke(messages)messages.append(ai_msg)for tool_call in ai_msg.tool_calls:    selected_tool = {\"add\": add, \"multiply\": multiply}[tool_call[\"name\"].lower()]    tool_output = selected_tool.invoke(tool_call[\"args\"])    messages.append(ToolMessage(tool_output, tool_call_id=tool_call[\"id\"]))messages\n```\n\n```\n[HumanMessage(content='What is 3 * 12? Also, what is 11 + 49?'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_K5DsWEmgt6D08EI9AFu9NaL1', 'function': {'arguments': '{\"a\": 3, \"b\": 12}', 'name': 'Multiply'}, 'type': 'function'}, {'id': 'call_qywVrsplg0ZMv7LHYYMjyG81', 'function': {'arguments': '{\"a\": 11, \"b\": 49}', 'name': 'Add'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 105, 'total_tokens': 155}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-1a0b8cdd-9221-4d94-b2ed-5701f67ce9fe-0', tool_calls=[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_K5DsWEmgt6D08EI9AFu9NaL1'}, {'name': 'Add', 'args': {'a': 11, 'b': 49}, 'id': 'call_qywVrsplg0ZMv7LHYYMjyG81'}]), ToolMessage(content='36', tool_call_id='call_K5DsWEmgt6D08EI9AFu9NaL1'), ToolMessage(content='60', tool_call_id='call_qywVrsplg0ZMv7LHYYMjyG81')]\n```\n\n```\nllm_with_tools.invoke(messages)\n```\n\n```\nAIMessage(content='3 * 12 is 36 and 11 + 49 is 60.', response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 171, 'total_tokens': 189}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None}, id='run-a6c8093c-b16a-4c92-8308-7c9ac998118c-0')\n```", "url_final": "https://python.langchain.com/docs/how_to/function_calling/", "url_citable": "https://python.langchain.com/docs/how_to/function_calling/#passing-tool-outputs-to-model", "title": "How to do tool/function calling | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Passing tool outputs to model‚Äã", "section_anchor": "passing-tool-outputs-to-model", "fetched_at": "2025-10-06T21:54:27.107285", "has_code": true}
{"id": "6cf6a18a8d1f6ad97e890f2529d1de3f0bb56be2", "text": "Few-shot prompting‚Äã\n\nFor more complex tool use it's very useful to add few-shot examples to the prompt. We can do this by adding AIMessages with ToolCalls and corresponding ToolMessages to our prompt.\n\nFor example, even with some special instructions our model can get tripped up by order of operations:\n\n```\nllm_with_tools.invoke(    \"Whats 119 times 8 minus 20. Don't do any math yourself, only use tools for math. Respect order of operations\").tool_calls\n```\n\n```\n[{'name': 'Multiply',  'args': {'a': 119, 'b': 8},  'id': 'call_Dl3FXRVkQCFW4sUNYOe4rFr7'}, {'name': 'Add',  'args': {'a': 952, 'b': -20},  'id': 'call_n03l4hmka7VZTCiP387Wud2C'}]\n```\n\nThe model shouldn't be trying to add anything yet, since it technically can't know the results of 119 * 8 yet.\n\nBy adding a prompt with some examples we can correct this behavior:\n\n```\nfrom langchain_core.messages import AIMessagefrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnablePassthroughexamples = [    HumanMessage(        \"What's the product of 317253 and 128472 plus four\", name=\"example_user\"    ),    AIMessage(        \"\",        name=\"example_assistant\",        tool_calls=[            {\"name\": \"Multiply\", \"args\": {\"x\": 317253, \"y\": 128472}, \"id\": \"1\"}        ],    ),    ToolMessage(\"16505054784\", tool_call_id=\"1\"),    AIMessage(        \"\",        name=\"example_assistant\",        tool_calls=[{\"name\": \"Add\", \"args\": {\"x\": 16505054784, \"y\": 4}, \"id\": \"2\"}],    ),    ToolMessage(\"16505054788\", tool_call_id=\"2\"),    AIMessage(        \"The product of 317253 and 128472 plus four is 16505054788\",        name=\"example_assistant\",    ),]system = \"\"\"You are bad at math but are an expert at using a calculator. Use past tool usage as an example of how to correctly use the tools.\"\"\"few_shot_prompt = ChatPromptTemplate.from_messages(    [        (\"system\", system),        *examples,        (\"human\", \"{query}\"),    ])chain = {\"query\": RunnablePassthrough()} | few_shot_prompt | llm_with_toolschain.invoke(\"Whats 119 times 8 minus 20\").tool_calls\n```\n\n```\n[{'name': 'Multiply',  'args': {'a': 119, 'b': 8},  'id': 'call_MoSgwzIhPxhclfygkYaKIsGZ'}]\n```\n\nSeems like we get the correct output this time.\n\nHere's what the LangSmith trace looks like.", "url_final": "https://python.langchain.com/docs/how_to/function_calling/", "url_citable": "https://python.langchain.com/docs/how_to/function_calling/#few-shot-prompting", "title": "How to do tool/function calling | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Few-shot prompting‚Äã", "section_anchor": "few-shot-prompting", "fetched_at": "2025-10-06T21:54:27.107285", "has_code": true}
{"id": "431f1d7ffd5f1efe791a8ef35b948c7e7de7a918", "text": "Next steps‚Äã\n\n- Output parsing: See OpenAI Tools output parsers to learn about extracting the function calling API responses into various formats.\n- Structured output chains: Some models have constructors that handle creating a structured output chain for you.\n- Tool use: See how to construct chains and agents that call the invoked tools in these guides.\n\n- Streaming", "url_final": "https://python.langchain.com/docs/how_to/function_calling/", "url_citable": "https://python.langchain.com/docs/how_to/function_calling/#next-steps", "title": "How to do tool/function calling | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Next steps‚Äã", "section_anchor": "next-steps", "fetched_at": "2025-10-06T21:54:27.107285", "has_code": false}
{"id": "b5ebcd5d10aa006eec3d097a5b551168eb7bcc57", "text": "Overview‚Äã\n\nApplications based on LLMs frequently entail extracting data from databases or files, like PDFs, and converting it into a format that LLMs can utilize. In LangChain, this usually involves creating Document objects, which encapsulate the extracted text (page_content) along with metadata‚Äîa dictionary containing details about the document, such as the author's name or the date of publication.\n\nDocument objects are often formatted into prompts that are fed into an LLM, allowing the LLM to use the information in the Document to generate a desired response (e.g., summarizing the document). Documents can be either used immediately or indexed into a vectorstore for future retrieval and use.\n\nThe main abstractions for Document Loading are:\n\nThis guide will demonstrate how to write custom document loading and file parsing logic; specifically, we'll see how to:\n\n- Create a standard document Loader by sub-classing from BaseLoader.\n- Create a parser using BaseBlobParser and use it in conjunction with Blob and BlobLoaders. This is useful primarily when working with files.", "url_final": "https://python.langchain.com/docs/how_to/document_loader_custom/", "url_citable": "https://python.langchain.com/docs/how_to/document_loader_custom/#overview", "title": "Custom Document Loader | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Overview‚Äã", "section_anchor": "overview", "fetched_at": "2025-10-06T21:54:53.753256", "has_code": false}
{"id": "9532b2008eae87a3163f3d4d6258ea2b05687c4d", "text": "Standard Document Loader‚Äã\n\nA document loader can be implemented by sub-classing from a BaseLoader which provides a standard interface for loading documents.\n\nInterface‚Äã\n\n- The load methods is a convenience method meant solely for prototyping work -- it just invokes list(self.lazy_load()).\n- The alazy_load has a default implementation that will delegate to lazy_load. If you're using async, we recommend overriding the default implementation and providing a native async implementation.\n\nWhen implementing a document loader do NOT provide parameters via the lazy_load or alazy_load methods.\n\nAll configuration is expected to be passed through the initializer (init). This was a design choice made by LangChain to make sure that once a document loader has been instantiated it has all the information needed to load documents.\n\nInstallation‚Äã\n\nInstall langchain-core and langchain_community.\n\n```\n%pip install -qU langchain_core langchain_community\n```\n\nImplementation‚Äã\n\nLet's create an example of a standard document loader that loads a file and creates a document from each line in the file.\n\n```\nfrom typing import AsyncIterator, Iteratorfrom langchain_core.document_loaders import BaseLoaderfrom langchain_core.documents import Documentclass CustomDocumentLoader(BaseLoader):    \"\"\"An example document loader that reads a file line by line.\"\"\"    def __init__(self, file_path: str) -> None:        \"\"\"Initialize the loader with a file path.        Args:            file_path: The path to the file to load.        \"\"\"        self.file_path = file_path    def lazy_load(self) -> Iterator[Document]:  # <-- Does not take any arguments        \"\"\"A lazy loader that reads a file line by line.        When you're implementing lazy load methods, you should use a generator        to yield documents one by one.        \"\"\"        with open(self.file_path, encoding=\"utf-8\") as f:            line_number = 0            for line in f:                yield Document(                    page_content=line,                    metadata={\"line_number\": line_number, \"source\": self.file_path},                )                line_number += 1    # alazy_load is OPTIONAL.    # If you leave out the implementation, a default implementation which delegates to lazy_load will be used!    async def alazy_load(        self,    ) -> AsyncIterator[Document]:  # <-- Does not take any arguments        \"\"\"An async lazy loader that reads a file line by line.\"\"\"        # Requires aiofiles        # https://github.com/Tinche/aiofiles        import aiofiles        async with aiofiles.open(self.file_path, encoding=\"utf-8\") as f:            line_number = 0            async for line in f:                yield Document(                    page_content=line,                    metadata={\"line_number\": line_number, \"source\": self.file_path},                )                line_number += 1\n```", "url_final": "https://python.langchain.com/docs/how_to/document_loader_custom/", "url_citable": "https://python.langchain.com/docs/how_to/document_loader_custom/#standard-document-loader", "title": "Custom Document Loader | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Standard Document Loader‚Äã", "section_anchor": "standard-document-loader", "fetched_at": "2025-10-06T21:54:53.753256", "has_code": true}
{"id": "e94e9d836acc2468dcc62167b9dfaa75a8de5101", "text": "Test üß™‚Äã\n\nTo test out the document loader, we need a file with some quality content.\n\n```\nwith open(\"./meow.txt\", \"w\", encoding=\"utf-8\") as f:    quality_content = \"meow meowüê± \\n meow meowüê± \\n meowüòªüòª\"    f.write(quality_content)loader = CustomDocumentLoader(\"./meow.txt\")\n```\n\n```\n%pip install -q aiofiles\n```\n\n```\n## Test out the lazy load interfacefor doc in loader.lazy_load():    print()    print(type(doc))    print(doc)\n```\n\n```\n<class 'langchain_core.documents.base.Document'>page_content='meow meowüê± ' metadata={'line_number': 0, 'source': './meow.txt'}<class 'langchain_core.documents.base.Document'>page_content=' meow meowüê± ' metadata={'line_number': 1, 'source': './meow.txt'}<class 'langchain_core.documents.base.Document'>page_content=' meowüòªüòª' metadata={'line_number': 2, 'source': './meow.txt'}\n```\n\n```\n## Test out the async implementationasync for doc in loader.alazy_load():    print()    print(type(doc))    print(doc)\n```\n\n```\n<class 'langchain_core.documents.base.Document'>page_content='meow meowüê± ' metadata={'line_number': 0, 'source': './meow.txt'}<class 'langchain_core.documents.base.Document'>page_content=' meow meowüê± ' metadata={'line_number': 1, 'source': './meow.txt'}<class 'langchain_core.documents.base.Document'>page_content=' meowüòªüòª' metadata={'line_number': 2, 'source': './meow.txt'}\n```\n\nload() can be helpful in an interactive environment such as a jupyter notebook.\n\nAvoid using it for production code since eager loading assumes that all the content can fit into memory, which is not always the case, especially for enterprise data.\n\n```\nloader.load()\n```\n\n```\n[Document(metadata={'line_number': 0, 'source': './meow.txt'}, page_content='meow meowüê± \\n'), Document(metadata={'line_number': 1, 'source': './meow.txt'}, page_content=' meow meowüê± \\n'), Document(metadata={'line_number': 2, 'source': './meow.txt'}, page_content=' meowüòªüòª')]\n```", "url_final": "https://python.langchain.com/docs/how_to/document_loader_custom/", "url_citable": "https://python.langchain.com/docs/how_to/document_loader_custom/#test-", "title": "Custom Document Loader | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Test üß™‚Äã", "section_anchor": "test-", "fetched_at": "2025-10-06T21:54:53.753256", "has_code": true}
{"id": "1098d983c8198421410f56bdd0b33752c39e8114", "text": "Working with Files‚Äã\n\nMany document loaders involve parsing files. The difference between such loaders usually stems from how the file is parsed, rather than how the file is loaded. For example, you can use open to read the binary content of either a PDF or a markdown file, but you need different parsing logic to convert that binary data into text.\n\nAs a result, it can be helpful to decouple the parsing logic from the loading logic, which makes it easier to re-use a given parser regardless of how the data was loaded.\n\nBaseBlobParser‚Äã\n\nA BaseBlobParser is an interface that accepts a blob and outputs a list of Document objects. A blob is a representation of data that lives either in memory or in a file. LangChain python has a Blob primitive which is inspired by the Blob WebAPI spec.\n\n```\nfrom langchain_core.document_loaders import BaseBlobParser, Blobclass MyParser(BaseBlobParser):    \"\"\"A simple parser that creates a document from each line.\"\"\"    def lazy_parse(self, blob: Blob) -> Iterator[Document]:        \"\"\"Parse a blob into a document line by line.\"\"\"        line_number = 0        with blob.as_bytes_io() as f:            for line in f:                line_number += 1                yield Document(                    page_content=line,                    metadata={\"line_number\": line_number, \"source\": blob.source},                )\n```\n\n```\nblob = Blob.from_path(\"./meow.txt\")parser = MyParser()\n```\n\n```\nlist(parser.lazy_parse(blob))\n```\n\n```\n[Document(metadata={'line_number': 1, 'source': './meow.txt'}, page_content='meow meowüê± \\n'), Document(metadata={'line_number': 2, 'source': './meow.txt'}, page_content=' meow meowüê± \\n'), Document(metadata={'line_number': 3, 'source': './meow.txt'}, page_content=' meowüòªüòª')]\n```\n\nUsing the blob API also allows one to load content directly from memory without having to read it from a file!\n\n```\nblob = Blob(data=b\"some data from memory\\nmeow\")list(parser.lazy_parse(blob))\n```\n\n```\n[Document(metadata={'line_number': 1, 'source': None}, page_content='some data from memory\\n'), Document(metadata={'line_number': 2, 'source': None}, page_content='meow')]\n```\n\nBlob‚Äã\n\nLet's take a quick look through some of the Blob API.\n\n```\nblob = Blob.from_path(\"./meow.txt\", metadata={\"foo\": \"bar\"})\n```\n\n```\nblob.encoding\n```\n\n```\n'utf-8'\n```\n\n```\nblob.as_bytes()\n```\n\n```\nb'meow meow\\xf0\\x9f\\x90\\xb1 \\n meow meow\\xf0\\x9f\\x90\\xb1 \\n meow\\xf0\\x9f\\x98\\xbb\\xf0\\x9f\\x98\\xbb'\n```\n\n```\nblob.as_string()\n```\n\n```\n'meow meowüê± \\n meow meowüê± \\n meowüòªüòª'\n```\n\n```\nblob.as_bytes_io()\n```\n\n```\n<contextlib._GeneratorContextManager at 0x74b8d42e9940>\n```\n\n```\nblob.metadata\n```\n\n```\n{'foo': 'bar'}\n```\n\n```\nblob.source\n```\n\n```\n'./meow.txt'\n```", "url_final": "https://python.langchain.com/docs/how_to/document_loader_custom/", "url_citable": "https://python.langchain.com/docs/how_to/document_loader_custom/#working-with-files", "title": "Custom Document Loader | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Working with Files‚Äã", "section_anchor": "working-with-files", "fetched_at": "2025-10-06T21:54:53.753256", "has_code": true}
{"id": "f621344ada5f27b1a3b35128eccc6e6b089129ab", "text": "Blob Loaders‚Äã\n\nWhile a parser encapsulates the logic needed to parse binary data into documents, blob loaders encapsulate the logic that's necessary to load blobs from a given storage location.\n\nAt the moment, LangChain supports FileSystemBlobLoader and CloudBlobLoader.\n\nYou can use the FileSystemBlobLoader to load blobs and then use the parser to parse them.\n\n```\nfrom langchain_community.document_loaders.blob_loaders import FileSystemBlobLoaderfilesystem_blob_loader = FileSystemBlobLoader(    path=\".\", glob=\"*.mdx\", show_progress=True)\n```\n\n```\n%pip install -q tqdm\n```\n\n```\nparser = MyParser()for blob in filesystem_blob_loader.yield_blobs():    for doc in parser.lazy_parse(blob):        print(doc)        break\n```\n\nOr, you can use CloudBlobLoader to load blobs from a cloud storage location (Supports s3://, az://, gs://, file:// schemes).\n\n```\n%pip install -q 'cloudpathlib[s3]'\n```\n\n```\nfrom cloudpathlib import S3Client, S3Pathfrom langchain_community.document_loaders.blob_loaders import CloudBlobLoaderclient = S3Client(no_sign_request=True)client.set_as_default_client()path = S3Path(    \"s3://bucket-01\", client=client)  # Supports s3://, az://, gs://, file:// schemes.cloud_loader = CloudBlobLoader(path, glob=\"**/*.pdf\", show_progress=True)for blob in cloud_loader.yield_blobs():    print(blob)\n```", "url_final": "https://python.langchain.com/docs/how_to/document_loader_custom/", "url_citable": "https://python.langchain.com/docs/how_to/document_loader_custom/#blob-loaders", "title": "Custom Document Loader | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Blob Loaders‚Äã", "section_anchor": "blob-loaders", "fetched_at": "2025-10-06T21:54:53.753256", "has_code": true}
{"id": "b55fd63b93f56d5a1436b6263ad6793e93b74b12", "text": "Generic Loader\u0000‚Äã\n\nLangChain has a GenericLoader abstraction which composes a BlobLoader with a BaseBlobParser.\n\nGenericLoader is meant to provide standardized classmethods that make it easy to use existing BlobLoader implementations. At the moment, the FileSystemBlobLoader and CloudBlobLoader are supported. See example below:\n\n```\nfrom langchain_community.document_loaders.generic import GenericLoadergeneric_loader_filesystem = GenericLoader(    blob_loader=filesystem_blob_loader, blob_parser=parser)for idx, doc in enumerate(generic_loader_filesystem.lazy_load()):    if idx < 5:        print(doc)print(\"... output truncated for demo purposes\")\n```\n\n```\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00, 1224.82it/s]``````outputpage_content='# Text embedding models' metadata={'line_number': 1, 'source': 'embed_text.mdx'}page_content='' metadata={'line_number': 2, 'source': 'embed_text.mdx'}page_content=':::info' metadata={'line_number': 3, 'source': 'embed_text.mdx'}page_content='Head to [Integrations](/docs/integrations/text_embedding/) for documentation on built-in integrations with text embedding model providers.' metadata={'line_number': 4, 'source': 'embed_text.mdx'}page_content=':::' metadata={'line_number': 5, 'source': 'embed_text.mdx'}... output truncated for demo purposes\n```\n\nIf you really like creating classes, you can sub-class and create a class to encapsulate the logic together.\n\nYou can sub-class from this class to load content using an existing loader.\n\n```\nfrom typing import Anyclass MyCustomLoader(GenericLoader):    @staticmethod    def get_parser(**kwargs: Any) -> BaseBlobParser:        \"\"\"Override this method to associate a default parser with the class.\"\"\"        return MyParser()\n```\n\n```\nloader = MyCustomLoader.from_filesystem(path=\".\", glob=\"*.mdx\", show_progress=True)for idx, doc in enumerate(loader.lazy_load()):    if idx < 5:        print(doc)print(\"... output truncated for demo purposes\")\n```\n\n```\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00, 814.86it/s]``````outputpage_content='# Text embedding models' metadata={'line_number': 1, 'source': 'embed_text.mdx'}page_content='' metadata={'line_number': 2, 'source': 'embed_text.mdx'}page_content=':::info' metadata={'line_number': 3, 'source': 'embed_text.mdx'}page_content='Head to [Integrations](/docs/integrations/text_embedding/) for documentation on built-in integrations with text embedding model providers.' metadata={'line_number': 4, 'source': 'embed_text.mdx'}page_content=':::' metadata={'line_number': 5, 'source': 'embed_text.mdx'}... output truncated for demo purposes\n```\n\n- Interface\n- Installation\n- Implementation\n- Test üß™\n\n- BaseBlobParser\n- Blob\n- Blob Loaders\n- Generic Loader", "url_final": "https://python.langchain.com/docs/how_to/document_loader_custom/", "url_citable": "https://python.langchain.com/docs/how_to/document_loader_custom/#generic-loader", "title": "Custom Document Loader | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Generic Loader\u0000‚Äã", "section_anchor": "generic-loader", "fetched_at": "2025-10-06T21:54:53.753256", "has_code": true}
{"id": "80de25bd04ea55d75fda94e5c74bf5e5a5f90850", "text": "Tracing‚Äã\n\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\n\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\n\n```\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"\n```\n\nOr, if in a notebook, you can set them with:\n\n```\nimport getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n```\n\nLet's suppose we have an agent, and want to visualize the actions it takes and tool outputs it receives. Without any debugging, here's what we see:\n\n```\npip install -qU \"langchain[google-genai]\"\n```\n\n```\nimport getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n```\n\n```\nfrom langchain.agents import AgentExecutor, create_tool_calling_agentfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_tavily import TavilySearchtools = [TavilySearch(max_results=5, topic=\"general\")]prompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"You are a helpful assistant.\",        ),        (\"placeholder\", \"{chat_history}\"),        (\"human\", \"{input}\"),        (\"placeholder\", \"{agent_scratchpad}\"),    ])# Construct the Tools agentagent = create_tool_calling_agent(llm, tools, prompt)# Create an agent executor by passing in the agent and toolsagent_executor = AgentExecutor(agent=agent, tools=tools)agent_executor.invoke(    {\"input\": \"Who directed the 2023 film Oppenheimer and what is their age in days?\"})\n```\n\n```\n{'input': 'Who directed the 2023 film Oppenheimer and what is their age in days?', 'output': \"The 2023 film **Oppenheimer** was directed by **Christopher Nolan**.\\n\\nChristopher Nolan was born on **July 30, 1970**. To calculate his age in days as of today:\\n\\n1. First, determine the total number of days from his birthdate to today.\\n2. Use the formula: \\\\[ \\\\text{Age in days} = (\\\\text{Current Year} - \\\\text{Birth Year}) \\\\times 365 + \\\\text{Extra Days for Leap Years} + \\\\text{Days from Birthday to Today's Date} \\\\]\\n\\nLet's calculate:\\n\\n- From July 30, 1970, to July 30, 2023, is 53 years.\\n- From July 30, 2023, to December 7, 2023, is 130 days.\\n\\nLeap years between 1970 and 2023 (every 4 years, except century years not divisible by 400):\\n1972, 1976, 1980, 1984, 1988, 1992, 1996, 2000, 2004, 2008, 2012, 2016, 2020. That's 13 leap years.\\n\\nSo, his age in days is:\\n\\\\[ 53 \\\\times 365 + 13 + 130 = 19345 + 13 + 130 = 19488 \\\\text{ days} \\\\]\\n\\nChristopher Nolan is **19,488 days old** as of today.\"}\n```\n\nWe don't get much output, but since we set up LangSmith we can easily see what happened under the hood:\n\nhttps://smith.langchain.com/public/a89ff88f-9ddc-4757-a395-3a1b365655bf/r", "url_final": "https://python.langchain.com/docs/how_to/debugging/", "url_citable": "https://python.langchain.com/docs/how_to/debugging/#tracing", "title": "How to debug your LLM apps | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Tracing‚Äã", "section_anchor": "tracing", "fetched_at": "2025-10-06T21:55:18.414131", "has_code": true}
{"id": "4d9cb5c39563d206b3194465d5b3c80c7d9e752d", "text": "set_debug and set_verbose‚Äã\n\nIf you're prototyping in Jupyter Notebooks or running Python scripts, it can be helpful to print out the intermediate steps of a chain run.\n\nThere are a number of ways to enable printing at varying degrees of verbosity.\n\nNote: These still work even with LangSmith enabled, so you can have both turned on and running at the same time", "url_final": "https://python.langchain.com/docs/how_to/debugging/", "url_citable": "https://python.langchain.com/docs/how_to/debugging/#set_debug-and-set_verbose", "title": "How to debug your LLM apps | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "set_debug and set_verbose‚Äã", "section_anchor": "set_debug-and-set_verbose", "fetched_at": "2025-10-06T21:55:18.414131", "has_code": false}
{"id": "6714d1b086598d7659adf9dc4c8556b793cb4f1f", "text": "set_verbose(True)‚Äã\n\nSetting the verbose flag will print out inputs and outputs in a slightly more readable format and will skip logging certain raw outputs (like the token usage stats for an LLM call) so that you can focus on application logic.\n\n```\nfrom langchain.globals import set_verboseset_verbose(True)agent_executor = AgentExecutor(agent=agent, tools=tools)agent_executor.invoke(    {\"input\": \"Who directed the 2023 film Oppenheimer and what is their age in days?\"})\n```\n\n```\n\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\u001b[32;1m\u001b[1;3mInvoking: `tavily_search` with `{'query': 'director of the 2023 film Oppenheimer'}`\u001b[0m\u001b[36;1m\u001b[1;3m{'query': 'director of the 2023 film Oppenheimer', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'Oppenheimer (film) - Wikipedia', 'url': 'https://en.wikipedia.org/wiki/Oppenheimer_(film)', 'content': \"Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Toggle the table of contents Contents move to sidebar hide (Top) 1 Plot 2 Cast 3 ProductionToggle Production subsection 3.1 Development 3.2 Writing 3.3 Casting 3.4 Filming 3.5 Post-production 4 Music 5 Marketing 6 ReleaseToggle Release subsection 6.1 Theatrical 6.1.1 Classifications and censorship 6.1.2 Bhagavad Gita controversy 6.2 Home media 7 ReceptionToggle Reception subsection 7.1 Box office 7.1.1 United States and Canada 7.1.2 Japan 7.1.3 Other territories 7.2 Critical response 7.3 Influence on legislation 8 Accuracy and omissions 9 Accolades 10 See also 11 Notes 12 References 13 Further reading 14 External links Oppenheimer (film) 70 languages ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ‡¶Ö‡¶∏‡¶Æ‡ßÄ‡¶Ø‡¶º‡¶æ Az…ôrbaycanca ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ –ë–µ–ª–∞—Ä—É—Å–∫–∞—è ‡§≠‡•ã‡§ú‡§™‡•Å‡§∞‡•Ä –ë—ä–ª–≥–∞—Ä—Å–∫–∏ Bosanski Catal√† ƒåe≈°tina Cymraeg Dansk Deutsch ‡§°‡•ã‡§ü‡•á‡§≤‡•Ä Eesti ŒïŒªŒªŒ∑ŒΩŒπŒ∫Œ¨ Espa√±ol Euskara ŸÅÿßÿ±ÿ≥€å Fran√ßais Gaeilge Galego ÌïúÍµ≠Ïñ¥ ’Ä’°’µ’•÷Ä’•’∂ ‡§π‡§ø‡§®‡•ç‡§¶‡•Ä Ido Bahasa Indonesia Italiano ◊¢◊ë◊®◊ô◊™ Jawa ·É•·Éê·É†·Éó·É£·Éö·Éò “ö–∞–∑–∞“õ—à–∞ Latina Latvie≈°u Lietuvi≈≥ Magyar –ú–∞–∫–µ–¥–æ–Ω—Å–∫–∏ ‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç ‡§Æ‡§∞‡§æ‡§†‡•Ä ŸÖÿµÿ±Ÿâ ŸÖÿßÿ≤Ÿêÿ±ŸàŸÜ€å Bahasa Melayu Nederlands ‡§®‡•á‡§™‡§æ‡§≤‡•Ä Êó•Êú¨Ë™û Norsk bokm√•l O ªzbekcha / —û–∑–±–µ–∫—á–∞ ‡®™‡©∞‡®ú‡®æ‡®¨‡©Ä Polski Portugu√™s Rom√¢nƒÉ –†—É—Å—Å–∫–∏–π Shqip Simple English Slovenƒçina Sloven≈°ƒçina ⁄©Ÿàÿ±ÿØ€å –°—Ä–ø—Å–∫–∏ / srpski Suomi Svenska ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç ‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å ‡πÑ‡∏ó‡∏¢ –¢–æ“∑–∏–∫”£ T√ºrk√ße –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ ÿßÿ±ÿØŸà Ti·∫øng Vi·ªát Á≤µË™û ‰∏≠Êñá Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Expand all Edit interlanguage links Print/export Download as PDF Printable version In other projects Wikimedia Commons Wikiquote Wikidata item From Wikipedia, the free encyclopedia 2023 film by Christopher Nolan | Oppenheimer | | --- | | Theatrical release poster | | Directed by | Christopher Nolan | | Screenplay by | Christopher Nolan | | Based on | American Prometheus by Kai Bird Martin J. Sherwin | | Produced by | Emma Thomas Charles Roven Christopher Nolan | | Starring | Cillian Murphy Emily Blunt Matt Damon Robert Downey Jr. Florence Pugh Josh Hartnett Casey Affleck Rami Malek Kenneth Branagh | | Cinematography | Hoyte van Hoytema | | Edited by | Jennifer Lame | | Music by | Ludwig G√∂ransson | | Production companies | Universal Pictures[1][2] Syncopy[1][2] Atlas Entertainment[1][2] Breakheart Films[2] Peters Creek Entertainment[2] Gadget Films[1][3] | | Distributed by | Universal Pictures | | Release dates | July 11, 2023 (2023-07-11) (Le Grand Rex) July 21, 2023 (2023-07-21) (United States and United Kingdom) | | Running time | 180 minutes[4] | | Countries | United States United Kingdom | | Language | English | | Budget | $100 million[5] | | Box office | $975.8 million[6][7] | Oppenheimer is a 2023 epic biographical drama film written, produced, and directed by Christopher Nolan. [8] It follows the life of J. Robert Oppenheimer, the American theoretical physicist who helped develop the first nuclear weapons during World War II. Based on the 2005 biography American Prometheus by Kai Bird and Martin J. Sherwin, the film dramatizes Oppenheimer's studies, his direction of the Los Alamos Laboratory and his 1954 security hearing. Oppenheimer received critical acclaim and grossed $975 million worldwide, becoming the third-highest-grossing film of 2023, the highest-grossing World War II-related film, the highest-grossing biographical film and the second-highest-grossing R-rated film of all time at the time of its release.\", 'score': 0.9475027, 'raw_content': None}, {'title': 'Oppenheimer | Cast, Film, Length, Plot, Actors, Awards, & Facts ...', 'url': 'https://www.britannica.com/topic/Oppenheimer-film', 'content': 'J. Robert Oppenheimer Robert Downey, Jr. Oppenheimer # Oppenheimer Oppenheimer, American and British dramatic biographical film, released in 2023, that explores the life and legacy of the American physicist J. Robert Oppenheimer, who played a key role in the development of the atomic bomb. Robert Oppenheimer (2005). Film critics‚Äô reaction to Oppenheimer was overwhelmingly positive. Oppenheimer grossed more than $300 million domestically and more than $600 million internationally by the end of November 2023, making it the second highest grossing R-rated film of all time. The film also dominated the Academy Awards nominations, garnering 13 nominations compared with the 8 for Greta Gerwig‚Äôs Barbie, which opened the same weekend as Oppenheimer but topped Nolan‚Äôs film at the box office.', 'score': 0.76194656, 'raw_content': None}, {'title': 'Oppenheimer (2023) - Full cast & crew - IMDb', 'url': 'https://www.imdb.com/title/tt15398776/fullcredits/', 'content': 'Oppenheimer (2023) - Cast and crew credits, including actors, actresses, directors, writers and more. Menu. ... Oscars Pride Month American Black Film Festival Summer Watch Guide STARmeter Awards Awards Central Festival Central All Events. ... second unit director: visual effects (uncredited) Francesca Kaimer Millea.', 'score': 0.683948, 'raw_content': None}, {'title': \"'Oppenheimer' director Christopher Nolan says the film is his darkest - NPR\", 'url': 'https://www.npr.org/2023/08/14/1193448291/oppenheimer-director-christopher-nolan', 'content': '# \\'Like it or not, we live in Oppenheimer\\'s world,\\' says director Christopher Nolan #### \\'Like it or not, we live in Oppenheimer\\'s world,\\' says director Christopher Nolan But he says the story of Robert Oppenheimer, known as the father of the atomic bomb, stayed with him in a way his other films didn\\'t. Nolan says he was drawn to the tension of Oppenheimer\\'s story ‚Äî particularly the disconnect between the joy the physicist felt at the success of the Trinity test, and the horror that later resulted. Writer, director and producer Christopher Nolan says Oppenheimer is the \"darkest\" of all the films he\\'s worked on. Writer, director and producer Christopher Nolan says Oppenheimer is the \"darkest\" of all the films he\\'s worked on.', 'score': 0.6255073, 'raw_content': None}, {'title': 'An extended interview with Christopher Nolan, director of Oppenheimer', 'url': 'https://thebulletin.org/premium/2023-07/an-extended-interview-with-christopher-nolan-director-of-oppenheimer/', 'content': 'A group of Manhattan Project scientists and engineers also focused on wider public education on nuclear weapons and energy (and science generally) through the creation of the Bulletin of the Atomic Scientists; Oppenheimer served as the first chair of the magazine‚Äôs Board of Sponsors.[5] As time has passed, more evidence has come to light of the bias and unfairness of the process that Dr. Oppenheimer was subjected to while the evidence of his loyalty and love of country have only been further affirmed.‚Äù[8] Decades after the fact, records of the Oppenheimer security hearing made it clear that, rather than any disloyalty to the nation, it was his principled opposition to development of the hydrogen bomb‚Äîa nuclear fusion-based weapon of immensely greater power than the fission weapons used to decimate Hiroshima and Nagasaki in 1945‚Äîthat was key to the decision to essentially bar him from government service. Robert Oppenheimer, Los Alamos, Manhattan Project, Nolan, atomic bomb, movie', 'score': 0.32472825, 'raw_content': None}], 'response_time': 0.94}\u001b[0m\u001b[32;1m\u001b[1;3mInvoking: `tavily_search` with `{'query': 'birthdate of the director of the 2023 film Oppenheimer'}`\u001b[0m\u001b[36;1m\u001b[1;3m{'query': 'birthdate of the director of the 2023 film Oppenheimer', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'Oppenheimer (film) - Wikipedia', 'url': 'https://en.wikipedia.org/wiki/Oppenheimer_(film)', 'content': \"Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Toggle the table of contents Contents move to sidebar hide (Top) 1 Plot 2 Cast 3 ProductionToggle Production subsection 3.1 Development 3.2 Writing 3.3 Casting 3.4 Filming 3.5 Post-production 4 Music 5 Marketing 6 ReleaseToggle Release subsection 6.1 Theatrical 6.1.1 Classifications and censorship 6.1.2 Bhagavad Gita controversy 6.2 Home media 7 ReceptionToggle Reception subsection 7.1 Box office 7.1.1 United States and Canada 7.1.2 Japan 7.1.3 Other territories 7.2 Critical response 7.3 Influence on legislation 8 Accuracy and omissions 9 Accolades 10 See also 11 Notes 12 References 13 Further reading 14 External links Oppenheimer (film) 70 languages ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ‡¶Ö‡¶∏‡¶Æ‡ßÄ‡¶Ø‡¶º‡¶æ Az…ôrbaycanca ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ –ë–µ–ª–∞—Ä—É—Å–∫–∞—è ‡§≠‡•ã‡§ú‡§™‡•Å‡§∞‡•Ä –ë—ä–ª–≥–∞—Ä—Å–∫–∏ Bosanski Catal√† ƒåe≈°tina Cymraeg Dansk Deutsch ‡§°‡•ã‡§ü‡•á‡§≤‡•Ä Eesti ŒïŒªŒªŒ∑ŒΩŒπŒ∫Œ¨ Espa√±ol Euskara ŸÅÿßÿ±ÿ≥€å Fran√ßais Gaeilge Galego ÌïúÍµ≠Ïñ¥ ’Ä’°’µ’•÷Ä’•’∂ ‡§π‡§ø‡§®‡•ç‡§¶‡•Ä Ido Bahasa Indonesia Italiano ◊¢◊ë◊®◊ô◊™ Jawa ·É•·Éê·É†·Éó·É£·Éö·Éò “ö–∞–∑–∞“õ—à–∞ Latina Latvie≈°u Lietuvi≈≥ Magyar –ú–∞–∫–µ–¥–æ–Ω—Å–∫–∏ ‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç ‡§Æ‡§∞‡§æ‡§†‡•Ä ŸÖÿµÿ±Ÿâ ŸÖÿßÿ≤Ÿêÿ±ŸàŸÜ€å Bahasa Melayu Nederlands ‡§®‡•á‡§™‡§æ‡§≤‡•Ä Êó•Êú¨Ë™û Norsk bokm√•l O ªzbekcha / —û–∑–±–µ–∫—á–∞ ‡®™‡©∞‡®ú‡®æ‡®¨‡©Ä Polski Portugu√™s Rom√¢nƒÉ –†—É—Å—Å–∫–∏–π Shqip Simple English Slovenƒçina Sloven≈°ƒçina ⁄©Ÿàÿ±ÿØ€å –°—Ä–ø—Å–∫–∏ / srpski Suomi Svenska ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç ‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å ‡πÑ‡∏ó‡∏¢ –¢–æ“∑–∏–∫”£ T√ºrk√ße –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ ÿßÿ±ÿØŸà Ti·∫øng Vi·ªát Á≤µË™û ‰∏≠Êñá Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Expand all Edit interlanguage links Print/export Download as PDF Printable version In other projects Wikimedia Commons Wikiquote Wikidata item From Wikipedia, the free encyclopedia 2023 film by Christopher Nolan | Oppenheimer | | --- | | Theatrical release poster | | Directed by | Christopher Nolan | | Screenplay by | Christopher Nolan | | Based on | American Prometheus by Kai Bird Martin J. Sherwin | | Produced by | Emma Thomas Charles Roven Christopher Nolan | | Starring | Cillian Murphy Emily Blunt Matt Damon Robert Downey Jr. Florence Pugh Josh Hartnett Casey Affleck Rami Malek Kenneth Branagh | | Cinematography | Hoyte van Hoytema | | Edited by | Jennifer Lame | | Music by | Ludwig G√∂ransson | | Production companies | Universal Pictures[1][2] Syncopy[1][2] Atlas Entertainment[1][2] Breakheart Films[2] Peters Creek Entertainment[2] Gadget Films[1][3] | | Distributed by | Universal Pictures | | Release dates | July 11, 2023 (2023-07-11) (Le Grand Rex) July 21, 2023 (2023-07-21) (United States and United Kingdom) | | Running time | 180 minutes[4] | | Countries | United States United Kingdom | | Language | English | | Budget | $100 million[5] | | Box office | $975.8 million[6][7] | Oppenheimer is a 2023 epic biographical drama film written, produced, and directed by Christopher Nolan. [8] It follows the life of J. Robert Oppenheimer, the American theoretical physicist who helped develop the first nuclear weapons during World War II. Based on the 2005 biography American Prometheus by Kai Bird and Martin J. Sherwin, the film dramatizes Oppenheimer's studies, his direction of the Los Alamos Laboratory and his 1954 security hearing. Oppenheimer received critical acclaim and grossed $975 million worldwide, becoming the third-highest-grossing film of 2023, the highest-grossing World War II-related film, the highest-grossing biographical film and the second-highest-grossing R-rated film of all time at the time of its release.\", 'score': 0.9092728, 'raw_content': None}, {'title': 'Oppenheimer (movie) - Simple English Wikipedia, the free encyclopedia', 'url': 'https://simple.wikipedia.org/wiki/Oppenheimer_(movie)', 'content': 'Oppenheimer (movie) - Simple English Wikipedia, the free encyclopedia Oppenheimer (movie) Oppenheimer is a 2023 epic biographical thriller movie written and directed by Christopher Nolan. Robert Oppenheimer, a theoretical physicist who helped create the first nuclear weapons as part of the Manhattan Project. With $975 million at the box office, Oppenheimer is the highest-grossing biographical movie of all time, beating Bohemian Rhapsody (2018).[5][6] Josh Hartnett as Ernest Lawrence, a Nobel-winning nuclear physicist who worked with Oppenheimer at the University of California, Berkeley. Dylan Arnold as Frank Oppenheimer, Robert‚Äôs younger brother and a particle physicist who worked on the Manhattan Project. Retrieved from \"https://simple.wikipedia.org/w/index.php?title=Oppenheimer_(movie)&oldid=10077836\" *   2023 movies Oppenheimer (movie)', 'score': 0.7961819, 'raw_content': None}, {'title': 'Oppenheimer | Cast, Film, Length, Plot, Actors, Awards, & Facts ...', 'url': 'https://www.britannica.com/topic/Oppenheimer-film', 'content': 'J. Robert Oppenheimer Robert Downey, Jr. Oppenheimer # Oppenheimer Oppenheimer, American and British dramatic biographical film, released in 2023, that explores the life and legacy of the American physicist J. Robert Oppenheimer, who played a key role in the development of the atomic bomb. Robert Oppenheimer (2005). Film critics‚Äô reaction to Oppenheimer was overwhelmingly positive. Oppenheimer grossed more than $300 million domestically and more than $600 million internationally by the end of November 2023, making it the second highest grossing R-rated film of all time. The film also dominated the Academy Awards nominations, garnering 13 nominations compared with the 8 for Greta Gerwig‚Äôs Barbie, which opened the same weekend as Oppenheimer but topped Nolan‚Äôs film at the box office.', 'score': 0.6854659, 'raw_content': None}, {'title': 'Oppenheimer (2023) - IMDb', 'url': 'https://www.imdb.com/title/tt15398776/', 'content': \"Oppenheimer (2023) - IMDb Oppenheimer IMDb RATING Robert Oppenheimer, the physicist who had a large hand in the development of the atomic bombs that brought an end to World War II.A dramatization of the life story of J. Robert Oppenheimer, the physicist who had a large hand in the development of the atomic bombs that brought an end to World War II.A dramatization of the life story of J. Robert Oppenheimer, the physicist who had a large hand in the development of the atomic bombs that brought an end to World War II. J. Robert Oppenheimer Cillian Murphy and the cast of Oppenheimer discuss what it's like to work with a singular director like Christopher Nolan. J. Robert Oppenheimer: Albert?\", 'score': 0.5951402, 'raw_content': None}, {'title': 'Oppenheimer (film) - Wikiwand', 'url': 'https://www.wikiwand.com/en/articles/Oppenheimer_(2023_film)', 'content': \"Development Kai Bird (pictured) and Martin J. Sherwin are the authors of J. Robert Oppenheimer's biography American Prometheus (2005), on which the film is based.. Director Sam Mendes was interested in adapting the 2005 J. Robert Oppenheimer biography American Prometheus by Kai Bird and Martin J. Sherwin.After that project failed to materialize, the book was optioned by various filmmakers over\", 'score': 0.3386242, 'raw_content': None}], 'response_time': 4.11}\u001b[0m\u001b[32;1m\u001b[1;3mInvoking: `tavily_search` with `{'query': 'birthdate of Christopher Nolan'}`responded: The 2023 film **Oppenheimer** was directed by **Christopher Nolan**.To calculate Christopher Nolan's age in days, I need to find his birthdate. Let me find that information for you.\u001b[0m\u001b[36;1m\u001b[1;3m{'query': 'birthdate of Christopher Nolan', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'Christopher Nolan - Age, Family, Bio | Famous Birthdays', 'url': 'https://www.famousbirthdays.com/people/christopher-nolan.html', 'content': 'Christopher Nolan Director Birthday July 30, 1970 Birth Sign Leo Birthplace London, England Age 54 years old #10,366 Most Popular About British-American director, screenwriter, and producer who first received acclaim for his 2000 indie suspense thriller Memento. He then shifted from art-house films to blockbusters with the box office hits The Dark Knight, Inception, and Interstellar. He won his first Academy Awards for Best Director and Best Picture for his 2023 film Oppenheimer. Trivia In 2003, he approached Warner Bros. with his pitch for a new Batman franchise more grounded in a realistic world than a comic book world. He signed a contract with the studio, and produced three Batman features from 2005 to 2012: Batman Begins, The Dark Knight and The Dark Knight Rises.', 'score': 0.8939131, 'raw_content': None}, {'title': 'Christopher Nolan | Biography, Movies, Batman, Oppenheimer, & Facts ...', 'url': 'https://www.britannica.com/biography/Christopher-Nolan-British-director', 'content': 'Christopher Nolan (born July 30, 1970, London, England) is a British film director and writer acclaimed for his noirish visual aesthetic and unconventional, often highly conceptual narratives. In 2024 Nolan won an Academy Award for best director for Oppenheimer (2023), which was also named best picture. Nolan‚Äôs breakthrough came with the 2000 film Memento, a sleeper hit that he adapted from a short story written by his brother Jonathan Nolan. The film was a critical and popular success and garnered the Nolan brothers an Academy Award nomination for best original screenplay. Nolan‚Äôs highly anticipated Batman Begins (2005), starring Christian Bale, focuses on the superhero‚Äôs origins and features settings and a tone that are grimmer and more realistic than those of previous Batman films. Nolan‚Äôs 2023 film, Oppenheimer, depicts American theoretical physicist  J.', 'score': 0.88822687, 'raw_content': None}, {'title': 'Christopher Nolan: Biography, Movie Director, Filmmaker', 'url': 'https://www.biography.com/movies-tv/christopher-nolan', 'content': 'Opt-Out Icon Christopher Nolan is an Academy Award-winning movie director and screenwriter who‚Äôs helmed several hit films, including Inception, The Dark Knight, Interstellar, and Oppenheimer. We may earn commission from links on this page, but we only recommend products we back. Christopher Nolan is a British-American filmmaker known for his complex storytelling in big-budget movies such as Inception (2010), Interstellar (2014) and Tenet (2020). Play Icon We may earn commission from links on this page, but we only recommend products we back. Biography and associated logos are trademarks of A+E Networks¬Æprotected in the US and other countries around the globe. Opt-Out Icon', 'score': 0.29651213, 'raw_content': None}, {'title': 'Christopher Nolan \"Film Director\" - Biography, Age and Married', 'url': 'https://biographyhost.com/p/christopher-nolan-biography.html', 'content': 'Christopher Nolan is a renowned British-American filmmaker celebrated for his innovative storytelling in films like Oppenheimer and Inception. Christopher Nolan is a British-American filmmaker renowned for his innovative storytelling and visually stunning films. Christopher Nolan is a British-American filmmaker renowned for his innovative storytelling and visually stunning films. Christopher Nolan is a British-American filmmaker renowned for his innovative storytelling and visually stunning films. Christopher Nolan is a British-American filmmaker renowned for his innovative storytelling and visually stunning films. Christopher Nolan is a British-American filmmaker renowned for his innovative storytelling and visually stunning films. Christopher Nolan is a British-American filmmaker renowned for his innovative storytelling and visually stunning films. Christopher Nolan is a British-American filmmaker renowned for his innovative storytelling and visually stunning films.', 'score': 0.21290259, 'raw_content': None}, {'title': 'Christopher Nolan - Wikipedia', 'url': 'https://en.wikipedia.org/wiki/Christopher_Nolan', 'content': 'Following a positive word of mouth and screenings in 500 theatres, it earned $40\\xa0million.[41] Memento premiered at the Venice Film Festival in September 2000 to critical acclaim.[42] Joe Morgenstern of The Wall Street Journal wrote in his review, \"I can\\'t remember when a movie has seemed so clever, strangely affecting and slyly funny at the very same time.\"[43] In the book The Philosophy of Neo-Noir, Basil Smith drew a comparison with John Locke\\'s An Essay Concerning Human Understanding, which argues that conscious memories constitute our identities ‚Äì a theme Nolan explores in the film.[44] Memento earned Nolan many accolades, including nominations for an Academy Award and a Golden Globe Award for Best Screenplay, as well as two Independent Spirit Awards: Best Director and Best Screenplay.[45][46] Six critics listed it as one of the best films of the 2000s.[47] In 2001, Nolan and Emma Thomas founded the production company Syncopy Inc.[48][b]', 'score': 0.15323243, 'raw_content': None}], 'response_time': 2.47}\u001b[0m\u001b[32;1m\u001b[1;3mInvoking: `tavily_search` with `{'query': 'current date'}`responded: Christopher Nolan, the director of the 2023 film **Oppenheimer**, was born on **July 30, 1970**.To calculate his age in days as of today, we can use the following formula:\\[ \\text{Age in days} = (\\text{Current Date} - \\text{Birthdate}) \\]Let's calculate this now.\u001b[0m\u001b[36;1m\u001b[1;3m{'query': 'current date', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': \"Today's Date - CalendarDate.com\", 'url': 'https://www.calendardate.com/todays.htm', 'content': \"Details about today's date with count of days, weeks, and months, Sun and Moon cycles, Zodiac signs and holidays. Sunday June 15, 2025 . Home; Calendars. 2025 Calendar; ... Current Season Today: Spring with 6 days until the start of Summer. S. Hemishpere flip seasons - i.e. Winter is Summer.\", 'score': 0.63152665, 'raw_content': None}, {'title': \"What is the date today | Today's Date\", 'url': 'https://www.datetoday.info/', 'content': 'Find out the current date and time in different time zones and formats, such as UTC, America/Los_Angeles, ISO 8601, RFC 2822, Unix Epoch, etc. Learn more about the day of the week, the day of the year, the week number, the month number, and the remaining days of the year.', 'score': 0.60049355, 'raw_content': None}, {'title': 'Current Time', 'url': 'https://www.timeanddate.com/', 'content': 'Current Time. Monday Jun 9, 2025 Washington DC, District of Columbia, USA. Set home location. 5:39: 55 am. World Clock. World Clock. Current local time around the world. Personal World Clock. Set the current time of your favorite locations across time zones. World Clock: current time around the globe', 'score': 0.45914948, 'raw_content': None}, {'title': 'What time is it - Exact time - Any time zone - vClock', 'url': 'https://vclock.com/time/', 'content': 'Online clock. What time is it in different regions of United States, Canada, Australia, Europe and the World. What time is it - Exact time - Any time zone - vClock ... On this website, you can find out the current time and date in any country and city in the world. You can also view the time difference between your location and that of another', 'score': 0.15111576, 'raw_content': None}, {'title': 'Time.is - exact time, any time zone', 'url': 'https://time.is/', 'content': '7 million locations, 58 languages, synchronized with atomic clock time.', 'score': 0.08800977, 'raw_content': None}], 'response_time': 2.62}\u001b[0m\u001b[32;1m\u001b[1;3mInvoking: `tavily_search` with `{'query': 'days between July 30, 1970 and current date'}`responded: Christopher Nolan, the director of the 2023 film **Oppenheimer**, was born on **July 30, 1970**.To calculate his age in days as of today, we can use the following formula:\\[ \\text{Age in days} = (\\text{Current Date} - \\text{Birthdate}) \\]Let's calculate this now.\u001b[0m\u001b[36;1m\u001b[1;3m{'query': 'days between July 30, 1970 and current date', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'Date Calculator', 'url': 'https://www.calculator.net/date-calculator.html', 'content': 'Days Between Two Dates. Find the number of years, months, weeks, and days between dates. Click \"Settings\" to define holidays. ... The months of April, June, September, and November have 30 days, while the rest have 31 days except for February, which has 28 days in a standard year, and 29 in a leap year. ... to 1 day in 3,030 years with respect', 'score': 0.15738304, 'raw_content': None}, {'title': 'Days Calculator (Days Between Dates)', 'url': 'https://www.gigacalculator.com/calculators/days-between-dates-calculator.php', 'content': 'Days calculator to count how many days between any two dates. Find out how many days there are between any two dates, e.g. days between today and date X in the future, or date Y in the past and today. Calculate how many days you have to a deadline with this free days between dates calculator. Days calculator online for time between dates, including days since or days from a given date.', 'score': 0.15232232, 'raw_content': None}, {'title': 'Days Calculator', 'url': 'https://time-calculator.net/days.html', 'content': 'The days calculator can find the days or duration between two dates and also gives the time interval in years, months, and days. Start Date: Today. End Date: Today. Include last day (+1 day) = Calculate. √ó Reset. Result:', 'score': 0.1465877, 'raw_content': None}, {'title': 'Date Calculator - Add Days to Date & Days Between Dates', 'url': 'https://timedatecalc.com/date-calculator', 'content': \"How to Add Days to Date. Enter the start date To get started, enter the start date to which you need to add/subtract days (today's date is initially displayed). Use the calendar for more convenient date selection. Enter the number of days Next, enter the time value you need to add or subtract from the start date (years, months, weeks, days).\", 'score': 0.14245868, 'raw_content': None}, {'title': 'Date Duration Calculator: Days Between Dates - timeanddate.com', 'url': 'https://www.timeanddate.com/date/duration.html', 'content': 'Help and Example Use. Some typical uses for the Date Calculators; API Services for Developers. API for Business Date Calculators; Date Calculators. Time and Date Duration - Calculate duration, with both date and time included; Date Calculator - Add or subtract days, months, years; Weekday Calculator - What day is this date?; Birthday Calculator - Find when you are 1 billion seconds old', 'score': 0.12024263, 'raw_content': None}], 'response_time': 2.27}\u001b[0m\u001b[32;1m\u001b[1;3mChristopher Nolan was born on July 30, 1970. As of today, June 15, 2025, he is 19,944 days old.\u001b[0m\u001b[1m> Finished chain.\u001b[0m\n```\n\n```\n{'input': 'Who directed the 2023 film Oppenheimer and what is their age in days?', 'output': 'Christopher Nolan was born on July 30, 1970. As of today, June 15, 2025, he is 19,944 days old.'}\n```", "url_final": "https://python.langchain.com/docs/how_to/debugging/", "url_citable": "https://python.langchain.com/docs/how_to/debugging/#set_verbosetrue", "title": "How to debug your LLM apps | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "set_verbose(True)‚Äã", "section_anchor": "set_verbosetrue", "fetched_at": "2025-10-06T21:55:18.414131", "has_code": true}
{"id": "1b4553a4265359ff64af36b4b6d5b3026b248a63", "text": "set_debug(True)‚Äã\n\nSetting the global debug flag will cause all LangChain components with callback support (chains, models, agents, tools, retrievers) to print the inputs they receive and outputs they generate. This is the most verbose setting and will fully log raw inputs and outputs.\n\n```\nfrom langchain.globals import set_debugset_debug(True)set_verbose(False)agent_executor = AgentExecutor(agent=agent, tools=tools)agent_executor.invoke(    {\"input\": \"Who directed the 2023 film Oppenheimer and what is their age in days?\"})\n```\n\n```\n\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor] Entering Chain run with input:\u001b[0m{  \"input\": \"Who directed the 2023 film Oppenheimer and what is their age in days?\"}\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence] Entering Chain run with input:\u001b[0m{  \"input\": \"\"}\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad>] Entering Chain run with input:\u001b[0m{  \"input\": \"\"}\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad> > chain:RunnableParallel<agent_scratchpad>] Entering Chain run with input:\u001b[0m{  \"input\": \"\"}\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad> > chain:RunnableParallel<agent_scratchpad> > chain:RunnableLambda] Entering Chain run with input:\u001b[0m{  \"input\": \"\"}\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad> > chain:RunnableParallel<agent_scratchpad> > chain:RunnableLambda] [0ms] Exiting Chain run with output:\u001b[0m{  \"output\": []}\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad> > chain:RunnableParallel<agent_scratchpad>] [1ms] Exiting Chain run with output:\u001b[0m{  \"agent_scratchpad\": []}\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad>] [2ms] Exiting Chain run with output:\u001b[0m{  \"input\": \"Who directed the 2023 film Oppenheimer and what is their age in days?\",  \"intermediate_steps\": [],  \"agent_scratchpad\": []}\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\u001b[0m{  \"input\": \"Who directed the 2023 film Oppenheimer and what is their age in days?\",  \"intermediate_steps\": [],  \"agent_scratchpad\": []}\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\u001b[0m[outputs]\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\u001b[0m{  \"prompts\": [    \"System: You are a helpful assistant.\\nHuman: Who directed the 2023 film Oppenheimer and what is their age in days?\"  ]}\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > llm:ChatOpenAI] [2.87s] Exiting LLM run with output:\u001b[0m{  \"generations\": [    [      {        \"text\": \"\",        \"generation_info\": {          \"finish_reason\": \"tool_calls\",          \"model_name\": \"gpt-4-turbo-2024-04-09\",          \"system_fingerprint\": \"fp_de235176ee\",          \"service_tier\": \"default\"        },        \"type\": \"ChatGenerationChunk\",        \"message\": {          \"lc\": 1,          \"type\": \"constructor\",          \"id\": [            \"langchain\",            \"schema\",            \"messages\",            \"AIMessageChunk\"          ],          \"kwargs\": {            \"content\": \"\",            \"additional_kwargs\": {              \"tool_calls\": [                {                  \"index\": 0,                  \"id\": \"call_7470602CBXe0TCtzU9kNddmI\",                  \"function\": {                    \"arguments\": \"{\\\"query\\\": \\\"director of the 2023 film Oppenheimer\\\"}\",                    \"name\": \"tavily_search\"                  },                  \"type\": \"function\"                },                {                  \"index\": 1,                  \"id\": \"call_NcqiDSEUVpwfSKBTSUDRwJTQ\",                  \"function\": {                    \"arguments\": \"{\\\"query\\\": \\\"birth date of Christopher Nolan\\\"}\",                    \"name\": \"tavily_search\"                  },                  \"type\": \"function\"                }              ]            },            \"response_metadata\": {              \"finish_reason\": \"tool_calls\",              \"model_name\": \"gpt-4-turbo-2024-04-09\",              \"system_fingerprint\": \"fp_de235176ee\",              \"service_tier\": \"default\"            },            \"type\": \"AIMessageChunk\",            \"id\": \"run--421b146e-04d7-4e72-8c1d-68c9b92995fe\",            \"tool_calls\": [              {                \"name\": \"tavily_search\",                \"args\": {                  \"query\": \"director of the 2023 film Oppenheimer\"                },                \"id\": \"call_7470602CBXe0TCtzU9kNddmI\",                \"type\": \"tool_call\"              },              {                \"name\": \"tavily_search\",                \"args\": {                  \"query\": \"birth date of Christopher Nolan\"                },                \"id\": \"call_NcqiDSEUVpwfSKBTSUDRwJTQ\",                \"type\": \"tool_call\"              }            ],            \"tool_call_chunks\": [              {                \"name\": \"tavily_search\",                \"args\": \"{\\\"query\\\": \\\"director of the 2023 film Oppenheimer\\\"}\",                \"id\": \"call_7470602CBXe0TCtzU9kNddmI\",                \"index\": 0,                \"type\": \"tool_call_chunk\"              },              {                \"name\": \"tavily_search\",                \"args\": \"{\\\"query\\\": \\\"birth date of Christopher Nolan\\\"}\",                \"id\": \"call_NcqiDSEUVpwfSKBTSUDRwJTQ\",                \"index\": 1,                \"type\": \"tool_call_chunk\"              }            ],            \"invalid_tool_calls\": []          }        }      }    ]  ],  \"llm_output\": null,  \"run\": null,  \"type\": \"LLMResult\"}\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > parser:ToolsAgentOutputParser] Entering Parser run with input:\u001b[0m[inputs]\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > parser:ToolsAgentOutputParser] [0ms] Exiting Parser run with output:\u001b[0m[outputs]\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence] [2.88s] Exiting Chain run with output:\u001b[0m[outputs]\u001b[32;1m\u001b[1;3m[tool/start]\u001b[0m \u001b[1m[chain:AgentExecutor > tool:tavily_search] Entering Tool run with input:\u001b[0m\"{'query': 'director of the 2023 film Oppenheimer'}\"\u001b[36;1m\u001b[1;3m[tool/end]\u001b[0m \u001b[1m[chain:AgentExecutor > tool:tavily_search] [2.11s] Exiting Tool run with output:\u001b[0m\"{'query': 'director of the 2023 film Oppenheimer', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'Oppenheimer (film) - Wikipedia', 'url': 'https://en.wikipedia.org/wiki/Oppenheimer_(film)', 'content': \"Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Toggle the table of contents Contents move to sidebar hide (Top) 1 Plot 2 Cast 3 ProductionToggle Production subsection 3.1 Development 3.2 Writing 3.3 Casting 3.4 Filming 3.5 Post-production 4 Music 5 Marketing 6 ReleaseToggle Release subsection 6.1 Theatrical 6.1.1 Classifications and censorship 6.1.2 Bhagavad Gita controversy 6.2 Home media 7 ReceptionToggle Reception subsection 7.1 Box office 7.1.1 United States and Canada 7.1.2 Japan 7.1.3 Other territories 7.2 Critical response 7.3 Influence on legislation 8 Accuracy and omissions 9 Accolades 10 See also 11 Notes 12 References 13 Further reading 14 External links Oppenheimer (film) 70 languages ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ‡¶Ö‡¶∏‡¶Æ‡ßÄ‡¶Ø‡¶º‡¶æ Az…ôrbaycanca ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ –ë–µ–ª–∞—Ä—É—Å–∫–∞—è ‡§≠‡•ã‡§ú‡§™‡•Å‡§∞‡•Ä –ë—ä–ª–≥–∞—Ä—Å–∫–∏ Bosanski Catal√† ƒåe≈°tina Cymraeg Dansk Deutsch ‡§°‡•ã‡§ü‡•á‡§≤‡•Ä Eesti ŒïŒªŒªŒ∑ŒΩŒπŒ∫Œ¨ Espa√±ol Euskara ŸÅÿßÿ±ÿ≥€å Fran√ßais Gaeilge Galego ÌïúÍµ≠Ïñ¥ ’Ä’°’µ’•÷Ä’•’∂ ‡§π‡§ø‡§®‡•ç‡§¶‡•Ä Ido Bahasa Indonesia Italiano ◊¢◊ë◊®◊ô◊™ Jawa ·É•·Éê·É†·Éó·É£·Éö·Éò “ö–∞–∑–∞“õ—à–∞ Latina Latvie≈°u Lietuvi≈≥ Magyar –ú–∞–∫–µ–¥–æ–Ω—Å–∫–∏ ‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç ‡§Æ‡§∞‡§æ‡§†‡•Ä ŸÖÿµÿ±Ÿâ ŸÖÿßÿ≤Ÿêÿ±ŸàŸÜ€å Bahasa Melayu Nederlands ‡§®‡•á‡§™‡§æ‡§≤‡•Ä Êó•Êú¨Ë™û Norsk bokm√•l O ªzbekcha / —û–∑–±–µ–∫—á–∞ ‡®™‡©∞‡®ú‡®æ‡®¨‡©Ä Polski Portugu√™s Rom√¢nƒÉ –†—É—Å—Å–∫–∏–π Shqip Simple English Slovenƒçina Sloven≈°ƒçina ⁄©Ÿàÿ±ÿØ€å –°—Ä–ø—Å–∫–∏ / srpski Suomi Svenska ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç ‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å ‡πÑ‡∏ó‡∏¢ –¢–æ“∑–∏–∫”£ T√ºrk√ße –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ ÿßÿ±ÿØŸà Ti·∫øng Vi·ªát Á≤µË™û ‰∏≠Êñá Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Expand all Edit interlanguage links Print/export Download as PDF Printable version In other projects Wikimedia Commons Wikiquote Wikidata item From Wikipedia, the free encyclopedia 2023 film by Christopher Nolan | Oppenheimer | | --- | | Theatrical release poster | | Directed by | Christopher Nolan | | Screenplay by | Christopher Nolan | | Based on | American Prometheus by Kai Bird Martin J. Sherwin | | Produced by | Emma Thomas Charles Roven Christopher Nolan | | Starring | Cillian Murphy Emily Blunt Matt Damon Robert Downey Jr. Florence Pugh Josh Hartnett Casey Affleck Rami Malek Kenneth Branagh | | Cinematography | Hoyte van Hoytema | | Edited by | Jennifer Lame | | Music by | Ludwig G√∂ransson | | Production companies | Universal Pictures[1][2] Syncopy[1][2] Atlas Entertainment[1][2] Breakheart Films[2] Peters Creek Entertainment[2] Gadget Films[1][3] | | Distributed by | Universal Pictures | | Release dates | July 11, 2023 (2023-07-11) (Le Grand Rex) July 21, 2023 (2023-07-21) (United States and United Kingdom) | | Running time | 180 minutes[4] | | Countries | United States United Kingdom | | Language | English | | Budget | $100 million[5] | | Box office | $975.8 million[6][7] | Oppenheimer is a 2023 epic biographical drama film written, produced, and directed by Christopher Nolan. [8] It follows the life of J. Robert Oppenheimer, the American theoretical physicist who helped develop the first nuclear weapons during World War II. Based on the 2005 biography American Prometheus by Kai Bird and Martin J. Sherwin, the film dramatizes Oppenheimer's studies, his direction of the Los Alamos Laboratory and his 1954 security hearing. Oppenheimer received critical acclaim and grossed $975 million worldwide, becoming the third-highest-grossing film of 2023, the highest-grossing World War II-related film, the highest-grossing biographical film and the second-highest-grossing R-rated film of all time at the time of its release.\", 'score': 0.9475027, 'raw_content': None}, {'title': 'Oppenheimer | Cast, Film, Length, Plot, Actors, Awards, & Facts ...', 'url': 'https://www.britannica.com/topic/Oppenheimer-film', 'content': 'J. Robert Oppenheimer Robert Downey, Jr. Oppenheimer # Oppenheimer Oppenheimer, American and British dramatic biographical film, released in 2023, that explores the life and legacy of the American physicist J. Robert Oppenheimer, who played a key role in the development of the atomic bomb. Robert Oppenheimer (2005). Film critics‚Äô reaction to Oppenheimer was overwhelmingly positive. Oppenheimer grossed more than $300 million domestically and more than $600 million internationally by the end of November 2023, making it the second highest grossing R-rated film of all time. The film also dominated the Academy Awards nominations, garnering 13 nominations compared with the 8 for Greta Gerwig‚Äôs Barbie, which opened the same weekend as Oppenheimer but topped Nolan‚Äôs film at the box office.', 'score': 0.76194656, 'raw_content': None}, {'title': 'Oppenheimer (2023) - Full cast & crew - IMDb', 'url': 'https://www.imdb.com/title/tt15398776/fullcredits/', 'content': 'Oppenheimer (2023) - Cast and crew credits, including actors, actresses, directors, writers and more. Menu. ... Oscars Pride Month American Black Film Festival Summer Watch Guide STARmeter Awards Awards Central Festival Central All Events. ... second unit director: visual effects (uncredited) Francesca Kaimer Millea.', 'score': 0.683948, 'raw_content': None}, {'title': \"'Oppenheimer' director Christopher Nolan says the film is his darkest - NPR\", 'url': 'https://www.npr.org/2023/08/14/1193448291/oppenheimer-director-christopher-nolan', 'content': '# \\'Like it or not, we live in Oppenheimer\\'s world,\\' says director Christopher Nolan #### \\'Like it or not, we live in Oppenheimer\\'s world,\\' says director Christopher Nolan But he says the story of Robert Oppenheimer, known as the father of the atomic bomb, stayed with him in a way his other films didn\\'t. Nolan says he was drawn to the tension of Oppenheimer\\'s story ‚Äî particularly the disconnect between the joy the physicist felt at the success of the Trinity test, and the horror that later resulted. Writer, director and producer Christopher Nolan says Oppenheimer is the \"darkest\" of all the films he\\'s worked on. Writer, director and producer Christopher Nolan says Oppenheimer is the \"darkest\" of all the films he\\'s worked on.', 'score': 0.6255073, 'raw_content': None}, {'title': 'An extended interview with Christopher Nolan, director of Oppenheimer', 'url': 'https://thebulletin.org/premium/2023-07/an-extended-interview-with-christopher-nolan-director-of-oppenheimer/', 'content': 'A group of Manhattan Project scientists and engineers also focused on wider public education on nuclear weapons and energy (and science generally) through the creation of the Bulletin of the Atomic Scientists; Oppenheimer served as the first chair of the magazine‚Äôs Board of Sponsors.[5] As time has passed, more evidence has come to light of the bias and unfairness of the process that Dr. Oppenheimer was subjected to while the evidence of his loyalty and love of country have only been further affirmed.‚Äù[8] Decades after the fact, records of the Oppenheimer security hearing made it clear that, rather than any disloyalty to the nation, it was his principled opposition to development of the hydrogen bomb‚Äîa nuclear fusion-based weapon of immensely greater power than the fission weapons used to decimate Hiroshima and Nagasaki in 1945‚Äîthat was key to the decision to essentially bar him from government service. Robert Oppenheimer, Los Alamos, Manhattan Project, Nolan, atomic bomb, movie', 'score': 0.32472825, 'raw_content': None}], 'response_time': 1.39}\"\u001b[32;1m\u001b[1;3m[tool/start]\u001b[0m \u001b[1m[chain:AgentExecutor > tool:tavily_search] Entering Tool run with input:\u001b[0m\"{'query': 'birth date of Christopher Nolan'}\"\u001b[36;1m\u001b[1;3m[tool/end]\u001b[0m \u001b[1m[chain:AgentExecutor > tool:tavily_search] [1.11s] Exiting Tool run with output:\u001b[0m\"{'query': 'birth date of Christopher Nolan', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'Christopher Nolan | Biography, Movies, Batman, Oppenheimer, & Facts ...', 'url': 'https://www.britannica.com/biography/Christopher-Nolan-British-director', 'content': 'Christopher Nolan (born July 30, 1970, London, England) is a British film director and writer acclaimed for his noirish visual aesthetic and unconventional, often highly conceptual narratives. In 2024 Nolan won an Academy Award for best director for Oppenheimer (2023), which was also named best picture. Nolan‚Äôs breakthrough came with the 2000 film Memento, a sleeper hit that he adapted from a short story written by his brother Jonathan Nolan. The film was a critical and popular success and garnered the Nolan brothers an Academy Award nomination for best original screenplay. Nolan‚Äôs highly anticipated Batman Begins (2005), starring Christian Bale, focuses on the superhero‚Äôs origins and features settings and a tone that are grimmer and more realistic than those of previous Batman films. Nolan‚Äôs 2023 film, Oppenheimer, depicts American theoretical physicist  J.', 'score': 0.8974172, 'raw_content': None}, {'title': 'Christopher Nolan - IMDb', 'url': 'https://m.imdb.com/name/nm0634240/', 'content': 'Christopher Nolan. Writer: Tenet. Best known for his cerebral, often nonlinear, storytelling, acclaimed Academy Award winner writer/director/producer Sir Christopher Nolan CBE was born in London, England. Over the course of more than 25 years of filmmaking, Nolan has gone from low-budget independent films to working on some of the biggest blockbusters ever made and became one of the most', 'score': 0.5087155, 'raw_content': None}, {'title': 'Christopher Nolan: Biography, Movie Director, Filmmaker', 'url': 'https://www.biography.com/movies-tv/christopher-nolan', 'content': 'Opt-Out Icon Christopher Nolan is an Academy Award-winning movie director and screenwriter who‚Äôs helmed several hit films, including Inception, The Dark Knight, Interstellar, and Oppenheimer. We may earn commission from links on this page, but we only recommend products we back. Christopher Nolan is a British-American filmmaker known for his complex storytelling in big-budget movies such as Inception (2010), Interstellar (2014) and Tenet (2020). Play Icon We may earn commission from links on this page, but we only recommend products we back. Biography and associated logos are trademarks of A+E Networks¬Æprotected in the US and other countries around the globe. Opt-Out Icon', 'score': 0.28185803, 'raw_content': None}, {'title': 'Christopher Nolan \"Film Director\" - Biography, Age and Married', 'url': 'https://biographyhost.com/p/christopher-nolan-biography.html', 'content': 'Christopher Nolan is a renowned British-American filmmaker celebrated for his innovative storytelling in films like Oppenheimer and Inception. Christopher Nolan is a British-American filmmaker renowned for his innovative storytelling and visually stunning films. Christopher Nolan is a British-American filmmaker renowned for his innovative storytelling and visually stunning films. Christopher Nolan is a British-American filmmaker renowned for his innovative storytelling and visually stunning films. Christopher Nolan is a British-American filmmaker renowned for his innovative storytelling and visually stunning films. Christopher Nolan is a British-American filmmaker renowned for his innovative storytelling and visually stunning films. Christopher Nolan is a British-American filmmaker renowned for his innovative storytelling and visually stunning films. Christopher Nolan is a British-American filmmaker renowned for his innovative storytelling and visually stunning films.', 'score': 0.19905913, 'raw_content': None}, {'title': 'Christopher Nolan - Wikipedia', 'url': 'https://en.wikipedia.org/wiki/Christopher_Nolan', 'content': 'Following a positive word of mouth and screenings in 500 theatres, it earned $40\\xa0million.[41] Memento premiered at the Venice Film Festival in September 2000 to critical acclaim.[42] Joe Morgenstern of The Wall Street Journal wrote in his review, \"I can\\'t remember when a movie has seemed so clever, strangely affecting and slyly funny at the very same time.\"[43] In the book The Philosophy of Neo-Noir, Basil Smith drew a comparison with John Locke\\'s An Essay Concerning Human Understanding, which argues that conscious memories constitute our identities ‚Äì a theme Nolan explores in the film.[44] Memento earned Nolan many accolades, including nominations for an Academy Award and a Golden Globe Award for Best Screenplay, as well as two Independent Spirit Awards: Best Director and Best Screenplay.[45][46] Six critics listed it as one of the best films of the 2000s.[47] In 2001, Nolan and Emma Thomas founded the production company Syncopy Inc.[48][b]', 'score': 0.1508904, 'raw_content': None}], 'response_time': 0.74}\"\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence] Entering Chain run with input:\u001b[0m{  \"input\": \"\"}\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad>] Entering Chain run with input:\u001b[0m{  \"input\": \"\"}\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad> > chain:RunnableParallel<agent_scratchpad>] Entering Chain run with input:\u001b[0m{  \"input\": \"\"}\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad> > chain:RunnableParallel<agent_scratchpad> > chain:RunnableLambda] Entering Chain run with input:\u001b[0m{  \"input\": \"\"}\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad> > chain:RunnableParallel<agent_scratchpad> > chain:RunnableLambda] [0ms] Exiting Chain run with output:\u001b[0m[outputs]\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad> > chain:RunnableParallel<agent_scratchpad>] [1ms] Exiting Chain run with output:\u001b[0m[outputs]\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad>] [2ms] Exiting Chain run with output:\u001b[0m[outputs]\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\u001b[0m[inputs]\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\u001b[0m[outputs]\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\u001b[0m{  \"prompts\": [    \"System: You are a helpful assistant.\\nHuman: Who directed the 2023 film Oppenheimer and what is their age in days?\\nAI: \\nTool: {\\\"query\\\": \\\"director of the 2023 film Oppenheimer\\\", \\\"follow_up_questions\\\": null, \\\"answer\\\": null, \\\"images\\\": [], \\\"results\\\": [{\\\"title\\\": \\\"Oppenheimer (film) - Wikipedia\\\", \\\"url\\\": \\\"https://en.wikipedia.org/wiki/Oppenheimer_(film)\\\", \\\"content\\\": \\\"Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Toggle the table of contents Contents move to sidebar hide (Top) 1 Plot 2 Cast 3 ProductionToggle Production subsection 3.1 Development 3.2 Writing 3.3 Casting 3.4 Filming 3.5 Post-production 4 Music 5 Marketing 6 ReleaseToggle Release subsection 6.1 Theatrical 6.1.1 Classifications and censorship 6.1.2 Bhagavad Gita controversy 6.2 Home media 7 ReceptionToggle Reception subsection 7.1 Box office 7.1.1 United States and Canada 7.1.2 Japan 7.1.3 Other territories 7.2 Critical response 7.3 Influence on legislation 8 Accuracy and omissions 9 Accolades 10 See also 11 Notes 12 References 13 Further reading 14 External links Oppenheimer (film) 70 languages ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ‡¶Ö‡¶∏‡¶Æ‡ßÄ‡¶Ø‡¶º‡¶æ Az…ôrbaycanca ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ –ë–µ–ª–∞—Ä—É—Å–∫–∞—è ‡§≠‡•ã‡§ú‡§™‡•Å‡§∞‡•Ä –ë—ä–ª–≥–∞—Ä—Å–∫–∏ Bosanski Catal√† ƒåe≈°tina Cymraeg Dansk Deutsch ‡§°‡•ã‡§ü‡•á‡§≤‡•Ä Eesti ŒïŒªŒªŒ∑ŒΩŒπŒ∫Œ¨ Espa√±ol Euskara ŸÅÿßÿ±ÿ≥€å Fran√ßais Gaeilge Galego ÌïúÍµ≠Ïñ¥ ’Ä’°’µ’•÷Ä’•’∂ ‡§π‡§ø‡§®‡•ç‡§¶‡•Ä Ido Bahasa Indonesia Italiano ◊¢◊ë◊®◊ô◊™ Jawa ·É•·Éê·É†·Éó·É£·Éö·Éò “ö–∞–∑–∞“õ—à–∞ Latina Latvie≈°u Lietuvi≈≥ Magyar –ú–∞–∫–µ–¥–æ–Ω—Å–∫–∏ ‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç ‡§Æ‡§∞‡§æ‡§†‡•Ä ŸÖÿµÿ±Ÿâ ŸÖÿßÿ≤Ÿêÿ±ŸàŸÜ€å Bahasa Melayu Nederlands ‡§®‡•á‡§™‡§æ‡§≤‡•Ä Êó•Êú¨Ë™û Norsk bokm√•l O ªzbekcha / —û–∑–±–µ–∫—á–∞ ‡®™‡©∞‡®ú‡®æ‡®¨‡©Ä Polski Portugu√™s Rom√¢nƒÉ –†—É—Å—Å–∫–∏–π Shqip Simple English Slovenƒçina Sloven≈°ƒçina ⁄©Ÿàÿ±ÿØ€å –°—Ä–ø—Å–∫–∏ / srpski Suomi Svenska ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç ‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å ‡πÑ‡∏ó‡∏¢ –¢–æ“∑–∏–∫”£ T√ºrk√ße –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ ÿßÿ±ÿØŸà Ti·∫øng Vi·ªát Á≤µË™û ‰∏≠Êñá Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Expand all Edit interlanguage links Print/export Download as PDF Printable version In other projects Wikimedia Commons Wikiquote Wikidata item From Wikipedia, the free encyclopedia 2023 film by Christopher Nolan | Oppenheimer | | --- | | Theatrical release poster | | Directed by | Christopher Nolan | | Screenplay by | Christopher Nolan | | Based on | American Prometheus by Kai Bird Martin J. Sherwin | | Produced by | Emma Thomas Charles Roven Christopher Nolan | | Starring | Cillian Murphy Emily Blunt Matt Damon Robert Downey Jr. Florence Pugh Josh Hartnett Casey Affleck Rami Malek Kenneth Branagh | | Cinematography | Hoyte van Hoytema | | Edited by | Jennifer Lame | | Music by | Ludwig G√∂ransson | | Production companies | Universal Pictures[1][2] Syncopy[1][2] Atlas Entertainment[1][2] Breakheart Films[2] Peters Creek Entertainment[2] Gadget Films[1][3] | | Distributed by | Universal Pictures | | Release dates | July 11, 2023 (2023-07-11) (Le Grand Rex) July 21, 2023 (2023-07-21) (United States and United Kingdom) | | Running time | 180 minutes[4] | | Countries | United States United Kingdom | | Language | English | | Budget | $100 million[5] | | Box office | $975.8 million[6][7] | Oppenheimer is a 2023 epic biographical drama film written, produced, and directed by Christopher Nolan. [8] It follows the life of J. Robert Oppenheimer, the American theoretical physicist who helped develop the first nuclear weapons during World War II. Based on the 2005 biography American Prometheus by Kai Bird and Martin J. Sherwin, the film dramatizes Oppenheimer's studies, his direction of the Los Alamos Laboratory and his 1954 security hearing. Oppenheimer received critical acclaim and grossed $975 million worldwide, becoming the third-highest-grossing film of 2023, the highest-grossing World War II-related film, the highest-grossing biographical film and the second-highest-grossing R-rated film of all time at the time of its release.\\\", \\\"score\\\": 0.9475027, \\\"raw_content\\\": null}, {\\\"title\\\": \\\"Oppenheimer | Cast, Film, Length, Plot, Actors, Awards, & Facts ...\\\", \\\"url\\\": \\\"https://www.britannica.com/topic/Oppenheimer-film\\\", \\\"content\\\": \\\"J. Robert Oppenheimer Robert Downey, Jr. Oppenheimer # Oppenheimer Oppenheimer, American and British dramatic biographical film, released in 2023, that explores the life and legacy of the American physicist J. Robert Oppenheimer, who played a key role in the development of the atomic bomb. Robert Oppenheimer (2005). Film critics‚Äô reaction to Oppenheimer was overwhelmingly positive. Oppenheimer grossed more than $300 million domestically and more than $600 million internationally by the end of November 2023, making it the second highest grossing R-rated film of all time. The film also dominated the Academy Awards nominations, garnering 13 nominations compared with the 8 for Greta Gerwig‚Äôs Barbie, which opened the same weekend as Oppenheimer but topped Nolan‚Äôs film at the box office.\\\", \\\"score\\\": 0.76194656, \\\"raw_content\\\": null}, {\\\"title\\\": \\\"Oppenheimer (2023) - Full cast & crew - IMDb\\\", \\\"url\\\": \\\"https://www.imdb.com/title/tt15398776/fullcredits/\\\", \\\"content\\\": \\\"Oppenheimer (2023) - Cast and crew credits, including actors, actresses, directors, writers and more. Menu. ... Oscars Pride Month American Black Film Festival Summer Watch Guide STARmeter Awards Awards Central Festival Central All Events. ... second unit director: visual effects (uncredited) Francesca Kaimer Millea.\\\", \\\"score\\\": 0.683948, \\\"raw_content\\\": null}, {\\\"title\\\": \\\"'Oppenheimer' director Christopher Nolan says the film is his darkest - NPR\\\", \\\"url\\\": \\\"https://www.npr.org/2023/08/14/1193448291/oppenheimer-director-christopher-nolan\\\", \\\"content\\\": \\\"# 'Like it or not, we live in Oppenheimer's world,' says director Christopher Nolan #### 'Like it or not, we live in Oppenheimer's world,' says director Christopher Nolan But he says the story of Robert Oppenheimer, known as the father of the atomic bomb, stayed with him in a way his other films didn't. Nolan says he was drawn to the tension of Oppenheimer's story ‚Äî particularly the disconnect between the joy the physicist felt at the success of the Trinity test, and the horror that later resulted. Writer, director and producer Christopher Nolan says Oppenheimer is the \\\\\\\"darkest\\\\\\\" of all the films he's worked on. Writer, director and producer Christopher Nolan says Oppenheimer is the \\\\\\\"darkest\\\\\\\" of all the films he's worked on.\\\", \\\"score\\\": 0.6255073, \\\"raw_content\\\": null}, {\\\"title\\\": \\\"An extended interview with Christopher Nolan, director of Oppenheimer\\\", \\\"url\\\": \\\"https://thebulletin.org/premium/2023-07/an-extended-interview-with-christopher-nolan-director-of-oppenheimer/\\\", \\\"content\\\": \\\"A group of Manhattan Project scientists and engineers also focused on wider public education on nuclear weapons and energy (and science generally) through the creation of the Bulletin of the Atomic Scientists; Oppenheimer served as the first chair of the magazine‚Äôs Board of Sponsors.[5] As time has passed, more evidence has come to light of the bias and unfairness of the process that Dr. Oppenheimer was subjected to while the evidence of his loyalty and love of country have only been further affirmed.‚Äù[8] Decades after the fact, records of the Oppenheimer security hearing made it clear that, rather than any disloyalty to the nation, it was his principled opposition to development of the hydrogen bomb‚Äîa nuclear fusion-based weapon of immensely greater power than the fission weapons used to decimate Hiroshima and Nagasaki in 1945‚Äîthat was key to the decision to essentially bar him from government service. Robert Oppenheimer, Los Alamos, Manhattan Project, Nolan, atomic bomb, movie\\\", \\\"score\\\": 0.32472825, \\\"raw_content\\\": null}], \\\"response_time\\\": 1.39}\\nTool: {\\\"query\\\": \\\"birth date of Christopher Nolan\\\", \\\"follow_up_questions\\\": null, \\\"answer\\\": null, \\\"images\\\": [], \\\"results\\\": [{\\\"title\\\": \\\"Christopher Nolan | Biography, Movies, Batman, Oppenheimer, & Facts ...\\\", \\\"url\\\": \\\"https://www.britannica.com/biography/Christopher-Nolan-British-director\\\", \\\"content\\\": \\\"Christopher Nolan (born July 30, 1970, London, England) is a British film director and writer acclaimed for his noirish visual aesthetic and unconventional, often highly conceptual narratives. In 2024 Nolan won an Academy Award for best director for Oppenheimer (2023), which was also named best picture. Nolan‚Äôs breakthrough came with the 2000 film Memento, a sleeper hit that he adapted from a short story written by his brother Jonathan Nolan. The film was a critical and popular success and garnered the Nolan brothers an Academy Award nomination for best original screenplay. Nolan‚Äôs highly anticipated Batman Begins (2005), starring Christian Bale, focuses on the superhero‚Äôs origins and features settings and a tone that are grimmer and more realistic than those of previous Batman films. Nolan‚Äôs 2023 film, Oppenheimer, depicts American theoretical physicist  J.\\\", \\\"score\\\": 0.8974172, \\\"raw_content\\\": null}, {\\\"title\\\": \\\"Christopher Nolan - IMDb\\\", \\\"url\\\": \\\"https://m.imdb.com/name/nm0634240/\\\", \\\"content\\\": \\\"Christopher Nolan. Writer: Tenet. Best known for his cerebral, often nonlinear, storytelling, acclaimed Academy Award winner writer/director/producer Sir Christopher Nolan CBE was born in London, England. Over the course of more than 25 years of filmmaking, Nolan has gone from low-budget independent films to working on some of the biggest blockbusters ever made and became one of the most\\\", \\\"score\\\": 0.5087155, \\\"raw_content\\\": null}, {\\\"title\\\": \\\"Christopher Nolan: Biography, Movie Director, Filmmaker\\\", \\\"url\\\": \\\"https://www.biography.com/movies-tv/christopher-nolan\\\", \\\"content\\\": \\\"Opt-Out Icon Christopher Nolan is an Academy Award-winning movie director and screenwriter who‚Äôs helmed several hit films, including Inception, The Dark Knight, Interstellar, and Oppenheimer. We may earn commission from links on this page, but we only recommend products we back. Christopher Nolan is a British-American filmmaker known for his complex storytelling in big-budget movies such as Inception (2010), Interstellar (2014) and Tenet (2020). Play Icon We may earn commission from links on this page, but we only recommend products we back. Biography and associated logos are trademarks of A+E Networks¬Æprotected in the US and other countries around the globe. Opt-Out Icon\\\", \\\"score\\\": 0.28185803, \\\"raw_content\\\": null}, {\\\"title\\\": \\\"Christopher Nolan \\\\\\\"Film Director\\\\\\\" - Biography, Age and Married\\\", \\\"url\\\": \\\"https://biographyhost.com/p/christopher-nolan-biography.html\\\", \\\"content\\\": \\\"Christopher Nolan is a renowned British-American filmmaker celebrated for his innovative storytelling in films like Oppenheimer and Inception. Christopher Nolan is a British-American filmmaker renowned for his innovative storytelling and visually stunning films. Christopher Nolan is a British-American filmmaker renowned for his innovative storytelling and visually stunning films. Christopher Nolan is a British-American filmmaker renowned for his innovative storytelling and visually stunning films. Christopher Nolan is a British-American filmmaker renowned for his innovative storytelling and visually stunning films. Christopher Nolan is a British-American filmmaker renowned for his innovative storytelling and visually stunning films. Christopher Nolan is a British-American filmmaker renowned for his innovative storytelling and visually stunning films. Christopher Nolan is a British-American filmmaker renowned for his innovative storytelling and visually stunning films.\\\", \\\"score\\\": 0.19905913, \\\"raw_content\\\": null}, {\\\"title\\\": \\\"Christopher Nolan - Wikipedia\\\", \\\"url\\\": \\\"https://en.wikipedia.org/wiki/Christopher_Nolan\\\", \\\"content\\\": \\\"Following a positive word of mouth and screenings in 500 theatres, it earned $40¬†million.[41] Memento premiered at the Venice Film Festival in September 2000 to critical acclaim.[42] Joe Morgenstern of The Wall Street Journal wrote in his review, \\\\\\\"I can't remember when a movie has seemed so clever, strangely affecting and slyly funny at the very same time.\\\\\\\"[43] In the book The Philosophy of Neo-Noir, Basil Smith drew a comparison with John Locke's An Essay Concerning Human Understanding, which argues that conscious memories constitute our identities ‚Äì a theme Nolan explores in the film.[44] Memento earned Nolan many accolades, including nominations for an Academy Award and a Golden Globe Award for Best Screenplay, as well as two Independent Spirit Awards: Best Director and Best Screenplay.[45][46] Six critics listed it as one of the best films of the 2000s.[47] In 2001, Nolan and Emma Thomas founded the production company Syncopy Inc.[48][b]\\\", \\\"score\\\": 0.1508904, \\\"raw_content\\\": null}], \\\"response_time\\\": 0.74}\"  ]}\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > llm:ChatOpenAI] [10.98s] Exiting LLM run with output:\u001b[0m{  \"generations\": [    [      {        \"text\": \"The 2023 film **Oppenheimer** was directed by **Christopher Nolan**.\\n\\nChristopher Nolan was born on **July 30, 1970**. To calculate his age in days as of today:\\n\\n1. First, determine the total number of days from his birthdate to today.\\n2. Use the formula: \\\\[ \\\\text{Age in days} = \\\\text{Current Date} - \\\\text{Birth Date} \\\\]\\n\\nLet's calculate:\\n\\n- Birthdate: July 30, 1970\\n- Today's Date: December 7, 2023\\n\\nUsing a date calculator or similar method, we find that Christopher Nolan is approximately **19,480 days old** as of today.\",        \"generation_info\": {          \"finish_reason\": \"stop\",          \"model_name\": \"gpt-4-turbo-2024-04-09\",          \"system_fingerprint\": \"fp_de235176ee\",          \"service_tier\": \"default\"        },        \"type\": \"ChatGenerationChunk\",        \"message\": {          \"lc\": 1,          \"type\": \"constructor\",          \"id\": [            \"langchain\",            \"schema\",            \"messages\",            \"AIMessageChunk\"          ],          \"kwargs\": {            \"content\": \"The 2023 film **Oppenheimer** was directed by **Christopher Nolan**.\\n\\nChristopher Nolan was born on **July 30, 1970**. To calculate his age in days as of today:\\n\\n1. First, determine the total number of days from his birthdate to today.\\n2. Use the formula: \\\\[ \\\\text{Age in days} = \\\\text{Current Date} - \\\\text{Birth Date} \\\\]\\n\\nLet's calculate:\\n\\n- Birthdate: July 30, 1970\\n- Today's Date: December 7, 2023\\n\\nUsing a date calculator or similar method, we find that Christopher Nolan is approximately **19,480 days old** as of today.\",            \"response_metadata\": {              \"finish_reason\": \"stop\",              \"model_name\": \"gpt-4-turbo-2024-04-09\",              \"system_fingerprint\": \"fp_de235176ee\",              \"service_tier\": \"default\"            },            \"type\": \"AIMessageChunk\",            \"id\": \"run--21b0c760-dbf4-45e1-89fd-d1edfa1eb9d5\",            \"tool_calls\": [],            \"invalid_tool_calls\": []          }        }      }    ]  ],  \"llm_output\": null,  \"run\": null,  \"type\": \"LLMResult\"}\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > parser:ToolsAgentOutputParser] Entering Parser run with input:\u001b[0m[inputs]\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > parser:ToolsAgentOutputParser] [0ms] Exiting Parser run with output:\u001b[0m[outputs]\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence] [10.99s] Exiting Chain run with output:\u001b[0m[outputs]\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor] [17.09s] Exiting Chain run with output:\u001b[0m{  \"output\": \"The 2023 film **Oppenheimer** was directed by **Christopher Nolan**.\\n\\nChristopher Nolan was born on **July 30, 1970**. To calculate his age in days as of today:\\n\\n1. First, determine the total number of days from his birthdate to today.\\n2. Use the formula: \\\\[ \\\\text{Age in days} = \\\\text{Current Date} - \\\\text{Birth Date} \\\\]\\n\\nLet's calculate:\\n\\n- Birthdate: July 30, 1970\\n- Today's Date: December 7, 2023\\n\\nUsing a date calculator or similar method, we find that Christopher Nolan is approximately **19,480 days old** as of today.\"}\n```\n\n```\n{'input': 'Who directed the 2023 film Oppenheimer and what is their age in days?', 'output': \"The 2023 film **Oppenheimer** was directed by **Christopher Nolan**.\\n\\nChristopher Nolan was born on **July 30, 1970**. To calculate his age in days as of today:\\n\\n1. First, determine the total number of days from his birthdate to today.\\n2. Use the formula: \\\\[ \\\\text{Age in days} = \\\\text{Current Date} - \\\\text{Birth Date} \\\\]\\n\\nLet's calculate:\\n\\n- Birthdate: July 30, 1970\\n- Today's Date: December 7, 2023\\n\\nUsing a date calculator or similar method, we find that Christopher Nolan is approximately **19,480 days old** as of today.\"}\n```\n\n- set_verbose(True)\n- set_debug(True)", "url_final": "https://python.langchain.com/docs/how_to/debugging/", "url_citable": "https://python.langchain.com/docs/how_to/debugging/#set_debugtrue", "title": "How to debug your LLM apps | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "set_debug(True)‚Äã", "section_anchor": "set_debugtrue", "fetched_at": "2025-10-06T21:55:18.414131", "has_code": true}
{"id": "3d694775500c313bd38b99d512cd5cfcfed92e55", "text": "Interface‚Äã\n\nThe current Embeddings abstraction in LangChain is designed to operate on text data. In this implementation, the inputs are either single strings or lists of strings, and the outputs are lists of numerical arrays (vectors), where each vector represents an embedding of the input text into some n-dimensional space.\n\nYour custom embedding must implement the following methods:\n\nThese methods ensure that your embedding model can be integrated seamlessly into the LangChain framework, providing both synchronous and asynchronous capabilities for scalability and performance optimization.\n\nEmbeddings do not currently implement the Runnable interface and are also not instances of pydantic BaseModel.\n\nEmbedding queries vs documents‚Äã\n\nThe embed_query and embed_documents methods are required. These methods both operate on string inputs. The accessing of Document.page_content attributes is handled by the vector store using the embedding model for legacy reasons.\n\nembed_query takes in a single string and returns a single embedding as a list of floats. If your model has different modes for embedding queries vs the underlying documents, you can implement this method to handle that.\n\nembed_documents takes in a list of strings and returns a list of embeddings as a list of lists of floats.\n\nembed_documents takes in a list of plain text, not a list of LangChain Document objects. The name of this method may change in future versions of LangChain.", "url_final": "https://python.langchain.com/docs/how_to/custom_embeddings/", "url_citable": "https://python.langchain.com/docs/how_to/custom_embeddings/#interface", "title": "Custom Embeddings | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Interface‚Äã", "section_anchor": "interface", "fetched_at": "2025-10-06T21:55:14.176550", "has_code": false}
{"id": "f75e56fac81a3690d48cf24bb752065830972f48", "text": "Implementation‚Äã\n\nAs an example, we'll implement a simple embeddings model that returns a constant vector. This model is for illustrative purposes only.\n\n```\nfrom typing import Listfrom langchain_core.embeddings import Embeddingsclass ParrotLinkEmbeddings(Embeddings):    \"\"\"ParrotLink embedding model integration.    # TODO: Populate with relevant params.    Key init args ‚Äî completion params:        model: str            Name of ParrotLink model to use.    See full list of supported init args and their descriptions in the params section.    # TODO: Replace with relevant init params.    Instantiate:        .. code-block:: python            from langchain_parrot_link import ParrotLinkEmbeddings            embed = ParrotLinkEmbeddings(                model=\"...\",                # api_key=\"...\",                # other params...            )    Embed single text:        .. code-block:: python            input_text = \"The meaning of life is 42\"            embed.embed_query(input_text)        .. code-block:: python            # TODO: Example output.    # TODO: Delete if token-level streaming isn't supported.    Embed multiple text:        .. code-block:: python             input_texts = [\"Document 1...\", \"Document 2...\"]            embed.embed_documents(input_texts)        .. code-block:: python            # TODO: Example output.    # TODO: Delete if native async isn't supported.    Async:        .. code-block:: python            await embed.aembed_query(input_text)            # multiple:            # await embed.aembed_documents(input_texts)        .. code-block:: python            # TODO: Example output.    \"\"\"    def __init__(self, model: str):        self.model = model    def embed_documents(self, texts: List[str]) -> List[List[float]]:        \"\"\"Embed search docs.\"\"\"        return [[0.5, 0.6, 0.7] for _ in texts]    def embed_query(self, text: str) -> List[float]:        \"\"\"Embed query text.\"\"\"        return self.embed_documents([text])[0]    # optional: add custom async implementations here    # you can also delete these, and the base class will    # use the default implementation, which calls the sync    # version in an async executor:    # async def aembed_documents(self, texts: List[str]) -> List[List[float]]:    #     \"\"\"Asynchronous Embed search docs.\"\"\"    #     ...    # async def aembed_query(self, text: str) -> List[float]:    #     \"\"\"Asynchronous Embed query text.\"\"\"    #     ...\n```\n\nLet's test it üß™‚Äã\n\n```\nembeddings = ParrotLinkEmbeddings(\"test-model\")print(embeddings.embed_documents([\"Hello\", \"world\"]))print(embeddings.embed_query(\"Hello\"))\n```\n\n```\n[[0.5, 0.6, 0.7], [0.5, 0.6, 0.7]][0.5, 0.6, 0.7]\n```", "url_final": "https://python.langchain.com/docs/how_to/custom_embeddings/", "url_citable": "https://python.langchain.com/docs/how_to/custom_embeddings/#implementation", "title": "Custom Embeddings | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Implementation‚Äã", "section_anchor": "implementation", "fetched_at": "2025-10-06T21:55:14.176550", "has_code": true}
{"id": "f09d8a8f51f225ad19928d95e05f8ed93ee7c2fb", "text": "Contributing‚Äã\n\nWe welcome contributions of Embedding models to the LangChain code base.\n\nIf you aim to contribute an embedding model for a new provider (e.g., with a new set of dependencies or SDK), we encourage you to publish your implementation in a separate langchain-* integration package. This will enable you to appropriately manage dependencies and version your package. Please refer to our contributing guide for a walkthrough of this process.\n\n- Embedding queries vs documents\n\n- Let's test it üß™", "url_final": "https://python.langchain.com/docs/how_to/custom_embeddings/", "url_citable": "https://python.langchain.com/docs/how_to/custom_embeddings/#contributing", "title": "Custom Embeddings | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Contributing‚Äã", "section_anchor": "contributing", "fetched_at": "2025-10-06T21:55:14.176550", "has_code": false}
{"id": "caf4167438a73b14b7d00d503118337819f88019", "text": "Setup‚Äã\n\n```\n%pip install -qU langchain langchain-community langchain-openai langchain-chroma\n```\n\n```\nNote: you may need to restart the kernel to use updated packages.\n```\n\nWe'll use OpenAI in this example:\n\n```\nimport getpassimport osif \"OPENAI_API_KEY\" not in os.environ:    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()# Optional, uncomment to trace runs with LangSmith. Sign up here: https://smith.langchain.com.# os.environ[\"LANGSMITH_TRACING\"] = \"true\"# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n```\n\nCreate Index‚Äã\n\nWe will create a vectorstore over fake information.\n\n```\nfrom langchain_chroma import Chromafrom langchain_openai import OpenAIEmbeddingsfrom langchain_text_splitters import RecursiveCharacterTextSplittertexts = [\"Harrison worked at Kensho\", \"Ankush worked at Facebook\"]embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")vectorstore = Chroma.from_texts(    texts,    embeddings,)retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n```", "url_final": "https://python.langchain.com/docs/how_to/query_multiple_queries/", "url_citable": "https://python.langchain.com/docs/how_to/query_multiple_queries/#setup", "title": "How to handle multiple queries when doing query analysis | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Setup‚Äã", "section_anchor": "setup", "fetched_at": "2025-10-06T21:54:44.614554", "has_code": true}
{"id": "cf15aca56b207713776653389dfae7d970ec7d93", "text": "Query analysis‚Äã\n\nWe will use function calling to structure the output. We will let it return multiple queries.\n\n```\nfrom typing import List, Optionalfrom pydantic import BaseModel, Fieldclass Search(BaseModel):    \"\"\"Search over a database of job records.\"\"\"    queries: List[str] = Field(        ...,        description=\"Distinct queries to search for\",    )\n```\n\n```\nfrom langchain_core.output_parsers.openai_tools import PydanticToolsParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnablePassthroughfrom langchain_openai import ChatOpenAIoutput_parser = PydanticToolsParser(tools=[Search])system = \"\"\"You have the ability to issue search queries to get information to help answer user information.If you need to look up two distinct pieces of information, you are allowed to do that!\"\"\"prompt = ChatPromptTemplate.from_messages(    [        (\"system\", system),        (\"human\", \"{question}\"),    ])llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)structured_llm = llm.with_structured_output(Search)query_analyzer = {\"question\": RunnablePassthrough()} | prompt | structured_llm\n```\n\nWe can see that this allows for creating multiple queries\n\n```\nquery_analyzer.invoke(\"where did Harrison Work\")\n```\n\n```\nSearch(queries=['Harrison Work', 'Harrison employment history'])\n```\n\n```\nquery_analyzer.invoke(\"where did Harrison and ankush Work\")\n```\n\n```\nSearch(queries=['Harrison work history', 'Ankush work history'])\n```", "url_final": "https://python.langchain.com/docs/how_to/query_multiple_queries/", "url_citable": "https://python.langchain.com/docs/how_to/query_multiple_queries/#query-analysis", "title": "How to handle multiple queries when doing query analysis | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Query analysis‚Äã", "section_anchor": "query-analysis", "fetched_at": "2025-10-06T21:54:44.614554", "has_code": true}
{"id": "c1fa533071ae10d1ddbdad4b5079975d01fa7910", "text": "Retrieval with query analysis‚Äã\n\nSo how would we include this in a chain? One thing that will make this a lot easier is if we call our retriever asynchronously - this will let us loop over the queries and not get blocked on the response time.\n\n```\nfrom langchain_core.runnables import chain\n```\n\n```\n@chainasync def custom_chain(question):    response = await query_analyzer.ainvoke(question)    docs = []    for query in response.queries:        new_docs = await retriever.ainvoke(query)        docs.extend(new_docs)    # You probably want to think about reranking or deduplicating documents here    # But that is a separate topic    return docs\n```\n\n```\nawait custom_chain.ainvoke(\"where did Harrison Work\")\n```\n\n```\n[Document(page_content='Harrison worked at Kensho'), Document(page_content='Harrison worked at Kensho')]\n```\n\n```\nawait custom_chain.ainvoke(\"where did Harrison and ankush Work\")\n```\n\n```\n[Document(page_content='Harrison worked at Kensho'), Document(page_content='Ankush worked at Facebook')]\n```\n\n- Create Index", "url_final": "https://python.langchain.com/docs/how_to/query_multiple_queries/", "url_citable": "https://python.langchain.com/docs/how_to/query_multiple_queries/#retrieval-with-query-analysis", "title": "How to handle multiple queries when doing query analysis | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Retrieval with query analysis‚Äã", "section_anchor": "retrieval-with-query-analysis", "fetched_at": "2025-10-06T21:54:44.614554", "has_code": true}
{"id": "a2b608400352558e33e8c456fd42e8f18f082044", "text": "Basic usage‚Äã\n\nSpecify max_chunk_size to constrain chunk sizes:\n\n```\nfrom langchain_text_splitters import RecursiveJsonSplittersplitter = RecursiveJsonSplitter(max_chunk_size=300)\n```\n\nTo obtain json chunks, use the .split_json method:\n\n```\n# Recursively split json data - If you need to access/manipulate the smaller json chunksjson_chunks = splitter.split_json(json_data=json_data)for chunk in json_chunks[:3]:    print(chunk)\n```\n\n```\n{'openapi': '3.1.0', 'info': {'title': 'LangSmith', 'version': '0.1.0'}, 'servers': [{'url': 'https://api.smith.langchain.com', 'description': 'LangSmith API endpoint.'}]}{'paths': {'/api/v1/sessions/{session_id}': {'get': {'tags': ['tracer-sessions'], 'summary': 'Read Tracer Session', 'description': 'Get a specific session.', 'operationId': 'read_tracer_session_api_v1_sessions__session_id__get'}}}}{'paths': {'/api/v1/sessions/{session_id}': {'get': {'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}}}}\n```\n\nTo obtain LangChain Document objects, use the .create_documents method:\n\n```\n# The splitter can also output documentsdocs = splitter.create_documents(texts=[json_data])for doc in docs[:3]:    print(doc)\n```\n\n```\npage_content='{\"openapi\": \"3.1.0\", \"info\": {\"title\": \"LangSmith\", \"version\": \"0.1.0\"}, \"servers\": [{\"url\": \"https://api.smith.langchain.com\", \"description\": \"LangSmith API endpoint.\"}]}'page_content='{\"paths\": {\"/api/v1/sessions/{session_id}\": {\"get\": {\"tags\": [\"tracer-sessions\"], \"summary\": \"Read Tracer Session\", \"description\": \"Get a specific session.\", \"operationId\": \"read_tracer_session_api_v1_sessions__session_id__get\"}}}}'page_content='{\"paths\": {\"/api/v1/sessions/{session_id}\": {\"get\": {\"security\": [{\"API Key\": []}, {\"Tenant ID\": []}, {\"Bearer Auth\": []}]}}}}'\n```\n\nOr use .split_text to obtain string content directly:\n\n```\ntexts = splitter.split_text(json_data=json_data)print(texts[0])print(texts[1])\n```\n\n```\n{\"openapi\": \"3.1.0\", \"info\": {\"title\": \"LangSmith\", \"version\": \"0.1.0\"}, \"servers\": [{\"url\": \"https://api.smith.langchain.com\", \"description\": \"LangSmith API endpoint.\"}]}{\"paths\": {\"/api/v1/sessions/{session_id}\": {\"get\": {\"tags\": [\"tracer-sessions\"], \"summary\": \"Read Tracer Session\", \"description\": \"Get a specific session.\", \"operationId\": \"read_tracer_session_api_v1_sessions__session_id__get\"}}}}\n```", "url_final": "https://python.langchain.com/docs/how_to/recursive_json_splitter/", "url_citable": "https://python.langchain.com/docs/how_to/recursive_json_splitter/#basic-usage", "title": "How to split JSON data | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Basic usage‚Äã", "section_anchor": "basic-usage", "fetched_at": "2025-10-06T21:56:14.050076", "has_code": true}
{"id": "d0092cc61f2b27eabd3b4c16d9e7a8292dfb925b", "text": "How to manage chunk sizes from list content‚Äã\n\nNote that one of the chunks in this example is larger than the specified max_chunk_size of 300. Reviewing one of these chunks that was bigger we see there is a list object there:\n\n```\nprint([len(text) for text in texts][:10])print()print(texts[3])\n```\n\n```\n[171, 231, 126, 469, 210, 213, 237, 271, 191, 232]{\"paths\": {\"/api/v1/sessions/{session_id}\": {\"get\": {\"parameters\": [{\"name\": \"session_id\", \"in\": \"path\", \"required\": true, \"schema\": {\"type\": \"string\", \"format\": \"uuid\", \"title\": \"Session Id\"}}, {\"name\": \"include_stats\", \"in\": \"query\", \"required\": false, \"schema\": {\"type\": \"boolean\", \"default\": false, \"title\": \"Include Stats\"}}, {\"name\": \"accept\", \"in\": \"header\", \"required\": false, \"schema\": {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"null\"}], \"title\": \"Accept\"}}]}}}}\n```\n\nThe json splitter by default does not split lists.\n\nSpecify convert_lists=True to preprocess the json, converting list content to dicts with index:item as key:val pairs:\n\n```\ntexts = splitter.split_text(json_data=json_data, convert_lists=True)\n```\n\nLet's look at the size of the chunks. Now they are all under the max\n\n```\nprint([len(text) for text in texts][:10])\n```\n\n```\n[176, 236, 141, 203, 212, 221, 210, 213, 242, 291]\n```\n\nThe list has been converted to a dict, but retains all the needed contextual information even if split into many chunks:\n\n```\nprint(texts[1])\n```\n\n```\n{\"paths\": {\"/api/v1/sessions/{session_id}\": {\"get\": {\"tags\": {\"0\": \"tracer-sessions\"}, \"summary\": \"Read Tracer Session\", \"description\": \"Get a specific session.\", \"operationId\": \"read_tracer_session_api_v1_sessions__session_id__get\"}}}}\n```\n\n```\n# We can also look at the documentsdocs[1]\n```\n\n```\nDocument(page_content='{\"paths\": {\"/api/v1/sessions/{session_id}\": {\"get\": {\"tags\": [\"tracer-sessions\"], \"summary\": \"Read Tracer Session\", \"description\": \"Get a specific session.\", \"operationId\": \"read_tracer_session_api_v1_sessions__session_id__get\"}}}}')\n```\n\n- Basic usage\n- How to manage chunk sizes from list content", "url_final": "https://python.langchain.com/docs/how_to/recursive_json_splitter/", "url_citable": "https://python.langchain.com/docs/how_to/recursive_json_splitter/#how-to-manage-chunk-sizes-from-list-content", "title": "How to split JSON data | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "How to manage chunk sizes from list content‚Äã", "section_anchor": "how-to-manage-chunk-sizes-from-list-content", "fetched_at": "2025-10-06T21:56:14.050076", "has_code": true}
{"id": "3a30357754136bed9fd1da3584345442ed455c3e", "text": "Using PydanticOutputParser‚Äã\n\nThe following example uses the built-in PydanticOutputParser to parse the output of a chat model.\n\n```\nfrom typing import List, Optionalfrom langchain_core.output_parsers import PydanticOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom pydantic import BaseModel, Field, validatorclass Person(BaseModel):    \"\"\"Information about a person.\"\"\"    name: str = Field(..., description=\"The name of the person\")    height_in_meters: float = Field(        ..., description=\"The height of the person expressed in meters.\"    )class People(BaseModel):    \"\"\"Identifying information about all people in a text.\"\"\"    people: List[Person]# Set up a parserparser = PydanticOutputParser(pydantic_object=People)# Promptprompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"Answer the user query. Wrap the output in `json` tags\\n{format_instructions}\",        ),        (\"human\", \"{query}\"),    ]).partial(format_instructions=parser.get_format_instructions())\n```\n\nLet's take a look at what information is sent to the model\n\n```\nquery = \"Anna is 23 years old and she is 6 feet tall\"\n```\n\n```\nprint(prompt.format_prompt(query=query).to_string())\n```\n\n```\nSystem: Answer the user query. Wrap the output in `json` tagsThe output should be formatted as a JSON instance that conforms to the JSON schema below.As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.Here is the output schema:\\`\\`\\`{\"$defs\": {\"Person\": {\"description\": \"Information about a person.\", \"properties\": {\"name\": {\"description\": \"The name of the person\", \"title\": \"Name\", \"type\": \"string\"}, \"height_in_meters\": {\"description\": \"The height of the person expressed in meters.\", \"title\": \"Height In Meters\", \"type\": \"number\"}}, \"required\": [\"name\", \"height_in_meters\"], \"title\": \"Person\", \"type\": \"object\"}}, \"description\": \"Identifying information about all people in a text.\", \"properties\": {\"people\": {\"items\": {\"$ref\": \"#/$defs/Person\"}, \"title\": \"People\", \"type\": \"array\"}}, \"required\": [\"people\"]}\\`\\`\\`Human: Anna is 23 years old and she is 6 feet tall\n```\n\nHaving defined our prompt, we simply chain together the prompt, model and output parser:\n\n```\nchain = prompt | model | parserchain.invoke({\"query\": query})\n```\n\n```\nPeople(people=[Person(name='Anna', height_in_meters=1.8288)])\n```\n\nCheck out the associated Langsmith trace.\n\nNote that the schema shows up in two places:\n\n- In the prompt, via parser.get_format_instructions();\n- In the chain, to receive the formatted output and structure it into a Python object (in this case, the Pydantic object People).", "url_final": "https://python.langchain.com/docs/how_to/extraction_parse/", "url_citable": "https://python.langchain.com/docs/how_to/extraction_parse/#using-pydanticoutputparser", "title": "How to use prompting alone (no tool calling) to do extraction | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Using PydanticOutputParser‚Äã", "section_anchor": "using-pydanticoutputparser", "fetched_at": "2025-10-06T21:55:39.697399", "has_code": true}
{"id": "b6b0ad8fc396dcf15fa039c8c160b61f14c4d6e8", "text": "Custom Parsing‚Äã\n\nIf desired, it's easy to create a custom prompt and parser with LangChain and LCEL.\n\nTo create a custom parser, define a function to parse the output from the model (typically an AIMessage) into an object of your choice.\n\nSee below for a simple implementation of a JSON parser.\n\n```\nimport jsonimport refrom typing import List, Optionalfrom langchain_anthropic.chat_models import ChatAnthropicfrom langchain_core.messages import AIMessagefrom langchain_core.prompts import ChatPromptTemplatefrom pydantic import BaseModel, Field, validatorclass Person(BaseModel):    \"\"\"Information about a person.\"\"\"    name: str = Field(..., description=\"The name of the person\")    height_in_meters: float = Field(        ..., description=\"The height of the person expressed in meters.\"    )class People(BaseModel):    \"\"\"Identifying information about all people in a text.\"\"\"    people: List[Person]# Promptprompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"Answer the user query. Output your answer as JSON that  \"            \"matches the given schema: \\`\\`\\`json\\n{schema}\\n\\`\\`\\`. \"            \"Make sure to wrap the answer in \\`\\`\\`json and \\`\\`\\` tags\",        ),        (\"human\", \"{query}\"),    ]).partial(schema=People.schema())# Custom parserdef extract_json(message: AIMessage) -> List[dict]:    \"\"\"Extracts JSON content from a string where JSON is embedded between \\`\\`\\`json and \\`\\`\\` tags.    Parameters:        text (str): The text containing the JSON content.    Returns:        list: A list of extracted JSON strings.    \"\"\"    text = message.content    # Define the regular expression pattern to match JSON blocks    pattern = r\"\\`\\`\\`json(.*?)\\`\\`\\`\"    # Find all non-overlapping matches of the pattern in the string    matches = re.findall(pattern, text, re.DOTALL)    # Return the list of matched JSON strings, stripping any leading or trailing whitespace    try:        return [json.loads(match.strip()) for match in matches]    except Exception:        raise ValueError(f\"Failed to parse: {message}\")\n```\n\n```\nquery = \"Anna is 23 years old and she is 6 feet tall\"print(prompt.format_prompt(query=query).to_string())\n```\n\n```\nSystem: Answer the user query. Output your answer as JSON that  matches the given schema: \\`\\`\\`json{'$defs': {'Person': {'description': 'Information about a person.', 'properties': {'name': {'description': 'The name of the person', 'title': 'Name', 'type': 'string'}, 'height_in_meters': {'description': 'The height of the person expressed in meters.', 'title': 'Height In Meters', 'type': 'number'}}, 'required': ['name', 'height_in_meters'], 'title': 'Person', 'type': 'object'}}, 'description': 'Identifying information about all people in a text.', 'properties': {'people': {'items': {'$ref': '#/$defs/Person'}, 'title': 'People', 'type': 'array'}}, 'required': ['people'], 'title': 'People', 'type': 'object'}\\`\\`\\`. Make sure to wrap the answer in \\`\\`\\`json and \\`\\`\\` tagsHuman: Anna is 23 years old and she is 6 feet tall\n```\n\n```\nchain = prompt | model | extract_jsonchain.invoke({\"query\": query})\n```\n\n```\n[{'people': [{'name': 'Anna', 'height_in_meters': 1.83}]}]\n```", "url_final": "https://python.langchain.com/docs/how_to/extraction_parse/", "url_citable": "https://python.langchain.com/docs/how_to/extraction_parse/#custom-parsing", "title": "How to use prompting alone (no tool calling) to do extraction | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Custom Parsing‚Äã", "section_anchor": "custom-parsing", "fetched_at": "2025-10-06T21:55:39.697399", "has_code": true}
{"id": "23c6044496a9f14fdffd25810fd7e4fbf9a49807", "text": "Other Libraries‚Äã\n\nIf you're looking at extracting using a parsing approach, check out the Kor library. It's written by one of the LangChain maintainers and it helps to craft a prompt that takes examples into account, allows controlling formats (e.g., JSON or CSV) and expresses the schema in TypeScript. It seems to work pretty!\n\n- Using PydanticOutputParser\n- Custom Parsing\n- Other Libraries", "url_final": "https://python.langchain.com/docs/how_to/extraction_parse/", "url_citable": "https://python.langchain.com/docs/how_to/extraction_parse/#other-libraries", "title": "How to use prompting alone (no tool calling) to do extraction | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Other Libraries‚Äã", "section_anchor": "other-libraries", "fetched_at": "2025-10-06T21:55:39.697399", "has_code": false}
{"id": "f065ebaeed652ab9e82426f618dcb2c3cbf31aa4", "text": "OpenAI‚Äã\n\n```\nfrom langchain_openai import ChatOpenAIllm = ChatOpenAI(model=\"gpt-4o-mini\")msg = llm.invoke(\"What's the oldest known example of cuneiform\")msg.response_metadata\n```\n\n```\n{'token_usage': {'completion_tokens': 88,  'prompt_tokens': 16,  'total_tokens': 104,  'completion_tokens_details': {'accepted_prediction_tokens': 0,   'audio_tokens': 0,   'reasoning_tokens': 0,   'rejected_prediction_tokens': 0},  'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-ByN1Qkvqb5fAGKKzXXxZ3rBlnqkWs', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}\n```", "url_final": "https://python.langchain.com/docs/how_to/response_metadata/", "url_citable": "https://python.langchain.com/docs/how_to/response_metadata/#openai", "title": "Response metadata | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "OpenAI‚Äã", "section_anchor": "openai", "fetched_at": "2025-10-06T21:56:16.051556", "has_code": true}
{"id": "1649b4125faa895e73f7d8353861996f2bd4d831", "text": "Anthropic‚Äã\n\n```\nfrom langchain_anthropic import ChatAnthropicllm = ChatAnthropic(model=\"claude-3-7-sonnet-20250219\")msg = llm.invoke(\"What's the oldest known example of cuneiform\")msg.response_metadata\n```\n\n```\n{'id': 'msg_01NTWnqvbNKSjGfqQL7xikau', 'model': 'claude-3-7-sonnet-20250219', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0,  'cache_read_input_tokens': 0,  'input_tokens': 17,  'output_tokens': 197,  'server_tool_use': None,  'service_tier': 'standard'}, 'model_name': 'claude-3-7-sonnet-20250219'}\n```", "url_final": "https://python.langchain.com/docs/how_to/response_metadata/", "url_citable": "https://python.langchain.com/docs/how_to/response_metadata/#anthropic", "title": "Response metadata | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Anthropic‚Äã", "section_anchor": "anthropic", "fetched_at": "2025-10-06T21:56:16.051556", "has_code": true}
{"id": "f79de7e1dd797182d3837bb3f018228fc22fe85b", "text": "Google Generative AI‚Äã\n\n```\nfrom langchain_google_genai import ChatGoogleGenerativeAIllm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")msg = llm.invoke(\"What's the oldest known example of cuneiform\")msg.response_metadata\n```\n\n```\n{'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}\n```", "url_final": "https://python.langchain.com/docs/how_to/response_metadata/", "url_citable": "https://python.langchain.com/docs/how_to/response_metadata/#google-generative-ai", "title": "Response metadata | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Google Generative AI‚Äã", "section_anchor": "google-generative-ai", "fetched_at": "2025-10-06T21:56:16.051556", "has_code": true}
{"id": "e1189c1c39cdaac6da0c3a6ed9e24267185b2d42", "text": "Bedrock (Anthropic)‚Äã\n\n```\nfrom langchain_aws import ChatBedrockConversellm = ChatBedrockConverse(model=\"anthropic.claude-3-7-sonnet-20250219-v1:0\")msg = llm.invoke(\"What's the oldest known example of cuneiform\")msg.response_metadata\n```\n\n```\n{'ResponseMetadata': {'RequestId': 'ea0ac2ad-3ad5-4a49-9647-274a0c73ac31',  'HTTPStatusCode': 200,  'HTTPHeaders': {'date': 'Sat, 22 Mar 2025 11:27:46 GMT',   'content-type': 'application/json',   'content-length': '1660',   'connection': 'keep-alive',   'x-amzn-requestid': 'ea0ac2ad-3ad5-4a49-9647-274a0c73ac31'},  'RetryAttempts': 0}, 'stopReason': 'end_turn', 'metrics': {'latencyMs': [11044]}}\n```", "url_final": "https://python.langchain.com/docs/how_to/response_metadata/", "url_citable": "https://python.langchain.com/docs/how_to/response_metadata/#bedrock-anthropic", "title": "Response metadata | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Bedrock (Anthropic)‚Äã", "section_anchor": "bedrock-anthropic", "fetched_at": "2025-10-06T21:56:16.051556", "has_code": true}
{"id": "3bb35defc05bd18e9c681600720052e4938342d0", "text": "MistralAI‚Äã\n\n```\nfrom langchain_mistralai import ChatMistralAIllm = ChatMistralAI(model=\"mistral-small-latest\")msg = llm.invoke([(\"human\", \"What's the oldest known example of cuneiform\")])msg.response_metadata\n```\n\n```\n{'token_usage': {'prompt_tokens': 13,  'total_tokens': 306,  'completion_tokens': 293}, 'model_name': 'mistral-small-latest', 'model': 'mistral-small-latest', 'finish_reason': 'stop'}\n```", "url_final": "https://python.langchain.com/docs/how_to/response_metadata/", "url_citable": "https://python.langchain.com/docs/how_to/response_metadata/#mistralai", "title": "Response metadata | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "MistralAI‚Äã", "section_anchor": "mistralai", "fetched_at": "2025-10-06T21:56:16.051556", "has_code": true}
{"id": "e6fb96af7e35ce2538edf493f601f83a96e54272", "text": "Groq‚Äã\n\n```\nfrom langchain_groq import ChatGroqllm = ChatGroq(model=\"llama-3.1-8b-instant\")msg = llm.invoke(\"What's the oldest known example of cuneiform\")msg.response_metadata\n```\n\n```\n{'token_usage': {'completion_tokens': 184,  'prompt_tokens': 45,  'total_tokens': 229,  'completion_time': 0.245333333,  'prompt_time': 0.002262803,  'queue_time': 0.19315161,  'total_time': 0.247596136}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_a56f6eea01', 'finish_reason': 'stop', 'logprobs': None}\n```", "url_final": "https://python.langchain.com/docs/how_to/response_metadata/", "url_citable": "https://python.langchain.com/docs/how_to/response_metadata/#groq", "title": "Response metadata | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Groq‚Äã", "section_anchor": "groq", "fetched_at": "2025-10-06T21:56:16.051556", "has_code": true}
{"id": "4d10dd51855cc2bb99298757d1624f0c494d6f5d", "text": "FireworksAI‚Äã\n\n```\nfrom langchain_fireworks import ChatFireworksllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")msg = llm.invoke(\"What's the oldest known example of cuneiform\")msg.response_metadata\n```\n\n```\n{'token_usage': {'prompt_tokens': 25,  'total_tokens': 352,  'completion_tokens': 327}, 'model_name': 'accounts/fireworks/models/llama-v3p1-70b-instruct', 'system_fingerprint': '', 'finish_reason': 'stop', 'logprobs': None}\n```\n\n- OpenAI\n- Anthropic\n- Google Generative AI\n- Bedrock (Anthropic)\n- MistralAI\n- Groq\n- FireworksAI", "url_final": "https://python.langchain.com/docs/how_to/response_metadata/", "url_citable": "https://python.langchain.com/docs/how_to/response_metadata/#fireworksai", "title": "Response metadata | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "FireworksAI‚Äã", "section_anchor": "fireworksai", "fetched_at": "2025-10-06T21:56:16.051556", "has_code": true}
{"id": "6b5e0bf65e42633ba61bc3c8205c41a063a0959f", "text": "SQLite Cache‚Äã\n\n```\n!rm .langchain.db\n```\n\n```\n# We can do the same thing with a SQLite cachefrom langchain_community.cache import SQLiteCacheset_llm_cache(SQLiteCache(database_path=\".langchain.db\"))\n```\n\n```\n%%time# The first time, it is not yet in cache, so it should take longerllm.invoke(\"Tell me a joke\")\n```\n\n```\nCPU times: user 10.6 ms, sys: 4.21 ms, total: 14.8 msWall time: 851 ms\n```\n\n```\n\"\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything!\"\n```\n\n```\n%%time# The second time it is, so it goes fasterllm.invoke(\"Tell me a joke\")\n```\n\n```\nCPU times: user 59.7 ms, sys: 63.6 ms, total: 123 msWall time: 134 ms\n```\n\n```\n\"\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything!\"\n```\n\n- SQLite Cache", "url_final": "https://python.langchain.com/docs/how_to/llm_caching/", "url_citable": "https://python.langchain.com/docs/how_to/llm_caching/#sqlite-cache", "title": "How to cache LLM responses | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "SQLite Cache‚Äã", "section_anchor": "sqlite-cache", "fetched_at": "2025-10-06T21:55:46.984800", "has_code": true}
{"id": "b10b5fff75605def724ccfc5eb83e17a19f5dbab", "text": "Setup‚Äã\n\nWe'll need to install the following packages for this guide:\n\n```\n%pip install --upgrade --quiet langchain\n```\n\nIf you'd like to trace your runs in LangSmith uncomment and set the following environment variables:\n\n```\nimport getpassimport os# os.environ[\"LANGSMITH_TRACING\"] = \"true\"# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n```", "url_final": "https://python.langchain.com/docs/how_to/tools_chain/", "url_citable": "https://python.langchain.com/docs/how_to/tools_chain/#setup", "title": "How to use tools in a chain | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Setup‚Äã", "section_anchor": "setup", "fetched_at": "2025-10-06T21:54:16.434276", "has_code": true}
{"id": "b22b688a961d058e70ab58a5e962375d393dc33f", "text": "Create a tool‚Äã\n\nFirst, we need to create a tool to call. For this example, we will create a custom tool from a function. For more information on creating custom tools, please see this guide.\n\n```\nfrom langchain_core.tools import tool@tooldef multiply(first_int: int, second_int: int) -> int:    \"\"\"Multiply two integers together.\"\"\"    return first_int * second_int\n```\n\n```\nprint(multiply.name)print(multiply.description)print(multiply.args)\n```\n\n```\nmultiplyMultiply two integers together.{'first_int': {'title': 'First Int', 'type': 'integer'}, 'second_int': {'title': 'Second Int', 'type': 'integer'}}\n```\n\n```\nmultiply.invoke({\"first_int\": 4, \"second_int\": 5})\n```\n\n```\n20\n```", "url_final": "https://python.langchain.com/docs/how_to/tools_chain/", "url_citable": "https://python.langchain.com/docs/how_to/tools_chain/#create-a-tool", "title": "How to use tools in a chain | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Create a tool‚Äã", "section_anchor": "create-a-tool", "fetched_at": "2025-10-06T21:54:16.434276", "has_code": true}
{"id": "58af4f0b8703f65884ea1d8f01ba2905294d0641", "text": "Chains‚Äã\n\nIf we know that we only need to use a tool a fixed number of times, we can create a chain for doing so. Let's create a simple chain that just multiplies user-specified numbers.\n\nTool/function calling‚Äã\n\nOne of the most reliable ways to use tools with LLMs is with tool calling APIs (also sometimes called function calling). This only works with models that explicitly support tool calling. You can see which models support tool calling here, and learn more about how to use tool calling in this guide.\n\nFirst we'll define our model and tools. We'll start with just a single tool, multiply.\n\n```\npip install -qU \"langchain[google-genai]\"\n```\n\n```\nimport getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n```\n\nWe'll use bind_tools to pass the definition of our tool in as part of each call to the model, so that the model can invoke the tool when appropriate:\n\n```\nllm_with_tools = llm.bind_tools([multiply])\n```\n\nWhen the model invokes the tool, this will show up in the AIMessage.tool_calls attribute of the output:\n\n```\nmsg = llm_with_tools.invoke(\"whats 5 times forty two\")msg.tool_calls\n```\n\n```\n[{'name': 'multiply',  'args': {'first_int': 5, 'second_int': 42},  'id': 'call_8QIg4QVFVAEeC1orWAgB2036',  'type': 'tool_call'}]\n```\n\nCheck out the LangSmith trace here.\n\nInvoking the tool‚Äã\n\nGreat! We're able to generate tool invocations. But what if we want to actually call the tool? To do so we'll need to pass the generated tool args to our tool. As a simple example we'll just extract the arguments of the first tool_call:\n\n```\nfrom operator import itemgetterchain = llm_with_tools | (lambda x: x.tool_calls[0][\"args\"]) | multiplychain.invoke(\"What's four times 23\")\n```\n\n```\n92\n```\n\nCheck out the LangSmith trace here.", "url_final": "https://python.langchain.com/docs/how_to/tools_chain/", "url_citable": "https://python.langchain.com/docs/how_to/tools_chain/#chains", "title": "How to use tools in a chain | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Chains‚Äã", "section_anchor": "chains", "fetched_at": "2025-10-06T21:54:16.434276", "has_code": true}
{"id": "4ef09442dd5dbca41728cc63b161568b1b9e609e", "text": "Agents‚Äã\n\nChains are great when we know the specific sequence of tool usage needed for any user input. But for certain use cases, how many times we use tools depends on the input. In these cases, we want to let the model itself decide how many times to use tools and in what order. Agents let us do just this.\n\nWe'll demonstrate a simple example using a LangGraph agent. See this tutorial for more detail.\n\n```\n!pip install -qU langgraph\n```\n\n```\nfrom langgraph.prebuilt import create_react_agent\n```\n\nAgents are also great because they make it easy to use multiple tools.\n\n```\n@tooldef add(first_int: int, second_int: int) -> int:    \"Add two integers.\"    return first_int + second_int@tooldef exponentiate(base: int, exponent: int) -> int:    \"Exponentiate the base to the exponent power.\"    return base**exponenttools = [multiply, add, exponentiate]\n```\n\n```\n# Construct the tool calling agentagent = create_react_agent(llm, tools)\n```\n\nWith an agent, we can ask questions that require arbitrarily-many uses of our tools:\n\n```\n# Use the agentquery = (    \"Take 3 to the fifth power and multiply that by the sum of twelve and \"    \"three, then square the whole result.\")input_message = {\"role\": \"user\", \"content\": query}for step in agent.stream({\"messages\": [input_message]}, stream_mode=\"values\"):    step[\"messages\"][-1].pretty_print()\n```\n\n```\n================================\u001b[1m Human Message \u001b[0m=================================Take 3 to the fifth power and multiply that by the sum of twelve and three, then square the whole result.==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls:  exponentiate (call_EHGS8gnEVNCJQ9rVOk11KCQH) Call ID: call_EHGS8gnEVNCJQ9rVOk11KCQH  Args:    base: 3    exponent: 5  add (call_s2cxOrXEKqI6z7LWbMUG6s8c) Call ID: call_s2cxOrXEKqI6z7LWbMUG6s8c  Args:    first_int: 12    second_int: 3=================================\u001b[1m Tool Message \u001b[0m=================================Name: add15==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls:  multiply (call_25v5JEfDWuKNgmVoGBan0d7J) Call ID: call_25v5JEfDWuKNgmVoGBan0d7J  Args:    first_int: 243    second_int: 15=================================\u001b[1m Tool Message \u001b[0m=================================Name: multiply3645==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls:  exponentiate (call_x1yKEeBPrFYmCp2z5Kn8705r) Call ID: call_x1yKEeBPrFYmCp2z5Kn8705r  Args:    base: 3645    exponent: 2=================================\u001b[1m Tool Message \u001b[0m=================================Name: exponentiate13286025==================================\u001b[1m Ai Message \u001b[0m==================================The final result of taking 3 to the fifth power, multiplying it by the sum of twelve and three, and then squaring the whole result is **13,286,025**.\n```\n\nCheck out the LangSmith trace here.\n\n- Tool/function calling\n- Invoking the tool", "url_final": "https://python.langchain.com/docs/how_to/tools_chain/", "url_citable": "https://python.langchain.com/docs/how_to/tools_chain/#agents", "title": "How to use tools in a chain | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Agents‚Äã", "section_anchor": "agents", "fetched_at": "2025-10-06T21:54:16.434276", "has_code": true}
{"id": "88eba7e6e8e6796105d075b693ac6c11aade1014", "text": "Setup‚Äã\n\nFor the \"simple and fast\" parsing, we will need langchain-community and the beautifulsoup4 library:\n\n```\n%pip install -qU langchain-community beautifulsoup4\n```\n\nFor advanced parsing, we will use langchain-unstructured:\n\n```\n%pip install -qU langchain-unstructured\n```", "url_final": "https://python.langchain.com/docs/how_to/document_loader_web/", "url_citable": "https://python.langchain.com/docs/how_to/document_loader_web/#setup", "title": "How to load web pages | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Setup‚Äã", "section_anchor": "setup", "fetched_at": "2025-10-06T21:55:27.905553", "has_code": true}
{"id": "f050f4a87966f5f7a2af9a8a9b616ca72c523000", "text": "Simple and fast text extraction‚Äã\n\nIf you are looking for a simple string representation of text that is embedded in a web page, the method below is appropriate. It will return a list of Document objects -- one per page -- containing a single string of the page's text. Under the hood it uses the beautifulsoup4 Python library.\n\nLangChain document loaders implement lazy_load and its async variant, alazy_load, which return iterators of Document objects. We will use these below.\n\n```\nimport bs4from langchain_community.document_loaders import WebBaseLoaderpage_url = \"https://python.langchain.com/docs/how_to/chatbots_memory/\"loader = WebBaseLoader(web_paths=[page_url])docs = []async for doc in loader.alazy_load():    docs.append(doc)assert len(docs) == 1doc = docs[0]\n```\n\n```\nUSER_AGENT environment variable not set, consider setting it to identify your requests.\n```\n\n```\nprint(f\"{doc.metadata}\\n\")print(doc.page_content[:500].strip())\n```\n\n```\n{'source': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'title': 'How to add memory to chatbots | \\uf8ff√º¬∂√∫√î‚àè√®\\uf8ff√º√Æ√≥ LangChain', 'description': 'A key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including:', 'language': 'en'}How to add memory to chatbots | Ô£ø√º¬∂√∫√î‚àè√®Ô£ø√º√Æ√≥ LangChainSkip to main contentShare your thoughts on AI agents. Take the 3-min survey.IntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1Ô£ø√º√≠¬®SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingd\n```\n\nThis is essentially a dump of the text from the page's HTML. It may contain extraneous information like headings and navigation bars. If you are familiar with the expected HTML, you can specify desired <div> classes and other parameters via BeautifulSoup. Below we parse only the body text of the article:\n\n```\nloader = WebBaseLoader(    web_paths=[page_url],    bs_kwargs={        \"parse_only\": bs4.SoupStrainer(class_=\"theme-doc-markdown markdown\"),    },    bs_get_text_kwargs={\"separator\": \" | \", \"strip\": True},)docs = []async for doc in loader.alazy_load():    docs.append(doc)assert len(docs) == 1doc = docs[0]\n```\n\n```\nprint(f\"{doc.metadata}\\n\")print(doc.page_content[:500])\n```\n\n```\n{'source': 'https://python.langchain.com/docs/how_to/chatbots_memory/'}How to add memory to chatbots | A key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including: | Simply stuffing previous messages into a chat model prompt. | The above, but trimming old messages to reduce the amount of distracting information the model has to deal with. | More complex modifications like synthesizing summaries for long running conversations. | We'll go into more detail on a few techniq\n```\n\n```\nprint(doc.page_content[-500:])\n```\n\n```\na greeting. Nemo then asks the AI how it is doing, and the AI responds that it is fine.'), | HumanMessage(content='What did I say my name was?'), | AIMessage(content='You introduced yourself as Nemo. How can I assist you today, Nemo?')] | Note that invoking the chain again will generate another summary generated from the initial summary plus new messages and so on. You could also design a hybrid approach where a certain number of messages are retained in chat history while others are summarized.\n```\n\nNote that this required advance technical knowledge of how the body text is represented in the underlying HTML.\n\nWe can parameterize WebBaseLoader with a variety of settings, allowing for specification of request headers, rate limits, and parsers and other kwargs for BeautifulSoup. See its API reference for detail.", "url_final": "https://python.langchain.com/docs/how_to/document_loader_web/", "url_citable": "https://python.langchain.com/docs/how_to/document_loader_web/#simple-and-fast-text-extraction", "title": "How to load web pages | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Simple and fast text extraction‚Äã", "section_anchor": "simple-and-fast-text-extraction", "fetched_at": "2025-10-06T21:55:27.905553", "has_code": true}
{"id": "384a849a0ddbd2b0c08a85a84e988b2ff0438ead", "text": "Advanced parsing‚Äã\n\nThis method is appropriate if we want more granular control or processing of the page content. Below, instead of generating one Document per page and controlling its content via BeautifulSoup, we generate multiple Document objects representing distinct structures on a page. These structures can include section titles and their corresponding body texts, lists or enumerations, tables, and more.\n\nUnder the hood it uses the langchain-unstructured library. See the integration docs for more information about using Unstructured with LangChain.\n\n```\nfrom langchain_unstructured import UnstructuredLoaderpage_url = \"https://python.langchain.com/docs/how_to/chatbots_memory/\"loader = UnstructuredLoader(web_url=page_url)docs = []async for doc in loader.alazy_load():    docs.append(doc)\n```\n\n```\nINFO: Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.INFO: NumExpr defaulting to 8 threads.\n```\n\nNote that with no advance knowledge of the page HTML structure, we recover a natural organization of the body text:\n\n```\nfor doc in docs[:5]:    print(doc.page_content)\n```\n\n```\nHow to add memory to chatbotsA key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including:Simply stuffing previous messages into a chat model prompt.The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.More complex modifications like synthesizing summaries for long running conversations.ERROR! Session/line number was not unique in database. History logging moved to new session 2747\n```", "url_final": "https://python.langchain.com/docs/how_to/document_loader_web/", "url_citable": "https://python.langchain.com/docs/how_to/document_loader_web/#advanced-parsing", "title": "How to load web pages | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Advanced parsing‚Äã", "section_anchor": "advanced-parsing", "fetched_at": "2025-10-06T21:55:27.905553", "has_code": true}
{"id": "0b3dfc59cd058b8efcb51e2410d378007884decd", "text": "Extracting content from specific sections‚Äã\n\nEach Document object represents an element of the page. Its metadata contains useful information, such as its category:\n\n```\nfor doc in docs[:5]:    print(f\"{doc.metadata['category']}: {doc.page_content}\")\n```\n\n```\nTitle: How to add memory to chatbotsNarrativeText: A key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including:ListItem: Simply stuffing previous messages into a chat model prompt.ListItem: The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.ListItem: More complex modifications like synthesizing summaries for long running conversations.\n```\n\nElements may also have parent-child relationships -- for example, a paragraph might belong to a section with a title. If a section is of particular interest (e.g., for indexing) we can isolate the corresponding Document objects.\n\nAs an example, below we load the content of the \"Setup\" sections for two web pages:\n\n```\nfrom typing import Listfrom langchain_core.documents import Documentasync def _get_setup_docs_from_url(url: str) -> List[Document]:    loader = UnstructuredLoader(web_url=url)    setup_docs = []    parent_id = -1    async for doc in loader.alazy_load():        if doc.metadata[\"category\"] == \"Title\" and doc.page_content.startswith(\"Setup\"):            parent_id = doc.metadata[\"element_id\"]        if doc.metadata.get(\"parent_id\") == parent_id:            setup_docs.append(doc)    return setup_docspage_urls = [    \"https://python.langchain.com/docs/how_to/chatbots_memory/\",    \"https://python.langchain.com/docs/how_to/chatbots_tools/\",]setup_docs = []for url in page_urls:    page_setup_docs = await _get_setup_docs_from_url(url)    setup_docs.extend(page_setup_docs)\n```\n\n```\nfrom collections import defaultdictsetup_text = defaultdict(str)for doc in setup_docs:    url = doc.metadata[\"url\"]    setup_text[url] += f\"{doc.page_content}\\n\"dict(setup_text)\n```\n\n```\n{'https://python.langchain.com/docs/how_to/chatbots_memory/': \"You'll need to install a few packages, and have your OpenAI API key set as an environment variable named OPENAI_API_KEY:\\n%pip install --upgrade --quiet langchain langchain-openai\\n\\n# Set env var OPENAI_API_KEY or load from a .env file:\\nimport dotenv\\n\\ndotenv.load_dotenv()\\n[33mWARNING: You are using pip version 22.0.4; however, version 23.3.2 is available.\\nYou should consider upgrading via the '/Users/jacoblee/.pyenv/versions/3.10.5/bin/python -m pip install --upgrade pip' command.[0m[33m\\n[0mNote: you may need to restart the kernel to use updated packages.\\n\", 'https://python.langchain.com/docs/how_to/chatbots_tools/': \"For this guide, we'll be using a tool calling agent with a single tool for searching the web. The default will be powered by Tavily, but you can switch it out for any similar tool. The rest of this section will assume you're using Tavily.\\nYou'll need to sign up for an account on the Tavily website, and install the following packages:\\n%pip install --upgrade --quiet langchain-community langchain-openai tavily-python\\n\\n# Set env var OPENAI_API_KEY or load from a .env file:\\nimport dotenv\\n\\ndotenv.load_dotenv()\\nYou will also need your OpenAI key set as OPENAI_API_KEY and your Tavily API key set as TAVILY_API_KEY.\\n\"}\n```", "url_final": "https://python.langchain.com/docs/how_to/document_loader_web/", "url_citable": "https://python.langchain.com/docs/how_to/document_loader_web/#extracting-content-from-specific-sections", "title": "How to load web pages | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Extracting content from specific sections‚Äã", "section_anchor": "extracting-content-from-specific-sections", "fetched_at": "2025-10-06T21:55:27.905553", "has_code": true}
{"id": "4f468fdf4d0c0305b8cfbea369956ef2765d20da", "text": "Vector search over page content‚Äã\n\nOnce we have loaded the page contents into LangChain Document objects, we can index them (e.g., for a RAG application) in the usual way. Below we use OpenAI embeddings, although any LangChain embeddings model will suffice.\n\n```\n%pip install -qU langchain-openai\n```\n\n```\nimport getpassimport osif \"OPENAI_API_KEY\" not in os.environ:    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n```\n\n```\nfrom langchain_core.vectorstores import InMemoryVectorStorefrom langchain_openai import OpenAIEmbeddingsvector_store = InMemoryVectorStore.from_documents(setup_docs, OpenAIEmbeddings())retrieved_docs = vector_store.similarity_search(\"Install Tavily\", k=2)for doc in retrieved_docs:    print(f\"Page {doc.metadata['url']}: {doc.page_content[:300]}\\n\")\n```\n\n```\nINFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"``````outputPage https://python.langchain.com/docs/how_to/chatbots_tools/: You'll need to sign up for an account on the Tavily website, and install the following packages:Page https://python.langchain.com/docs/how_to/chatbots_tools/: For this guide, we'll be using a tool calling agent with a single tool for searching the web. The default will be powered by Tavily, but you can switch it out for any similar tool. The rest of this section will assume you're using Tavily.\n```", "url_final": "https://python.langchain.com/docs/how_to/document_loader_web/", "url_citable": "https://python.langchain.com/docs/how_to/document_loader_web/#vector-search-over-page-content", "title": "How to load web pages | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Vector search over page content‚Äã", "section_anchor": "vector-search-over-page-content", "fetched_at": "2025-10-06T21:55:27.905553", "has_code": true}
{"id": "3e359b1743d5056cbb5835252e4ea10aac73062e", "text": "Other web page loaders‚Äã\n\nFor a list of available LangChain web page loaders, please see this table.\n\n- Extracting content from specific sections\n- Vector search over page content", "url_final": "https://python.langchain.com/docs/how_to/document_loader_web/", "url_citable": "https://python.langchain.com/docs/how_to/document_loader_web/#other-web-page-loaders", "title": "How to load web pages | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Other web page loaders‚Äã", "section_anchor": "other-web-page-loaders", "fetched_at": "2025-10-06T21:55:27.905553", "has_code": false}
{"id": "e0b1f70b66b33be85659a607c5610a9d83e0c1cd", "text": "Load chat model‚Äã\n\nLet's first load a chat model:\n\n```\npip install -qU \"langchain[google-genai]\"\n```\n\n```\nimport getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n```", "url_final": "https://python.langchain.com/docs/how_to/summarize_map_reduce/", "url_citable": "https://python.langchain.com/docs/how_to/summarize_map_reduce/#load-chat-model", "title": "How to summarize text through parallelization | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Load chat model‚Äã", "section_anchor": "load-chat-model", "fetched_at": "2025-10-06T21:54:36.279599", "has_code": true}
{"id": "5e96ac867c6350d06cf56a341daaaa366f3f452e", "text": "Load documents‚Äã\n\nFirst we load in our documents. We will use WebBaseLoader to load a blog post, and split the documents into smaller sub-documents.\n\n```\nfrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_text_splitters import CharacterTextSplittertext_splitter = CharacterTextSplitter.from_tiktoken_encoder(    chunk_size=1000, chunk_overlap=0)loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")docs = loader.load()split_docs = text_splitter.split_documents(docs)print(f\"Generated {len(split_docs)} documents.\")\n```\n\n```\nCreated a chunk of size 1003, which is longer than the specified 1000``````outputGenerated 14 documents.\n```", "url_final": "https://python.langchain.com/docs/how_to/summarize_map_reduce/", "url_citable": "https://python.langchain.com/docs/how_to/summarize_map_reduce/#load-documents", "title": "How to summarize text through parallelization | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Load documents‚Äã", "section_anchor": "load-documents", "fetched_at": "2025-10-06T21:54:36.279599", "has_code": true}
{"id": "7a0e93f6953c5f3d7bd363f57d35f716fcbe3f89", "text": "Create graph‚Äã\n\nMap step‚Äã\n\nLet's first define the prompt associated with the map step, and associated it with the LLM via a chain:\n\n```\nfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatemap_prompt = ChatPromptTemplate.from_messages(    [(\"human\", \"Write a concise summary of the following:\\\\n\\\\n{context}\")])map_chain = map_prompt | llm | StrOutputParser()\n```\n\nReduce step‚Äã\n\nWe also define a chain that takes the document mapping results and reduces them into a single output.\n\n```\nreduce_template = \"\"\"The following is a set of summaries:{docs}Take these and distill it into a final, consolidated summaryof the main themes.\"\"\"reduce_prompt = ChatPromptTemplate([(\"human\", reduce_template)])reduce_chain = reduce_prompt | llm | StrOutputParser()\n```", "url_final": "https://python.langchain.com/docs/how_to/summarize_map_reduce/", "url_citable": "https://python.langchain.com/docs/how_to/summarize_map_reduce/#create-graph", "title": "How to summarize text through parallelization | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Create graph‚Äã", "section_anchor": "create-graph", "fetched_at": "2025-10-06T21:54:36.279599", "has_code": true}
{"id": "229acf3be8bd1a04fb9eef595edcdf5dccb2163c", "text": "Orchestration via LangGraph‚Äã\n\nBelow we implement a simple application that maps the summarization step on a list of documents, then reduces them using the above prompts.\n\nMap-reduce flows are particularly useful when texts are long compared to the context window of a LLM. For long texts, we need a mechanism that ensures that the context to be summarized in the reduce step does not exceed a model's context window size. Here we implement a recursive \"collapsing\" of the summaries: the inputs are partitioned based on a token limit, and summaries are generated of the partitions. This step is repeated until the total length of the summaries is within a desired limit, allowing for the summarization of arbitrary-length text.\n\nWe will need to install langgraph:\n\n```\npip install -qU langgraph\n```\n\n```\nimport operatorfrom typing import Annotated, List, Literal, TypedDictfrom langchain.chains.combine_documents.reduce import (    acollapse_docs,    split_list_of_docs,)from langchain_core.documents import Documentfrom langgraph.constants import Sendfrom langgraph.graph import END, START, StateGraphtoken_max = 1000def length_function(documents: List[Document]) -> int:    \"\"\"Get number of tokens for input contents.\"\"\"    return sum(llm.get_num_tokens(doc.page_content) for doc in documents)# This will be the overall state of the main graph.# It will contain the input document contents, corresponding# summaries, and a final summary.class OverallState(TypedDict):    # Notice here we use the operator.add    # This is because we want combine all the summaries we generate    # from individual nodes back into one list - this is essentially    # the \"reduce\" part    contents: List[str]    summaries: Annotated[list, operator.add]    collapsed_summaries: List[Document]    final_summary: str# This will be the state of the node that we will \"map\" all# documents to in order to generate summariesclass SummaryState(TypedDict):    content: str# Here we generate a summary, given a documentasync def generate_summary(state: SummaryState):    response = await map_chain.ainvoke(state[\"content\"])    return {\"summaries\": [response]}# Here we define the logic to map out over the documents# We will use this an edge in the graphdef map_summaries(state: OverallState):    # We will return a list of `Send` objects    # Each `Send` object consists of the name of a node in the graph    # as well as the state to send to that node    return [        Send(\"generate_summary\", {\"content\": content}) for content in state[\"contents\"]    ]def collect_summaries(state: OverallState):    return {        \"collapsed_summaries\": [Document(summary) for summary in state[\"summaries\"]]    }# Add node to collapse summariesasync def collapse_summaries(state: OverallState):    doc_lists = split_list_of_docs(        state[\"collapsed_summaries\"], length_function, token_max    )    results = []    for doc_list in doc_lists:        results.append(await acollapse_docs(doc_list, reduce_chain.ainvoke))    return {\"collapsed_summaries\": results}# This represents a conditional edge in the graph that determines# if we should collapse the summaries or notdef should_collapse(    state: OverallState,) -> Literal[\"collapse_summaries\", \"generate_final_summary\"]:    num_tokens = length_function(state[\"collapsed_summaries\"])    if num_tokens > token_max:        return \"collapse_summaries\"    else:        return \"generate_final_summary\"# Here we will generate the final summaryasync def generate_final_summary(state: OverallState):    response = await reduce_chain.ainvoke(state[\"collapsed_summaries\"])    return {\"final_summary\": response}# Construct the graph# Nodes:graph = StateGraph(OverallState)graph.add_node(\"generate_summary\", generate_summary)  # same as beforegraph.add_node(\"collect_summaries\", collect_summaries)graph.add_node(\"collapse_summaries\", collapse_summaries)graph.add_node(\"generate_final_summary\", generate_final_summary)# Edges:graph.add_conditional_edges(START, map_summaries, [\"generate_summary\"])graph.add_edge(\"generate_summary\", \"collect_summaries\")graph.add_conditional_edges(\"collect_summaries\", should_collapse)graph.add_conditional_edges(\"collapse_summaries\", should_collapse)graph.add_edge(\"generate_final_summary\", END)app = graph.compile()\n```\n\nLangGraph allows the graph structure to be plotted to help visualize its function:\n\n```\nfrom IPython.display import ImageImage(app.get_graph().draw_mermaid_png())\n```", "url_final": "https://python.langchain.com/docs/how_to/summarize_map_reduce/", "url_citable": "https://python.langchain.com/docs/how_to/summarize_map_reduce/#orchestration-via-langgraph", "title": "How to summarize text through parallelization | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Orchestration via LangGraph‚Äã", "section_anchor": "orchestration-via-langgraph", "fetched_at": "2025-10-06T21:54:36.279599", "has_code": true}
{"id": "8096908d1e8d236df1a17f2233e8036344fb4681", "text": "Invoke graph‚Äã\n\nWhen running the application, we can stream the graph to observe its sequence of steps. Below, we will simply print out the name of the step.\n\nNote that because we have a loop in the graph, it can be helpful to specify a recursion_limit on its execution. This will raise a specific error when the specified limit is exceeded.\n\n```\nasync for step in app.astream(    {\"contents\": [doc.page_content for doc in split_docs]},    {\"recursion_limit\": 10},):    print(list(step.keys()))\n```\n\n```\n['generate_summary']['generate_summary']['generate_summary']['generate_summary']['generate_summary']['generate_summary']['generate_summary']['generate_summary']['generate_summary']['generate_summary']['generate_summary']['generate_summary']['generate_summary']['generate_summary']['collect_summaries']['collapse_summaries']['collapse_summaries']['generate_final_summary']\n```\n\n```\nprint(step)\n```\n\n```\n{'generate_final_summary': {'final_summary': 'The consolidated summary of the main themes from the provided documents highlights the advancements and applications of large language models (LLMs) in artificial intelligence, particularly in autonomous agents and software development. Key themes include:\\n\\n1. **Integration of LLMs**: LLMs play a crucial role in enabling autonomous agents to perform complex tasks through advanced reasoning and decision-making techniques, such as Chain of Thought (CoT) and Tree of Thoughts.\\n\\n2. **Memory Management**: The categorization of memory into sensory, short-term, and long-term types parallels machine learning concepts, with short-term memory facilitating in-context learning and long-term memory enhanced by external storage solutions.\\n\\n3. **Tool Use and APIs**: Autonomous agents utilize external APIs to expand their capabilities, demonstrating adaptability and improved problem-solving skills.\\n\\n4. **Search Algorithms**: Various approximate nearest neighbor search algorithms, including Locality-Sensitive Hashing (LSH) and FAISS, are discussed for enhancing search efficiency in high-dimensional spaces.\\n\\n5. **Neuro-Symbolic Architectures**: The integration of neuro-symbolic systems, such as the MRKL framework, combines expert modules with LLMs to improve problem-solving, particularly in complex tasks.\\n\\n6. **Challenges and Innovations**: The documents address challenges like hallucination and inefficient planning in LLMs, alongside innovative methods such as Chain of Hindsight (CoH) and Algorithm Distillation (AD) for performance enhancement.\\n\\n7. **Software Development Practices**: The use of LLMs in software development is explored, particularly in creating structured applications like a Super Mario game using the model-view-controller (MVC) architecture, emphasizing task management, component organization, and documentation.\\n\\n8. **Limitations of LLMs**: Constraints such as finite context length and challenges in long-term planning are acknowledged, along with concerns regarding the reliability of natural language as an interface.\\n\\nOverall, the integration of LLMs and neuro-symbolic architectures signifies a significant evolution in AI, with ongoing research focused on enhancing planning, memory management, and problem-solving capabilities across various applications.'}}\n```", "url_final": "https://python.langchain.com/docs/how_to/summarize_map_reduce/", "url_citable": "https://python.langchain.com/docs/how_to/summarize_map_reduce/#invoke-graph", "title": "How to summarize text through parallelization | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Invoke graph‚Äã", "section_anchor": "invoke-graph", "fetched_at": "2025-10-06T21:54:36.279599", "has_code": true}
{"id": "8fd598fc1982af16538f01df3140073500bf817c", "text": "Next steps‚Äã\n\nCheck out the LangGraph documentation for detail on building with LangGraph, including this guide on the details of map-reduce in LangGraph.\n\nSee the summarization how-to guides for additional summarization strategies, including those designed for larger volumes of text.\n\nSee also this tutorial for more detail on summarization.\n\n- Map step\n- Reduce step\n- Orchestration via LangGraph", "url_final": "https://python.langchain.com/docs/how_to/summarize_map_reduce/", "url_citable": "https://python.langchain.com/docs/how_to/summarize_map_reduce/#next-steps", "title": "How to summarize text through parallelization | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Next steps‚Äã", "section_anchor": "next-steps", "fetched_at": "2025-10-06T21:54:36.279599", "has_code": false}
{"id": "da000c82c626fd94dd296d6528f7c8ba8a556855", "text": "Inferring by parameter type‚Äã\n\nTo access reference the active config object from your custom tool, you'll need to add a parameter to your tool's signature typed as RunnableConfig. When you invoke your tool, LangChain will inspect your tool's signature, look for a parameter typed as RunnableConfig, and if it exists, populate that parameter with the correct value.\n\nNote: The actual name of the parameter doesn't matter, only the typing.\n\nTo illustrate this, define a custom tool that takes a two parameters - one typed as a string, the other typed as RunnableConfig:\n\n```\n%pip install -qU langchain_core\n```\n\n```\nfrom langchain_core.runnables import RunnableConfigfrom langchain_core.tools import tool@toolasync def reverse_tool(text: str, special_config_param: RunnableConfig) -> str:    \"\"\"A test tool that combines input text with a configurable parameter.\"\"\"    return (text + special_config_param[\"configurable\"][\"additional_field\"])[::-1]\n```\n\nThen, if we invoke the tool with a config containing a configurable field, we can see that additional_field is passed through correctly:\n\n```\nawait reverse_tool.ainvoke(    {\"text\": \"abc\"}, config={\"configurable\": {\"additional_field\": \"123\"}})\n```\n\n```\n'321cba'\n```", "url_final": "https://python.langchain.com/docs/how_to/tool_configure/", "url_citable": "https://python.langchain.com/docs/how_to/tool_configure/#inferring-by-parameter-type", "title": "How to access the RunnableConfig from a tool | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Inferring by parameter type‚Äã", "section_anchor": "inferring-by-parameter-type", "fetched_at": "2025-10-06T21:56:36.663551", "has_code": true}
{"id": "c6374c5bf7916fe2790eb8d780027301f68730ff", "text": "Next steps‚Äã\n\nYou've now seen how to configure and stream events from within a tool. Next, check out the following guides for more on using tools:\n\n- Stream events from child runs within a custom tool\n- Pass tool results back to a model\n\nYou can also check out some more specific uses of tool calling:\n\n- Building tool-using chains and agents\n- Getting structured outputs from models\n\n- Inferring by parameter type\n- Next steps", "url_final": "https://python.langchain.com/docs/how_to/tool_configure/", "url_citable": "https://python.langchain.com/docs/how_to/tool_configure/#next-steps", "title": "How to access the RunnableConfig from a tool | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Next steps‚Äã", "section_anchor": "next-steps", "fetched_at": "2025-10-06T21:56:36.663551", "has_code": false}
{"id": "d10756af87c92c11eafb7dd70462679a7ed2f670", "text": "Create vector store‚Äã\n\nFirst we populate a vector store with some data. We will use a PineconeVectorStore, but this guide is compatible with any LangChain vector store that implements a .similarity_search_with_score method.\n\n```\nfrom langchain_core.documents import Documentfrom langchain_openai import OpenAIEmbeddingsfrom langchain_pinecone import PineconeVectorStoredocs = [    Document(        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},    ),    Document(        page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},    ),    Document(        page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",        metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},    ),    Document(        page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},    ),    Document(        page_content=\"Toys come alive and have a blast doing so\",        metadata={\"year\": 1995, \"genre\": \"animated\"},    ),    Document(        page_content=\"Three men walk into the Zone, three men walk out of the Zone\",        metadata={            \"year\": 1979,            \"director\": \"Andrei Tarkovsky\",            \"genre\": \"thriller\",            \"rating\": 9.9,        },    ),]vectorstore = PineconeVectorStore.from_documents(    docs, index_name=\"sample\", embedding=OpenAIEmbeddings())\n```", "url_final": "https://python.langchain.com/docs/how_to/add_scores_retriever/", "url_citable": "https://python.langchain.com/docs/how_to/add_scores_retriever/#create-vector-store", "title": "How to add scores to retriever results | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Create vector store‚Äã", "section_anchor": "create-vector-store", "fetched_at": "2025-10-06T21:54:55.773584", "has_code": true}
{"id": "5342f8ef21a90b424cee5ec91320382c2b8cbfc6", "text": "Retriever‚Äã\n\nTo obtain scores from a vector store retriever, we wrap the underlying vector store's .similarity_search_with_score method in a short function that packages scores into the associated document's metadata.\n\nWe add a @chain decorator to the function to create a Runnable that can be used similarly to a typical retriever.\n\n```\nfrom typing import Listfrom langchain_core.documents import Documentfrom langchain_core.runnables import chain@chaindef retriever(query: str) -> List[Document]:    docs, scores = zip(*vectorstore.similarity_search_with_score(query))    for doc, score in zip(docs, scores):        doc.metadata[\"score\"] = score    return docs\n```\n\n```\nresult = retriever.invoke(\"dinosaur\")result\n```\n\n```\n(Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'genre': 'science fiction', 'rating': 7.7, 'year': 1993.0, 'score': 0.84429127}), Document(page_content='Toys come alive and have a blast doing so', metadata={'genre': 'animated', 'year': 1995.0, 'score': 0.792038262}), Document(page_content='Three men walk into the Zone, three men walk out of the Zone', metadata={'director': 'Andrei Tarkovsky', 'genre': 'thriller', 'rating': 9.9, 'year': 1979.0, 'score': 0.751571238}), Document(page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', metadata={'director': 'Satoshi Kon', 'rating': 8.6, 'year': 2006.0, 'score': 0.747471571}))\n```\n\nNote that similarity scores from the retrieval step are included in the metadata of the above documents.", "url_final": "https://python.langchain.com/docs/how_to/add_scores_retriever/", "url_citable": "https://python.langchain.com/docs/how_to/add_scores_retriever/#retriever", "title": "How to add scores to retriever results | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Retriever‚Äã", "section_anchor": "retriever", "fetched_at": "2025-10-06T21:54:55.773584", "has_code": true}
{"id": "88f7ff08a4057e4b387add469b371ded428926d1", "text": "SelfQueryRetriever‚Äã\n\nSelfQueryRetriever will use a LLM to generate a query that is potentially structured-- for example, it can construct filters for the retrieval on top of the usual semantic-similarity driven selection. See this guide for more detail.\n\nSelfQueryRetriever includes a short (1 - 2 line) method _get_docs_with_query that executes the vectorstore search. We can subclass SelfQueryRetriever and override this method to propagate similarity scores.\n\nFirst, following the how-to guide, we will need to establish some metadata on which to filter:\n\n```\nfrom langchain.chains.query_constructor.base import AttributeInfofrom langchain.retrievers.self_query.base import SelfQueryRetrieverfrom langchain_openai import ChatOpenAImetadata_field_info = [    AttributeInfo(        name=\"genre\",        description=\"The genre of the movie. One of ['science fiction', 'comedy', 'drama', 'thriller', 'romance', 'action', 'animated']\",        type=\"string\",    ),    AttributeInfo(        name=\"year\",        description=\"The year the movie was released\",        type=\"integer\",    ),    AttributeInfo(        name=\"director\",        description=\"The name of the movie director\",        type=\"string\",    ),    AttributeInfo(        name=\"rating\", description=\"A 1-10 rating for the movie\", type=\"float\"    ),]document_content_description = \"Brief summary of a movie\"llm = ChatOpenAI(temperature=0)\n```\n\nWe then override the _get_docs_with_query to use the similarity_search_with_score method of the underlying vector store:\n\n```\nfrom typing import Any, Dictclass CustomSelfQueryRetriever(SelfQueryRetriever):    def _get_docs_with_query(        self, query: str, search_kwargs: Dict[str, Any]    ) -> List[Document]:        \"\"\"Get docs, adding score information.\"\"\"        docs, scores = zip(            *self.vectorstore.similarity_search_with_score(query, **search_kwargs)        )        for doc, score in zip(docs, scores):            doc.metadata[\"score\"] = score        return docs\n```\n\nInvoking this retriever will now include similarity scores in the document metadata. Note that the underlying structured-query capabilities of SelfQueryRetriever are retained.\n\n```\nretriever = CustomSelfQueryRetriever.from_llm(    llm,    vectorstore,    document_content_description,    metadata_field_info,)result = retriever.invoke(\"dinosaur movie with rating less than 8\")result\n```\n\n```\n(Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'genre': 'science fiction', 'rating': 7.7, 'year': 1993.0, 'score': 0.84429127}),)\n```", "url_final": "https://python.langchain.com/docs/how_to/add_scores_retriever/", "url_citable": "https://python.langchain.com/docs/how_to/add_scores_retriever/#selfqueryretriever", "title": "How to add scores to retriever results | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "SelfQueryRetriever‚Äã", "section_anchor": "selfqueryretriever", "fetched_at": "2025-10-06T21:54:55.773584", "has_code": true}
{"id": "4b1b7b7c81f3d100e7e891cb528e9ff940355e52", "text": "MultiVectorRetriever‚Äã\n\nMultiVectorRetriever allows you to associate multiple vectors with a single document. This can be useful in a number of applications. For example, we can index small chunks of a larger document and run the retrieval on the chunks, but return the larger \"parent\" document when invoking the retriever. ParentDocumentRetriever, a subclass of MultiVectorRetriever, includes convenience methods for populating a vector store to support this. Further applications are detailed in this how-to guide.\n\nTo propagate similarity scores through this retriever, we can again subclass MultiVectorRetriever and override a method. This time we will override _get_relevant_documents.\n\nFirst, we prepare some fake data. We generate fake \"whole documents\" and store them in a document store; here we will use a simple InMemoryStore.\n\n```\nfrom langchain.storage import InMemoryStorefrom langchain_text_splitters import RecursiveCharacterTextSplitter# The storage layer for the parent documentsdocstore = InMemoryStore()fake_whole_documents = [    (\"fake_id_1\", Document(page_content=\"fake whole document 1\")),    (\"fake_id_2\", Document(page_content=\"fake whole document 2\")),]docstore.mset(fake_whole_documents)\n```\n\nNext we will add some fake \"sub-documents\" to our vector store. We can link these sub-documents to the parent documents by populating the \"doc_id\" key in its metadata.\n\n```\ndocs = [    Document(        page_content=\"A snippet from a larger document discussing cats.\",        metadata={\"doc_id\": \"fake_id_1\"},    ),    Document(        page_content=\"A snippet from a larger document discussing discourse.\",        metadata={\"doc_id\": \"fake_id_1\"},    ),    Document(        page_content=\"A snippet from a larger document discussing chocolate.\",        metadata={\"doc_id\": \"fake_id_2\"},    ),]vectorstore.add_documents(docs)\n```\n\n```\n['62a85353-41ff-4346-bff7-be6c8ec2ed89', '5d4a0e83-4cc5-40f1-bc73-ed9cbad0ee15', '8c1d9a56-120f-45e4-ba70-a19cd19a38f4']\n```\n\nTo propagate the scores, we subclass MultiVectorRetriever and override its _get_relevant_documents method. Here we will make two changes:\n\n- We will add similarity scores to the metadata of the corresponding \"sub-documents\" using the similarity_search_with_score method of the underlying vector store as above;\n- We will include a list of these sub-documents in the metadata of the retrieved parent document. This surfaces what snippets of text were identified by the retrieval, together with their corresponding similarity scores.\n\n```\nfrom collections import defaultdictfrom langchain.retrievers import MultiVectorRetrieverfrom langchain_core.callbacks import CallbackManagerForRetrieverRunclass CustomMultiVectorRetriever(MultiVectorRetriever):    def _get_relevant_documents(        self, query: str, *, run_manager: CallbackManagerForRetrieverRun    ) -> List[Document]:        \"\"\"Get documents relevant to a query.        Args:            query: String to find relevant documents for            run_manager: The callbacks handler to use        Returns:            List of relevant documents        \"\"\"        results = self.vectorstore.similarity_search_with_score(            query, **self.search_kwargs        )        # Map doc_ids to list of sub-documents, adding scores to metadata        id_to_doc = defaultdict(list)        for doc, score in results:            doc_id = doc.metadata.get(\"doc_id\")            if doc_id:                doc.metadata[\"score\"] = score                id_to_doc[doc_id].append(doc)        # Fetch documents corresponding to doc_ids, retaining sub_docs in metadata        docs = []        for _id, sub_docs in id_to_doc.items():            docstore_docs = self.docstore.mget([_id])            if docstore_docs:                if doc := docstore_docs[0]:                    doc.metadata[\"sub_docs\"] = sub_docs                    docs.append(doc)        return docs\n```\n\nInvoking this retriever, we can see that it identifies the correct parent document, including the relevant snippet from the sub-document with similarity score.\n\n```\nretriever = CustomMultiVectorRetriever(vectorstore=vectorstore, docstore=docstore)retriever.invoke(\"cat\")\n```\n\n```\n[Document(page_content='fake whole document 1', metadata={'sub_docs': [Document(page_content='A snippet from a larger document discussing cats.', metadata={'doc_id': 'fake_id_1', 'score': 0.831276655})]})]\n```\n\n- Create vector store\n- Retriever\n- SelfQueryRetriever\n- MultiVectorRetriever", "url_final": "https://python.langchain.com/docs/how_to/add_scores_retriever/", "url_citable": "https://python.langchain.com/docs/how_to/add_scores_retriever/#multivectorretriever", "title": "How to add scores to retriever results | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "MultiVectorRetriever‚Äã", "section_anchor": "multivectorretriever", "fetched_at": "2025-10-06T21:54:55.773584", "has_code": true}
{"id": "426cb3a3c1760c1c0ae0a8f3ecd750bc61aedc66", "text": "How it works‚Äã\n\nLangChain indexing makes use of a record manager (RecordManager) that keeps track of document writes into the vector store.\n\nWhen indexing content, hashes are computed for each document, and the following information is stored in the record manager:\n\n- the document hash (hash of both page content and metadata)\n- write time\n- the source id -- each document should include information in its metadata to allow us to determine the ultimate source of this document", "url_final": "https://python.langchain.com/docs/how_to/indexing/", "url_citable": "https://python.langchain.com/docs/how_to/indexing/#how-it-works", "title": "How to use the LangChain indexing API | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "How it works‚Äã", "section_anchor": "how-it-works", "fetched_at": "2025-10-06T21:55:43.782566", "has_code": false}
{"id": "4289a813f7a3585164de64a0f0b1ba5e24cc753a", "text": "Deletion modes‚Äã\n\nWhen indexing documents into a vector store, it's possible that some existing documents in the vector store should be deleted. In certain situations you may want to remove any existing documents that are derived from the same sources as the new documents being indexed. In others you may want to delete all existing documents wholesale. The indexing API deletion modes let you pick the behavior you want:\n\nNone does not do any automatic clean up, allowing the user to manually do clean up of old content.\n\nincremental, full and scoped_full offer the following automated clean up:\n\n- If the content of the source document or derived documents has changed, all 3 modes will clean up (delete) previous versions of the content.\n- If the source document has been deleted (meaning it is not included in the documents currently being indexed), the full cleanup mode will delete it from the vector store correctly, but the incremental and scoped_full mode will not.\n\nWhen content is mutated (e.g., the source PDF file was revised) there will be a period of time during indexing when both the new and old versions may be returned to the user. This happens after the new content was written, but before the old version was deleted.\n\n- incremental indexing minimizes this period of time as it is able to do clean up continuously, as it writes.\n- full and scoped_full mode does the clean up after all batches have been written.", "url_final": "https://python.langchain.com/docs/how_to/indexing/", "url_citable": "https://python.langchain.com/docs/how_to/indexing/#deletion-modes", "title": "How to use the LangChain indexing API | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Deletion modes‚Äã", "section_anchor": "deletion-modes", "fetched_at": "2025-10-06T21:55:43.782566", "has_code": false}
{"id": "29d614c14f68cb15d01c0b646738e4ff2e0d8d98", "text": "Requirements‚Äã\n\n- document addition by id (add_documents method with ids argument)\n- delete by id (delete method with ids argument)\n\nCompatible Vectorstores: AnalyticDB, AstraDB, AwaDB, AzureCosmosDBNoSqlVectorSearch, AzureCosmosDBVectorSearch, AzureSearch, Bagel, Cassandra, Chroma, CouchbaseVectorStore, DashVector, DatabricksVectorSearch, DeepLake, Dingo, ElasticVectorSearch, ElasticsearchStore, FAISS, HanaDB, Milvus, MongoDBAtlasVectorSearch, MyScale, OpenSearchVectorSearch, PGVector, Pinecone, Qdrant, Redis, Rockset, ScaNN, SingleStoreDB, SupabaseVectorStore, SurrealDBStore, TimescaleVector, Vald, VDMS, Vearch, VespaStore, Weaviate, Yellowbrick, ZepVectorStore, TencentVectorDB, OpenSearchVectorSearch.", "url_final": "https://python.langchain.com/docs/how_to/indexing/", "url_citable": "https://python.langchain.com/docs/how_to/indexing/#requirements", "title": "How to use the LangChain indexing API | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Requirements‚Äã", "section_anchor": "requirements", "fetched_at": "2025-10-06T21:55:43.782566", "has_code": false}
{"id": "d5be4d1a3af01e47657fd94409a243e6c54a71ad", "text": "Caution‚Äã\n\nThe record manager relies on a time-based mechanism to determine what content can be cleaned up (when using full or incremental or scoped_full cleanup modes).\n\nIf two tasks run back-to-back, and the first task finishes before the clock time changes, then the second task may not be able to clean up content.\n\nThis is unlikely to be an issue in actual settings for the following reasons:\n\n- The RecordManager uses higher resolution timestamps.\n- The data would need to change between the first and the second tasks runs, which becomes unlikely if the time interval between the tasks is small.\n- Indexing tasks typically take more than a few ms.", "url_final": "https://python.langchain.com/docs/how_to/indexing/", "url_citable": "https://python.langchain.com/docs/how_to/indexing/#caution", "title": "How to use the LangChain indexing API | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Caution‚Äã", "section_anchor": "caution", "fetched_at": "2025-10-06T21:55:43.782566", "has_code": false}
{"id": "073fdac349334f27211d237f331eeedcec003443", "text": "Quickstart‚Äã\n\n```\nfrom langchain.indexes import SQLRecordManager, indexfrom langchain_core.documents import Documentfrom langchain_elasticsearch import ElasticsearchStorefrom langchain_openai import OpenAIEmbeddings\n```\n\nInitialize a vector store and set up the embeddings:\n\n```\ncollection_name = \"test_index\"embedding = OpenAIEmbeddings()vectorstore = ElasticsearchStore(    es_url=\"http://localhost:9200\", index_name=\"test_index\", embedding=embedding)\n```\n\nInitialize a record manager with an appropriate namespace.\n\nSuggestion: Use a namespace that takes into account both the vector store and the collection name in the vector store; e.g., 'redis/my_docs', 'chromadb/my_docs' or 'postgres/my_docs'.\n\n```\nnamespace = f\"elasticsearch/{collection_name}\"record_manager = SQLRecordManager(    namespace, db_url=\"sqlite:///record_manager_cache.sql\")\n```\n\nCreate a schema before using the record manager.\n\n```\nrecord_manager.create_schema()\n```\n\nLet's index some test documents:\n\n```\ndoc1 = Document(page_content=\"kitty\", metadata={\"source\": \"kitty.txt\"})doc2 = Document(page_content=\"doggy\", metadata={\"source\": \"doggy.txt\"})\n```\n\nIndexing into an empty vector store:\n\n```\ndef _clear():    \"\"\"Hacky helper method to clear content. See the `full` mode section to understand why it works.\"\"\"    index([], record_manager, vectorstore, cleanup=\"full\", source_id_key=\"source\")\n```\n\nNone deletion mode‚Äã\n\nThis mode does not do automatic clean up of old versions of content; however, it still takes care of content de-duplication.\n\n```\n_clear()\n```\n\n```\nindex(    [doc1, doc1, doc1, doc1, doc1],    record_manager,    vectorstore,    cleanup=None,    source_id_key=\"source\",)\n```\n\n```\n{'num_added': 1, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}\n```\n\n```\n_clear()\n```\n\n```\nindex([doc1, doc2], record_manager, vectorstore, cleanup=None, source_id_key=\"source\")\n```\n\n```\n{'num_added': 2, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}\n```\n\nSecond time around all content will be skipped:\n\n```\nindex([doc1, doc2], record_manager, vectorstore, cleanup=None, source_id_key=\"source\")\n```\n\n```\n{'num_added': 0, 'num_updated': 0, 'num_skipped': 2, 'num_deleted': 0}\n```\n\n\"incremental\" deletion mode‚Äã\n\n```\n_clear()\n```\n\n```\nindex(    [doc1, doc2],    record_manager,    vectorstore,    cleanup=\"incremental\",    source_id_key=\"source\",)\n```\n\n```\n{'num_added': 2, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}\n```\n\nIndexing again should result in both documents getting skipped -- also skipping the embedding operation!\n\n```\nindex(    [doc1, doc2],    record_manager,    vectorstore,    cleanup=\"incremental\",    source_id_key=\"source\",)\n```\n\n```\n{'num_added': 0, 'num_updated': 0, 'num_skipped': 2, 'num_deleted': 0}\n```\n\nIf we provide no documents with incremental indexing mode, nothing will change.\n\n```\nindex([], record_manager, vectorstore, cleanup=\"incremental\", source_id_key=\"source\")\n```\n\n```\n{'num_added': 0, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}\n```\n\nIf we mutate a document, the new version will be written and all old versions sharing the same source will be deleted.\n\n```\nchanged_doc_2 = Document(page_content=\"puppy\", metadata={\"source\": \"doggy.txt\"})\n```\n\n```\nindex(    [changed_doc_2],    record_manager,    vectorstore,    cleanup=\"incremental\",    source_id_key=\"source\",)\n```\n\n```\n{'num_added': 1, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 1}\n```", "url_final": "https://python.langchain.com/docs/how_to/indexing/", "url_citable": "https://python.langchain.com/docs/how_to/indexing/#quickstart", "title": "How to use the LangChain indexing API | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Quickstart‚Äã", "section_anchor": "quickstart", "fetched_at": "2025-10-06T21:55:43.782566", "has_code": true}
{"id": "03d189b3aeeec32c25a3117af082dc51bc06cfcb", "text": "\"full\" deletion mode‚Äã\n\nIn full mode the user should pass the full universe of content that should be indexed into the indexing function.\n\nAny documents that are not passed into the indexing function and are present in the vectorstore will be deleted!\n\nThis behavior is useful to handle deletions of source documents.\n\n```\n_clear()\n```\n\n```\nall_docs = [doc1, doc2]\n```\n\n```\nindex(all_docs, record_manager, vectorstore, cleanup=\"full\", source_id_key=\"source\")\n```\n\n```\n{'num_added': 2, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}\n```\n\nSay someone deleted the first doc:\n\n```\ndel all_docs[0]\n```\n\n```\nall_docs\n```\n\n```\n[Document(page_content='doggy', metadata={'source': 'doggy.txt'})]\n```\n\nUsing full mode will clean up the deleted content as well.\n\n```\nindex(all_docs, record_manager, vectorstore, cleanup=\"full\", source_id_key=\"source\")\n```\n\n```\n{'num_added': 0, 'num_updated': 0, 'num_skipped': 1, 'num_deleted': 1}\n```", "url_final": "https://python.langchain.com/docs/how_to/indexing/", "url_citable": "https://python.langchain.com/docs/how_to/indexing/#full-deletion-mode", "title": "How to use the LangChain indexing API | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "\"full\" deletion mode‚Äã", "section_anchor": "full-deletion-mode", "fetched_at": "2025-10-06T21:55:43.782566", "has_code": true}
{"id": "b948c42552283dc3897b11affefcee4d5c46c279", "text": "Source‚Äã\n\nThe metadata attribute contains a field called source. This source should be pointing at the ultimate provenance associated with the given document.\n\nFor example, if these documents are representing chunks of some parent document, the source for both documents should be the same and reference the parent document.\n\nIn general, source should always be specified. Only use a None, if you never intend to use incremental mode, and for some reason can't specify the source field correctly.\n\n```\nfrom langchain_text_splitters import CharacterTextSplitter\n```\n\n```\ndoc1 = Document(    page_content=\"kitty kitty kitty kitty kitty\", metadata={\"source\": \"kitty.txt\"})doc2 = Document(page_content=\"doggy doggy the doggy\", metadata={\"source\": \"doggy.txt\"})\n```\n\n```\nnew_docs = CharacterTextSplitter(    separator=\"t\", keep_separator=True, chunk_size=12, chunk_overlap=2).split_documents([doc1, doc2])new_docs\n```\n\n```\n[Document(page_content='kitty kit', metadata={'source': 'kitty.txt'}), Document(page_content='tty kitty ki', metadata={'source': 'kitty.txt'}), Document(page_content='tty kitty', metadata={'source': 'kitty.txt'}), Document(page_content='doggy doggy', metadata={'source': 'doggy.txt'}), Document(page_content='the doggy', metadata={'source': 'doggy.txt'})]\n```\n\n```\n_clear()\n```\n\n```\nindex(    new_docs,    record_manager,    vectorstore,    cleanup=\"incremental\",    source_id_key=\"source\",)\n```\n\n```\n{'num_added': 5, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}\n```\n\n```\nchanged_doggy_docs = [    Document(page_content=\"woof woof\", metadata={\"source\": \"doggy.txt\"}),    Document(page_content=\"woof woof woof\", metadata={\"source\": \"doggy.txt\"}),]\n```\n\nThis should delete the old versions of documents associated with doggy.txt source and replace them with the new versions.\n\n```\nindex(    changed_doggy_docs,    record_manager,    vectorstore,    cleanup=\"incremental\",    source_id_key=\"source\",)\n```\n\n```\n{'num_added': 2, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 2}\n```\n\n```\nvectorstore.similarity_search(\"dog\", k=30)\n```\n\n```\n[Document(page_content='woof woof', metadata={'source': 'doggy.txt'}), Document(page_content='woof woof woof', metadata={'source': 'doggy.txt'}), Document(page_content='tty kitty', metadata={'source': 'kitty.txt'}), Document(page_content='tty kitty ki', metadata={'source': 'kitty.txt'}), Document(page_content='kitty kit', metadata={'source': 'kitty.txt'})]\n```", "url_final": "https://python.langchain.com/docs/how_to/indexing/", "url_citable": "https://python.langchain.com/docs/how_to/indexing/#source", "title": "How to use the LangChain indexing API | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Source‚Äã", "section_anchor": "source", "fetched_at": "2025-10-06T21:55:43.782566", "has_code": true}
{"id": "d7b83a95fc737bb3a3b65b2366ebe5736405c187", "text": "Using with loaders‚Äã\n\nIndexing can accept either an iterable of documents or else any loader.\n\nAttention: The loader must set source keys correctly.\n\n```\nfrom langchain_core.document_loaders import BaseLoaderclass MyCustomLoader(BaseLoader):    def lazy_load(self):        text_splitter = CharacterTextSplitter(            separator=\"t\", keep_separator=True, chunk_size=12, chunk_overlap=2        )        docs = [            Document(page_content=\"woof woof\", metadata={\"source\": \"doggy.txt\"}),            Document(page_content=\"woof woof woof\", metadata={\"source\": \"doggy.txt\"}),        ]        yield from text_splitter.split_documents(docs)    def load(self):        return list(self.lazy_load())\n```\n\n```\n_clear()\n```\n\n```\nloader = MyCustomLoader()\n```\n\n```\nloader.load()\n```\n\n```\n[Document(page_content='woof woof', metadata={'source': 'doggy.txt'}), Document(page_content='woof woof woof', metadata={'source': 'doggy.txt'})]\n```\n\n```\nindex(loader, record_manager, vectorstore, cleanup=\"full\", source_id_key=\"source\")\n```\n\n```\n{'num_added': 2, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}\n```\n\n```\nvectorstore.similarity_search(\"dog\", k=30)\n```\n\n```\n[Document(page_content='woof woof', metadata={'source': 'doggy.txt'}), Document(page_content='woof woof woof', metadata={'source': 'doggy.txt'})]\n```\n\n- None deletion mode\n- \"incremental\" deletion mode\n- \"full\" deletion mode", "url_final": "https://python.langchain.com/docs/how_to/indexing/", "url_citable": "https://python.langchain.com/docs/how_to/indexing/#using-with-loaders", "title": "How to use the LangChain indexing API | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Using with loaders‚Äã", "section_anchor": "using-with-loaders", "fetched_at": "2025-10-06T21:55:43.782566", "has_code": true}
{"id": "a478b12123e119e09052c9039c9a66775a5019ce", "text": "How to use toolkits | ü¶úÔ∏èüîó LangChain\n\nToolkits are collections of tools that are designed to be used together for specific tasks. They have convenient loading methods.\n\nAll Toolkits expose a get_tools method which returns a list of tools. You can therefore do:\n\n```\n# Initialize a toolkittoolkit = ExampleTookit(...)# Get list of toolstools = toolkit.get_tools()# Create agentagent = create_agent_method(llm, tools, prompt)\n```", "url_final": "https://python.langchain.com/docs/how_to/toolkits/", "url_citable": "https://python.langchain.com/docs/how_to/toolkits/", "title": "How to use toolkits | ü¶úÔ∏èüîó LangChain", "section_level": 1, "section_heading": "How to use toolkits | ü¶úÔ∏èüîó LangChain", "section_anchor": null, "fetched_at": "2025-10-06T21:54:39.472216", "has_code": true}
{"id": "55fc3d3883e725cb7acbf0b02fb3746485502235", "text": "Next steps‚Äã\n\nYou've now learned how to attach callbacks to a chain.\n\nNext, check out the other how-to guides in this section, such as how to pass callbacks in at runtime.\n\n- Next steps", "url_final": "https://python.langchain.com/docs/how_to/callbacks_attach/", "url_citable": "https://python.langchain.com/docs/how_to/callbacks_attach/#next-steps", "title": "How to attach callbacks to a runnable | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Next steps‚Äã", "section_anchor": "next-steps", "fetched_at": "2025-10-06T21:54:58.849906", "has_code": false}
{"id": "e3fe0791cf2c5084f8cad599c11f886fc18c27d9", "text": "How to convert tools to OpenAI Functions | ü¶úÔ∏èüîó LangChain\n\nThis notebook goes over how to use LangChain tools as OpenAI functions.\n\n```\n%pip install -qU langchain-community langchain-openai\n```\n\n```\nfrom langchain_community.tools import MoveFileToolfrom langchain_core.messages import HumanMessagefrom langchain_core.utils.function_calling import convert_to_openai_functionfrom langchain_openai import ChatOpenAI\n```\n\n```\nmodel = ChatOpenAI(model=\"gpt-3.5-turbo\")\n```\n\n```\ntools = [MoveFileTool()]functions = [convert_to_openai_function(t) for t in tools]\n```\n\n```\nfunctions[0]\n```\n\n```\n{'name': 'move_file', 'description': 'Move or rename a file from one location to another', 'parameters': {'type': 'object',  'properties': {'source_path': {'description': 'Path of the file to move',    'type': 'string'},   'destination_path': {'description': 'New path for the moved file',    'type': 'string'}},  'required': ['source_path', 'destination_path']}}\n```\n\n```\nmessage = model.invoke(    [HumanMessage(content=\"move file foo to bar\")], functions=functions)\n```\n\n```\nmessage\n```\n\n```\nAIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"source_path\": \"foo\",\\n  \"destination_path\": \"bar\"\\n}', 'name': 'move_file'}})\n```\n\n```\nmessage.additional_kwargs[\"function_call\"]\n```\n\n```\n{'name': 'move_file', 'arguments': '{\\n  \"source_path\": \"foo\",\\n  \"destination_path\": \"bar\"\\n}'}\n```\n\nWith OpenAI chat models we can also automatically bind and convert function-like objects with bind_functions\n\n```\nmodel_with_functions = model.bind_functions(tools)model_with_functions.invoke([HumanMessage(content=\"move file foo to bar\")])\n```\n\n```\nAIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"source_path\": \"foo\",\\n  \"destination_path\": \"bar\"\\n}', 'name': 'move_file'}})\n```\n\nOr we can use the update OpenAI API that uses tools and tool_choice instead of functions and function_call by using ChatOpenAI.bind_tools:\n\n```\nmodel_with_tools = model.bind_tools(tools)model_with_tools.invoke([HumanMessage(content=\"move file foo to bar\")])\n```\n\n```\nAIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_btkY3xV71cEVAOHnNa5qwo44', 'function': {'arguments': '{\\n  \"source_path\": \"foo\",\\n  \"destination_path\": \"bar\"\\n}', 'name': 'move_file'}, 'type': 'function'}]})\n```", "url_final": "https://python.langchain.com/docs/how_to/tools_as_openai_functions/", "url_citable": "https://python.langchain.com/docs/how_to/tools_as_openai_functions/", "title": "How to convert tools to OpenAI Functions | ü¶úÔ∏èüîó LangChain", "section_level": 1, "section_heading": "How to convert tools to OpenAI Functions | ü¶úÔ∏èüîó LangChain", "section_anchor": null, "fetched_at": "2025-10-06T21:56:41.882727", "has_code": true}
{"id": "eb23d40da912a5d828d271a0cdbf5ab0df6577a0", "text": "Setup‚Äã\n\nJupyter Notebook‚Äã\n\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebooks. Going through guides in an interactive environment is a great way to better understand them. See here for instructions on how to install.\n\nInstallation‚Äã\n\nTo install LangChain run:\n\n- Pip\n- Conda\n\n```\npip install langchain\n```\n\n```\nconda install langchain -c conda-forge\n```\n\nFor more details, see our Installation guide.\n\nLangSmith‚Äã\n\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\n\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\n\n```\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"export LANGSMITH_PROJECT=\"default\" # or any other project name\n```\n\nOr, if in a notebook, you can set them with:\n\n```\nimport getpassimport ostry:    # load environment variables from .env file (requires `python-dotenv`)    from dotenv import load_dotenv    load_dotenv()except ImportError:    passos.environ[\"LANGSMITH_TRACING\"] = \"true\"if \"LANGSMITH_API_KEY\" not in os.environ:    os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(        prompt=\"Enter your LangSmith API key (optional): \"    )if \"LANGSMITH_PROJECT\" not in os.environ:    os.environ[\"LANGSMITH_PROJECT\"] = getpass.getpass(        prompt='Enter your LangSmith Project Name (default = \"default\"): '    )    if not os.environ.get(\"LANGSMITH_PROJECT\"):        os.environ[\"LANGSMITH_PROJECT\"] = \"default\"\n```", "url_final": "https://python.langchain.com/docs/tutorials/llm_chain/", "url_citable": "https://python.langchain.com/docs/tutorials/llm_chain/#setup", "title": "Build a simple LLM application with chat models and prompt templates | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Setup‚Äã", "section_anchor": "setup", "fetched_at": "2025-10-06T21:54:05.425859", "has_code": true}
{"id": "fb90df04c174afadee3f3d0bd1e81754f0ce8343", "text": "Using Language Models‚Äã\n\nFirst up, let's learn how to use a language model by itself. LangChain supports many different language models that you can use interchangeably. For details on getting started with a specific model, refer to supported integrations.\n\n```\npip install -qU \"langchain[google-genai]\"\n```\n\n```\nimport getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelmodel = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n```\n\nLet's first use the model directly. ChatModels are instances of LangChain Runnables, which means they expose a standard interface for interacting with them. To simply call the model, we can pass in a list of messages to the .invoke method.\n\n```\nfrom langchain_core.messages import HumanMessage, SystemMessagemessages = [    SystemMessage(content=\"Translate the following from English into Italian\"),    HumanMessage(content=\"hi!\"),]model.invoke(messages)\n```\n\n```\nAIMessage(content='Ciao!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 20, 'total_tokens': 23, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-32654a56-627c-40e1-a141-ad9350bbfd3e-0', usage_metadata={'input_tokens': 20, 'output_tokens': 3, 'total_tokens': 23, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n```\n\nIf we've enabled LangSmith, we can see that this run is logged to LangSmith, and can see the LangSmith trace. The LangSmith trace reports token usage information, latency, standard model parameters (such as temperature), and other information.\n\nNote that ChatModels receive message objects as input and generate message objects as output. In addition to text content, message objects convey conversational roles and hold important data, such as tool calls and token usage counts.\n\nLangChain also supports chat model inputs via strings or OpenAI format. The following are equivalent:\n\n```\nmodel.invoke(\"Hello\")model.invoke([{\"role\": \"user\", \"content\": \"Hello\"}])model.invoke([HumanMessage(\"Hello\")])\n```\n\nStreaming‚Äã\n\nBecause chat models are Runnables, they expose a standard interface that includes async and streaming modes of invocation. This allows us to stream individual tokens from a chat model:\n\n```\nfor token in model.stream(messages):    print(token.content, end=\"|\")\n```\n\n```\n|C|iao|!||\n```\n\nYou can find more details on streaming chat model outputs in this guide.", "url_final": "https://python.langchain.com/docs/tutorials/llm_chain/", "url_citable": "https://python.langchain.com/docs/tutorials/llm_chain/#using-language-models", "title": "Build a simple LLM application with chat models and prompt templates | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Using Language Models‚Äã", "section_anchor": "using-language-models", "fetched_at": "2025-10-06T21:54:05.425859", "has_code": true}
{"id": "a526e9ea35e18453436203d83caac27323520156", "text": "Prompt Templates‚Äã\n\nRight now we are passing a list of messages directly into the language model. Where does this list of messages come from? Usually, it is constructed from a combination of user input and application logic. This application logic usually takes the raw user input and transforms it into a list of messages ready to pass to the language model. Common transformations include adding a system message or formatting a template with the user input.\n\nPrompt templates are a concept in LangChain designed to assist with this transformation. They take in raw user input and return data (a prompt) that is ready to pass into a language model.\n\nLet's create a prompt template here. It will take in two user variables:\n\n- language: The language to translate text into\n- text: The text to translate\n\n```\nfrom langchain_core.prompts import ChatPromptTemplatesystem_template = \"Translate the following from English into {language}\"prompt_template = ChatPromptTemplate.from_messages(    [(\"system\", system_template), (\"user\", \"{text}\")])\n```\n\nNote that ChatPromptTemplate supports multiple message roles in a single template. We format the language parameter into the system message, and the user text into a user message.\n\nThe input to this prompt template is a dictionary. We can play around with this prompt template by itself to see what it does by itself\n\n```\nprompt = prompt_template.invoke({\"language\": \"Italian\", \"text\": \"hi!\"})prompt\n```\n\n```\nChatPromptValue(messages=[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})])\n```\n\nWe can see that it returns a ChatPromptValue that consists of two messages. If we want to access the messages directly we do:\n\n```\nprompt.to_messages()\n```\n\n```\n[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})]\n```\n\nFinally, we can invoke the chat model on the formatted prompt:\n\n```\nresponse = model.invoke(prompt)print(response.content)\n```\n\n```\nCiao!\n```\n\nMessage content can contain both text and content blocks with additional structure. See this guide for more information.\n\nIf we take a look at the LangSmith trace, we can see exactly what prompt the chat model receives, along with token usage information, latency, standard model parameters (such as temperature), and other information.", "url_final": "https://python.langchain.com/docs/tutorials/llm_chain/", "url_citable": "https://python.langchain.com/docs/tutorials/llm_chain/#prompt-templates", "title": "Build a simple LLM application with chat models and prompt templates | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Prompt Templates‚Äã", "section_anchor": "prompt-templates", "fetched_at": "2025-10-06T21:54:05.425859", "has_code": true}
{"id": "a1361b4d6e96e0583fddb2455cec10c895fc89b4", "text": "Conclusion‚Äã\n\nThat's it! In this tutorial you've learned how to create your first simple LLM application. You've learned how to work with language models, how to create a prompt template, and how to get great observability into applications you create with LangSmith.\n\nThis just scratches the surface of what you will want to learn to become a proficient AI Engineer. Luckily - we've got a lot of other resources!\n\nFor further reading on the core concepts of LangChain, we've got detailed Conceptual Guides.\n\nIf you have more specific questions on these concepts, check out the following sections of the how-to guides:\n\n- Chat models\n- Prompt templates\n\nAnd the LangSmith docs:\n\n- LangSmith\n\n- Jupyter Notebook\n- Installation\n- LangSmith\n\n- Streaming", "url_final": "https://python.langchain.com/docs/tutorials/llm_chain/", "url_citable": "https://python.langchain.com/docs/tutorials/llm_chain/#conclusion", "title": "Build a simple LLM application with chat models and prompt templates | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Conclusion‚Äã", "section_anchor": "conclusion", "fetched_at": "2025-10-06T21:54:05.425859", "has_code": false}
{"id": "17f01228346887587613eb7e2fff143074738ba6", "text": "Basic usage‚Äã\n\n```\nfrom langchain_core.messages import (    AIMessage,    HumanMessage,    SystemMessage,    merge_message_runs,)messages = [    SystemMessage(\"you're a good assistant.\"),    SystemMessage(\"you always respond with a joke.\"),    HumanMessage([{\"type\": \"text\", \"text\": \"i wonder why it's called langchain\"}]),    HumanMessage(\"and who is harrison chasing anyways\"),    AIMessage(        'Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it!'    ),    AIMessage(\"Why, he's probably chasing after the last cup of coffee in the office!\"),]merged = merge_message_runs(messages)print(\"\\n\\n\".join([repr(x) for x in merged]))\n```\n\n```\nSystemMessage(content=\"you're a good assistant.\\nyou always respond with a joke.\", additional_kwargs={}, response_metadata={})HumanMessage(content=[{'type': 'text', 'text': \"i wonder why it's called langchain\"}, 'and who is harrison chasing anyways'], additional_kwargs={}, response_metadata={})AIMessage(content='Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it!\\nWhy, he\\'s probably chasing after the last cup of coffee in the office!', additional_kwargs={}, response_metadata={})\n```\n\nNotice that if the contents of one of the messages to merge is a list of content blocks then the merged message will have a list of content blocks. And if both messages to merge have string contents then those are concatenated with a newline character.", "url_final": "https://python.langchain.com/docs/how_to/merge_message_runs/", "url_citable": "https://python.langchain.com/docs/how_to/merge_message_runs/#basic-usage", "title": "How to merge consecutive messages of the same type | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Basic usage‚Äã", "section_anchor": "basic-usage", "fetched_at": "2025-10-06T21:55:53.211868", "has_code": true}
{"id": "e9a4c0199c1291fc13814597da86a9745a796f48", "text": "Chaining‚Äã\n\nmerge_message_runs can be used imperatively (like above) or declaratively, making it easy to compose with other components in a chain:\n\n```\nfrom langchain_anthropic import ChatAnthropicllm = ChatAnthropic(model=\"claude-3-7-sonnet-20250219\", temperature=0)# Notice we don't pass in messages. This creates# a RunnableLambda that takes messages as inputmerger = merge_message_runs()chain = merger | llmchain.invoke(messages)\n```\n\n```\nAIMessage(content='\\n\\nAs for the actual answer, LangChain is named for connecting (chaining) language models together with other components. And Harrison Chase is one of the co-founders of LangChain, not someone being chased! \\n\\nBut I like to think he\\'s running after runaway tokens that escaped from the embedding space. \"Come back here, you vectors!\"', additional_kwargs={}, response_metadata={'id': 'msg_018MF8xBrM1ztw69XTx3Uxcy', 'model': 'claude-3-7-sonnet-20250219', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 84, 'output_tokens': 80, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-7-sonnet-20250219'}, id='run--caa1b9d6-a554-40ad-95cd-268938d8223b-0', usage_metadata={'input_tokens': 84, 'output_tokens': 80, 'total_tokens': 164, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})\n```\n\nLooking at the LangSmith trace we can see that before the messages are passed to the model they are merged: https://smith.langchain.com/public/ab558677-cac9-4c59-9066-1ecce5bcd87c/r\n\nLooking at just the merger, we can see that it's a Runnable object that can be invoked like all Runnables:\n\n```\nmerger.invoke(messages)\n```\n\n```\n[SystemMessage(content=\"you're a good assistant.\\nyou always respond with a joke.\", additional_kwargs={}, response_metadata={}), HumanMessage(content=[{'type': 'text', 'text': \"i wonder why it's called langchain\"}, 'and who is harrison chasing anyways'], additional_kwargs={}, response_metadata={}), AIMessage(content='Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it!\\nWhy, he\\'s probably chasing after the last cup of coffee in the office!', additional_kwargs={}, response_metadata={})]\n```\n\nmerge_message_runs can also be placed after a prompt:\n\n```\nfrom langchain_core.prompts import ChatPromptTemplateprompt = ChatPromptTemplate(    [        (\"system\", \"You're great a {skill}\"),        (\"system\", \"You're also great at explaining things\"),        (\"human\", \"{query}\"),    ])chain = prompt | merger | llmchain.invoke({\"skill\": \"math\", \"query\": \"what's the definition of a convergent series\"})\n```\n\n```\nAIMessage(content=\"# Definition of a Convergent Series\\n\\nA series is a sum of terms in a sequence, typically written as:\\n\\n$$\\\\sum_{n=1}^{\\\\infty} a_n = a_1 + a_2 + a_3 + \\\\ldots$$\\n\\nA series is called **convergent** if the sequence of partial sums approaches a finite limit.\\n\\n## Formal Definition\\n\\nLet's define the sequence of partial sums:\\n$$S_N = \\\\sum_{n=1}^{N} a_n = a_1 + a_2 + \\\\ldots + a_N$$\\n\\nA series $\\\\sum_{n=1}^{\\\\infty} a_n$ is convergent if and only if:\\n- The limit of the partial sums exists and is finite\\n- That is, there exists a finite number $S$ such that $\\\\lim_{N \\\\to \\\\infty} S_N = S$\\n\\nIf this limit exists, we say the series converges to $S$, and we write:\\n$$\\\\sum_{n=1}^{\\\\infty} a_n = S$$\\n\\nIf the limit doesn't exist or is infinite, the series is called divergent.\", additional_kwargs={}, response_metadata={'id': 'msg_018ypyi2MTjV6S7jCydSqDn9', 'model': 'claude-3-7-sonnet-20250219', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 29, 'output_tokens': 273, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-7-sonnet-20250219'}, id='run--5de0ca29-d031-48f7-bc75-671eade20b74-0', usage_metadata={'input_tokens': 29, 'output_tokens': 273, 'total_tokens': 302, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})\n```\n\nLangSmith Trace", "url_final": "https://python.langchain.com/docs/how_to/merge_message_runs/", "url_citable": "https://python.langchain.com/docs/how_to/merge_message_runs/#chaining", "title": "How to merge consecutive messages of the same type | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Chaining‚Äã", "section_anchor": "chaining", "fetched_at": "2025-10-06T21:55:53.211868", "has_code": true}
{"id": "d871165f0d192c7334cc4c3a0fdbfc6ca3bc833a", "text": "API reference‚Äã\n\nFor a complete description of all arguments head to the API reference\n\n- Setup\n- Basic usage\n- Chaining\n- API reference", "url_final": "https://python.langchain.com/docs/how_to/merge_message_runs/", "url_citable": "https://python.langchain.com/docs/how_to/merge_message_runs/#api-reference", "title": "How to merge consecutive messages of the same type | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "API reference‚Äã", "section_anchor": "api-reference", "fetched_at": "2025-10-06T21:55:53.211868", "has_code": false}
{"id": "c8933ab62e28db555e07d6d426c14b883698c79b", "text": "Examples‚Äã\n\nIn order to use an example selector, we need to create a list of examples. These should generally be example inputs and outputs. For this demo purpose, let's imagine we are selecting examples of how to translate English to Italian.\n\n```\nexamples = [    {\"input\": \"hi\", \"output\": \"ciao\"},    {\"input\": \"bye\", \"output\": \"arrivederci\"},    {\"input\": \"soccer\", \"output\": \"calcio\"},]\n```", "url_final": "https://python.langchain.com/docs/how_to/example_selectors/", "url_citable": "https://python.langchain.com/docs/how_to/example_selectors/#examples", "title": "How to use example selectors | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Examples‚Äã", "section_anchor": "examples", "fetched_at": "2025-10-06T21:54:19.652556", "has_code": true}
{"id": "3aeecd02cf30cfc05cb9acfe7b5e4b14d037a808", "text": "Custom Example Selector‚Äã\n\nLet's write an example selector that chooses what example to pick based on the length of the word.\n\n```\nfrom langchain_core.example_selectors.base import BaseExampleSelectorclass CustomExampleSelector(BaseExampleSelector):    def __init__(self, examples):        self.examples = examples    def add_example(self, example):        self.examples.append(example)    def select_examples(self, input_variables):        # This assumes knowledge that part of the input will be a 'text' key        new_word = input_variables[\"input\"]        new_word_length = len(new_word)        # Initialize variables to store the best match and its length difference        best_match = None        smallest_diff = float(\"inf\")        # Iterate through each example        for example in self.examples:            # Calculate the length difference with the first word of the example            current_diff = abs(len(example[\"input\"]) - new_word_length)            # Update the best match if the current one is closer in length            if current_diff < smallest_diff:                smallest_diff = current_diff                best_match = example        return [best_match]\n```\n\n```\nexample_selector = CustomExampleSelector(examples)\n```\n\n```\nexample_selector.select_examples({\"input\": \"okay\"})\n```\n\n```\n[{'input': 'bye', 'output': 'arrivederci'}]\n```\n\n```\nexample_selector.add_example({\"input\": \"hand\", \"output\": \"mano\"})\n```\n\n```\nexample_selector.select_examples({\"input\": \"okay\"})\n```\n\n```\n[{'input': 'hand', 'output': 'mano'}]\n```", "url_final": "https://python.langchain.com/docs/how_to/example_selectors/", "url_citable": "https://python.langchain.com/docs/how_to/example_selectors/#custom-example-selector", "title": "How to use example selectors | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Custom Example Selector‚Äã", "section_anchor": "custom-example-selector", "fetched_at": "2025-10-06T21:54:19.652556", "has_code": true}
{"id": "a287568136c8a517aab421496e8121b5e9425287", "text": "Use in a Prompt‚Äã\n\nWe can now use this example selector in a prompt\n\n```\nfrom langchain_core.prompts.few_shot import FewShotPromptTemplatefrom langchain_core.prompts.prompt import PromptTemplateexample_prompt = PromptTemplate.from_template(\"Input: {input} -> Output: {output}\")\n```\n\n```\nprompt = FewShotPromptTemplate(    example_selector=example_selector,    example_prompt=example_prompt,    suffix=\"Input: {input} -> Output:\",    prefix=\"Translate the following words from English to Italian:\",    input_variables=[\"input\"],)print(prompt.format(input=\"word\"))\n```\n\n```\nTranslate the following words from English to Italian:Input: hand -> Output: manoInput: word -> Output:\n```", "url_final": "https://python.langchain.com/docs/how_to/example_selectors/", "url_citable": "https://python.langchain.com/docs/how_to/example_selectors/#use-in-a-prompt", "title": "How to use example selectors | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Use in a Prompt‚Äã", "section_anchor": "use-in-a-prompt", "fetched_at": "2025-10-06T21:54:19.652556", "has_code": true}
{"id": "4a66c186a78582b269b4a7737efdbd3791a42453", "text": "Example Selector Types‚Äã\n\n- Examples\n- Custom Example Selector\n- Use in a Prompt\n- Example Selector Types", "url_final": "https://python.langchain.com/docs/how_to/example_selectors/", "url_citable": "https://python.langchain.com/docs/how_to/example_selectors/#example-selector-types", "title": "How to use example selectors | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Example Selector Types‚Äã", "section_anchor": "example-selector-types", "fetched_at": "2025-10-06T21:54:19.652556", "has_code": false}
{"id": "3b6db95689a5986506c137e3e503174df9133375", "text": "Retrieval Example‚Äã\n\nIn the example below, we see a more real-world use case where we use RunnablePassthrough along with RunnableParallel in a chain to properly format inputs to a prompt:\n\n```\nfrom langchain_community.vectorstores import FAISSfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnablePassthroughfrom langchain_openai import ChatOpenAI, OpenAIEmbeddingsvectorstore = FAISS.from_texts(    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())retriever = vectorstore.as_retriever()template = \"\"\"Answer the question based only on the following context:{context}Question: {question}\"\"\"prompt = ChatPromptTemplate.from_template(template)model = ChatOpenAI()retrieval_chain = (    {\"context\": retriever, \"question\": RunnablePassthrough()}    | prompt    | model    | StrOutputParser())retrieval_chain.invoke(\"where did harrison work?\")\n```\n\n```\n'Harrison worked at Kensho.'\n```\n\nHere the input to prompt is expected to be a map with keys \"context\" and \"question\". The user input is just the question. So we need to get the context using our retriever and passthrough the user input under the \"question\" key. The RunnablePassthrough allows us to pass on the user's question to the prompt and model.", "url_final": "https://python.langchain.com/docs/how_to/passthrough/", "url_citable": "https://python.langchain.com/docs/how_to/passthrough/#retrieval-example", "title": "How to pass through arguments from one step to the next | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Retrieval Example‚Äã", "section_anchor": "retrieval-example", "fetched_at": "2025-10-06T21:54:46.654276", "has_code": true}
{"id": "1a711bd0991bcd9b4f3ba41bcf227c39b14a9b4c", "text": "Next steps‚Äã\n\nNow you've learned how to pass data through your chains to help format the data flowing through your chains.\n\nTo learn more, see the other how-to guides on runnables in this section.\n\n- Retrieval Example\n- Next steps", "url_final": "https://python.langchain.com/docs/how_to/passthrough/", "url_citable": "https://python.langchain.com/docs/how_to/passthrough/#next-steps", "title": "How to pass through arguments from one step to the next | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Next steps‚Äã", "section_anchor": "next-steps", "fetched_at": "2025-10-06T21:54:46.654276", "has_code": false}
{"id": "9e300c70548dde7d27f6ae433dea6b705288fe35", "text": "Overview‚Äã\n\nA typical RAG application has two main components:\n\nIndexing: a pipeline for ingesting data from a source and indexing it. This usually happens offline.\n\nRetrieval and generation: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n\nNote: the indexing portion of this tutorial will largely follow the semantic search tutorial.\n\nThe most common full sequence from raw data to answer looks like:\n\nIndexing‚Äã\n\n- Load: First we need to load our data. This is done with Document Loaders.\n- Split: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won't fit in a model's finite context window.\n- Store: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model.\n\nRetrieval and generation‚Äã\n\n- Retrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\n- Generate: A ChatModel / LLM produces an answer using a prompt that includes both the question with the retrieved data\n\nOnce we've indexed our data, we will use LangGraph as our orchestration framework to implement the retrieval and generation steps.", "url_final": "https://python.langchain.com/docs/tutorials/rag/", "url_citable": "https://python.langchain.com/docs/tutorials/rag/#overview", "title": "Build a Retrieval Augmented Generation (RAG) App: Part 1 | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Overview‚Äã", "section_anchor": "overview", "fetched_at": "2025-10-06T21:54:11.442119", "has_code": false}
{"id": "7e0a2130d30986ae823bb53010dc5725de65009f", "text": "Setup‚Äã\n\nJupyter Notebook‚Äã\n\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebooks. Going through guides in an interactive environment is a great way to better understand them. See here for instructions on how to install.\n\nInstallation‚Äã\n\nThis tutorial requires these langchain dependencies:\n\n- Pip\n- Conda\n\n```\n%pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph\n```\n\n```\nconda install langchain-text-splitters langchain-community langgraph -c conda-forge\n```\n\nFor more details, see our Installation guide.\n\nLangSmith‚Äã\n\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\n\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\n\n```\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"\n```\n\nOr, if in a notebook, you can set them with:\n\n```\nimport getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n```", "url_final": "https://python.langchain.com/docs/tutorials/rag/", "url_citable": "https://python.langchain.com/docs/tutorials/rag/#setup", "title": "Build a Retrieval Augmented Generation (RAG) App: Part 1 | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Setup‚Äã", "section_anchor": "setup", "fetched_at": "2025-10-06T21:54:11.442119", "has_code": true}
{"id": "9738d002ccc0e823bc2a83a26bcb045f39fdf252", "text": "Components‚Äã\n\nWe will need to select three components from LangChain's suite of integrations.\n\n```\npip install -qU \"langchain[google-genai]\"\n```\n\n```\nimport getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n```\n\n```\npip install -qU langchain-openai\n```\n\n```\nimport getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"):  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")from langchain_openai import OpenAIEmbeddingsembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n```\n\n```\npip install -qU langchain-core\n```\n\n```\nfrom langchain_core.vectorstores import InMemoryVectorStorevector_store = InMemoryVectorStore(embeddings)\n```", "url_final": "https://python.langchain.com/docs/tutorials/rag/", "url_citable": "https://python.langchain.com/docs/tutorials/rag/#components", "title": "Build a Retrieval Augmented Generation (RAG) App: Part 1 | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Components‚Äã", "section_anchor": "components", "fetched_at": "2025-10-06T21:54:11.442119", "has_code": true}
{"id": "0d8490e59b25e717465e1528865cfd4dff36b867", "text": "Preview‚Äã\n\nIn this guide we‚Äôll build an app that answers questions about the website's content. The specific website we will use is the LLM Powered Autonomous Agents blog post by Lilian Weng, which allows us to ask questions about the contents of the post.\n\nWe can create a simple indexing pipeline and RAG chain to do this in ~50 lines of code.\n\n```\nimport bs4from langchain import hubfrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_core.documents import Documentfrom langchain_text_splitters import RecursiveCharacterTextSplitterfrom langgraph.graph import START, StateGraphfrom typing_extensions import List, TypedDict# Load and chunk contents of the blogloader = WebBaseLoader(    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),    bs_kwargs=dict(        parse_only=bs4.SoupStrainer(            class_=(\"post-content\", \"post-title\", \"post-header\")        )    ),)docs = loader.load()text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)all_splits = text_splitter.split_documents(docs)# Index chunks_ = vector_store.add_documents(documents=all_splits)# Define prompt for question-answering# N.B. for non-US LangSmith endpoints, you may need to specify# api_url=\"https://api.smith.langchain.com\" in hub.pull.prompt = hub.pull(\"rlm/rag-prompt\")# Define state for applicationclass State(TypedDict):    question: str    context: List[Document]    answer: str# Define application stepsdef retrieve(state: State):    retrieved_docs = vector_store.similarity_search(state[\"question\"])    return {\"context\": retrieved_docs}def generate(state: State):    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})    response = llm.invoke(messages)    return {\"answer\": response.content}# Compile application and testgraph_builder = StateGraph(State).add_sequence([retrieve, generate])graph_builder.add_edge(START, \"retrieve\")graph = graph_builder.compile()\n```\n\n```\nresponse = graph.invoke({\"question\": \"What is Task Decomposition?\"})print(response[\"answer\"])\n```\n\n```\nTask Decomposition is the process of breaking down a complicated task into smaller, manageable steps to facilitate easier execution and understanding. Techniques like Chain of Thought (CoT) and Tree of Thoughts (ToT) guide models to think step-by-step, allowing them to explore multiple reasoning possibilities. This method enhances performance on complex tasks and provides insight into the model's thinking process.\n```\n\nCheck out the LangSmith trace.", "url_final": "https://python.langchain.com/docs/tutorials/rag/", "url_citable": "https://python.langchain.com/docs/tutorials/rag/#preview", "title": "Build a Retrieval Augmented Generation (RAG) App: Part 1 | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Preview‚Äã", "section_anchor": "preview", "fetched_at": "2025-10-06T21:54:11.442119", "has_code": true}
{"id": "a46793fe9a2eec3c5c45a482f65fcfd8d048bb9e", "text": "Detailed walkthrough‚Äã\n\nLet‚Äôs go through the above code step-by-step to really understand what‚Äôs going on.", "url_final": "https://python.langchain.com/docs/tutorials/rag/", "url_citable": "https://python.langchain.com/docs/tutorials/rag/#detailed-walkthrough", "title": "Build a Retrieval Augmented Generation (RAG) App: Part 1 | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Detailed walkthrough‚Äã", "section_anchor": "detailed-walkthrough", "fetched_at": "2025-10-06T21:54:11.442119", "has_code": false}
{"id": "4a71cb7465560d0ac4d02fddead5de03d608c771", "text": "1. Indexing‚Äã\n\nThis section is an abbreviated version of the content in the semantic search tutorial. If you're comfortable with document loaders, embeddings, and vector stores, feel free to skip to the next section on retrieval and generation.\n\nLoading documents‚Äã\n\nWe need to first load the blog post contents. We can use DocumentLoaders for this, which are objects that load in data from a source and return a list of Document objects.\n\nIn this case we‚Äôll use the WebBaseLoader, which uses urllib to load HTML from web URLs and BeautifulSoup to parse it to text. We can customize the HTML -> text parsing by passing in parameters into the BeautifulSoup parser via bs_kwargs (see BeautifulSoup docs). In this case only HTML tags with class ‚Äúpost-content‚Äù, ‚Äúpost-title‚Äù, or ‚Äúpost-header‚Äù are relevant, so we‚Äôll remove all others.\n\n```\nimport bs4from langchain_community.document_loaders import WebBaseLoader# Only keep post title, headers, and content from the full HTML.bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))loader = WebBaseLoader(    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),    bs_kwargs={\"parse_only\": bs4_strainer},)docs = loader.load()assert len(docs) == 1print(f\"Total characters: {len(docs[0].page_content)}\")\n```\n\n```\nTotal characters: 43131\n```\n\n```\nprint(docs[0].page_content[:500])\n```\n\n```\n      LLM Powered Autonomous Agents    Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian WengBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.Agent System Overview#In\n```\n\nDocumentLoader: Object that loads data from a source as list of Documents.\n\n- Docs: Detailed documentation on how to use DocumentLoaders.\n- Integrations: 160+ integrations to choose from.\n- Interface: API reference for the base interface.\n\nSplitting documents‚Äã\n\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\n\nTo handle this we‚Äôll split the Document into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\n\nAs in the semantic search tutorial, we use a RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\n\n```\nfrom langchain_text_splitters import RecursiveCharacterTextSplittertext_splitter = RecursiveCharacterTextSplitter(    chunk_size=1000,  # chunk size (characters)    chunk_overlap=200,  # chunk overlap (characters)    add_start_index=True,  # track index in original document)all_splits = text_splitter.split_documents(docs)print(f\"Split blog post into {len(all_splits)} sub-documents.\")\n```\n\n```\nSplit blog post into 66 sub-documents.\n```\n\nTextSplitter: Object that splits a list of Documents into smaller chunks. Subclass of DocumentTransformers.\n\n- Learn more about splitting text using different methods by reading the how-to docs\n- Code (py or js)\n- Scientific papers\n- Interface: API reference for the base interface.\n\nDocumentTransformer: Object that performs a transformation on a list of Document objects.\n\n- Docs: Detailed documentation on how to use DocumentTransformers\n- Integrations\n- Interface: API reference for the base interface.", "url_final": "https://python.langchain.com/docs/tutorials/rag/", "url_citable": "https://python.langchain.com/docs/tutorials/rag/#indexing", "title": "Build a Retrieval Augmented Generation (RAG) App: Part 1 | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "1. Indexing‚Äã", "section_anchor": "indexing", "fetched_at": "2025-10-06T21:54:11.442119", "has_code": true}
{"id": "66f09355a629e32b51a7050f7cfc457d33fd18b5", "text": "Storing documents‚Äã\n\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the semantic search tutorial, our approach is to embed the contents of each document split and insert these embeddings into a vector store. Given an input query, we can then use vector search to retrieve relevant documents.\n\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the start of the tutorial.\n\n```\ndocument_ids = vector_store.add_documents(documents=all_splits)print(document_ids[:3])\n```\n\n```\n['07c18af6-ad58-479a-bfb1-d508033f9c64', '9000bf8e-1993-446f-8d4d-f4e507ba4b8f', 'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6']\n```\n\nEmbeddings: Wrapper around a text embedding model, used for converting text to embeddings.\n\n- Docs: Detailed documentation on how to use embeddings.\n- Integrations: 30+ integrations to choose from.\n- Interface: API reference for the base interface.\n\nVectorStore: Wrapper around a vector database, used for storing and querying embeddings.\n\n- Docs: Detailed documentation on how to use vector stores.\n- Integrations: 40+ integrations to choose from.\n- Interface: API reference for the base interface.\n\nThis completes the Indexing portion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.", "url_final": "https://python.langchain.com/docs/tutorials/rag/", "url_citable": "https://python.langchain.com/docs/tutorials/rag/#storing-documents", "title": "Build a Retrieval Augmented Generation (RAG) App: Part 1 | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Storing documents‚Äã", "section_anchor": "storing-documents", "fetched_at": "2025-10-06T21:54:11.442119", "has_code": true}
{"id": "0734adc3c4207ab5a51c543c69a692db06fb9c33", "text": "2. Retrieval and Generation‚Äã\n\nNow let‚Äôs write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\n\nFor generation, we will use the chat model selected at the start of the tutorial.\n\nWe‚Äôll use a prompt for RAG that is checked into the LangChain prompt hub (here).\n\n```\nfrom langchain import hub# N.B. for non-US LangSmith endpoints, you may need to specify# api_url=\"https://api.smith.langchain.com\" in hub.pull.prompt = hub.pull(\"rlm/rag-prompt\")example_messages = prompt.invoke(    {\"context\": \"(context goes here)\", \"question\": \"(question goes here)\"}).to_messages()assert len(example_messages) == 1print(example_messages[0].content)\n```\n\n```\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.Question: (question goes here) Context: (context goes here) Answer:\n```\n\nWe'll use LangGraph to tie together the retrieval and generation steps into a single application. This will bring a number of benefits:\n\n- We can define our application logic once and automatically support multiple invocation modes, including streaming, async, and batched calls.\n- We get streamlined deployments via LangGraph Platform.\n- LangSmith will automatically trace the steps of our application together.\n- We can easily add key features to our application, including persistence and human-in-the-loop approval, with minimal code changes.\n\nTo use LangGraph, we need to define three things:\n\n- The state of our application;\n- The nodes of our application (i.e., application steps);\n- The \"control flow\" of our application (e.g., the ordering of the steps).\n\nThe state of our application controls what data is input to the application, transferred between steps, and output by the application. It is typically a TypedDict, but can also be a Pydantic BaseModel.\n\nFor a simple RAG application, we can just keep track of the input question, retrieved context, and generated answer:\n\n```\nfrom langchain_core.documents import Documentfrom typing_extensions import List, TypedDictclass State(TypedDict):    question: str    context: List[Document]    answer: str\n```\n\nLet's start with a simple sequence of two steps: retrieval and generation.\n\n```\ndef retrieve(state: State):    retrieved_docs = vector_store.similarity_search(state[\"question\"])    return {\"context\": retrieved_docs}def generate(state: State):    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})    response = llm.invoke(messages)    return {\"answer\": response.content}\n```\n\nOur retrieval step simply runs a similarity search using the input question, and the generation step formats the retrieved context and original question into a prompt for the chat model.\n\nFinally, we compile our application into a single graph object. In this case, we are just connecting the retrieval and generation steps into a single sequence.\n\n```\nfrom langgraph.graph import START, StateGraphgraph_builder = StateGraph(State).add_sequence([retrieve, generate])graph_builder.add_edge(START, \"retrieve\")graph = graph_builder.compile()\n```\n\nLangGraph also comes with built-in utilities for visualizing the control flow of your application:\n\n```\nfrom IPython.display import Image, displaydisplay(Image(graph.get_graph().draw_mermaid_png()))\n```\n\nLangGraph is not required to build a RAG application. Indeed, we can implement the same application logic through invocations of the individual components:\n\n```\nquestion = \"...\"retrieved_docs = vector_store.similarity_search(question)docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)prompt = prompt.invoke({\"question\": question, \"context\": docs_content})answer = llm.invoke(prompt)\n```\n\nThe benefits of LangGraph include:\n\n- Support for multiple invocation modes: this logic would need to be rewritten if we wanted to stream output tokens, or stream the results of individual steps;\n- Automatic support for tracing via LangSmith and deployments via LangGraph Platform;\n- Support for persistence, human-in-the-loop, and other features.\n\nMany use-cases demand RAG in a conversational experience, such that a user can receive context-informed answers via a stateful conversation. As we will see in Part 2 of the tutorial, LangGraph's management and persistence of state simplifies these applications enormously.\n\nLet's test our application! LangGraph supports multiple invocation modes, including sync, async, and streaming.\n\nInvoke:\n\n```\nresult = graph.invoke({\"question\": \"What is Task Decomposition?\"})print(f\"Context: {result['context']}\\n\\n\")print(f\"Answer: {result['answer']}\")\n```\n\n```\nContext: [Document(id='a42dc78b-8f76-472a-9e25-180508af74f3', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1585}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.'), Document(id='c0e45887-d0b0-483d-821a-bb5d8316d51d', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2192}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'), Document(id='4cc7f318-35f5-440f-a4a4-145b5f0b918d', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 29630}, page_content='Resources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.'), Document(id='f621ade4-9b0d-471f-a522-44eb5feeba0c', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 19373}, page_content=\"(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\")]Answer: Task decomposition is a technique used to break down complex tasks into smaller, manageable steps, allowing for more efficient problem-solving. This can be achieved through methods like chain of thought prompting or the tree of thoughts approach, which explores multiple reasoning possibilities at each step. It can be initiated through simple prompts, task-specific instructions, or human inputs.\n```\n\nStream steps:\n\n```\nfor step in graph.stream(    {\"question\": \"What is Task Decomposition?\"}, stream_mode=\"updates\"):    print(f\"{step}\\n\\n----------------\\n\")\n```\n\n```\n{'retrieve': {'context': [Document(id='a42dc78b-8f76-472a-9e25-180508af74f3', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1585}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.'), Document(id='c0e45887-d0b0-483d-821a-bb5d8316d51d', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2192}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'), Document(id='4cc7f318-35f5-440f-a4a4-145b5f0b918d', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 29630}, page_content='Resources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.'), Document(id='f621ade4-9b0d-471f-a522-44eb5feeba0c', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 19373}, page_content=\"(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\")]}}----------------{'generate': {'answer': 'Task decomposition is the process of breaking down a complex task into smaller, more manageable steps. This technique, often enhanced by methods like Chain of Thought (CoT) or Tree of Thoughts, allows models to reason through tasks systematically and improves performance by clarifying the thought process. It can be achieved through simple prompts, task-specific instructions, or human inputs.'}}----------------\n```\n\nStream tokens:\n\n```\nfor message, metadata in graph.stream(    {\"question\": \"What is Task Decomposition?\"}, stream_mode=\"messages\"):    print(message.content, end=\"|\")\n```\n\n```\n|Task| decomposition| is| the| process| of| breaking| down| complex| tasks| into| smaller|,| more| manageable| steps|.| It| can| be| achieved| through| techniques| like| Chain| of| Thought| (|Co|T|)| prompting|,| which| encourages| the| model| to| think| step| by| step|,| or| through| more| structured| methods| like| the| Tree| of| Thoughts|.| This| approach| not| only| simplifies| task| execution| but| also| provides| insights| into| the| model|'s| reasoning| process|.||\n```\n\nFor async invocations, use:\n\n```\nresult = await graph.ainvoke(...)\n```\n\nand\n\n```\nasync for step in graph.astream(...):\n```\n\nNote that by storing the retrieved context in the state of the graph, we recover sources for the model's generated answer in the \"context\" field of the state. See this guide on returning sources for more detail.\n\nChat models take in a sequence of messages and return a message.\n\n- Docs\n- Integrations: 25+ integrations to choose from.\n- Interface: API reference for the base interface.\n\nCustomizing the prompt\n\nAs shown above, we can load prompts (e.g., this RAG prompt) from the prompt hub. The prompt can also be easily customized. For example:\n\n```\nfrom langchain_core.prompts import PromptTemplatetemplate = \"\"\"Use the following pieces of context to answer the question at the end.If you don't know the answer, just say that you don't know, don't try to make up an answer.Use three sentences maximum and keep the answer as concise as possible.Always say \"thanks for asking!\" at the end of the answer.{context}Question: {question}Helpful Answer:\"\"\"custom_rag_prompt = PromptTemplate.from_template(template)\n```", "url_final": "https://python.langchain.com/docs/tutorials/rag/", "url_citable": "https://python.langchain.com/docs/tutorials/rag/#orchestration", "title": "Build a Retrieval Augmented Generation (RAG) App: Part 1 | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "2. Retrieval and Generation‚Äã", "section_anchor": "orchestration", "fetched_at": "2025-10-06T21:54:11.442119", "has_code": true}
{"id": "11850fee95d5afe48b9f4f19790eb140c2951cad", "text": "Query analysis‚Äã\n\nSo far, we are executing the retrieval using the raw input query. However, there are some advantages to allowing a model to generate the query for retrieval purposes. For example:\n\n- In addition to semantic search, we can build in structured filters (e.g., \"Find documents since the year 2020.\");\n- The model can rewrite user queries, which may be multifaceted or include irrelevant language, into more effective search queries.\n\nQuery analysis employs models to transform or construct optimized search queries from raw user input. We can easily incorporate a query analysis step into our application. For illustrative purposes, let's add some metadata to the documents in our vector store. We will add some (contrived) sections to the document which we can filter on later.\n\n```\ntotal_documents = len(all_splits)third = total_documents // 3for i, document in enumerate(all_splits):    if i < third:        document.metadata[\"section\"] = \"beginning\"    elif i < 2 * third:        document.metadata[\"section\"] = \"middle\"    else:        document.metadata[\"section\"] = \"end\"all_splits[0].metadata\n```\n\n```\n{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 8, 'section': 'beginning'}\n```\n\nWe will need to update the documents in our vector store. We will use a simple InMemoryVectorStore for this, as we will use some of its specific features (i.e., metadata filtering). Refer to the vector store integration documentation for relevant features of your chosen vector store.\n\n```\nfrom langchain_core.vectorstores import InMemoryVectorStorevector_store = InMemoryVectorStore(embeddings)_ = vector_store.add_documents(all_splits)\n```\n\nLet's next define a schema for our search query. We will use structured output for this purpose. Here we define a query as containing a string query and a document section (either \"beginning\", \"middle\", or \"end\"), but this can be defined however you like.\n\n```\nfrom typing import Literalfrom typing_extensions import Annotatedclass Search(TypedDict):    \"\"\"Search query.\"\"\"    query: Annotated[str, ..., \"Search query to run.\"]    section: Annotated[        Literal[\"beginning\", \"middle\", \"end\"],        ...,        \"Section to query.\",    ]\n```\n\nFinally, we add a step to our LangGraph application to generate a query from the user's raw input:\n\n```\nclass State(TypedDict):    question: str    query: Search    context: List[Document]    answer: strdef analyze_query(state: State):    structured_llm = llm.with_structured_output(Search)    query = structured_llm.invoke(state[\"question\"])    return {\"query\": query}def retrieve(state: State):    query = state[\"query\"]    retrieved_docs = vector_store.similarity_search(        query[\"query\"],        filter=lambda doc: doc.metadata.get(\"section\") == query[\"section\"],    )    return {\"context\": retrieved_docs}def generate(state: State):    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})    response = llm.invoke(messages)    return {\"answer\": response.content}graph_builder = StateGraph(State).add_sequence([analyze_query, retrieve, generate])graph_builder.add_edge(START, \"analyze_query\")graph = graph_builder.compile()\n```\n\n```\nfrom typing import Literalimport bs4from langchain import hubfrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_core.documents import Documentfrom langchain_core.vectorstores import InMemoryVectorStorefrom langchain_text_splitters import RecursiveCharacterTextSplitterfrom langgraph.graph import START, StateGraphfrom typing_extensions import Annotated, List, TypedDict# Load and chunk contents of the blogloader = WebBaseLoader(    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),    bs_kwargs=dict(        parse_only=bs4.SoupStrainer(            class_=(\"post-content\", \"post-title\", \"post-header\")        )    ),)docs = loader.load()text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)all_splits = text_splitter.split_documents(docs)# Update metadata (illustration purposes)total_documents = len(all_splits)third = total_documents // 3for i, document in enumerate(all_splits):    if i < third:        document.metadata[\"section\"] = \"beginning\"    elif i < 2 * third:        document.metadata[\"section\"] = \"middle\"    else:        document.metadata[\"section\"] = \"end\"# Index chunksvector_store = InMemoryVectorStore(embeddings)_ = vector_store.add_documents(all_splits)# Define schema for searchclass Search(TypedDict):    \"\"\"Search query.\"\"\"    query: Annotated[str, ..., \"Search query to run.\"]    section: Annotated[        Literal[\"beginning\", \"middle\", \"end\"],        ...,        \"Section to query.\",    ]# Define prompt for question-answeringprompt = hub.pull(\"rlm/rag-prompt\")# Define state for applicationclass State(TypedDict):    question: str    query: Search    context: List[Document]    answer: strdef analyze_query(state: State):    structured_llm = llm.with_structured_output(Search)    query = structured_llm.invoke(state[\"question\"])    return {\"query\": query}def retrieve(state: State):    query = state[\"query\"]    retrieved_docs = vector_store.similarity_search(        query[\"query\"],        filter=lambda doc: doc.metadata.get(\"section\") == query[\"section\"],    )    return {\"context\": retrieved_docs}def generate(state: State):    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})    response = llm.invoke(messages)    return {\"answer\": response.content}graph_builder = StateGraph(State).add_sequence([analyze_query, retrieve, generate])graph_builder.add_edge(START, \"analyze_query\")graph = graph_builder.compile()\n```\n\n```\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n```\n\nWe can test our implementation by specifically asking for context from the end of the post. Note that the model includes different information in its answer.\n\n```\nfor step in graph.stream(    {\"question\": \"What does the end of the post say about Task Decomposition?\"},    stream_mode=\"updates\",):    print(f\"{step}\\n\\n----------------\\n\")\n```\n\n```\n{'analyze_query': {'query': {'query': 'Task Decomposition', 'section': 'end'}}}----------------{'retrieve': {'context': [Document(id='d6cef137-e1e8-4ddc-91dc-b62bd33c6020', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 39221, 'section': 'end'}, page_content='Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.'), Document(id='d1834ae1-eb6a-43d7-a023-08dfa5028799', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 39086, 'section': 'end'}, page_content='}\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:'), Document(id='ca7f06e4-2c2e-4788-9a81-2418d82213d9', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 32942, 'section': 'end'}, page_content='}\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:'), Document(id='1fcc2736-30f4-4ef6-90f2-c64af92118cb', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 35127, 'section': 'end'}, page_content='\"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease')]}}----------------{'generate': {'answer': 'The end of the post highlights that task decomposition faces challenges in long-term planning and adapting to unexpected errors. LLMs struggle with adjusting their plans, making them less robust compared to humans who learn from trial and error. This indicates a limitation in effectively exploring the solution space and handling complex tasks.'}}----------------\n```\n\nIn both the streamed steps and the LangSmith trace, we can now observe the structured query that was fed into the retrieval step.\n\nQuery Analysis is a rich problem with a wide range of approaches. Refer to the how-to guides for more examples.", "url_final": "https://python.langchain.com/docs/tutorials/rag/", "url_citable": "https://python.langchain.com/docs/tutorials/rag/#query-analysis", "title": "Build a Retrieval Augmented Generation (RAG) App: Part 1 | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Query analysis‚Äã", "section_anchor": "query-analysis", "fetched_at": "2025-10-06T21:54:11.442119", "has_code": true}
{"id": "b75f52b849d93c934d9bd0ca54c2ab032e9090e7", "text": "Next steps‚Äã\n\nWe've covered the steps to build a basic Q&A app over data:\n\n- Loading data with a Document Loader\n- Chunking the indexed data with a Text Splitter to make it more easily usable by a model\n- Embedding the data and storing the data in a vectorstore\n- Retrieving the previously stored chunks in response to incoming questions\n- Generating an answer using the retrieved chunks as context.\n\nIn Part 2 of the tutorial, we will extend the implementation here to accommodate conversation-style interactions and multi-step retrieval processes.\n\nFurther reading:\n\n- Return sources: Learn how to return source documents\n- Streaming: Learn how to stream outputs and intermediate steps\n- Add chat history: Learn how to add chat history to your app\n- Retrieval conceptual guide: A high-level overview of specific retrieval techniques\n\n- Indexing\n- Retrieval and generation\n\n- Jupyter Notebook\n- Installation\n- LangSmith\n\n- Loading documents\n- Splitting documents\n- Storing documents", "url_final": "https://python.langchain.com/docs/tutorials/rag/", "url_citable": "https://python.langchain.com/docs/tutorials/rag/#next-steps", "title": "Build a Retrieval Augmented Generation (RAG) App: Part 1 | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Next steps‚Äã", "section_anchor": "next-steps", "fetched_at": "2025-10-06T21:54:11.442119", "has_code": false}
{"id": "82e6c858411e578e72e0356f9d26e7b3a3b2167d", "text": "How to reorder retrieved results to mitigate the \"lost in the middle\" effect | ü¶úÔ∏èüîó LangChain\n\nSubstantial performance degradations in RAG applications have been documented as the number of retrieved documents grows (e.g., beyond ten). In brief: models are liable to miss relevant information in the middle of long contexts.\n\nBy contrast, queries against vector stores will typically return documents in descending order of relevance (e.g., as measured by cosine similarity of embeddings).\n\nTo mitigate the \"lost in the middle\" effect, you can re-order documents after retrieval such that the most relevant documents are positioned at extrema (e.g., the first and last pieces of context), and the least relevant documents are positioned in the middle. In some cases this can help surface the most relevant information to LLMs.\n\nThe LongContextReorder document transformer implements this re-ordering procedure. Below we demonstrate an example.\n\n```\n%pip install -qU langchain langchain-community langchain-openai\n```\n\nFirst we embed some artificial documents and index them in a basic in-memory vector store. We will use OpenAI embeddings, but any LangChain vector store or embeddings model will suffice.\n\n```\nfrom langchain_core.vectorstores import InMemoryVectorStorefrom langchain_openai import OpenAIEmbeddings# Get embeddings.embeddings = OpenAIEmbeddings()texts = [    \"Basquetball is a great sport.\",    \"Fly me to the moon is one of my favourite songs.\",    \"The Celtics are my favourite team.\",    \"This is a document about the Boston Celtics\",    \"I simply love going to the movies\",    \"The Boston Celtics won the game by 20 points\",    \"This is just a random text.\",    \"Elden Ring is one of the best games in the last 15 years.\",    \"L. Kornet is one of the best Celtics players.\",    \"Larry Bird was an iconic NBA player.\",]# Create a retrieverretriever = InMemoryVectorStore.from_texts(texts, embedding=embeddings).as_retriever(    search_kwargs={\"k\": 10})query = \"What can you tell me about the Celtics?\"# Get relevant documents ordered by relevance scoredocs = retriever.invoke(query)for doc in docs:    print(f\"- {doc.page_content}\")\n```\n\n```\n- The Celtics are my favourite team.- This is a document about the Boston Celtics- The Boston Celtics won the game by 20 points- L. Kornet is one of the best Celtics players.- Basquetball is a great sport.- Larry Bird was an iconic NBA player.- This is just a random text.- I simply love going to the movies- Fly me to the moon is one of my favourite songs.- Elden Ring is one of the best games in the last 15 years.\n```\n\nNote that documents are returned in descending order of relevance to the query. The LongContextReorder document transformer will implement the re-ordering described above:\n\n```\nfrom langchain_community.document_transformers import LongContextReorder# Reorder the documents:# Less relevant document will be at the middle of the list and more# relevant elements at beginning / end.reordering = LongContextReorder()reordered_docs = reordering.transform_documents(docs)# Confirm that the 4 relevant documents are at beginning and end.for doc in reordered_docs:    print(f\"- {doc.page_content}\")\n```\n\n```\n- This is a document about the Boston Celtics- L. Kornet is one of the best Celtics players.- Larry Bird was an iconic NBA player.- I simply love going to the movies- Elden Ring is one of the best games in the last 15 years.- Fly me to the moon is one of my favourite songs.- This is just a random text.- Basquetball is a great sport.- The Boston Celtics won the game by 20 points- The Celtics are my favourite team.\n```\n\nBelow, we show how to incorporate the re-ordered documents into a simple question-answering chain:\n\n```\nfrom langchain.chains.combine_documents import create_stuff_documents_chainfrom langchain_core.prompts import PromptTemplatefrom langchain_openai import ChatOpenAIllm = ChatOpenAI(model=\"gpt-4o-mini\")prompt_template = \"\"\"Given these texts:-----{context}-----Please answer the following question:{query}\"\"\"prompt = PromptTemplate(    template=prompt_template,    input_variables=[\"context\", \"query\"],)# Create and invoke the chain:chain = create_stuff_documents_chain(llm, prompt)response = chain.invoke({\"context\": reordered_docs, \"query\": query})print(response)\n```\n\n```\nThe Boston Celtics are a professional basketball team known for their rich history and success in the NBA. L. Kornet is recognized as one of the best players on the team, and the Celtics recently won a game by 20 points. The Celtics are favored by some fans, as indicated by the statement, \"The Celtics are my favourite team.\" Overall, they have a strong following and are considered a significant part of basketball culture.\n```", "url_final": "https://python.langchain.com/docs/how_to/long_context_reorder/", "url_citable": "https://python.langchain.com/docs/how_to/long_context_reorder/", "title": "How to reorder retrieved results to mitigate the \"lost in the middle\" effect | ü¶úÔ∏èüîó LangChain", "section_level": 1, "section_heading": "How to reorder retrieved results to mitigate the \"lost in the middle\" effect | ü¶úÔ∏èüîó LangChain", "section_anchor": null, "fetched_at": "2025-10-06T21:55:50.989916", "has_code": true}
{"id": "6ba803bb621c68a97570b8d298d7347b753fc298", "text": "Using LangSmith‚Äã\n\nYou can use LangSmith to help track token usage in your LLM application. See the LangSmith quick start guide.", "url_final": "https://python.langchain.com/docs/how_to/llm_token_usage_tracking/", "url_citable": "https://python.langchain.com/docs/how_to/llm_token_usage_tracking/#using-langsmith", "title": "How to track token usage for LLMs | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Using LangSmith‚Äã", "section_anchor": "using-langsmith", "fetched_at": "2025-10-06T21:55:47.992592", "has_code": false}
{"id": "fca37c9700f363f463dfe635372d975b7ab605b9", "text": "Using callbacks‚Äã\n\nThere are some API-specific callback context managers that allow you to track token usage across multiple calls. You'll need to check whether such an integration is available for your particular model.\n\nIf such an integration is not available for your model, you can create a custom callback manager by adapting the implementation of the OpenAI callback manager.\n\nOpenAI‚Äã\n\nLet's first look at an extremely simple example of tracking token usage for a single Chat model call.\n\nThe callback handler does not currently support streaming token counts for legacy language models (e.g., langchain_openai.OpenAI). For support in a streaming context, refer to the corresponding guide for chat models here.\n\nSingle call‚Äã\n\n```\nfrom langchain_community.callbacks import get_openai_callbackfrom langchain_openai import OpenAIllm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\")with get_openai_callback() as cb:    result = llm.invoke(\"Tell me a joke\")    print(result)    print(\"---\")print()print(f\"Total Tokens: {cb.total_tokens}\")print(f\"Prompt Tokens: {cb.prompt_tokens}\")print(f\"Completion Tokens: {cb.completion_tokens}\")print(f\"Total Cost (USD): ${cb.total_cost}\")\n```\n\n```\nWhy don't scientists trust atoms?Because they make up everything.---Total Tokens: 18Prompt Tokens: 4Completion Tokens: 14Total Cost (USD): $3.4e-05\n```\n\nMultiple calls‚Äã\n\nAnything inside the context manager will get tracked. Here's an example of using it to track multiple calls in sequence to a chain. This will also work for an agent which may use multiple steps.\n\n```\nfrom langchain_community.callbacks import get_openai_callbackfrom langchain_core.prompts import PromptTemplatefrom langchain_openai import OpenAIllm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\")template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")chain = template | llmwith get_openai_callback() as cb:    response = chain.invoke({\"topic\": \"birds\"})    print(response)    response = chain.invoke({\"topic\": \"fish\"})    print(\"--\")    print(response)print()print(\"---\")print(f\"Total Tokens: {cb.total_tokens}\")print(f\"Prompt Tokens: {cb.prompt_tokens}\")print(f\"Completion Tokens: {cb.completion_tokens}\")print(f\"Total Cost (USD): ${cb.total_cost}\")\n```\n\n```\nWhy did the chicken go to the seance?To talk to the other side of the road!--Why did the fish need a lawyer?Because it got caught in a net!---Total Tokens: 50Prompt Tokens: 12Completion Tokens: 38Total Cost (USD): $9.400000000000001e-05\n```", "url_final": "https://python.langchain.com/docs/how_to/llm_token_usage_tracking/", "url_citable": "https://python.langchain.com/docs/how_to/llm_token_usage_tracking/#using-callbacks", "title": "How to track token usage for LLMs | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Using callbacks‚Äã", "section_anchor": "using-callbacks", "fetched_at": "2025-10-06T21:55:47.992592", "has_code": true}
{"id": "432593bcf2915a2c263411d9f67db4c19af36e69", "text": "Streaming‚Äã\n\nget_openai_callback does not currently support streaming token counts for legacy language models (e.g., langchain_openai.OpenAI). If you want to count tokens correctly in a streaming context, there are a number of options:\n\n- Use chat models as described in this guide;\n- Implement a custom callback handler that uses appropriate tokenizers to count the tokens;\n- Use a monitoring platform such as LangSmith.\n\nNote that when using legacy language models in a streaming context, token counts are not updated:\n\n```\nfrom langchain_community.callbacks import get_openai_callbackfrom langchain_openai import OpenAIllm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\")with get_openai_callback() as cb:    for chunk in llm.stream(\"Tell me a joke\"):        print(chunk, end=\"\", flush=True)    print(result)    print(\"---\")print()print(f\"Total Tokens: {cb.total_tokens}\")print(f\"Prompt Tokens: {cb.prompt_tokens}\")print(f\"Completion Tokens: {cb.completion_tokens}\")print(f\"Total Cost (USD): ${cb.total_cost}\")\n```\n\n```\nWhy don't scientists trust atoms?Because they make up everything!Why don't scientists trust atoms?Because they make up everything.---Total Tokens: 0Prompt Tokens: 0Completion Tokens: 0Total Cost (USD): $0.0\n```\n\n- OpenAI\n- Single call\n- Multiple calls", "url_final": "https://python.langchain.com/docs/how_to/llm_token_usage_tracking/", "url_citable": "https://python.langchain.com/docs/how_to/llm_token_usage_tracking/#streaming", "title": "How to track token usage for LLMs | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Streaming‚Äã", "section_anchor": "streaming", "fetched_at": "2025-10-06T21:55:47.992592", "has_code": true}
{"id": "4d6f7e8872c1b258ac20c3f33b614238e04cfa87", "text": "String prompt composition‚Äã\n\nWhen working with string prompts, each template is joined together. You can work with either prompts directly or strings (the first element in the list needs to be a prompt).\n\n```\nfrom langchain_core.prompts import PromptTemplateprompt = (    PromptTemplate.from_template(\"Tell me a joke about {topic}\")    + \", make it funny\"    + \"\\n\\nand in {language}\")prompt\n```\n\n```\nPromptTemplate(input_variables=['language', 'topic'], template='Tell me a joke about {topic}, make it funny\\n\\nand in {language}')\n```\n\n```\nprompt.format(topic=\"sports\", language=\"spanish\")\n```\n\n```\n'Tell me a joke about sports, make it funny\\n\\nand in spanish'\n```", "url_final": "https://python.langchain.com/docs/how_to/prompts_composition/", "url_citable": "https://python.langchain.com/docs/how_to/prompts_composition/#string-prompt-composition", "title": "How to compose prompts together | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "String prompt composition‚Äã", "section_anchor": "string-prompt-composition", "fetched_at": "2025-10-06T21:54:47.664578", "has_code": true}
{"id": "35acdd2315f1114dec9d906ca2244849b5539d8f", "text": "Chat prompt composition‚Äã\n\nA chat prompt is made up of a list of messages. Similarly to the above example, we can concatenate chat prompt templates. Each new element is a new message in the final prompt.\n\nFirst, let's initialize the a ChatPromptTemplate with a SystemMessage.\n\n```\nfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessageprompt = SystemMessage(content=\"You are a nice pirate\")\n```\n\nYou can then easily create a pipeline combining it with other messages or message templates. Use a Message when there is no variables to be formatted, use a MessageTemplate when there are variables to be formatted. You can also use just a string (note: this will automatically get inferred as a HumanMessagePromptTemplate.)\n\n```\nnew_prompt = (    prompt + HumanMessage(content=\"hi\") + AIMessage(content=\"what?\") + \"{input}\")\n```\n\nUnder the hood, this creates an instance of the ChatPromptTemplate class, so you can use it just as you did before!\n\n```\nnew_prompt.format_messages(input=\"i said hi\")\n```\n\n```\n[SystemMessage(content='You are a nice pirate'), HumanMessage(content='hi'), AIMessage(content='what?'), HumanMessage(content='i said hi')]\n```", "url_final": "https://python.langchain.com/docs/how_to/prompts_composition/", "url_citable": "https://python.langchain.com/docs/how_to/prompts_composition/#chat-prompt-composition", "title": "How to compose prompts together | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Chat prompt composition‚Äã", "section_anchor": "chat-prompt-composition", "fetched_at": "2025-10-06T21:54:47.664578", "has_code": true}
{"id": "dd266e9c8a38d039671105bc7f5460efdce60e9a", "text": "Using PipelinePrompt‚Äã\n\nPipelinePromptTemplate is deprecated; for more information, please refer to PipelinePromptTemplate.\n\nLangChain includes a class called PipelinePromptTemplate, which can be useful when you want to reuse parts of prompts. A PipelinePrompt consists of two main parts:\n\n- Final prompt: The final prompt that is returned\n- Pipeline prompts: A list of tuples, consisting of a string name and a prompt template. Each prompt template will be formatted and then passed to future prompt templates as a variable with the same name.\n\n```\nfrom langchain_core.prompts import PipelinePromptTemplate, PromptTemplatefull_template = \"\"\"{introduction}{example}{start}\"\"\"full_prompt = PromptTemplate.from_template(full_template)introduction_template = \"\"\"You are impersonating {person}.\"\"\"introduction_prompt = PromptTemplate.from_template(introduction_template)example_template = \"\"\"Here's an example of an interaction:Q: {example_q}A: {example_a}\"\"\"example_prompt = PromptTemplate.from_template(example_template)start_template = \"\"\"Now, do this for real!Q: {input}A:\"\"\"start_prompt = PromptTemplate.from_template(start_template)input_prompts = [    (\"introduction\", introduction_prompt),    (\"example\", example_prompt),    (\"start\", start_prompt),]pipeline_prompt = PipelinePromptTemplate(    final_prompt=full_prompt, pipeline_prompts=input_prompts)pipeline_prompt.input_variables\n```\n\n```\n['person', 'example_a', 'example_q', 'input']\n```\n\n```\nprint(    pipeline_prompt.format(        person=\"Elon Musk\",        example_q=\"What's your favorite car?\",        example_a=\"Tesla\",        input=\"What's your favorite social media site?\",    ))\n```\n\n```\nYou are impersonating Elon Musk.Here's an example of an interaction:Q: What's your favorite car?A: TeslaNow, do this for real!Q: What's your favorite social media site?A:\n```", "url_final": "https://python.langchain.com/docs/how_to/prompts_composition/", "url_citable": "https://python.langchain.com/docs/how_to/prompts_composition/#using-pipelineprompt", "title": "How to compose prompts together | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Using PipelinePrompt‚Äã", "section_anchor": "using-pipelineprompt", "fetched_at": "2025-10-06T21:54:47.664578", "has_code": true}
{"id": "a4d4d05e05ac775928eac760edf173b91a2c2e5c", "text": "Next steps‚Äã\n\nYou've now learned how to compose prompts together.\n\nNext, check out the other how-to guides on prompt templates in this section, like adding few-shot examples to your prompt templates.\n\n- String prompt composition\n- Chat prompt composition\n- Using PipelinePrompt\n- Next steps", "url_final": "https://python.langchain.com/docs/how_to/prompts_composition/", "url_citable": "https://python.langchain.com/docs/how_to/prompts_composition/#next-steps", "title": "How to compose prompts together | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Next steps‚Äã", "section_anchor": "next-steps", "fetched_at": "2025-10-06T21:54:47.664578", "has_code": false}
{"id": "f69bed616c47628a34fff24cc992eaea21b4ec9e", "text": "Using Stream‚Äã\n\nAll Runnable objects implement a sync method called stream and an async variant called astream.\n\nThese methods are designed to stream the final output in chunks, yielding each chunk as soon as it is available.\n\nStreaming is only possible if all steps in the program know how to process an input stream; i.e., process an input chunk one at a time, and yield a corresponding output chunk.\n\nThe complexity of this processing can vary, from straightforward tasks like emitting tokens produced by an LLM, to more challenging ones like streaming parts of JSON results before the entire JSON is complete.\n\nThe best place to start exploring streaming is with the single most important components in LLMs apps-- the LLMs themselves!", "url_final": "https://python.langchain.com/docs/how_to/streaming/", "url_citable": "https://python.langchain.com/docs/how_to/streaming/#using-stream", "title": "How to stream runnables | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Using Stream‚Äã", "section_anchor": "using-stream", "fetched_at": "2025-10-06T21:56:28.670573", "has_code": false}
{"id": "37b77d2ef5fc05f6ea6d937176f7c9c53293275a", "text": "LLMs and Chat Models‚Äã\n\nLarge language models and their chat variants are the primary bottleneck in LLM based apps.\n\nLarge language models can take several seconds to generate a complete response to a query. This is far slower than the ~200-300 ms threshold at which an application feels responsive to an end user.\n\nThe key strategy to make the application feel more responsive is to show intermediate progress; viz., to stream the output from the model token by token.\n\nWe will show examples of streaming using a chat model. Choose one from the options below:\n\n```\npip install -qU \"langchain[google-genai]\"\n```\n\n```\nimport getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelmodel = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n```\n\nLet's start with the sync stream API:\n\n```\nchunks = []for chunk in model.stream(\"what color is the sky?\"):    chunks.append(chunk)    print(chunk.content, end=\"|\", flush=True)\n```\n\n```\n|The sky typically| appears blue during the day due to a phenomenon| called Rayleigh scattering, where| air molecules scatter sunlight, with| blue light being scattered more than other colors. However|, the sky's color can vary|:- At sunrise/sunset:| Red, orange, pink, or purple-| During storms: Gray or dark blue|- At night: Dark| blue to black- In certain| atmospheric conditions: White, yellow, or green| (rare)The color we perceive depends| on weather conditions, time of day, pollution| levels, and our viewing angle.||\n```\n\nAlternatively, if you're working in an async environment, you may consider using the async astream API:\n\n```\nchunks = []async for chunk in model.astream(\"what color is the sky?\"):    chunks.append(chunk)    print(chunk.content, end=\"|\", flush=True)\n```\n\n```\n|The sky typically| appears blue during the day due to a phenomenon called Rayleigh| scattering, where air molecules scatter sunlight,| with blue light being scattered more than other colors. However|, the sky's color can vary - appearing re|d, orange, or pink during sunrise and sunset,| gray on cloudy days, and black at night.| The color you see depends on the time of| day, weather conditions, and your location.||\n```\n\nLet's inspect one of the chunks\n\n```\nchunks[0]\n```\n\n```\nAIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_name': 'claude-3-7-sonnet-20250219'}, id='run--c4f01565-8bb4-4f07-9b23-acfe46cbeca3', usage_metadata={'input_tokens': 13, 'output_tokens': 0, 'total_tokens': 13, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})\n```\n\nWe got back something called an AIMessageChunk. This chunk represents a part of an AIMessage.\n\nMessage chunks are additive by design -- one can simply add them up to get the state of the response so far!\n\n```\nchunks[0] + chunks[1] + chunks[2] + chunks[3] + chunks[4]\n```\n\n```\nAIMessageChunk(content='The sky typically appears blue during the day due to a phenomenon called Rayleigh scattering, where air molecules scatter sunlight, with blue light being scattered more than other colors. However', additional_kwargs={}, response_metadata={'model_name': 'claude-3-7-sonnet-20250219'}, id='run--c4f01565-8bb4-4f07-9b23-acfe46cbeca3', usage_metadata={'input_tokens': 13, 'output_tokens': 0, 'total_tokens': 13, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})\n```", "url_final": "https://python.langchain.com/docs/how_to/streaming/", "url_citable": "https://python.langchain.com/docs/how_to/streaming/#llms-and-chat-models", "title": "How to stream runnables | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "LLMs and Chat Models‚Äã", "section_anchor": "llms-and-chat-models", "fetched_at": "2025-10-06T21:56:28.670573", "has_code": true}
{"id": "8cb6a72f1cbf5a6e7cb2de942eaa3711bff865a4", "text": "Chains‚Äã\n\nVirtually all LLM applications involve more steps than just a call to a language model.\n\nLet's build a simple chain using LangChain Expression Language (LCEL) that combines a prompt, model and a parser and verify that streaming works.\n\nWe will use StrOutputParser to parse the output from the model. This is a simple parser that extracts the content field from an AIMessageChunk, giving us the token returned by the model.\n\nLCEL is a declarative way to specify a \"program\" by chainining together different LangChain primitives. Chains created using LCEL benefit from an automatic implementation of stream and astream allowing streaming of the final output. In fact, chains created with LCEL implement the entire standard Runnable interface.\n\n```\nfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplateprompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")parser = StrOutputParser()chain = prompt | model | parserasync for chunk in chain.astream({\"topic\": \"parrot\"}):    print(chunk, end=\"|\", flush=True)\n```\n\n```\n|Why| don't parrots use the internet?They|'re afraid of getting a virus from all the tweets|!||\n```\n\nNote that we're getting streaming output even though we're using parser at the end of the chain above. The parser operates on each streaming chunk individidually. Many of the LCEL primitives also support this kind of transform-style passthrough streaming, which can be very convenient when constructing apps.\n\nCustom functions can be designed to return generators, which are able to operate on streams.\n\nCertain runnables, like prompt templates and chat models, cannot process individual chunks and instead aggregate all previous steps. Such runnables can interrupt the streaming process.\n\nThe LangChain Expression language allows you to separate the construction of a chain from the mode in which it is used (e.g., sync/async, batch/streaming etc.). If this is not relevant to what you're building, you can also rely on a standard imperative programming approach by caling invoke, batch or stream on each component individually, assigning the results to variables and then using them downstream as you see fit.", "url_final": "https://python.langchain.com/docs/how_to/streaming/", "url_citable": "https://python.langchain.com/docs/how_to/streaming/#chains", "title": "How to stream runnables | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Chains‚Äã", "section_anchor": "chains", "fetched_at": "2025-10-06T21:56:28.670573", "has_code": true}
{"id": "a60c8c0563a38772a329a70b629b2a5216cf85a1", "text": "Working with Input Streams‚Äã\n\nWhat if you wanted to stream JSON from the output as it was being generated?\n\nIf you were to rely on json.loads to parse the partial json, the parsing would fail as the partial json wouldn't be valid json.\n\nYou'd likely be at a complete loss of what to do and claim that it wasn't possible to stream JSON.\n\nWell, turns out there is a way to do it -- the parser needs to operate on the input stream, and attempt to \"auto-complete\" the partial json into a valid state.\n\nLet's see such a parser in action to understand what this means.\n\n```\nfrom langchain_core.output_parsers import JsonOutputParserchain = (    model | JsonOutputParser())  # Due to a bug in older versions of Langchain, JsonOutputParser did not stream results from some modelsasync for text in chain.astream(    \"output a list of the countries france, spain and japan and their populations in JSON format. \"    'Use a dict with an outer key of \"countries\" which contains a list of countries. '    \"Each country should have the key `name` and `population`\"):    print(text, flush=True)\n```\n\n```\n{'countries': []}{'countries': [{'name': 'France'}]}{'countries': [{'name': 'France', 'population': 67750}]}{'countries': [{'name': 'France', 'population': 67750000}, {}]}{'countries': [{'name': 'France', 'population': 67750000}, {'name': 'Spain'}]}{'countries': [{'name': 'France', 'population': 67750000}, {'name': 'Spain', 'population': 47350000}, {}]}{'countries': [{'name': 'France', 'population': 67750000}, {'name': 'Spain', 'population': 47350000}, {'name': 'Japan'}]}{'countries': [{'name': 'France', 'population': 67750000}, {'name': 'Spain', 'population': 47350000}, {'name': 'Japan', 'population': 125700000}]}\n```\n\nNow, let's break streaming. We'll use the previous example and append an extraction function at the end that extracts the country names from the finalized JSON.\n\nAny steps in the chain that operate on finalized inputs rather than on input streams can break streaming functionality via stream or astream.\n\nLater, we will discuss the astream_events API which streams results from intermediate steps. This API will stream results from intermediate steps even if the chain contains steps that only operate on finalized inputs.\n\n```\nfrom langchain_core.output_parsers import (    JsonOutputParser,)# A function that operates on finalized inputs# rather than on an input_streamdef _extract_country_names(inputs):    \"\"\"A function that does not operates on input streams and breaks streaming.\"\"\"    if not isinstance(inputs, dict):        return \"\"    if \"countries\" not in inputs:        return \"\"    countries = inputs[\"countries\"]    if not isinstance(countries, list):        return \"\"    country_names = [        country.get(\"name\") for country in countries if isinstance(country, dict)    ]    return country_nameschain = model | JsonOutputParser() | _extract_country_namesasync for text in chain.astream(    \"output a list of the countries france, spain and japan and their populations in JSON format. \"    'Use a dict with an outer key of \"countries\" which contains a list of countries. '    \"Each country should have the key `name` and `population`\"):    print(text, end=\"|\", flush=True)\n```\n\n```\n['France', 'Spain', 'Japan']|\n```\n\nLet's fix the streaming using a generator function that can operate on the input stream.\n\nA generator function (a function that uses yield) allows writing code that operates on input streams\n\n```\nfrom langchain_core.output_parsers import JsonOutputParserasync def _extract_country_names_streaming(input_stream):    \"\"\"A function that operates on input streams.\"\"\"    country_names_so_far = set()    async for input in input_stream:        if not isinstance(input, dict):            continue        if \"countries\" not in input:            continue        countries = input[\"countries\"]        if not isinstance(countries, list):            continue        for country in countries:            name = country.get(\"name\")            if not name:                continue            if name not in country_names_so_far:                yield name                country_names_so_far.add(name)chain = model | JsonOutputParser() | _extract_country_names_streamingasync for text in chain.astream(    \"output a list of the countries france, spain and japan and their populations in JSON format. \"    'Use a dict with an outer key of \"countries\" which contains a list of countries. '    \"Each country should have the key `name` and `population`\",):    print(text, end=\"|\", flush=True)\n```\n\n```\nFrance|Spain|Japan|\n```\n\nBecause the code above is relying on JSON auto-completion, you may see partial names of countries (e.g., Sp and Spain), which is not what one would want for an extraction result!\n\nWe're focusing on streaming concepts, not necessarily the results of the chains.", "url_final": "https://python.langchain.com/docs/how_to/streaming/", "url_citable": "https://python.langchain.com/docs/how_to/streaming/#working-with-input-streams", "title": "How to stream runnables | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Working with Input Streams‚Äã", "section_anchor": "working-with-input-streams", "fetched_at": "2025-10-06T21:56:28.670573", "has_code": true}
{"id": "82c5706f5c51650a4872f7cb8dd67c230d7fea60", "text": "Non-streaming components‚Äã\n\nSome built-in components like Retrievers do not offer any streaming. What happens if we try to stream them? ü§®\n\n```\nfrom langchain_community.vectorstores import FAISSfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnablePassthroughfrom langchain_openai import OpenAIEmbeddingstemplate = \"\"\"Answer the question based only on the following context:{context}Question: {question}\"\"\"prompt = ChatPromptTemplate.from_template(template)vectorstore = FAISS.from_texts(    [\"harrison worked at kensho\", \"harrison likes spicy food\"],    embedding=OpenAIEmbeddings(),)retriever = vectorstore.as_retriever()chunks = [chunk for chunk in retriever.stream(\"where did harrison work?\")]chunks\n```\n\n```\n[[Document(id='2740a247-9738-48c4-8c8f-d879d4ed39f7', metadata={}, page_content='harrison worked at kensho'),  Document(id='1d3d012f-1cb0-4bee-928a-c8bf0f8b1b92', metadata={}, page_content='harrison likes spicy food')]]\n```\n\nStream just yielded the final result from that component.\n\nThis is OK ü•π! Not all components have to implement streaming -- in some cases streaming is either unnecessary, difficult or just doesn't make sense.\n\nAn LCEL chain constructed using non-streaming components, will still be able to stream in a lot of cases, with streaming of partial output starting after the last non-streaming step in the chain.\n\n```\nretrieval_chain = (    {        \"context\": retriever.with_config(run_name=\"Docs\"),        \"question\": RunnablePassthrough(),    }    | prompt    | model    | StrOutputParser())\n```\n\n```\nfor chunk in retrieval_chain.stream(    \"Where did harrison work? Write 3 made up sentences about this place.\"):    print(chunk, end=\"|\", flush=True)\n```\n\n```\n|Base|d on the provided context, Harrison worked at Kens|ho.Three made up sentences about Kens|ho:1. Kensho is a| cutting-edge technology company that specializes in| AI and data analytics for financial institutions.2|. The Kensho office features| an open floor plan with panoramic views of the city| skyline, creating an inspiring environment for its| employees.3. At Kensho,| team members often collaborate in innovative brainstorming sessions while| enjoying complimentary gourmet coffee from| their in-house caf√©.||\n```\n\nNow that we've seen how stream and astream work, let's venture into the world of streaming events. üèûÔ∏è", "url_final": "https://python.langchain.com/docs/how_to/streaming/", "url_citable": "https://python.langchain.com/docs/how_to/streaming/#non-streaming-components", "title": "How to stream runnables | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Non-streaming components‚Äã", "section_anchor": "non-streaming-components", "fetched_at": "2025-10-06T21:56:28.670573", "has_code": true}
{"id": "74c027124bfe413690918f8cba9095be595406cb", "text": "Using Stream Events‚Äã\n\nEvent Streaming is a beta API. This API may change a bit based on feedback.\n\nThis guide demonstrates the V2 API and requires langchain-core >= 0.2. For the V1 API compatible with older versions of LangChain, see here.\n\n```\nimport langchain_corelangchain_core.__version__\n```\n\nFor the astream_events API to work properly:\n\n- Use async throughout the code to the extent possible (e.g., async tools etc)\n- Propagate callbacks if defining custom functions / runnables\n- Whenever using runnables without LCEL, make sure to call .astream() on LLMs rather than .ainvoke to force the LLM to stream tokens.\n- Let us know if anything doesn't work as expected! :)\n\nEvent Reference‚Äã\n\nBelow is a reference table that shows some events that might be emitted by the various Runnable objects.\n\nWhen streaming is implemented properly, the inputs to a runnable will not be known until after the input stream has been entirely consumed. This means that inputs will often be included only for end events and rather than for start events.", "url_final": "https://python.langchain.com/docs/how_to/streaming/", "url_citable": "https://python.langchain.com/docs/how_to/streaming/#using-stream-events", "title": "How to stream runnables | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Using Stream Events‚Äã", "section_anchor": "using-stream-events", "fetched_at": "2025-10-06T21:56:28.670573", "has_code": true}
{"id": "f265e3096cb1ca3b3121d923d4af04bc50eca5b2", "text": "Chat Model‚Äã\n\nLet's start off by looking at the events produced by a chat model.\n\n```\nevents = []async for event in model.astream_events(\"hello\"):    events.append(event)\n```\n\nFor langchain-core<0.3.37, set the version kwarg explicitly (e.g., model.astream_events(\"hello\", version=\"v2\")).\n\nLet's take a look at the few of the start event and a few of the end events.\n\n```\nevents[:3]\n```\n\n```\n[{'event': 'on_chat_model_start',  'data': {'input': 'hello'},  'name': 'ChatAnthropic',  'tags': [],  'run_id': 'c35a72be-a5af-4bd5-bd9b-206135c28ef6',  'metadata': {'ls_provider': 'anthropic',   'ls_model_name': 'claude-3-7-sonnet-20250219',   'ls_model_type': 'chat',   'ls_temperature': 0.0,   'ls_max_tokens': 1024},  'parent_ids': []}, {'event': 'on_chat_model_stream',  'run_id': 'c35a72be-a5af-4bd5-bd9b-206135c28ef6',  'name': 'ChatAnthropic',  'tags': [],  'metadata': {'ls_provider': 'anthropic',   'ls_model_name': 'claude-3-7-sonnet-20250219',   'ls_model_type': 'chat',   'ls_temperature': 0.0,   'ls_max_tokens': 1024},  'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_name': 'claude-3-7-sonnet-20250219'}, id='run--c35a72be-a5af-4bd5-bd9b-206135c28ef6', usage_metadata={'input_tokens': 8, 'output_tokens': 0, 'total_tokens': 8, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})},  'parent_ids': []}, {'event': 'on_chat_model_stream',  'run_id': 'c35a72be-a5af-4bd5-bd9b-206135c28ef6',  'name': 'ChatAnthropic',  'tags': [],  'metadata': {'ls_provider': 'anthropic',   'ls_model_name': 'claude-3-7-sonnet-20250219',   'ls_model_type': 'chat',   'ls_temperature': 0.0,   'ls_max_tokens': 1024},  'data': {'chunk': AIMessageChunk(content='Hello! How', additional_kwargs={}, response_metadata={}, id='run--c35a72be-a5af-4bd5-bd9b-206135c28ef6')},  'parent_ids': []}]\n```\n\n```\nevents[-2:]\n```\n\n```\n[{'event': 'on_chat_model_stream',  'run_id': 'c35a72be-a5af-4bd5-bd9b-206135c28ef6',  'name': 'ChatAnthropic',  'tags': [],  'metadata': {'ls_provider': 'anthropic',   'ls_model_name': 'claude-3-7-sonnet-20250219',   'ls_model_type': 'chat',   'ls_temperature': 0.0,   'ls_max_tokens': 1024},  'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'stop_reason': 'end_turn', 'stop_sequence': None}, id='run--c35a72be-a5af-4bd5-bd9b-206135c28ef6', usage_metadata={'input_tokens': 0, 'output_tokens': 40, 'total_tokens': 40})},  'parent_ids': []}, {'event': 'on_chat_model_end',  'data': {'output': AIMessageChunk(content=\"Hello! How can I assist you today? Whether you have questions, need information, or just want to chat, I'm here to help. What would you like to talk about?\", additional_kwargs={}, response_metadata={'model_name': 'claude-3-7-sonnet-20250219', 'stop_reason': 'end_turn', 'stop_sequence': None}, id='run--c35a72be-a5af-4bd5-bd9b-206135c28ef6', usage_metadata={'input_tokens': 8, 'output_tokens': 40, 'total_tokens': 48, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})},  'run_id': 'c35a72be-a5af-4bd5-bd9b-206135c28ef6',  'name': 'ChatAnthropic',  'tags': [],  'metadata': {'ls_provider': 'anthropic',   'ls_model_name': 'claude-3-7-sonnet-20250219',   'ls_model_type': 'chat',   'ls_temperature': 0.0,   'ls_max_tokens': 1024},  'parent_ids': []}]\n```", "url_final": "https://python.langchain.com/docs/how_to/streaming/", "url_citable": "https://python.langchain.com/docs/how_to/streaming/#chat-model", "title": "How to stream runnables | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Chat Model‚Äã", "section_anchor": "chat-model", "fetched_at": "2025-10-06T21:56:28.670573", "has_code": true}
{"id": "c5808ec58477a706b16d324f7d4608a80418e4b9", "text": "Chain‚Äã\n\nLet's revisit the example chain that parsed streaming JSON to explore the streaming events API.\n\n```\nchain = (    model | JsonOutputParser())  # Due to a bug in older versions of Langchain, JsonOutputParser did not stream results from some modelsevents = [    event    async for event in chain.astream_events(        \"output a list of the countries france, spain and japan and their populations in JSON format. \"        'Use a dict with an outer key of \"countries\" which contains a list of countries. '        \"Each country should have the key `name` and `population`\",    )]\n```\n\nIf you examine at the first few events, you'll notice that there are 3 different start events rather than 2 start events.\n\nThe three start events correspond to:\n\n- The chain (model + parser)\n- The model\n- The parser\n\n```\nevents[:3]\n```\n\n```\n[{'event': 'on_chain_start',  'data': {'input': 'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`'},  'name': 'RunnableSequence',  'tags': [],  'run_id': 'f859e56f-a760-4670-a24e-040e11bcd7fc',  'metadata': {},  'parent_ids': []}, {'event': 'on_chat_model_start',  'data': {'input': {'messages': [[HumanMessage(content='output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`', additional_kwargs={}, response_metadata={})]]}},  'name': 'ChatAnthropic',  'tags': ['seq:step:1'],  'run_id': '2aa8c9e6-a5cd-4e94-b994-cb0e9bd8ab21',  'metadata': {'ls_provider': 'anthropic',   'ls_model_name': 'claude-3-7-sonnet-20250219',   'ls_model_type': 'chat',   'ls_temperature': 0.0,   'ls_max_tokens': 1024},  'parent_ids': ['f859e56f-a760-4670-a24e-040e11bcd7fc']}, {'event': 'on_chat_model_stream',  'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_name': 'claude-3-7-sonnet-20250219'}, id='run--2aa8c9e6-a5cd-4e94-b994-cb0e9bd8ab21', usage_metadata={'input_tokens': 56, 'output_tokens': 0, 'total_tokens': 56, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})},  'run_id': '2aa8c9e6-a5cd-4e94-b994-cb0e9bd8ab21',  'name': 'ChatAnthropic',  'tags': ['seq:step:1'],  'metadata': {'ls_provider': 'anthropic',   'ls_model_name': 'claude-3-7-sonnet-20250219',   'ls_model_type': 'chat',   'ls_temperature': 0.0,   'ls_max_tokens': 1024},  'parent_ids': ['f859e56f-a760-4670-a24e-040e11bcd7fc']}]\n```\n\nWhat do you think you'd see if you looked at the last 3 events? what about the middle?\n\nLet's use this API to take output the stream events from the model and the parser. We're ignoring start events, end events and events from the chain.\n\n```\nnum_events = 0async for event in chain.astream_events(    \"output a list of the countries france, spain and japan and their populations in JSON format. \"    'Use a dict with an outer key of \"countries\" which contains a list of countries. '    \"Each country should have the key `name` and `population`\",):    kind = event[\"event\"]    if kind == \"on_chat_model_stream\":        print(            f\"Chat model chunk: {repr(event['data']['chunk'].content)}\",            flush=True,        )    if kind == \"on_parser_stream\":        print(f\"Parser chunk: {event['data']['chunk']}\", flush=True)    num_events += 1    if num_events > 30:        # Truncate the output        print(\"...\")        break\n```\n\n```\nChat model chunk: ''Chat model chunk: '\\`\\`\\`'Chat model chunk: 'json\\n{\\n  \"countries\": ['Parser chunk: {'countries': []}Chat model chunk: '\\n    {\\n      \"name\": \"France\",'Parser chunk: {'countries': [{'name': 'France'}]}Chat model chunk: '\\n      \"population\": 67750000\\n    },'Parser chunk: {'countries': [{'name': 'France', 'population': 67750000}]}Chat model chunk: '\\n    {\\n      \"name\": \"Spain\",'Parser chunk: {'countries': [{'name': 'France', 'population': 67750000}, {'name': 'Spain'}]}Chat model chunk: '\\n      \"population\": 47350'Parser chunk: {'countries': [{'name': 'France', 'population': 67750000}, {'name': 'Spain', 'population': 47350}]}Chat model chunk: '000\\n    },\\n    {\\n      \"'Parser chunk: {'countries': [{'name': 'France', 'population': 67750000}, {'name': 'Spain', 'population': 47350000}, {}]}Chat model chunk: 'name\": \"Japan\",\\n      \"population\":'Parser chunk: {'countries': [{'name': 'France', 'population': 67750000}, {'name': 'Spain', 'population': 47350000}, {'name': 'Japan'}]}Chat model chunk: ' 125700000\\n    }'Parser chunk: {'countries': [{'name': 'France', 'population': 67750000}, {'name': 'Spain', 'population': 47350000}, {'name': 'Japan', 'population': 125700000}]}Chat model chunk: '\\n  ]\\n}\\n\\`\\`\\`'Chat model chunk: ''...\n```\n\nBecause both the model and the parser support streaming, we see streaming events from both components in real time! Kind of cool isn't it? ü¶ú", "url_final": "https://python.langchain.com/docs/how_to/streaming/", "url_citable": "https://python.langchain.com/docs/how_to/streaming/#chain", "title": "How to stream runnables | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Chain‚Äã", "section_anchor": "chain", "fetched_at": "2025-10-06T21:56:28.670573", "has_code": true}
{"id": "144bb91dca7f772213dcfccb4dfde1cdd4bfa398", "text": "Filtering Events‚Äã\n\nBecause this API produces so many events, it is useful to be able to filter on events.\n\nYou can filter by either component name, component tags or component type.\n\n```\nchain = model.with_config({\"run_name\": \"model\"}) | JsonOutputParser().with_config(    {\"run_name\": \"my_parser\"})max_events = 0async for event in chain.astream_events(    \"output a list of the countries france, spain and japan and their populations in JSON format. \"    'Use a dict with an outer key of \"countries\" which contains a list of countries. '    \"Each country should have the key `name` and `population`\",    include_names=[\"my_parser\"],):    print(event)    max_events += 1    if max_events > 10:        # Truncate output        print(\"...\")        break\n```\n\n```\n{'event': 'on_parser_start', 'data': {'input': 'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`'}, 'name': 'my_parser', 'tags': ['seq:step:2'], 'run_id': '781af9b6-31f8-47f2-ab79-52d17b000857', 'metadata': {}, 'parent_ids': ['82c918c6-d5f6-4d2d-b710-4668509fe2f0']}{'event': 'on_parser_stream', 'run_id': '781af9b6-31f8-47f2-ab79-52d17b000857', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': []}}, 'parent_ids': ['82c918c6-d5f6-4d2d-b710-4668509fe2f0']}{'event': 'on_parser_stream', 'run_id': '781af9b6-31f8-47f2-ab79-52d17b000857', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France'}]}}, 'parent_ids': ['82c918c6-d5f6-4d2d-b710-4668509fe2f0']}{'event': 'on_parser_stream', 'run_id': '781af9b6-31f8-47f2-ab79-52d17b000857', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67750}]}}, 'parent_ids': ['82c918c6-d5f6-4d2d-b710-4668509fe2f0']}{'event': 'on_parser_stream', 'run_id': '781af9b6-31f8-47f2-ab79-52d17b000857', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67750000}, {}]}}, 'parent_ids': ['82c918c6-d5f6-4d2d-b710-4668509fe2f0']}{'event': 'on_parser_stream', 'run_id': '781af9b6-31f8-47f2-ab79-52d17b000857', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67750000}, {'name': 'Spain'}]}}, 'parent_ids': ['82c918c6-d5f6-4d2d-b710-4668509fe2f0']}{'event': 'on_parser_stream', 'run_id': '781af9b6-31f8-47f2-ab79-52d17b000857', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67750000}, {'name': 'Spain', 'population': 47350000}]}}, 'parent_ids': ['82c918c6-d5f6-4d2d-b710-4668509fe2f0']}{'event': 'on_parser_stream', 'run_id': '781af9b6-31f8-47f2-ab79-52d17b000857', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67750000}, {'name': 'Spain', 'population': 47350000}, {'name': 'Japan'}]}}, 'parent_ids': ['82c918c6-d5f6-4d2d-b710-4668509fe2f0']}{'event': 'on_parser_stream', 'run_id': '781af9b6-31f8-47f2-ab79-52d17b000857', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67750000}, {'name': 'Spain', 'population': 47350000}, {'name': 'Japan', 'population': 125700000}]}}, 'parent_ids': ['82c918c6-d5f6-4d2d-b710-4668509fe2f0']}{'event': 'on_parser_end', 'data': {'output': {'countries': [{'name': 'France', 'population': 67750000}, {'name': 'Spain', 'population': 47350000}, {'name': 'Japan', 'population': 125700000}]}}, 'run_id': '781af9b6-31f8-47f2-ab79-52d17b000857', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'parent_ids': ['82c918c6-d5f6-4d2d-b710-4668509fe2f0']}\n```\n\n```\nchain = model.with_config({\"run_name\": \"model\"}) | JsonOutputParser().with_config(    {\"run_name\": \"my_parser\"})max_events = 0async for event in chain.astream_events(    'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`',    include_types=[\"chat_model\"],):    print(event)    max_events += 1    if max_events > 10:        # Truncate output        print(\"...\")        break\n```\n\n```\n{'event': 'on_chat_model_start', 'data': {'input': 'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`'}, 'name': 'model', 'tags': ['seq:step:1'], 'run_id': 'b7a08416-a629-4b42-b5d5-dbe48566e5d5', 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-7-sonnet-20250219', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['116a6506-5a19-4f60-a8c2-7b728d4b8248']}{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_name': 'claude-3-7-sonnet-20250219'}, id='run--b7a08416-a629-4b42-b5d5-dbe48566e5d5', usage_metadata={'input_tokens': 56, 'output_tokens': 0, 'total_tokens': 56, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})}, 'run_id': 'b7a08416-a629-4b42-b5d5-dbe48566e5d5', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-7-sonnet-20250219', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['116a6506-5a19-4f60-a8c2-7b728d4b8248']}{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='\\`\\`\\`', additional_kwargs={}, response_metadata={}, id='run--b7a08416-a629-4b42-b5d5-dbe48566e5d5')}, 'run_id': 'b7a08416-a629-4b42-b5d5-dbe48566e5d5', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-7-sonnet-20250219', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['116a6506-5a19-4f60-a8c2-7b728d4b8248']}{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='json\\n{\\n  \"countries\": [', additional_kwargs={}, response_metadata={}, id='run--b7a08416-a629-4b42-b5d5-dbe48566e5d5')}, 'run_id': 'b7a08416-a629-4b42-b5d5-dbe48566e5d5', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-7-sonnet-20250219', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['116a6506-5a19-4f60-a8c2-7b728d4b8248']}{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='\\n    {\\n      \"name\": \"France\",', additional_kwargs={}, response_metadata={}, id='run--b7a08416-a629-4b42-b5d5-dbe48566e5d5')}, 'run_id': 'b7a08416-a629-4b42-b5d5-dbe48566e5d5', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-7-sonnet-20250219', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['116a6506-5a19-4f60-a8c2-7b728d4b8248']}{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='\\n      \"population\": 67750', additional_kwargs={}, response_metadata={}, id='run--b7a08416-a629-4b42-b5d5-dbe48566e5d5')}, 'run_id': 'b7a08416-a629-4b42-b5d5-dbe48566e5d5', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-7-sonnet-20250219', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['116a6506-5a19-4f60-a8c2-7b728d4b8248']}{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='000\\n    },\\n    {\\n      \"', additional_kwargs={}, response_metadata={}, id='run--b7a08416-a629-4b42-b5d5-dbe48566e5d5')}, 'run_id': 'b7a08416-a629-4b42-b5d5-dbe48566e5d5', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-7-sonnet-20250219', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['116a6506-5a19-4f60-a8c2-7b728d4b8248']}{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='name\": \"Spain\",\\n      \"population\":', additional_kwargs={}, response_metadata={}, id='run--b7a08416-a629-4b42-b5d5-dbe48566e5d5')}, 'run_id': 'b7a08416-a629-4b42-b5d5-dbe48566e5d5', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-7-sonnet-20250219', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['116a6506-5a19-4f60-a8c2-7b728d4b8248']}{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' 47350000\\n    },', additional_kwargs={}, response_metadata={}, id='run--b7a08416-a629-4b42-b5d5-dbe48566e5d5')}, 'run_id': 'b7a08416-a629-4b42-b5d5-dbe48566e5d5', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-7-sonnet-20250219', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['116a6506-5a19-4f60-a8c2-7b728d4b8248']}{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='\\n    {\\n      \"name\": \"Japan\",', additional_kwargs={}, response_metadata={}, id='run--b7a08416-a629-4b42-b5d5-dbe48566e5d5')}, 'run_id': 'b7a08416-a629-4b42-b5d5-dbe48566e5d5', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-7-sonnet-20250219', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['116a6506-5a19-4f60-a8c2-7b728d4b8248']}{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='\\n      \"population\": 125700', additional_kwargs={}, response_metadata={}, id='run--b7a08416-a629-4b42-b5d5-dbe48566e5d5')}, 'run_id': 'b7a08416-a629-4b42-b5d5-dbe48566e5d5', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-7-sonnet-20250219', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['116a6506-5a19-4f60-a8c2-7b728d4b8248']}...\n```\n\nTags are inherited by child components of a given runnable.\n\nIf you're using tags to filter, make sure that this is what you want.\n\n```\nchain = (model | JsonOutputParser()).with_config({\"tags\": [\"my_chain\"]})max_events = 0async for event in chain.astream_events(    'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`',    include_tags=[\"my_chain\"],):    print(event)    max_events += 1    if max_events > 10:        # Truncate output        print(\"...\")        break\n```\n\n```\n{'event': 'on_chain_start', 'data': {'input': 'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`'}, 'name': 'RunnableSequence', 'tags': ['my_chain'], 'run_id': '3e4f8c37-a44a-46b7-a7e5-75182d1cca31', 'metadata': {}, 'parent_ids': []}{'event': 'on_chat_model_start', 'data': {'input': {'messages': [[HumanMessage(content='output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`', additional_kwargs={}, response_metadata={})]]}}, 'name': 'ChatAnthropic', 'tags': ['seq:step:1', 'my_chain'], 'run_id': '778846c9-acd3-43b7-b9c0-ac718761b2bc', 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-7-sonnet-20250219', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['3e4f8c37-a44a-46b7-a7e5-75182d1cca31']}{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_name': 'claude-3-7-sonnet-20250219'}, id='run--778846c9-acd3-43b7-b9c0-ac718761b2bc', usage_metadata={'input_tokens': 56, 'output_tokens': 0, 'total_tokens': 56, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})}, 'run_id': '778846c9-acd3-43b7-b9c0-ac718761b2bc', 'name': 'ChatAnthropic', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-7-sonnet-20250219', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['3e4f8c37-a44a-46b7-a7e5-75182d1cca31']}{'event': 'on_parser_start', 'data': {}, 'name': 'JsonOutputParser', 'tags': ['seq:step:2', 'my_chain'], 'run_id': '2c46d24f-231c-4062-a7ab-b7954840986d', 'metadata': {}, 'parent_ids': ['3e4f8c37-a44a-46b7-a7e5-75182d1cca31']}{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='\\`\\`\\`', additional_kwargs={}, response_metadata={}, id='run--778846c9-acd3-43b7-b9c0-ac718761b2bc')}, 'run_id': '778846c9-acd3-43b7-b9c0-ac718761b2bc', 'name': 'ChatAnthropic', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-7-sonnet-20250219', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['3e4f8c37-a44a-46b7-a7e5-75182d1cca31']}{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='json\\n{\\n  \"countries\": [', additional_kwargs={}, response_metadata={}, id='run--778846c9-acd3-43b7-b9c0-ac718761b2bc')}, 'run_id': '778846c9-acd3-43b7-b9c0-ac718761b2bc', 'name': 'ChatAnthropic', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-7-sonnet-20250219', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['3e4f8c37-a44a-46b7-a7e5-75182d1cca31']}{'event': 'on_parser_stream', 'run_id': '2c46d24f-231c-4062-a7ab-b7954840986d', 'name': 'JsonOutputParser', 'tags': ['seq:step:2', 'my_chain'], 'metadata': {}, 'data': {'chunk': {'countries': []}}, 'parent_ids': ['3e4f8c37-a44a-46b7-a7e5-75182d1cca31']}{'event': 'on_chain_stream', 'run_id': '3e4f8c37-a44a-46b7-a7e5-75182d1cca31', 'name': 'RunnableSequence', 'tags': ['my_chain'], 'metadata': {}, 'data': {'chunk': {'countries': []}}, 'parent_ids': []}{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='\\n    {\\n      \"name\": \"France\",', additional_kwargs={}, response_metadata={}, id='run--778846c9-acd3-43b7-b9c0-ac718761b2bc')}, 'run_id': '778846c9-acd3-43b7-b9c0-ac718761b2bc', 'name': 'ChatAnthropic', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-7-sonnet-20250219', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['3e4f8c37-a44a-46b7-a7e5-75182d1cca31']}{'event': 'on_parser_stream', 'run_id': '2c46d24f-231c-4062-a7ab-b7954840986d', 'name': 'JsonOutputParser', 'tags': ['seq:step:2', 'my_chain'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France'}]}}, 'parent_ids': ['3e4f8c37-a44a-46b7-a7e5-75182d1cca31']}{'event': 'on_chain_stream', 'run_id': '3e4f8c37-a44a-46b7-a7e5-75182d1cca31', 'name': 'RunnableSequence', 'tags': ['my_chain'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France'}]}}, 'parent_ids': []}...\n```", "url_final": "https://python.langchain.com/docs/how_to/streaming/", "url_citable": "https://python.langchain.com/docs/how_to/streaming/#filtering-events", "title": "How to stream runnables | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Filtering Events‚Äã", "section_anchor": "filtering-events", "fetched_at": "2025-10-06T21:56:28.670573", "has_code": true}
{"id": "46c02dd80a04b362cca96c7e801074f74a2effc2", "text": "Non-streaming components‚Äã\n\nRemember how some components don't stream well because they don't operate on input streams?\n\nWhile such components can break streaming of the final output when using astream, astream_events will still yield streaming events from intermediate steps that support streaming!\n\n```\n# Function that does not support streaming.# It operates on the finalizes inputs rather than# operating on the input stream.def _extract_country_names(inputs):    \"\"\"A function that does not operates on input streams and breaks streaming.\"\"\"    if not isinstance(inputs, dict):        return \"\"    if \"countries\" not in inputs:        return \"\"    countries = inputs[\"countries\"]    if not isinstance(countries, list):        return \"\"    country_names = [        country.get(\"name\") for country in countries if isinstance(country, dict)    ]    return country_nameschain = (    model | JsonOutputParser() | _extract_country_names)  # This parser only works with OpenAI right now\n```\n\nAs expected, the astream API doesn't work correctly because _extract_country_names doesn't operate on streams.\n\n```\nasync for chunk in chain.astream(    \"output a list of the countries france, spain and japan and their populations in JSON format. \"    'Use a dict with an outer key of \"countries\" which contains a list of countries. '    \"Each country should have the key `name` and `population`\",):    print(chunk, flush=True)\n```\n\n```\n['France', 'Spain', 'Japan']\n```\n\nNow, let's confirm that with astream_events we're still seeing streaming output from the model and the parser.\n\n```\nnum_events = 0async for event in chain.astream_events(    \"output a list of the countries france, spain and japan and their populations in JSON format. \"    'Use a dict with an outer key of \"countries\" which contains a list of countries. '    \"Each country should have the key `name` and `population`\",):    kind = event[\"event\"]    if kind == \"on_chat_model_stream\":        print(            f\"Chat model chunk: {repr(event['data']['chunk'].content)}\",            flush=True,        )    if kind == \"on_parser_stream\":        print(f\"Parser chunk: {event['data']['chunk']}\", flush=True)    num_events += 1    if num_events > 30:        # Truncate the output        print(\"...\")        break\n```\n\n```\nChat model chunk: ''Chat model chunk: '\\`\\`\\`'Chat model chunk: 'json\\n{\\n  \"countries\": ['Parser chunk: {'countries': []}Chat model chunk: '\\n    {\\n      \"name\": \"France\",'Parser chunk: {'countries': [{'name': 'France'}]}Chat model chunk: '\\n      \"population\": 67750'Parser chunk: {'countries': [{'name': 'France', 'population': 67750}]}Chat model chunk: '000\\n    },\\n    {\\n      \"'Parser chunk: {'countries': [{'name': 'France', 'population': 67750000}, {}]}Chat model chunk: 'name\": \"Spain\",\\n      \"population\":'Parser chunk: {'countries': [{'name': 'France', 'population': 67750000}, {'name': 'Spain'}]}Chat model chunk: ' 47350000\\n    },'Parser chunk: {'countries': [{'name': 'France', 'population': 67750000}, {'name': 'Spain', 'population': 47350000}]}Chat model chunk: '\\n    {\\n      \"name\": \"Japan\",'Parser chunk: {'countries': [{'name': 'France', 'population': 67750000}, {'name': 'Spain', 'population': 47350000}, {'name': 'Japan'}]}Chat model chunk: '\\n      \"population\": 125700'Parser chunk: {'countries': [{'name': 'France', 'population': 67750000}, {'name': 'Spain', 'population': 47350000}, {'name': 'Japan', 'population': 125700}]}Chat model chunk: '000\\n    }\\n  ]\\n}'Parser chunk: {'countries': [{'name': 'France', 'population': 67750000}, {'name': 'Spain', 'population': 47350000}, {'name': 'Japan', 'population': 125700000}]}Chat model chunk: '\\n\\`\\`\\`'Chat model chunk: ''...\n```", "url_final": "https://python.langchain.com/docs/how_to/streaming/", "url_citable": "https://python.langchain.com/docs/how_to/streaming/#non-streaming-components-1", "title": "How to stream runnables | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Non-streaming components‚Äã", "section_anchor": "non-streaming-components-1", "fetched_at": "2025-10-06T21:56:28.670573", "has_code": true}
{"id": "1b68d1a2ba5004bd58e8f932d3ca5eac65da8680", "text": "Propagating Callbacks‚Äã\n\nIf you're using invoking runnables inside your tools, you need to propagate callbacks to the runnable; otherwise, no stream events will be generated.\n\nWhen using RunnableLambdas or @chain decorator, callbacks are propagated automatically behind the scenes.\n\n```\nfrom langchain_core.runnables import RunnableLambdafrom langchain_core.tools import tooldef reverse_word(word: str):    return word[::-1]reverse_word = RunnableLambda(reverse_word)@tooldef bad_tool(word: str):    \"\"\"Custom tool that doesn't propagate callbacks.\"\"\"    return reverse_word.invoke(word)async for event in bad_tool.astream_events(\"hello\"):    print(event)\n```\n\n```\n{'event': 'on_tool_start', 'data': {'input': 'hello'}, 'name': 'bad_tool', 'tags': [], 'run_id': 'b1c6b79d-f94b-432f-a289-1ea68a7c3cea', 'metadata': {}, 'parent_ids': []}{'event': 'on_chain_start', 'data': {'input': 'hello'}, 'name': 'reverse_word', 'tags': [], 'run_id': 'e661c1ec-e6d2-4f9a-9620-b50645f2b194', 'metadata': {}, 'parent_ids': ['b1c6b79d-f94b-432f-a289-1ea68a7c3cea']}{'event': 'on_chain_end', 'data': {'output': 'olleh', 'input': 'hello'}, 'run_id': 'e661c1ec-e6d2-4f9a-9620-b50645f2b194', 'name': 'reverse_word', 'tags': [], 'metadata': {}, 'parent_ids': ['b1c6b79d-f94b-432f-a289-1ea68a7c3cea']}{'event': 'on_tool_end', 'data': {'output': 'olleh'}, 'run_id': 'b1c6b79d-f94b-432f-a289-1ea68a7c3cea', 'name': 'bad_tool', 'tags': [], 'metadata': {}, 'parent_ids': []}\n```\n\nHere's a re-implementation that does propagate callbacks correctly. You'll notice that now we're getting events from the reverse_word runnable as well.\n\n```\n@tooldef correct_tool(word: str, callbacks):    \"\"\"A tool that correctly propagates callbacks.\"\"\"    return reverse_word.invoke(word, {\"callbacks\": callbacks})async for event in correct_tool.astream_events(\"hello\"):    print(event)\n```\n\n```\n{'event': 'on_tool_start', 'data': {'input': 'hello'}, 'name': 'correct_tool', 'tags': [], 'run_id': '399c91f5-a40b-4173-943f-a9c583a04003', 'metadata': {}, 'parent_ids': []}{'event': 'on_chain_start', 'data': {'input': 'hello'}, 'name': 'reverse_word', 'tags': [], 'run_id': 'e9cc7db1-4587-40af-9c35-2d787b3f0956', 'metadata': {}, 'parent_ids': ['399c91f5-a40b-4173-943f-a9c583a04003']}{'event': 'on_chain_end', 'data': {'output': 'olleh', 'input': 'hello'}, 'run_id': 'e9cc7db1-4587-40af-9c35-2d787b3f0956', 'name': 'reverse_word', 'tags': [], 'metadata': {}, 'parent_ids': ['399c91f5-a40b-4173-943f-a9c583a04003']}{'event': 'on_tool_end', 'data': {'output': 'olleh'}, 'run_id': '399c91f5-a40b-4173-943f-a9c583a04003', 'name': 'correct_tool', 'tags': [], 'metadata': {}, 'parent_ids': []}\n```\n\nIf you're invoking runnables from within Runnable Lambdas or @chains, then callbacks will be passed automatically on your behalf.\n\n```\nfrom langchain_core.runnables import RunnableLambdaasync def reverse_and_double(word: str):    return await reverse_word.ainvoke(word) * 2reverse_and_double = RunnableLambda(reverse_and_double)await reverse_and_double.ainvoke(\"1234\")async for event in reverse_and_double.astream_events(\"1234\"):    print(event)\n```\n\n```\n{'event': 'on_chain_start', 'data': {'input': '1234'}, 'name': 'reverse_and_double', 'tags': [], 'run_id': '04726e2e-f508-4f90-9d4f-f88e588f0b39', 'metadata': {}, 'parent_ids': []}{'event': 'on_chain_stream', 'run_id': '04726e2e-f508-4f90-9d4f-f88e588f0b39', 'name': 'reverse_and_double', 'tags': [], 'metadata': {}, 'data': {'chunk': '43214321'}, 'parent_ids': []}{'event': 'on_chain_end', 'data': {'output': '43214321'}, 'run_id': '04726e2e-f508-4f90-9d4f-f88e588f0b39', 'name': 'reverse_and_double', 'tags': [], 'metadata': {}, 'parent_ids': []}\n```\n\nAnd with the @chain decorator:\n\n```\nfrom langchain_core.runnables import chain@chainasync def reverse_and_double(word: str):    return await reverse_word.ainvoke(word) * 2await reverse_and_double.ainvoke(\"1234\")async for event in reverse_and_double.astream_events(\"1234\"):    print(event)\n```\n\n```\n{'event': 'on_chain_start', 'data': {'input': '1234'}, 'name': 'reverse_and_double', 'tags': [], 'run_id': '25f72976-aa79-408d-bb42-6d0f038cde52', 'metadata': {}, 'parent_ids': []}{'event': 'on_chain_stream', 'run_id': '25f72976-aa79-408d-bb42-6d0f038cde52', 'name': 'reverse_and_double', 'tags': [], 'metadata': {}, 'data': {'chunk': '43214321'}, 'parent_ids': []}{'event': 'on_chain_end', 'data': {'output': '43214321'}, 'run_id': '25f72976-aa79-408d-bb42-6d0f038cde52', 'name': 'reverse_and_double', 'tags': [], 'metadata': {}, 'parent_ids': []}\n```", "url_final": "https://python.langchain.com/docs/how_to/streaming/", "url_citable": "https://python.langchain.com/docs/how_to/streaming/#propagating-callbacks", "title": "How to stream runnables | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Propagating Callbacks‚Äã", "section_anchor": "propagating-callbacks", "fetched_at": "2025-10-06T21:56:28.670573", "has_code": true}
{"id": "afafa86f915a7c68f372802df6b8d389da93fa94", "text": "Next steps‚Äã\n\nNow you've learned some ways to stream both final outputs and internal steps with LangChain.\n\nTo learn more, check out the other how-to guides in this section, or the conceptual guide on Langchain Expression Language.\n\n- LLMs and Chat Models\n- Chains\n- Working with Input Streams\n- Non-streaming components\n\n- Event Reference\n- Chat Model\n- Chain\n- Filtering Events\n- Non-streaming components\n- Propagating Callbacks", "url_final": "https://python.langchain.com/docs/how_to/streaming/", "url_citable": "https://python.langchain.com/docs/how_to/streaming/#next-steps", "title": "How to stream runnables | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Next steps‚Äã", "section_anchor": "next-steps", "fetched_at": "2025-10-06T21:56:28.670573", "has_code": false}
{"id": "c3d83e0eb9e820d5d08817ab0abb15b2ec1e6af5", "text": "Code Example‚Äã\n\nLet's see a concrete example of what this looks like in code. We will use the Cassandra/CQL interface of Astra DB for this example.\n\nInstall the following Python package:\n\n```\n!pip install \"cassio>=0.1.7\"\n```\n\nGet the connection secrets.\n\nInitialize cassio:\n\n```\nimport cassiocassio.init(    database_id=\"Your database ID\",    token=\"Your application token\",    keyspace=\"Your key space\",)\n```\n\nCreate the Cassandra VectorStore with a standard index analyzer. The index analyzer is needed to enable term matching.\n\n```\nfrom cassio.table.cql import STANDARD_ANALYZERfrom langchain_community.vectorstores import Cassandrafrom langchain_openai import OpenAIEmbeddingsembeddings = OpenAIEmbeddings()vectorstore = Cassandra(    embedding=embeddings,    table_name=\"test_hybrid\",    body_index_options=[STANDARD_ANALYZER],    session=None,    keyspace=None,)vectorstore.add_texts(    [        \"In 2023, I visited Paris\",        \"In 2022, I visited New York\",        \"In 2021, I visited New Orleans\",    ])\n```\n\nIf we do a standard similarity search, we get all the documents:\n\n```\nvectorstore.as_retriever().invoke(\"What city did I visit last?\")\n```\n\n```\n[Document(page_content='In 2022, I visited New York'),Document(page_content='In 2023, I visited Paris'),Document(page_content='In 2021, I visited New Orleans')]\n```\n\nThe Astra DB vectorstore body_search argument can be used to filter the search on the term new.\n\n```\nvectorstore.as_retriever(search_kwargs={\"body_search\": \"new\"}).invoke(    \"What city did I visit last?\")\n```\n\n```\n[Document(page_content='In 2022, I visited New York'),Document(page_content='In 2021, I visited New Orleans')]\n```\n\nWe can now create the chain that we will use to do question-answering over\n\n```\nfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import (    ConfigurableField,    RunnablePassthrough,)from langchain_openai import ChatOpenAI\n```\n\nThis is basic question-answering chain set up.\n\n```\ntemplate = \"\"\"Answer the question based only on the following context:{context}Question: {question}\"\"\"prompt = ChatPromptTemplate.from_template(template)model = ChatOpenAI()retriever = vectorstore.as_retriever()\n```\n\nHere we mark the retriever as having a configurable field. All vectorstore retrievers have search_kwargs as a field. This is just a dictionary, with vectorstore specific fields\n\n```\nconfigurable_retriever = retriever.configurable_fields(    search_kwargs=ConfigurableField(        id=\"search_kwargs\",        name=\"Search Kwargs\",        description=\"The search kwargs to use\",    ))\n```\n\nWe can now create the chain using our configurable retriever\n\n```\nchain = (    {\"context\": configurable_retriever, \"question\": RunnablePassthrough()}    | prompt    | model    | StrOutputParser())\n```\n\n```\nchain.invoke(\"What city did I visit last?\")\n```\n\n```\nParis\n```\n\nWe can now invoke the chain with configurable options. search_kwargs is the id of the configurable field. The value is the search kwargs to use for Astra DB.\n\n```\nchain.invoke(    \"What city did I visit last?\",    config={\"configurable\": {\"search_kwargs\": {\"body_search\": \"new\"}}},)\n```\n\n```\nNew York\n```\n\n- Code Example", "url_final": "https://python.langchain.com/docs/how_to/hybrid/", "url_citable": "https://python.langchain.com/docs/how_to/hybrid/#code-example", "title": "Hybrid Search | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Code Example‚Äã", "section_anchor": "code-example", "fetched_at": "2025-10-06T21:55:42.785719", "has_code": true}
{"id": "86a95c6a6be05a1c38194fc9c932961f6cd8bf5a", "text": "Setup‚Äã\n\nDependencies‚Äã\n\nWe'll use OpenAI embeddings and an InMemory vector store in this walkthrough, but everything shown here works with any Embeddings, and VectorStore or Retriever.\n\nWe'll use the following packages:\n\n```\n%%capture --no-stderr%pip install --upgrade --quiet langgraph langchain-community beautifulsoup4\n```\n\nLangSmith‚Äã\n\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\n\nNote that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:\n\n```\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"if not os.environ.get(\"LANGSMITH_API_KEY\"):    os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n```\n\nComponents‚Äã\n\nWe will need to select three components from LangChain's suite of integrations.\n\nA chat model:\n\n```\npip install -qU \"langchain[google-genai]\"\n```\n\n```\nimport getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n```\n\nAn embedding model:\n\n```\npip install -qU langchain-openai\n```\n\n```\nimport getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"):  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")from langchain_openai import OpenAIEmbeddingsembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n```\n\nAnd a vector store:\n\n```\npip install -qU langchain-core\n```\n\n```\nfrom langchain_core.vectorstores import InMemoryVectorStorevector_store = InMemoryVectorStore(embeddings)\n```", "url_final": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "url_citable": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/#setup", "title": "How to add chat history | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Setup‚Äã", "section_anchor": "setup", "fetched_at": "2025-10-06T21:56:08.888587", "has_code": true}
{"id": "a45fcc10d0ac4400f8933445ea78c5daac10a001", "text": "Chains‚Äã\n\nThe RAG Tutorial indexes an LLM Powered Autonomous Agents blog post by Lilian Weng. We will repeat that here. Below we load the content of the page, split it into sub-documents, and embed the documents into our vector store:\n\n```\nimport bs4from langchain import hubfrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_core.documents import Documentfrom langchain_text_splitters import RecursiveCharacterTextSplitterfrom typing_extensions import List, TypedDict# Load and chunk contents of the blogloader = WebBaseLoader(    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),    bs_kwargs=dict(        parse_only=bs4.SoupStrainer(            class_=(\"post-content\", \"post-title\", \"post-header\")        )    ),)docs = loader.load()text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)all_splits = text_splitter.split_documents(docs)\n```\n\n```\n# Index chunks_ = vector_store.add_documents(documents=all_splits)\n```\n\nAs detailed in Part 2 of the RAG tutorial, we can naturally support a conversational experience by representing the flow of the RAG application as a sequence of messages:\n\n- User input as a HumanMessage;\n- Vector store query as an AIMessage with tool calls;\n- Retrieved documents as a ToolMessage;\n- Final response as a AIMessage.\n\nWe will use tool-calling to facilitate this, which additionally allows the query to be generated by the LLM. We can build a tool to execute the retrieval step:\n\n```\nfrom langchain_core.tools import tool@tool(response_format=\"content_and_artifact\")def retrieve(query: str):    \"\"\"Retrieve information related to a query.\"\"\"    retrieved_docs = vector_store.similarity_search(query, k=2)    serialized = \"\\n\\n\".join(        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")        for doc in retrieved_docs    )    return serialized, retrieved_docs\n```\n\nWe can now build our LangGraph application.\n\nNote that we compile it with a checkpointer to support a back-and-forth conversation. LangGraph comes with a simple in-memory checkpointer, which we use below. See its documentation for more detail, including how to use different persistence backends (e.g., SQLite or Postgres).\n\n```\nfrom langchain_core.messages import SystemMessagefrom langgraph.checkpoint.memory import MemorySaverfrom langgraph.graph import END, MessagesState, StateGraphfrom langgraph.prebuilt import ToolNode, tools_condition# Step 1: Generate an AIMessage that may include a tool-call to be sent.def query_or_respond(state: MessagesState):    \"\"\"Generate tool call for retrieval or respond.\"\"\"    llm_with_tools = llm.bind_tools([retrieve])    response = llm_with_tools.invoke(state[\"messages\"])    # MessagesState appends messages to state instead of overwriting    return {\"messages\": [response]}# Step 2: Execute the retrieval.tools = ToolNode([retrieve])# Step 3: Generate a response using the retrieved content.def generate(state: MessagesState):    \"\"\"Generate answer.\"\"\"    # Get generated ToolMessages    recent_tool_messages = []    for message in reversed(state[\"messages\"]):        if message.type == \"tool\":            recent_tool_messages.append(message)        else:            break    tool_messages = recent_tool_messages[::-1]    # Format into prompt    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)    system_message_content = (        \"You are an assistant for question-answering tasks. \"        \"Use the following pieces of retrieved context to answer \"        \"the question. If you don't know the answer, say that you \"        \"don't know. Use three sentences maximum and keep the \"        \"answer concise.\"        \"\\n\\n\"        f\"{docs_content}\"    )    conversation_messages = [        message        for message in state[\"messages\"]        if message.type in (\"human\", \"system\")        or (message.type == \"ai\" and not message.tool_calls)    ]    prompt = [SystemMessage(system_message_content)] + conversation_messages    # Run    response = llm.invoke(prompt)    return {\"messages\": [response]}# Build graphgraph_builder = StateGraph(MessagesState)graph_builder.add_node(query_or_respond)graph_builder.add_node(tools)graph_builder.add_node(generate)graph_builder.set_entry_point(\"query_or_respond\")graph_builder.add_conditional_edges(    \"query_or_respond\",    tools_condition,    {END: END, \"tools\": \"tools\"},)graph_builder.add_edge(\"tools\", \"generate\")graph_builder.add_edge(\"generate\", END)memory = MemorySaver()graph = graph_builder.compile(checkpointer=memory)\n```\n\n```\nfrom IPython.display import Image, displaydisplay(Image(graph.get_graph().draw_mermaid_png()))\n```\n\nLet's test our application.\n\nNote that it responds appropriately to messages that do not require an additional retrieval step:\n\n```\n# Specify an ID for the threadconfig = {\"configurable\": {\"thread_id\": \"abc123\"}}\n```\n\n```\ninput_message = \"Hello\"for step in graph.stream(    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},    stream_mode=\"values\",    config=config,):    step[\"messages\"][-1].pretty_print()\n```\n\n```\n================================\u001b[1m Human Message \u001b[0m=================================Hello==================================\u001b[1m Ai Message \u001b[0m==================================Hello! How can I assist you today?\n```\n\nAnd when executing a search, we can stream the steps to observe the query generation, retrieval, and answer generation:\n\n```\ninput_message = \"What is Task Decomposition?\"for step in graph.stream(    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},    stream_mode=\"values\",    config=config,):    step[\"messages\"][-1].pretty_print()\n```\n\n```\n================================\u001b[1m Human Message \u001b[0m=================================What is Task Decomposition?==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls:  retrieve (call_RntwX5GMt531biEE9MqSbgLV) Call ID: call_RntwX5GMt531biEE9MqSbgLV  Args:    query: Task Decomposition=================================\u001b[1m Tool Message \u001b[0m=================================Name: retrieveSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}Content: Fig. 1. Overview of a LLM-powered autonomous agent system.Component One: Planning#A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.Task Decomposition#Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.==================================\u001b[1m Ai Message \u001b[0m==================================Task Decomposition is the process of breaking down a complicated task into smaller, more manageable steps. It often involves techniques like Chain of Thought (CoT), where the model is prompted to \"think step by step,\" allowing for better handling of complex tasks. This approach enhances model performance and provides insight into the model's reasoning process.\n```\n\nFinally, because we have compiled our application with a checkpointer, historical messages are maintained in the state. This allows the model to contextualize user queries:\n\n```\ninput_message = \"Can you look up some common ways of doing it?\"for step in graph.stream(    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},    stream_mode=\"values\",    config=config,):    step[\"messages\"][-1].pretty_print()\n```\n\n```\n================================\u001b[1m Human Message \u001b[0m=================================Can you look up some common ways of doing it?==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls:  retrieve (call_kwO5rYPyJ0MftYKoKRFjKpZM) Call ID: call_kwO5rYPyJ0MftYKoKRFjKpZM  Args:    query: common methods for task decomposition=================================\u001b[1m Tool Message \u001b[0m=================================Name: retrieveSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}Content: Fig. 1. Overview of a LLM-powered autonomous agent system.Component One: Planning#A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.Task Decomposition#Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.==================================\u001b[1m Ai Message \u001b[0m==================================Common ways of Task Decomposition include: (1) using large language models (LLMs) with simple prompts like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\"; (2) utilizing task-specific instructions, such as \"Write a story outline\" for creative tasks; and (3) incorporating human inputs to guide the decomposition process.\n```\n\nNote that we can observe the full sequence of messages sent to the chat model-- including tool calls and retrieved context-- in the LangSmith trace.\n\nThe conversation history can also be inspected via the state of the application:\n\n```\nchat_history = graph.get_state(config).values[\"messages\"]for message in chat_history:    message.pretty_print()\n```\n\n```\n================================\u001b[1m Human Message \u001b[0m=================================Hello==================================\u001b[1m Ai Message \u001b[0m==================================Hello! How can I assist you today?================================\u001b[1m Human Message \u001b[0m=================================What is Task Decomposition?==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls:  retrieve (call_RntwX5GMt531biEE9MqSbgLV) Call ID: call_RntwX5GMt531biEE9MqSbgLV  Args:    query: Task Decomposition=================================\u001b[1m Tool Message \u001b[0m=================================Name: retrieveSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}Content: Fig. 1. Overview of a LLM-powered autonomous agent system.Component One: Planning#A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.Task Decomposition#Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.==================================\u001b[1m Ai Message \u001b[0m==================================Task Decomposition is the process of breaking down a complicated task into smaller, more manageable steps. It often involves techniques like Chain of Thought (CoT), where the model is prompted to \"think step by step,\" allowing for better handling of complex tasks. This approach enhances model performance and provides insight into the model's reasoning process.================================\u001b[1m Human Message \u001b[0m=================================Can you look up some common ways of doing it?==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls:  retrieve (call_kwO5rYPyJ0MftYKoKRFjKpZM) Call ID: call_kwO5rYPyJ0MftYKoKRFjKpZM  Args:    query: common methods for task decomposition=================================\u001b[1m Tool Message \u001b[0m=================================Name: retrieveSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}Content: Fig. 1. Overview of a LLM-powered autonomous agent system.Component One: Planning#A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.Task Decomposition#Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.==================================\u001b[1m Ai Message \u001b[0m==================================Common ways of Task Decomposition include: (1) using large language models (LLMs) with simple prompts like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\"; (2) utilizing task-specific instructions, such as \"Write a story outline\" for creative tasks; and (3) incorporating human inputs to guide the decomposition process.\n```", "url_final": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "url_citable": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/#chains", "title": "How to add chat history | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Chains‚Äã", "section_anchor": "chains", "fetched_at": "2025-10-06T21:56:08.888587", "has_code": true}
{"id": "6fa1f0c12d68cf63aa7e77fea24ef5d7859a3a56", "text": "Agents‚Äã\n\nAgents leverage the reasoning capabilities of LLMs to make decisions during execution. Using agents allows you to offload additional discretion over the retrieval process. Although their behavior is less predictable than the above \"chain\", they are able to execute multiple retrieval steps in service of a query, or iterate on a single search.\n\nBelow we assemble a minimal RAG agent. Using LangGraph's pre-built ReAct agent constructor, we can do this in one line.\n\nCheck out LangGraph's Agentic RAG tutorial for more advanced formulations.\n\n```\nfrom langgraph.prebuilt import create_react_agentagent_executor = create_react_agent(llm, [retrieve], checkpointer=memory)\n```\n\nLet's inspect the graph:\n\n```\ndisplay(Image(agent_executor.get_graph().draw_mermaid_png()))\n```\n\nThe key difference from our earlier implementation is that instead of a final generation step that ends the run, here the tool invocation loops back to the original LLM call. The model can then either answer the question using the retrieved context, or generate another tool call to obtain more information.\n\nLet's test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\n\n```\nconfig = {\"configurable\": {\"thread_id\": \"def234\"}}input_message = (    \"What is the standard method for Task Decomposition?\\n\\n\"    \"Once you get the answer, look up common extensions of that method.\")for event in agent_executor.stream(    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},    stream_mode=\"values\",    config=config,):    event[\"messages\"][-1].pretty_print()\n```\n\n```\n================================\u001b[1m Human Message \u001b[0m=================================What is the standard method for Task Decomposition?Once you get the answer, look up common extensions of that method.==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls:  retrieve (call_rxBqio7dxthnMuzjr4AIquSZ) Call ID: call_rxBqio7dxthnMuzjr4AIquSZ  Args:    query: standard method for Task Decomposition=================================\u001b[1m Tool Message \u001b[0m=================================Name: retrieveSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}Content: Fig. 1. Overview of a LLM-powered autonomous agent system.Component One: Planning#A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.Task Decomposition#Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls:  retrieve (call_kmQMRWCKeBdtXdlJi8yZD9CO) Call ID: call_kmQMRWCKeBdtXdlJi8yZD9CO  Args:    query: common extensions of Task Decomposition methods=================================\u001b[1m Tool Message \u001b[0m=================================Name: retrieveSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}Content: Fig. 1. Overview of a LLM-powered autonomous agent system.Component One: Planning#A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.Task Decomposition#Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.==================================\u001b[1m Ai Message \u001b[0m==================================The standard method for Task Decomposition involves breaking down complex tasks into smaller, manageable steps. Here are the main techniques:1. **Chain of Thought (CoT)**: This prompting technique encourages a model to \"think step by step,\" allowing it to utilize more computational resources during testing to decompose challenging tasks into simpler parts. CoT not only simplifies tasks but also provides insights into the model's reasoning process.2. **Simple Prompting**: This can involve straightforward queries like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\" to guide the model in identifying the necessary steps.3. **Task-specific Instructions**: Using specific prompts tailored to the task at hand, such as \"Write a story outline\" for creative writing, allows for more directed decomposition.4. **Human Inputs**: Involving human expertise can also aid in breaking down tasks effectively.### Common Extensions of Task Decomposition Methods1. **Tree of Thoughts**: This method extends CoT by exploring multiple reasoning possibilities at each step. It decomposes the problem into various thought steps and generates multiple thoughts per step, forming a tree structure. This can utilize search processes like breadth-first search (BFS) or depth-first search (DFS) to evaluate states through classifiers or majority voting.These extensions build on the basic principles of task decomposition, enhancing the depth and breadth of reasoning applied to complex tasks.\n```\n\nNote that the agent:\n\n- Generates a query to search for a standard method for task decomposition;\n- Receiving the answer, generates a second query to search for common extensions of it;\n- Having received all necessary context, answers the question.\n\nWe can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.", "url_final": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "url_citable": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/#agents", "title": "How to add chat history | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Agents‚Äã", "section_anchor": "agents", "fetched_at": "2025-10-06T21:56:08.888587", "has_code": true}
{"id": "04f2bf870735878c9974c7e5403a447dbb9b276f", "text": "Next steps‚Äã\n\nWe've covered the steps to build a basic conversational Q&A application:\n\n- We used chains to build a predictable application that generates search queries for each user input;\n- We used agents to build an application that \"decides\" when and how to generate search queries.\n\nTo explore different types of retrievers and retrieval strategies, visit the retrievers section of the how-to guides.\n\nFor a detailed walkthrough of LangChain's conversation memory abstractions, visit the How to add message history (memory) LCEL page.\n\nTo learn more about agents, head to the Agents Modules.\n\n- Dependencies\n- LangSmith\n- Components", "url_final": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "url_citable": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/#next-steps", "title": "How to add chat history | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Next steps‚Äã", "section_anchor": "next-steps", "fetched_at": "2025-10-06T21:56:08.888587", "has_code": false}
{"id": "b8dc44c4048346ed93176c19be7d5b8f18c5ad20", "text": "Setup‚Äã\n\nWe'll need to install the following packages:\n\n```\n%pip install --upgrade --quiet langchain-core langchain-openai\n```\n\nIf you'd like to trace your runs in LangSmith uncomment and set the following environment variables:\n\n```\nimport getpassimport os# os.environ[\"LANGSMITH_TRACING\"] = \"true\"# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n```", "url_final": "https://python.langchain.com/docs/how_to/tools_error/", "url_citable": "https://python.langchain.com/docs/how_to/tools_error/#setup", "title": "How to handle tool errors | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Setup‚Äã", "section_anchor": "setup", "fetched_at": "2025-10-06T21:56:42.882964", "has_code": true}
{"id": "21dbc8773ce53d1628a2f8a2f600cac8a916e9e0", "text": "Chain‚Äã\n\nSuppose we have the following (dummy) tool and tool-calling chain. We'll make our tool intentionally convoluted to try and trip up the model.\n\n```\npip install -qU \"langchain[google-genai]\"\n```\n\n```\nimport getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n```\n\n```\n# Define toolfrom langchain_core.tools import tool@tooldef complex_tool(int_arg: int, float_arg: float, dict_arg: dict) -> int:    \"\"\"Do something complex with a complex tool.\"\"\"    return int_arg * float_argllm_with_tools = llm.bind_tools(    [complex_tool],)# Define chainchain = llm_with_tools | (lambda msg: msg.tool_calls[0][\"args\"]) | complex_tool\n```\n\nWe can see that when we try to invoke this chain with even a fairly explicit input, the model fails to correctly call the tool (it forgets the dict_arg argument).\n\n```\nchain.invoke(    \"use complex tool. the args are 5, 2.1, empty dictionary. don't forget dict_arg\")\n```\n\n```\n---------------------------------------------------------------------------``````outputValidationError                           Traceback (most recent call last)``````outputCell In[5], line 1----> 1 chain.invoke(      2     \"use complex tool. the args are 5, 2.1, empty dictionary. don't forget dict_arg\"      3 )``````outputFile ~/langchain/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:2998, in RunnableSequence.invoke(self, input, config, **kwargs)   2996             input = context.run(step.invoke, input, config, **kwargs)   2997         else:-> 2998             input = context.run(step.invoke, input, config)   2999 # finish the root run   3000 except BaseException as e:``````outputFile ~/langchain/.venv/lib/python3.11/site-packages/langchain_core/tools/base.py:456, in BaseTool.invoke(self, input, config, **kwargs)    449 def invoke(    450     self,    451     input: Union[str, Dict, ToolCall],    452     config: Optional[RunnableConfig] = None,    453     **kwargs: Any,    454 ) -> Any:    455     tool_input, kwargs = _prep_run_args(input, config, **kwargs)--> 456     return self.run(tool_input, **kwargs)``````outputFile ~/langchain/.venv/lib/python3.11/site-packages/langchain_core/tools/base.py:659, in BaseTool.run(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)    657 if error_to_raise:    658     run_manager.on_tool_error(error_to_raise)--> 659     raise error_to_raise    660 output = _format_output(content, artifact, tool_call_id, self.name, status)    661 run_manager.on_tool_end(output, color=color, name=self.name, **kwargs)``````outputFile ~/langchain/.venv/lib/python3.11/site-packages/langchain_core/tools/base.py:622, in BaseTool.run(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)    620 context = copy_context()    621 context.run(_set_config_context, child_config)--> 622 tool_args, tool_kwargs = self._to_args_and_kwargs(tool_input)    623 if signature(self._run).parameters.get(\"run_manager\"):    624     tool_kwargs[\"run_manager\"] = run_manager``````outputFile ~/langchain/.venv/lib/python3.11/site-packages/langchain_core/tools/base.py:545, in BaseTool._to_args_and_kwargs(self, tool_input)    544 def _to_args_and_kwargs(self, tool_input: Union[str, Dict]) -> Tuple[Tuple, Dict]:--> 545     tool_input = self._parse_input(tool_input)    546     # For backwards compatibility, if run_input is a string,    547     # pass as a positional argument.    548     if isinstance(tool_input, str):``````outputFile ~/langchain/.venv/lib/python3.11/site-packages/langchain_core/tools/base.py:487, in BaseTool._parse_input(self, tool_input)    485 if input_args is not None:    486     if issubclass(input_args, BaseModel):--> 487         result = input_args.model_validate(tool_input)    488         result_dict = result.model_dump()    489     elif issubclass(input_args, BaseModelV1):``````outputFile ~/langchain/.venv/lib/python3.11/site-packages/pydantic/main.py:568, in BaseModel.model_validate(cls, obj, strict, from_attributes, context)    566 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks    567 __tracebackhide__ = True--> 568 return cls.__pydantic_validator__.validate_python(    569     obj, strict=strict, from_attributes=from_attributes, context=context    570 )``````outputValidationError: 1 validation error for complex_toolSchemadict_arg  Field required [type=missing, input_value={'int_arg': 5, 'float_arg': 2.1}, input_type=dict]    For further information visit https://errors.pydantic.dev/2.8/v/missing\n```", "url_final": "https://python.langchain.com/docs/how_to/tools_error/", "url_citable": "https://python.langchain.com/docs/how_to/tools_error/#chain", "title": "How to handle tool errors | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Chain‚Äã", "section_anchor": "chain", "fetched_at": "2025-10-06T21:56:42.882964", "has_code": true}
{"id": "3fbe7ce1b064d617cf72d6709decc0ed976b4018", "text": "Try/except tool call‚Äã\n\nThe simplest way to more gracefully handle errors is to try/except the tool-calling step and return a helpful message on errors:\n\n```\nfrom typing import Anyfrom langchain_core.runnables import Runnable, RunnableConfigdef try_except_tool(tool_args: dict, config: RunnableConfig) -> Runnable:    try:        complex_tool.invoke(tool_args, config=config)    except Exception as e:        return f\"Calling tool with arguments:\\n\\n{tool_args}\\n\\nraised the following error:\\n\\n{type(e)}: {e}\"chain = llm_with_tools | (lambda msg: msg.tool_calls[0][\"args\"]) | try_except_toolprint(    chain.invoke(        \"use complex tool. the args are 5, 2.1, empty dictionary. don't forget dict_arg\"    ))\n```\n\n```\nCalling tool with arguments:{'int_arg': 5, 'float_arg': 2.1}raised the following error:<class 'pydantic_core._pydantic_core.ValidationError'>: 1 validation error for complex_toolSchemadict_arg  Field required [type=missing, input_value={'int_arg': 5, 'float_arg': 2.1}, input_type=dict]    For further information visit https://errors.pydantic.dev/2.8/v/missing\n```", "url_final": "https://python.langchain.com/docs/how_to/tools_error/", "url_citable": "https://python.langchain.com/docs/how_to/tools_error/#tryexcept-tool-call", "title": "How to handle tool errors | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Try/except tool call‚Äã", "section_anchor": "tryexcept-tool-call", "fetched_at": "2025-10-06T21:56:42.882964", "has_code": true}
{"id": "073f52ce8fba8ef5d77277b23e9994307134cffa", "text": "Fallbacks‚Äã\n\nWe can also try to fallback to a better model in the event of a tool invocation error. In this case we'll fall back to an identical chain that uses gpt-4-1106-preview instead of gpt-3.5-turbo.\n\n```\nchain = llm_with_tools | (lambda msg: msg.tool_calls[0][\"args\"]) | complex_toolbetter_model = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0).bind_tools(    [complex_tool], tool_choice=\"complex_tool\")better_chain = better_model | (lambda msg: msg.tool_calls[0][\"args\"]) | complex_toolchain_with_fallback = chain.with_fallbacks([better_chain])chain_with_fallback.invoke(    \"use complex tool. the args are 5, 2.1, empty dictionary. don't forget dict_arg\")\n```\n\n```\n10.5\n```\n\nLooking at the LangSmith trace for this chain run, we can see that the first chain call fails as expected and it's the fallback that succeeds.", "url_final": "https://python.langchain.com/docs/how_to/tools_error/", "url_citable": "https://python.langchain.com/docs/how_to/tools_error/#fallbacks", "title": "How to handle tool errors | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Fallbacks‚Äã", "section_anchor": "fallbacks", "fetched_at": "2025-10-06T21:56:42.882964", "has_code": true}
{"id": "00a55aea5f664481f1d7aae5f0b320676586f7f9", "text": "Retry with exception‚Äã\n\nTo take things one step further, we can try to automatically re-run the chain with the exception passed in, so that the model may be able to correct its behavior:\n\n```\nfrom langchain_core.messages import AIMessage, HumanMessage, ToolCall, ToolMessagefrom langchain_core.prompts import ChatPromptTemplateclass CustomToolException(Exception):    \"\"\"Custom LangChain tool exception.\"\"\"    def __init__(self, tool_call: ToolCall, exception: Exception) -> None:        super().__init__()        self.tool_call = tool_call        self.exception = exceptiondef tool_custom_exception(msg: AIMessage, config: RunnableConfig) -> Runnable:    try:        return complex_tool.invoke(msg.tool_calls[0][\"args\"], config=config)    except Exception as e:        raise CustomToolException(msg.tool_calls[0], e)def exception_to_messages(inputs: dict) -> dict:    exception = inputs.pop(\"exception\")    # Add historical messages to the original input, so the model knows that it made a mistake with the last tool call.    messages = [        AIMessage(content=\"\", tool_calls=[exception.tool_call]),        ToolMessage(            tool_call_id=exception.tool_call[\"id\"], content=str(exception.exception)        ),        HumanMessage(            content=\"The last tool call raised an exception. Try calling the tool again with corrected arguments. Do not repeat mistakes.\"        ),    ]    inputs[\"last_output\"] = messages    return inputs# We add a last_output MessagesPlaceholder to our prompt which if not passed in doesn't# affect the prompt at all, but gives us the option to insert an arbitrary list of Messages# into the prompt if needed. We'll use this on retries to insert the error message.prompt = ChatPromptTemplate.from_messages(    [(\"human\", \"{input}\"), (\"placeholder\", \"{last_output}\")])chain = prompt | llm_with_tools | tool_custom_exception# If the initial chain call fails, we rerun it withe the exception passed in as a message.self_correcting_chain = chain.with_fallbacks(    [exception_to_messages | chain], exception_key=\"exception\")\n```\n\n```\nself_correcting_chain.invoke(    {        \"input\": \"use complex tool. the args are 5, 2.1, empty dictionary. don't forget dict_arg\"    })\n```\n\n```\n10.5\n```\n\nAnd our chain succeeds! Looking at the LangSmith trace, we can see that indeed our initial chain still fails, and it's only on retrying that the chain succeeds.", "url_final": "https://python.langchain.com/docs/how_to/tools_error/", "url_citable": "https://python.langchain.com/docs/how_to/tools_error/#retry-with-exception", "title": "How to handle tool errors | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Retry with exception‚Äã", "section_anchor": "retry-with-exception", "fetched_at": "2025-10-06T21:56:42.882964", "has_code": true}
{"id": "00b5c2e60729e2ff4cd7a22ed69b6aabd44fb411", "text": "Next steps‚Äã\n\nNow you've seen some strategies how to handle tool calling errors. Next, you can learn more about how to use tools:\n\n- Few shot prompting with tools\n- Stream tool calls\n- Pass runtime values to tools\n\nYou can also check out some more specific uses of tool calling:\n\n- Getting structured outputs from models\n\n- Setup\n- Chain\n- Try/except tool call\n- Fallbacks\n- Retry with exception\n- Next steps", "url_final": "https://python.langchain.com/docs/how_to/tools_error/", "url_citable": "https://python.langchain.com/docs/how_to/tools_error/#next-steps", "title": "How to handle tool errors | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Next steps‚Äã", "section_anchor": "next-steps", "fetched_at": "2025-10-06T21:56:42.882964", "has_code": false}
{"id": "813bb4703b55d4a46da009f02fe1842618ee36d0", "text": "Setup‚Äã\n\n```\n# %pip install -qU langchain-core langchain-openai\n```\n\nWe'll use OpenAI in this example:\n\n```\nimport getpassimport osif \"OPENAI_API_KEY\" not in os.environ:    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()# Optional, uncomment to trace runs with LangSmith. Sign up here: https://smith.langchain.com.# os.environ[\"LANGSMITH_TRACING\"] = \"true\"# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n```", "url_final": "https://python.langchain.com/docs/how_to/query_few_shot/", "url_citable": "https://python.langchain.com/docs/how_to/query_few_shot/#setup", "title": "How to add examples to the prompt for query analysis | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Setup‚Äã", "section_anchor": "setup", "fetched_at": "2025-10-06T21:54:29.103643", "has_code": true}
{"id": "6293ca5c94f67fd1013ba3c736ded620b799178d", "text": "Query schema‚Äã\n\nWe'll define a query schema that we want our model to output. To make our query analysis a bit more interesting, we'll add a sub_queries field that contains more narrow questions derived from the top level question.\n\n```\nfrom typing import List, Optionalfrom pydantic import BaseModel, Fieldsub_queries_description = \"\"\"\\If the original question contains multiple distinct sub-questions, \\or if there are more generic questions that would be helpful to answer in \\order to answer the original question, write a list of all relevant sub-questions. \\Make sure this list is comprehensive and covers all parts of the original question. \\It's ok if there's redundancy in the sub-questions. \\Make sure the sub-questions are as narrowly focused as possible.\"\"\"class Search(BaseModel):    \"\"\"Search over a database of tutorial videos about a software library.\"\"\"    query: str = Field(        ...,        description=\"Primary similarity search query applied to video transcripts.\",    )    sub_queries: List[str] = Field(        default_factory=list, description=sub_queries_description    )    publish_year: Optional[int] = Field(None, description=\"Year video was published\")\n```", "url_final": "https://python.langchain.com/docs/how_to/query_few_shot/", "url_citable": "https://python.langchain.com/docs/how_to/query_few_shot/#query-schema", "title": "How to add examples to the prompt for query analysis | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Query schema‚Äã", "section_anchor": "query-schema", "fetched_at": "2025-10-06T21:54:29.103643", "has_code": true}
{"id": "b945a0cae47ffb4f29865cf635dc23a14cb0314c", "text": "Query generation‚Äã\n\n```\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholderfrom langchain_core.runnables import RunnablePassthroughfrom langchain_openai import ChatOpenAIsystem = \"\"\"You are an expert at converting user questions into database queries. \\You have access to a database of tutorial videos about a software library for building LLM-powered applications. \\Given a question, return a list of database queries optimized to retrieve the most relevant results.If there are acronyms or words you are not familiar with, do not try to rephrase them.\"\"\"prompt = ChatPromptTemplate.from_messages(    [        (\"system\", system),        MessagesPlaceholder(\"examples\", optional=True),        (\"human\", \"{question}\"),    ])llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)structured_llm = llm.with_structured_output(Search)query_analyzer = {\"question\": RunnablePassthrough()} | prompt | structured_llm\n```\n\nLet's try out our query analyzer without any examples in the prompt:\n\n```\nquery_analyzer.invoke(    \"what's the difference between web voyager and reflection agents? do both use langgraph?\")\n```\n\n```\nSearch(query='difference between web voyager and reflection agents', sub_queries=['what is web voyager', 'what are reflection agents', 'do both web voyager and reflection agents use langgraph?'], publish_year=None)\n```", "url_final": "https://python.langchain.com/docs/how_to/query_few_shot/", "url_citable": "https://python.langchain.com/docs/how_to/query_few_shot/#query-generation", "title": "How to add examples to the prompt for query analysis | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Query generation‚Äã", "section_anchor": "query-generation", "fetched_at": "2025-10-06T21:54:29.103643", "has_code": true}
{"id": "49fd07fb576e1e245c6c6de224e2f5c6435b0d0e", "text": "Adding examples and tuning the prompt‚Äã\n\nThis works pretty well, but we probably want it to decompose the question even further to separate the queries about Web Voyager and Reflection Agents.\n\nTo tune our query generation results, we can add some examples of inputs questions and gold standard output queries to our prompt.\n\n```\nexamples = []\n```\n\n```\nquestion = \"What's chat langchain, is it a langchain template?\"query = Search(    query=\"What is chat langchain and is it a langchain template?\",    sub_queries=[\"What is chat langchain\", \"What is a langchain template\"],)examples.append({\"input\": question, \"tool_calls\": [query]})\n```\n\n```\nquestion = \"How to build multi-agent system and stream intermediate steps from it\"query = Search(    query=\"How to build multi-agent system and stream intermediate steps from it\",    sub_queries=[        \"How to build multi-agent system\",        \"How to stream intermediate steps from multi-agent system\",        \"How to stream intermediate steps\",    ],)examples.append({\"input\": question, \"tool_calls\": [query]})\n```\n\n```\nquestion = \"LangChain agents vs LangGraph?\"query = Search(    query=\"What's the difference between LangChain agents and LangGraph? How do you deploy them?\",    sub_queries=[        \"What are LangChain agents\",        \"What is LangGraph\",        \"How do you deploy LangChain agents\",        \"How do you deploy LangGraph\",    ],)examples.append({\"input\": question, \"tool_calls\": [query]})\n```\n\nNow we need to update our prompt template and chain so that the examples are included in each prompt. Since we're working with OpenAI function-calling, we'll need to do a bit of extra structuring to send example inputs and outputs to the model. We'll create a tool_example_to_messages helper function to handle this for us:\n\n```\nimport uuidfrom typing import Dictfrom langchain_core.messages import (    AIMessage,    BaseMessage,    HumanMessage,    SystemMessage,    ToolMessage,)def tool_example_to_messages(example: Dict) -> List[BaseMessage]:    messages: List[BaseMessage] = [HumanMessage(content=example[\"input\"])]    openai_tool_calls = []    for tool_call in example[\"tool_calls\"]:        openai_tool_calls.append(            {                \"id\": str(uuid.uuid4()),                \"type\": \"function\",                \"function\": {                    \"name\": tool_call.__class__.__name__,                    \"arguments\": tool_call.json(),                },            }        )    messages.append(        AIMessage(content=\"\", additional_kwargs={\"tool_calls\": openai_tool_calls})    )    tool_outputs = example.get(\"tool_outputs\") or [        \"You have correctly called this tool.\"    ] * len(openai_tool_calls)    for output, tool_call in zip(tool_outputs, openai_tool_calls):        messages.append(ToolMessage(content=output, tool_call_id=tool_call[\"id\"]))    return messagesexample_msgs = [msg for ex in examples for msg in tool_example_to_messages(ex)]\n```\n\n```\nfrom langchain_core.prompts import MessagesPlaceholderquery_analyzer_with_examples = (    {\"question\": RunnablePassthrough()}    | prompt.partial(examples=example_msgs)    | structured_llm)\n```\n\n```\nquery_analyzer_with_examples.invoke(    \"what's the difference between web voyager and reflection agents? do both use langgraph?\")\n```\n\n```\nSearch(query=\"What's the difference between web voyager and reflection agents? Do both use langgraph?\", sub_queries=['What is web voyager', 'What are reflection agents', 'Do web voyager and reflection agents use langgraph?'], publish_year=None)\n```\n\nThanks to our examples we get a slightly more decomposed search query. With some more prompt engineering and tuning of our examples we could improve query generation even more.\n\nYou can see that the examples are passed to the model as messages in the LangSmith trace.\n\n- Setup\n- Query schema\n- Query generation\n- Adding examples and tuning the prompt", "url_final": "https://python.langchain.com/docs/how_to/query_few_shot/", "url_citable": "https://python.langchain.com/docs/how_to/query_few_shot/#adding-examples-and-tuning-the-prompt", "title": "How to add examples to the prompt for query analysis | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Adding examples and tuning the prompt‚Äã", "section_anchor": "adding-examples-and-tuning-the-prompt", "fetched_at": "2025-10-06T21:54:29.103643", "has_code": true}
{"id": "5be002054a228325f3720b9a126869adcbc52de1", "text": "The pipe operator: |‚Äã\n\nTo show off how this works, let's go through an example. We'll walk through a common pattern in LangChain: using a prompt template to format input into a chat model, and finally converting the chat message output into a string with an output parser.\n\n```\npip install -qU \"langchain[google-genai]\"\n```\n\n```\nimport getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelmodel = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n```\n\n```\nfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplateprompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")chain = prompt | model | StrOutputParser()\n```\n\nPrompts and models are both runnable, and the output type from the prompt call is the same as the input type of the chat model, so we can chain them together. We can then invoke the resulting sequence like any other runnable:\n\n```\nchain.invoke({\"topic\": \"bears\"})\n```\n\n```\n\"Why don't bears wear shoes?\\n\\nBecause they prefer to go bear-foot!\"\n```\n\nCoercion‚Äã\n\nWe can even combine this chain with more runnables to create another chain. This may involve some input/output formatting using other types of runnables, depending on the required inputs and outputs of the chain components.\n\nFor example, let's say we wanted to compose the joke generating chain with another chain that evaluates whether or not the generated joke was funny.\n\nWe would need to be careful with how we format the input into the next chain. In the below example, the dict in the chain is automatically parsed and converted into a RunnableParallel, which runs all of its values in parallel and returns a dict with the results.\n\nThis happens to be the same format the next prompt template expects. Here it is in action:\n\n```\nfrom langchain_core.output_parsers import StrOutputParseranalysis_prompt = ChatPromptTemplate.from_template(\"is this a funny joke? {joke}\")composed_chain = {\"joke\": chain} | analysis_prompt | model | StrOutputParser()composed_chain.invoke({\"topic\": \"bears\"})\n```\n\n```\n'Yes, that\\'s a funny joke! It\\'s a classic pun that plays on the homophone pair \"bare-foot\" and \"bear-foot.\" The humor comes from:\\n\\n1. The wordplay between \"barefoot\" (not wearing shoes) and \"bear-foot\" (the foot of a bear)\\n2. The logical connection to the setup (bears don\\'t wear shoes)\\n3. It\\'s family-friendly and accessible\\n4. It\\'s a simple, clean pun that creates an unexpected but satisfying punchline\\n\\nIt\\'s the kind of joke that might make you groan and smile at the same time - what people often call a \"dad joke.\"'\n```\n\nFunctions will also be coerced into runnables, so you can add custom logic to your chains too. The below chain results in the same logical flow as before:\n\n```\ncomposed_chain_with_lambda = (    chain    | (lambda input: {\"joke\": input})    | analysis_prompt    | model    | StrOutputParser())composed_chain_with_lambda.invoke({\"topic\": \"beets\"})\n```\n\n```\n'Yes, that\\'s a cute and funny joke! It works well because:\\n\\n1. It plays on the double meaning of \"roots\" - both the literal roots of the beet plant and the metaphorical sense of knowing one\\'s origins or foundation\\n2. It\\'s a simple, clean pun that doesn\\'t rely on offensive content\\n3. It has a satisfying logical connection (beets are root vegetables)\\n\\nIt\\'s the kind of wholesome food pun that might make people groan a little but also smile. Perfect for sharing in casual conversation or with kids!'\n```\n\nHowever, keep in mind that using functions like this may interfere with operations like streaming. See this section for more information.", "url_final": "https://python.langchain.com/docs/how_to/sequence/", "url_citable": "https://python.langchain.com/docs/how_to/sequence/#the-pipe-operator-", "title": "How to chain runnables | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "The pipe operator: |‚Äã", "section_anchor": "the-pipe-operator-", "fetched_at": "2025-10-06T21:56:20.088555", "has_code": true}
{"id": "9b0c6208727f17d158e73b7d510b4175c23b9f42", "text": "The .pipe() method‚Äã\n\nWe could also compose the same sequence using the .pipe() method. Here's what that looks like:\n\n```\nfrom langchain_core.runnables import RunnableParallelcomposed_chain_with_pipe = (    RunnableParallel({\"joke\": chain})    .pipe(analysis_prompt)    .pipe(model)    .pipe(StrOutputParser()))composed_chain_with_pipe.invoke({\"topic\": \"battlestar galactica\"})\n```\n\n```\n\"This joke is moderately funny! It plays on Battlestar Galactica lore where Cylons are robots with 12 different models trying to infiltrate human society. The humor comes from the idea of a Cylon accidentally revealing their non-human nature through a pickup line that references their artificial origins. It's a decent nerd-culture joke that would land well with fans of the show, though someone unfamiliar with Battlestar Galactica might not get the reference. The punchline effectively highlights the contradiction in a Cylon trying to blend in while simultaneously revealing their true identity.\"\n```\n\nOr the abbreviated:\n\n```\ncomposed_chain_with_pipe = RunnableParallel({\"joke\": chain}).pipe(    analysis_prompt, model, StrOutputParser())\n```", "url_final": "https://python.langchain.com/docs/how_to/sequence/", "url_citable": "https://python.langchain.com/docs/how_to/sequence/#the-pipe-method", "title": "How to chain runnables | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "The .pipe() method‚Äã", "section_anchor": "the-pipe-method", "fetched_at": "2025-10-06T21:56:20.088555", "has_code": true}
{"id": "db1b534b19ea2d13ac174b719d32cda1e1e53f18", "text": "Related‚Äã\n\n- Streaming: Check out the streaming guide to understand the streaming behavior of a chain\n\n- Coercion", "url_final": "https://python.langchain.com/docs/how_to/sequence/", "url_citable": "https://python.langchain.com/docs/how_to/sequence/#related", "title": "How to chain runnables | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Related‚Äã", "section_anchor": "related", "fetched_at": "2025-10-06T21:56:20.088555", "has_code": false}
{"id": "eeb32761d66c4f1ea321dd5c402ecfdad0e9da6d", "text": "How to stream tool calls | ü¶úÔ∏èüîó LangChain\n\nWhen tools are called in a streaming context, message chunks will be populated with tool call chunk objects in a list via the .tool_call_chunks attribute. A ToolCallChunk includes optional string fields for the tool name, args, and id, and includes an optional integer field index that can be used to join chunks together. Fields are optional because portions of a tool call may be streamed across different chunks (e.g., a chunk that includes a substring of the arguments may have null values for the tool name and id).\n\nBecause message chunks inherit from their parent message class, an AIMessageChunk with tool call chunks will also include .tool_calls and .invalid_tool_calls fields. These fields are parsed best-effort from the message's tool call chunks.\n\nNote that not all providers currently support streaming for tool calls. Before we start let's define our tools and our model.\n\n```\nfrom langchain_core.tools import tool@tooldef add(a: int, b: int) -> int:    \"\"\"Adds a and b.\"\"\"    return a + b@tooldef multiply(a: int, b: int) -> int:    \"\"\"Multiplies a and b.\"\"\"    return a * btools = [add, multiply]\n```\n\n```\nimport osfrom getpass import getpassfrom langchain_openai import ChatOpenAIif \"OPENAI_API_KEY\" not in os.environ:    os.environ[\"OPENAI_API_KEY\"] = getpass()llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)llm_with_tools = llm.bind_tools(tools)\n```\n\nNow let's define our query and stream our output:\n\n```\nquery = \"What is 3 * 12? Also, what is 11 + 49?\"async for chunk in llm_with_tools.astream(query):    print(chunk.tool_call_chunks)\n```\n\n```\n[][{'name': 'Multiply', 'args': '', 'id': 'call_3aQwTP9CYlFxwOvQZPHDu6wL', 'index': 0}][{'name': None, 'args': '{\"a\"', 'id': None, 'index': 0}][{'name': None, 'args': ': 3, ', 'id': None, 'index': 0}][{'name': None, 'args': '\"b\": 1', 'id': None, 'index': 0}][{'name': None, 'args': '2}', 'id': None, 'index': 0}][{'name': 'Add', 'args': '', 'id': 'call_SQUoSsJz2p9Kx2x73GOgN1ja', 'index': 1}][{'name': None, 'args': '{\"a\"', 'id': None, 'index': 1}][{'name': None, 'args': ': 11,', 'id': None, 'index': 1}][{'name': None, 'args': ' \"b\": ', 'id': None, 'index': 1}][{'name': None, 'args': '49}', 'id': None, 'index': 1}][]\n```\n\nNote that adding message chunks will merge their corresponding tool call chunks. This is the principle by which LangChain's various tool output parsers support streaming.\n\nFor example, below we accumulate tool call chunks:\n\n```\nfirst = Trueasync for chunk in llm_with_tools.astream(query):    if first:        gathered = chunk        first = False    else:        gathered = gathered + chunk    print(gathered.tool_call_chunks)\n```\n\n```\n[][{'name': 'Multiply', 'args': '', 'id': 'call_AkL3dVeCjjiqvjv8ckLxL3gP', 'index': 0}][{'name': 'Multiply', 'args': '{\"a\"', 'id': 'call_AkL3dVeCjjiqvjv8ckLxL3gP', 'index': 0}][{'name': 'Multiply', 'args': '{\"a\": 3, ', 'id': 'call_AkL3dVeCjjiqvjv8ckLxL3gP', 'index': 0}][{'name': 'Multiply', 'args': '{\"a\": 3, \"b\": 1', 'id': 'call_AkL3dVeCjjiqvjv8ckLxL3gP', 'index': 0}][{'name': 'Multiply', 'args': '{\"a\": 3, \"b\": 12}', 'id': 'call_AkL3dVeCjjiqvjv8ckLxL3gP', 'index': 0}][{'name': 'Multiply', 'args': '{\"a\": 3, \"b\": 12}', 'id': 'call_AkL3dVeCjjiqvjv8ckLxL3gP', 'index': 0}, {'name': 'Add', 'args': '', 'id': 'call_b4iMiB3chGNGqbt5SjqqD2Wh', 'index': 1}][{'name': 'Multiply', 'args': '{\"a\": 3, \"b\": 12}', 'id': 'call_AkL3dVeCjjiqvjv8ckLxL3gP', 'index': 0}, {'name': 'Add', 'args': '{\"a\"', 'id': 'call_b4iMiB3chGNGqbt5SjqqD2Wh', 'index': 1}][{'name': 'Multiply', 'args': '{\"a\": 3, \"b\": 12}', 'id': 'call_AkL3dVeCjjiqvjv8ckLxL3gP', 'index': 0}, {'name': 'Add', 'args': '{\"a\": 11,', 'id': 'call_b4iMiB3chGNGqbt5SjqqD2Wh', 'index': 1}][{'name': 'Multiply', 'args': '{\"a\": 3, \"b\": 12}', 'id': 'call_AkL3dVeCjjiqvjv8ckLxL3gP', 'index': 0}, {'name': 'Add', 'args': '{\"a\": 11, \"b\": ', 'id': 'call_b4iMiB3chGNGqbt5SjqqD2Wh', 'index': 1}][{'name': 'Multiply', 'args': '{\"a\": 3, \"b\": 12}', 'id': 'call_AkL3dVeCjjiqvjv8ckLxL3gP', 'index': 0}, {'name': 'Add', 'args': '{\"a\": 11, \"b\": 49}', 'id': 'call_b4iMiB3chGNGqbt5SjqqD2Wh', 'index': 1}][{'name': 'Multiply', 'args': '{\"a\": 3, \"b\": 12}', 'id': 'call_AkL3dVeCjjiqvjv8ckLxL3gP', 'index': 0}, {'name': 'Add', 'args': '{\"a\": 11, \"b\": 49}', 'id': 'call_b4iMiB3chGNGqbt5SjqqD2Wh', 'index': 1}]\n```\n\n```\nprint(type(gathered.tool_call_chunks[0][\"args\"]))\n```\n\n```\n<class 'str'>\n```\n\nAnd below we accumulate tool calls to demonstrate partial parsing:\n\n```\nfirst = Trueasync for chunk in llm_with_tools.astream(query):    if first:        gathered = chunk        first = False    else:        gathered = gathered + chunk    print(gathered.tool_calls)\n```\n\n```\n[][][{'name': 'Multiply', 'args': {}, 'id': 'call_4p0D4tHVXSiae9Mu0e8jlI1m'}][{'name': 'Multiply', 'args': {'a': 3}, 'id': 'call_4p0D4tHVXSiae9Mu0e8jlI1m'}][{'name': 'Multiply', 'args': {'a': 3, 'b': 1}, 'id': 'call_4p0D4tHVXSiae9Mu0e8jlI1m'}][{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_4p0D4tHVXSiae9Mu0e8jlI1m'}][{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_4p0D4tHVXSiae9Mu0e8jlI1m'}][{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_4p0D4tHVXSiae9Mu0e8jlI1m'}, {'name': 'Add', 'args': {}, 'id': 'call_54Hx3DGjZitFlEjgMe1DYonh'}][{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_4p0D4tHVXSiae9Mu0e8jlI1m'}, {'name': 'Add', 'args': {'a': 11}, 'id': 'call_54Hx3DGjZitFlEjgMe1DYonh'}][{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_4p0D4tHVXSiae9Mu0e8jlI1m'}, {'name': 'Add', 'args': {'a': 11}, 'id': 'call_54Hx3DGjZitFlEjgMe1DYonh'}][{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_4p0D4tHVXSiae9Mu0e8jlI1m'}, {'name': 'Add', 'args': {'a': 11, 'b': 49}, 'id': 'call_54Hx3DGjZitFlEjgMe1DYonh'}][{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_4p0D4tHVXSiae9Mu0e8jlI1m'}, {'name': 'Add', 'args': {'a': 11, 'b': 49}, 'id': 'call_54Hx3DGjZitFlEjgMe1DYonh'}]\n```\n\n```\nprint(type(gathered.tool_calls[0][\"args\"]))\n```\n\n```\n<class 'dict'>\n```\n\nNote the key difference: accumulating tool_call_chunks captures the raw tool arguments as an unparsed string as they are streamed. In contrast, accumulating tool_calls demonstrates partial parsing by progressively converting the streamed argument string into a valid, usable dictionary at each step of the process.", "url_final": "https://python.langchain.com/docs/how_to/tool_streaming/", "url_citable": "https://python.langchain.com/docs/how_to/tool_streaming/", "title": "How to stream tool calls | ü¶úÔ∏èüîó LangChain", "section_level": 1, "section_heading": "How to stream tool calls | ü¶úÔ∏èüîó LangChain", "section_anchor": null, "fetched_at": "2025-10-06T21:56:40.871865", "has_code": true}
{"id": "59e4dddeb7b196ad3e95b3ba189d1c97dfde20ed", "text": "Next steps‚Äã\n\nYou've now seen how to stream events from within a tool. Next, check out the following guides for more on using tools:\n\n- Pass runtime values to tools\n- Pass tool results back to a model\n- Dispatch custom callback events\n\nYou can also check out some more specific uses of tool calling:\n\n- Building tool-using chains and agents\n- Getting structured outputs from models\n\n- Next steps", "url_final": "https://python.langchain.com/docs/how_to/tool_stream_events/", "url_citable": "https://python.langchain.com/docs/how_to/tool_stream_events/#next-steps", "title": "How to stream events from a tool | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Next steps‚Äã", "section_anchor": "next-steps", "fetched_at": "2025-10-06T21:56:39.737464", "has_code": false}
{"id": "5cf4d2095f5f322a0ed41140dd8e556343ad1e93", "text": "Customizing the CSV parsing and loading‚Äã\n\nCSVLoader will accept a csv_args kwarg that supports customization of arguments passed to Python's csv.DictReader. See the csv module documentation for more information of what csv args are supported.\n\n```\nloader = CSVLoader(    file_path=file_path,    csv_args={        \"delimiter\": \",\",        \"quotechar\": '\"',        \"fieldnames\": [\"MLB Team\", \"Payroll in millions\", \"Wins\"],    },)data = loader.load()for record in data[:2]:    print(record)\n```\n\n```\npage_content='MLB Team: Team\\nPayroll in millions: \"Payroll (millions)\"\\nWins: \"Wins\"' metadata={'source': '../../../docs/integrations/document_loaders/example_data/mlb_teams_2012.csv', 'row': 0}page_content='MLB Team: Nationals\\nPayroll in millions: 81.34\\nWins: 98' metadata={'source': '../../../docs/integrations/document_loaders/example_data/mlb_teams_2012.csv', 'row': 1}\n```", "url_final": "https://python.langchain.com/docs/how_to/document_loader_csv/", "url_citable": "https://python.langchain.com/docs/how_to/document_loader_csv/#customizing-the-csv-parsing-and-loading", "title": "How to load CSVs | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Customizing the CSV parsing and loading‚Äã", "section_anchor": "customizing-the-csv-parsing-and-loading", "fetched_at": "2025-10-06T21:55:19.555228", "has_code": true}
{"id": "c17f0c61f951c3d56bd53268d6bb7f03356eb169", "text": "Specify a column to identify the document source‚Äã\n\nThe \"source\" key on Document metadata can be set using a column of the CSV. Use the source_column argument to specify a source for the document created from each row. Otherwise file_path will be used as the source for all documents created from the CSV file.\n\nThis is useful when using documents loaded from CSV files for chains that answer questions using sources.\n\n```\nloader = CSVLoader(file_path=file_path, source_column=\"Team\")data = loader.load()for record in data[:2]:    print(record)\n```\n\n```\npage_content='Team: Nationals\\n\"Payroll (millions)\": 81.34\\n\"Wins\": 98' metadata={'source': 'Nationals', 'row': 0}page_content='Team: Reds\\n\"Payroll (millions)\": 82.20\\n\"Wins\": 97' metadata={'source': 'Reds', 'row': 1}\n```", "url_final": "https://python.langchain.com/docs/how_to/document_loader_csv/", "url_citable": "https://python.langchain.com/docs/how_to/document_loader_csv/#specify-a-column-to-identify-the-document-source", "title": "How to load CSVs | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Specify a column to identify the document source‚Äã", "section_anchor": "specify-a-column-to-identify-the-document-source", "fetched_at": "2025-10-06T21:55:19.555228", "has_code": true}
{"id": "5b2961bce5943f2a3544ffbcba32846f02672cf8", "text": "Load from a string‚Äã\n\nPython's tempfile can be used when working with CSV strings directly.\n\n```\nimport tempfilefrom io import StringIOstring_data = \"\"\"\"Team\", \"Payroll (millions)\", \"Wins\"\"Nationals\",     81.34, 98\"Reds\",          82.20, 97\"Yankees\",      197.96, 95\"Giants\",       117.62, 94\"\"\".strip()with tempfile.NamedTemporaryFile(delete=False, mode=\"w+\") as temp_file:    temp_file.write(string_data)    temp_file_path = temp_file.nameloader = CSVLoader(file_path=temp_file_path)data = loader.load()for record in data[:2]:    print(record)\n```\n\n```\npage_content='Team: Nationals\\n\"Payroll (millions)\": 81.34\\n\"Wins\": 98' metadata={'source': 'Nationals', 'row': 0}page_content='Team: Reds\\n\"Payroll (millions)\": 82.20\\n\"Wins\": 97' metadata={'source': 'Reds', 'row': 1}\n```\n\n- Customizing the CSV parsing and loading\n- Specify a column to identify the document source\n- Load from a string", "url_final": "https://python.langchain.com/docs/how_to/document_loader_csv/", "url_citable": "https://python.langchain.com/docs/how_to/document_loader_csv/#load-from-a-string", "title": "How to load CSVs | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Load from a string‚Äã", "section_anchor": "load-from-a-string", "fetched_at": "2025-10-06T21:55:19.555228", "has_code": true}
{"id": "6281c109ceb3fc0658707c296fdac3f2dbf5b4d1", "text": "Basic Usage‚Äã\n\nFor basic creation and usage of a tool-calling ReAct-style agent, the functionality is the same. First, let's define a model and tool(s), then we'll use those to create an agent.\n\n```\nfrom langchain_core.tools import toolfrom langchain_openai import ChatOpenAImodel = ChatOpenAI(model=\"gpt-4o\")@tooldef magic_function(input: int) -> int:    \"\"\"Applies a magic function to an input.\"\"\"    return input + 2tools = [magic_function]query = \"what is the value of magic_function(3)?\"\n```\n\nFor the LangChain AgentExecutor, we define a prompt with a placeholder for the agent's scratchpad. The agent can be invoked as follows:\n\n```\nfrom langchain.agents import AgentExecutor, create_tool_calling_agentfrom langchain_core.prompts import ChatPromptTemplateprompt = ChatPromptTemplate.from_messages(    [        (\"system\", \"You are a helpful assistant\"),        (\"human\", \"{input}\"),        # Placeholders fill up a **list** of messages        (\"placeholder\", \"{agent_scratchpad}\"),    ])agent = create_tool_calling_agent(model, tools, prompt)agent_executor = AgentExecutor(agent=agent, tools=tools)agent_executor.invoke({\"input\": query})\n```\n\n```\n{'input': 'what is the value of magic_function(3)?', 'output': 'The value of `magic_function(3)` is 5.'}\n```\n\nLangGraph's react agent executor manages a state that is defined by a list of messages. It will continue to process the list until there are no tool calls in the agent's output. To kick it off, we input a list of messages. The output will contain the entire state of the graph-- in this case, the conversation history.\n\n```\nfrom langgraph.prebuilt import create_react_agentlanggraph_agent_executor = create_react_agent(model, tools)messages = langgraph_agent_executor.invoke({\"messages\": [(\"human\", query)]}){    \"input\": query,    \"output\": messages[\"messages\"][-1].content,}\n```\n\n```\n{'input': 'what is the value of magic_function(3)?', 'output': 'The value of `magic_function(3)` is 5.'}\n```\n\n```\nmessage_history = messages[\"messages\"]new_query = \"Pardon?\"messages = langgraph_agent_executor.invoke(    {\"messages\": message_history + [(\"human\", new_query)]}){    \"input\": new_query,    \"output\": messages[\"messages\"][-1].content,}\n```\n\n```\n{'input': 'Pardon?', 'output': 'The result of applying `magic_function` to the input value 3 is 5.'}\n```", "url_final": "https://python.langchain.com/docs/how_to/migrate_agent/", "url_citable": "https://python.langchain.com/docs/how_to/migrate_agent/#basic-usage", "title": "How to migrate from legacy LangChain agents to LangGraph | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Basic Usage‚Äã", "section_anchor": "basic-usage", "fetched_at": "2025-10-06T21:55:55.200740", "has_code": true}
{"id": "485f5283ab748d132a94767e69ee3d61a79244d5", "text": "Prompt Templates‚Äã\n\nWith legacy LangChain agents you have to pass in a prompt template. You can use this to control the agent.\n\nWith LangGraph react agent executor, by default there is no prompt. You can achieve similar control over the agent in a few ways:\n\n- Pass in a system message as input\n- Initialize the agent with a system message\n- Initialize the agent with a function to transform messages in the graph state before passing to the model.\n- Initialize the agent with a Runnable to transform messages in the graph state before passing to the model. This includes passing prompt templates as well.\n\nLet's take a look at all of these below. We will pass in custom instructions to get the agent to respond in Spanish.\n\nFirst up, using AgentExecutor:\n\n```\nprompt = ChatPromptTemplate.from_messages(    [        (\"system\", \"You are a helpful assistant. Respond only in Spanish.\"),        (\"human\", \"{input}\"),        # Placeholders fill up a **list** of messages        (\"placeholder\", \"{agent_scratchpad}\"),    ])agent = create_tool_calling_agent(model, tools, prompt)agent_executor = AgentExecutor(agent=agent, tools=tools)agent_executor.invoke({\"input\": query})\n```\n\n```\n{'input': 'what is the value of magic_function(3)?', 'output': 'El valor de magic_function(3) es 5.'}\n```\n\nNow, let's pass a custom system message to react agent executor.\n\nLangGraph's prebuilt create_react_agent does not take a prompt template directly as a parameter, but instead takes a prompt parameter. This modifies the graph state before the llm is called, and can be one of four values:\n\n- A SystemMessage, which is added to the beginning of the list of messages.\n- A string, which is converted to a SystemMessage and added to the beginning of the list of messages.\n- A Callable, which should take in full graph state. The output is then passed to the language model.\n- Or a Runnable, which should take in full graph state. The output is then passed to the language model.\n\nHere's how it looks in action:\n\n```\nfrom langchain_core.messages import SystemMessagefrom langgraph.prebuilt import create_react_agentsystem_message = \"You are a helpful assistant. Respond only in Spanish.\"# This could also be a SystemMessage object# system_message = SystemMessage(content=\"You are a helpful assistant. Respond only in Spanish.\")langgraph_agent_executor = create_react_agent(model, tools, prompt=system_message)messages = langgraph_agent_executor.invoke({\"messages\": [(\"user\", query)]})\n```\n\nWe can also pass in an arbitrary function or a runnable. This function/runnable should take in a graph state and output a list of messages. We can do all types of arbitrary formatting of messages here. In this case, let's add a SystemMessage to the start of the list of messages and append another user message at the end.\n\n```\nfrom langchain_core.messages import HumanMessage, SystemMessagefrom langgraph.prebuilt import create_react_agentfrom langgraph.prebuilt.chat_agent_executor import AgentStateprompt = ChatPromptTemplate.from_messages(    [        (\"system\", \"You are a helpful assistant. Respond only in Spanish.\"),        (\"placeholder\", \"{messages}\"),        (\"user\", \"Also say 'Pandamonium!' after the answer.\"),    ])# alternatively, this can be passed as a function, e.g.# def prompt(state: AgentState):#     return (#         [SystemMessage(content=\"You are a helpful assistant. Respond only in Spanish.\")] +#         state[\"messages\"] +#         [HumanMessage(content=\"Also say 'Pandamonium!' after the answer.\")]#     )langgraph_agent_executor = create_react_agent(model, tools, prompt=prompt)messages = langgraph_agent_executor.invoke({\"messages\": [(\"human\", query)]})print(    {        \"input\": query,        \"output\": messages[\"messages\"][-1].content,    })\n```\n\n```\n{'input': 'what is the value of magic_function(3)?', 'output': 'El valor de magic_function(3) es 5. ¬°Pandamonium!'}\n```", "url_final": "https://python.langchain.com/docs/how_to/migrate_agent/", "url_citable": "https://python.langchain.com/docs/how_to/migrate_agent/#prompt-templates", "title": "How to migrate from legacy LangChain agents to LangGraph | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Prompt Templates‚Äã", "section_anchor": "prompt-templates", "fetched_at": "2025-10-06T21:55:55.200740", "has_code": true}
{"id": "f6c0e2cbde56acaee089e21f3fc812d55d05fded", "text": "Memory‚Äã\n\nIn LangChain‚Äã\n\nWith LangChain's AgentExecutor, you could add chat Memory so it can engage in a multi-turn conversation.\n\n```\nfrom langchain.agents import AgentExecutor, create_tool_calling_agentfrom langchain_core.chat_history import InMemoryChatMessageHistoryfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables.history import RunnableWithMessageHistoryfrom langchain_core.tools import toolfrom langchain_openai import ChatOpenAImodel = ChatOpenAI(model=\"gpt-4o\")memory = InMemoryChatMessageHistory(session_id=\"test-session\")prompt = ChatPromptTemplate.from_messages(    [        (\"system\", \"You are a helpful assistant.\"),        # First put the history        (\"placeholder\", \"{chat_history}\"),        # Then the new input        (\"human\", \"{input}\"),        # Finally the scratchpad        (\"placeholder\", \"{agent_scratchpad}\"),    ])@tooldef magic_function(input: int) -> int:    \"\"\"Applies a magic function to an input.\"\"\"    return input + 2tools = [magic_function]agent = create_tool_calling_agent(model, tools, prompt)agent_executor = AgentExecutor(agent=agent, tools=tools)agent_with_chat_history = RunnableWithMessageHistory(    agent_executor,    # This is needed because in most real world scenarios, a session id is needed    # It isn't really used here because we are using a simple in memory ChatMessageHistory    lambda session_id: memory,    input_messages_key=\"input\",    history_messages_key=\"chat_history\",)config = {\"configurable\": {\"session_id\": \"test-session\"}}print(    agent_with_chat_history.invoke(        {\"input\": \"Hi, I'm polly! What's the output of magic_function of 3?\"}, config    )[\"output\"])print(\"---\")print(agent_with_chat_history.invoke({\"input\": \"Remember my name?\"}, config)[\"output\"])print(\"---\")print(    agent_with_chat_history.invoke({\"input\": \"what was that output again?\"}, config)[        \"output\"    ])\n```\n\n```\nThe output of the magic function when the input is 3 is 5.---Yes, you mentioned your name is Polly.---The output of the magic function when the input is 3 is 5.\n```\n\nIn LangGraph‚Äã\n\nMemory is just persistence, aka checkpointing.\n\nAdd a checkpointer to the agent and you get chat memory for free.\n\n```\nfrom langgraph.checkpoint.memory import MemorySaver  # an in-memory checkpointerfrom langgraph.prebuilt import create_react_agentsystem_message = \"You are a helpful assistant.\"# This could also be a SystemMessage object# system_message = SystemMessage(content=\"You are a helpful assistant. Respond only in Spanish.\")memory = MemorySaver()langgraph_agent_executor = create_react_agent(    model, tools, prompt=system_message, checkpointer=memory)config = {\"configurable\": {\"thread_id\": \"test-thread\"}}print(    langgraph_agent_executor.invoke(        {            \"messages\": [                (\"user\", \"Hi, I'm polly! What's the output of magic_function of 3?\")            ]        },        config,    )[\"messages\"][-1].content)print(\"---\")print(    langgraph_agent_executor.invoke(        {\"messages\": [(\"user\", \"Remember my name?\")]}, config    )[\"messages\"][-1].content)print(\"---\")print(    langgraph_agent_executor.invoke(        {\"messages\": [(\"user\", \"what was that output again?\")]}, config    )[\"messages\"][-1].content)\n```\n\n```\nThe output of the magic function for the input 3 is 5.---Yes, you mentioned that your name is Polly.---The output of the magic function for the input 3 was 5.\n```", "url_final": "https://python.langchain.com/docs/how_to/migrate_agent/", "url_citable": "https://python.langchain.com/docs/how_to/migrate_agent/#memory", "title": "How to migrate from legacy LangChain agents to LangGraph | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Memory‚Äã", "section_anchor": "memory", "fetched_at": "2025-10-06T21:55:55.200740", "has_code": true}
{"id": "a3bc0c758116c2d874f2f3446b723164a1e6664c", "text": "Iterating through steps‚Äã\n\nIn LangChain‚Äã\n\nWith LangChain's AgentExecutor, you could iterate over the steps using the stream (or async astream) methods or the iter method. LangGraph supports stepwise iteration using stream\n\n```\nfrom langchain.agents import AgentExecutor, create_tool_calling_agentfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.tools import toolfrom langchain_openai import ChatOpenAImodel = ChatOpenAI(model=\"gpt-4o\")prompt = ChatPromptTemplate.from_messages(    [        (\"system\", \"You are a helpful assistant.\"),        (\"human\", \"{input}\"),        # Placeholders fill up a **list** of messages        (\"placeholder\", \"{agent_scratchpad}\"),    ])@tooldef magic_function(input: int) -> int:    \"\"\"Applies a magic function to an input.\"\"\"    return input + 2tools = [magic_function]agent = create_tool_calling_agent(model, tools, prompt=prompt)agent_executor = AgentExecutor(agent=agent, tools=tools)for step in agent_executor.stream({\"input\": query}):    print(step)\n```\n\n```\n{'actions': [ToolAgentAction(tool='magic_function', tool_input={'input': 3}, log=\"\\nInvoking: `magic_function` with `{'input': 3}`\\n\\n\\n\", message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_yyetzabaDBRX9Ml2KyqfKzZM', 'function': {'arguments': '{\"input\":3}', 'name': 'magic_function'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7'}, id='run-7a3a5ada-52ec-4df0-bf7d-81e5051b01b4', tool_calls=[{'name': 'magic_function', 'args': {'input': 3}, 'id': 'call_yyetzabaDBRX9Ml2KyqfKzZM', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'magic_function', 'args': '{\"input\":3}', 'id': 'call_yyetzabaDBRX9Ml2KyqfKzZM', 'index': 0, 'type': 'tool_call_chunk'}])], tool_call_id='call_yyetzabaDBRX9Ml2KyqfKzZM')], 'messages': [AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_yyetzabaDBRX9Ml2KyqfKzZM', 'function': {'arguments': '{\"input\":3}', 'name': 'magic_function'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7'}, id='run-7a3a5ada-52ec-4df0-bf7d-81e5051b01b4', tool_calls=[{'name': 'magic_function', 'args': {'input': 3}, 'id': 'call_yyetzabaDBRX9Ml2KyqfKzZM', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'magic_function', 'args': '{\"input\":3}', 'id': 'call_yyetzabaDBRX9Ml2KyqfKzZM', 'index': 0, 'type': 'tool_call_chunk'}])]}{'steps': [AgentStep(action=ToolAgentAction(tool='magic_function', tool_input={'input': 3}, log=\"\\nInvoking: `magic_function` with `{'input': 3}`\\n\\n\\n\", message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_yyetzabaDBRX9Ml2KyqfKzZM', 'function': {'arguments': '{\"input\":3}', 'name': 'magic_function'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7'}, id='run-7a3a5ada-52ec-4df0-bf7d-81e5051b01b4', tool_calls=[{'name': 'magic_function', 'args': {'input': 3}, 'id': 'call_yyetzabaDBRX9Ml2KyqfKzZM', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'magic_function', 'args': '{\"input\":3}', 'id': 'call_yyetzabaDBRX9Ml2KyqfKzZM', 'index': 0, 'type': 'tool_call_chunk'}])], tool_call_id='call_yyetzabaDBRX9Ml2KyqfKzZM'), observation=5)], 'messages': [FunctionMessage(content='5', additional_kwargs={}, response_metadata={}, name='magic_function')]}{'output': 'The value of `magic_function(3)` is 5.', 'messages': [AIMessage(content='The value of `magic_function(3)` is 5.', additional_kwargs={}, response_metadata={})]}\n```", "url_final": "https://python.langchain.com/docs/how_to/migrate_agent/", "url_citable": "https://python.langchain.com/docs/how_to/migrate_agent/#iterating-through-steps", "title": "How to migrate from legacy LangChain agents to LangGraph | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Iterating through steps‚Äã", "section_anchor": "iterating-through-steps", "fetched_at": "2025-10-06T21:55:55.200740", "has_code": true}
{"id": "ff353af0c3ce12d41b7f68d971fd8ccd603464da", "text": "In LangGraph‚Äã\n\nIn LangGraph, things are handled natively using stream or the asynchronous astream method.\n\n```\nfrom langgraph.prebuilt import create_react_agentfrom langgraph.prebuilt.chat_agent_executor import AgentStateprompt = ChatPromptTemplate.from_messages(    [        (\"system\", \"You are a helpful assistant.\"),        (\"placeholder\", \"{messages}\"),    ])langgraph_agent_executor = create_react_agent(model, tools, prompt=prompt)for step in langgraph_agent_executor.stream(    {\"messages\": [(\"human\", query)]}, stream_mode=\"updates\"):    print(step)\n```\n\n```\n{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_IHTMrjvIHn8gFOX42FstIpr9', 'function': {'arguments': '{\"input\":3}', 'name': 'magic_function'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 61, 'total_tokens': 75, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-1a6970da-163a-4e4d-b9b7-7e73b1057f42-0', tool_calls=[{'name': 'magic_function', 'args': {'input': 3}, 'id': 'call_IHTMrjvIHn8gFOX42FstIpr9', 'type': 'tool_call'}], usage_metadata={'input_tokens': 61, 'output_tokens': 14, 'total_tokens': 75, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]}}{'tools': {'messages': [ToolMessage(content='5', name='magic_function', id='51a9d3e4-734d-426f-a5a1-c6597e4efe25', tool_call_id='call_IHTMrjvIHn8gFOX42FstIpr9')]}}{'agent': {'messages': [AIMessage(content='The value of `magic_function(3)` is 5.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 84, 'total_tokens': 98, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a20a4ee344', 'finish_reason': 'stop', 'logprobs': None}, id='run-73001576-a3dc-4552-8d81-c9ce8aec05b3-0', usage_metadata={'input_tokens': 84, 'output_tokens': 14, 'total_tokens': 98, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]}}\n```", "url_final": "https://python.langchain.com/docs/how_to/migrate_agent/", "url_citable": "https://python.langchain.com/docs/how_to/migrate_agent/#in-langgraph-1", "title": "How to migrate from legacy LangChain agents to LangGraph | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "In LangGraph‚Äã", "section_anchor": "in-langgraph-1", "fetched_at": "2025-10-06T21:55:55.200740", "has_code": true}
{"id": "1fe6d5157b5cff81040860c1af8dced19b414e7a", "text": "return_intermediate_steps‚Äã\n\nIn LangChain‚Äã\n\nSetting this parameter on AgentExecutor allows users to access intermediate_steps, which pairs agent actions (e.g., tool invocations) with their outcomes.\n\n```\nagent_executor = AgentExecutor(agent=agent, tools=tools, return_intermediate_steps=True)result = agent_executor.invoke({\"input\": query})print(result[\"intermediate_steps\"])\n```\n\n```\n[(ToolAgentAction(tool='magic_function', tool_input={'input': 3}, log=\"\\nInvoking: `magic_function` with `{'input': 3}`\\n\\n\\n\", message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_njTvl2RsVf4q1aMUxoYnJuK1', 'function': {'arguments': '{\"input\":3}', 'name': 'magic_function'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7'}, id='run-c9dfe3ab-2db6-4592-851e-89e056aeab32', tool_calls=[{'name': 'magic_function', 'args': {'input': 3}, 'id': 'call_njTvl2RsVf4q1aMUxoYnJuK1', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'magic_function', 'args': '{\"input\":3}', 'id': 'call_njTvl2RsVf4q1aMUxoYnJuK1', 'index': 0, 'type': 'tool_call_chunk'}])], tool_call_id='call_njTvl2RsVf4q1aMUxoYnJuK1'), 5)]\n```\n\nIn LangGraph‚Äã\n\nBy default the react agent executor in LangGraph appends all messages to the central state. Therefore, it is easy to see any intermediate steps by just looking at the full state.\n\n```\nfrom langgraph.prebuilt import create_react_agentlanggraph_agent_executor = create_react_agent(model, tools=tools)messages = langgraph_agent_executor.invoke({\"messages\": [(\"human\", query)]})messages\n```\n\n```\n{'messages': [HumanMessage(content='what is the value of magic_function(3)?', additional_kwargs={}, response_metadata={}, id='1abb52c2-4bc2-4d82-bd32-5a24c3976b0f'),  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_XfQD6C7rAalcmicQubkhJVFq', 'function': {'arguments': '{\"input\":3}', 'name': 'magic_function'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 55, 'total_tokens': 69, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a20a4ee344', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-34f02786-5b5c-4bb1-bd9e-406c81944a24-0', tool_calls=[{'name': 'magic_function', 'args': {'input': 3}, 'id': 'call_XfQD6C7rAalcmicQubkhJVFq', 'type': 'tool_call'}], usage_metadata={'input_tokens': 55, 'output_tokens': 14, 'total_tokens': 69, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}),  ToolMessage(content='5', name='magic_function', id='cbc9fadf-1962-4ed7-b476-348c774652be', tool_call_id='call_XfQD6C7rAalcmicQubkhJVFq'),  AIMessage(content='The value of `magic_function(3)` is 5.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 78, 'total_tokens': 92, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7', 'finish_reason': 'stop', 'logprobs': None}, id='run-547e03d2-872d-4008-a38d-b7f739a77df5-0', usage_metadata={'input_tokens': 78, 'output_tokens': 14, 'total_tokens': 92, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]}\n```", "url_final": "https://python.langchain.com/docs/how_to/migrate_agent/", "url_citable": "https://python.langchain.com/docs/how_to/migrate_agent/#return_intermediate_steps", "title": "How to migrate from legacy LangChain agents to LangGraph | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "return_intermediate_steps‚Äã", "section_anchor": "return_intermediate_steps", "fetched_at": "2025-10-06T21:55:55.200740", "has_code": true}
{"id": "b426ca50c1b75fe9cf2e4a5d9e052fcdebba876a", "text": "max_iterations‚Äã\n\nIn LangChain‚Äã\n\nAgentExecutor implements a max_iterations parameter, allowing users to abort a run that exceeds a specified number of iterations.\n\n```\n@tooldef magic_function(input: str) -> str:    \"\"\"Applies a magic function to an input.\"\"\"    return \"Sorry, there was an error. Please try again.\"tools = [magic_function]\n```\n\n```\nprompt = ChatPromptTemplate.from_messages(    [        (\"system\", \"You are a helpful assistant. Respond only in Spanish.\"),        (\"human\", \"{input}\"),        # Placeholders fill up a **list** of messages        (\"placeholder\", \"{agent_scratchpad}\"),    ])agent = create_tool_calling_agent(model, tools, prompt)agent_executor = AgentExecutor(    agent=agent,    tools=tools,    verbose=True,    max_iterations=3,)agent_executor.invoke({\"input\": query})\n```\n\n```\n\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\u001b[32;1m\u001b[1;3mLo siento, no puedo decirte directamente el valor de `magic_function(3)`. Si deseas, puedo usar la funci√≥n m√°gica para calcularlo. ¬øTe gustar√≠a que lo hiciera?\u001b[0m\u001b[1m> Finished chain.\u001b[0m\n```\n\n```\n{'input': 'what is the value of magic_function(3)?', 'output': 'Lo siento, no puedo decirte directamente el valor de `magic_function(3)`. Si deseas, puedo usar la funci√≥n m√°gica para calcularlo. ¬øTe gustar√≠a que lo hiciera?'}\n```", "url_final": "https://python.langchain.com/docs/how_to/migrate_agent/", "url_citable": "https://python.langchain.com/docs/how_to/migrate_agent/#max_iterations", "title": "How to migrate from legacy LangChain agents to LangGraph | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "max_iterations‚Äã", "section_anchor": "max_iterations", "fetched_at": "2025-10-06T21:55:55.200740", "has_code": true}
{"id": "434045ed461a5ae727a8fa8425cc87f03360e635", "text": "In LangGraph‚Äã\n\nIn LangGraph this is controlled via recursion_limit configuration parameter.\n\nNote that in AgentExecutor, an \"iteration\" includes a full turn of tool invocation and execution. In LangGraph, each step contributes to the recursion limit, so we will need to multiply by two (and add one) to get equivalent results.\n\nIf the recursion limit is reached, LangGraph raises a specific exception type, that we can catch and manage similarly to AgentExecutor.\n\n```\nfrom langgraph.errors import GraphRecursionErrorfrom langgraph.prebuilt import create_react_agentRECURSION_LIMIT = 2 * 3 + 1langgraph_agent_executor = create_react_agent(model, tools=tools)try:    for chunk in langgraph_agent_executor.stream(        {\"messages\": [(\"human\", query)]},        {\"recursion_limit\": RECURSION_LIMIT},        stream_mode=\"values\",    ):        print(chunk[\"messages\"][-1])except GraphRecursionError:    print({\"input\": query, \"output\": \"Agent stopped due to max iterations.\"})\n```\n\n```\ncontent='what is the value of magic_function(3)?' additional_kwargs={} response_metadata={} id='c2489fe8-e69c-4163-876d-3cce26b28521'content='' additional_kwargs={'tool_calls': [{'id': 'call_OyNTcO6SDAvZcBlIEknPRrTR', 'function': {'arguments': '{\"input\":\"3\"}', 'name': 'magic_function'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 55, 'total_tokens': 69, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-b65504bb-fa23-4f8a-8d6c-7edb6d16e7ff-0' tool_calls=[{'name': 'magic_function', 'args': {'input': '3'}, 'id': 'call_OyNTcO6SDAvZcBlIEknPRrTR', 'type': 'tool_call'}] usage_metadata={'input_tokens': 55, 'output_tokens': 14, 'total_tokens': 69, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}content='Sorry, there was an error. Please try again.' name='magic_function' id='f00e0bff-54fe-4726-a1a7-127a59d8f7ed' tool_call_id='call_OyNTcO6SDAvZcBlIEknPRrTR'content=\"It seems there was an error when trying to compute the value of the magic function with input 3. Let's try again.\" additional_kwargs={'tool_calls': [{'id': 'call_Q020rQoJh4cnh8WglIMnDm4z', 'function': {'arguments': '{\"input\":\"3\"}', 'name': 'magic_function'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 88, 'total_tokens': 128, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-556d8cb2-b47a-4826-b17d-b520982c2475-0' tool_calls=[{'name': 'magic_function', 'args': {'input': '3'}, 'id': 'call_Q020rQoJh4cnh8WglIMnDm4z', 'type': 'tool_call'}] usage_metadata={'input_tokens': 88, 'output_tokens': 40, 'total_tokens': 128, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}content='Sorry, there was an error. Please try again.' name='magic_function' id='777212cd-8381-44db-9762-3f81951ea73e' tool_call_id='call_Q020rQoJh4cnh8WglIMnDm4z'content=\"It seems there is a persistent issue in computing the value of the magic function with the input 3. Unfortunately, I can't provide the value at this time. If you have any other questions or need further assistance, feel free to ask!\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 150, 'total_tokens': 199, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7', 'finish_reason': 'stop', 'logprobs': None} id='run-92ec0b90-bc8e-4851-9139-f1d976145ab7-0' usage_metadata={'input_tokens': 150, 'output_tokens': 49, 'total_tokens': 199, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}\n```", "url_final": "https://python.langchain.com/docs/how_to/migrate_agent/", "url_citable": "https://python.langchain.com/docs/how_to/migrate_agent/#in-langgraph-3", "title": "How to migrate from legacy LangChain agents to LangGraph | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "In LangGraph‚Äã", "section_anchor": "in-langgraph-3", "fetched_at": "2025-10-06T21:55:55.200740", "has_code": true}
{"id": "0ce42c7974ce7a6b9b5007b7a59af52a1af54d2d", "text": "max_execution_time‚Äã\n\nIn LangChain‚Äã\n\nAgentExecutor implements a max_execution_time parameter, allowing users to abort a run that exceeds a total time limit.\n\n```\nimport time@tooldef magic_function(input: str) -> str:    \"\"\"Applies a magic function to an input.\"\"\"    time.sleep(2.5)    return \"Sorry, there was an error. Please try again.\"tools = [magic_function]agent = create_tool_calling_agent(model, tools, prompt)agent_executor = AgentExecutor(    agent=agent,    tools=tools,    max_execution_time=2,    verbose=True,)agent_executor.invoke({\"input\": query})\n```\n\n```\n\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\u001b[32;1m\u001b[1;3mLo siento, no tengo la capacidad de evaluar directamente una funci√≥n llamada \"magic_function\" con el valor 3. Sin embargo, si me proporcionas m√°s detalles sobre qu√© hace la funci√≥n o c√≥mo est√° definida, podr√≠a intentar ayudarte a comprender su comportamiento o resolverlo de otra manera.\u001b[0m\u001b[1m> Finished chain.\u001b[0m\n```\n\n```\n{'input': 'what is the value of magic_function(3)?', 'output': 'Lo siento, no tengo la capacidad de evaluar directamente una funci√≥n llamada \"magic_function\" con el valor 3. Sin embargo, si me proporcionas m√°s detalles sobre qu√© hace la funci√≥n o c√≥mo est√° definida, podr√≠a intentar ayudarte a comprender su comportamiento o resolverlo de otra manera.'}\n```", "url_final": "https://python.langchain.com/docs/how_to/migrate_agent/", "url_citable": "https://python.langchain.com/docs/how_to/migrate_agent/#max_execution_time", "title": "How to migrate from legacy LangChain agents to LangGraph | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "max_execution_time‚Äã", "section_anchor": "max_execution_time", "fetched_at": "2025-10-06T21:55:55.200740", "has_code": true}
{"id": "aeee1098cacbcc267d4d8f052809373dddd6b5ec", "text": "In LangGraph‚Äã\n\nWith LangGraph's react agent, you can control timeouts on two levels.\n\nYou can set a step_timeout to bound each step:\n\n```\nfrom langgraph.prebuilt import create_react_agentlanggraph_agent_executor = create_react_agent(model, tools=tools)# Set the max timeout for each step herelanggraph_agent_executor.step_timeout = 2try:    for chunk in langgraph_agent_executor.stream({\"messages\": [(\"human\", query)]}):        print(chunk)        print(\"------\")except TimeoutError:    print({\"input\": query, \"output\": \"Agent stopped due to a step timeout.\"})\n```\n\n```\n{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_UuxSgpGaqzX84sNlKzCVOiRO', 'function': {'arguments': '{\"input\":\"3\"}', 'name': 'magic_function'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 55, 'total_tokens': 69, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-24c94cbd-2962-48cf-a447-af888eb6ef86-0', tool_calls=[{'name': 'magic_function', 'args': {'input': '3'}, 'id': 'call_UuxSgpGaqzX84sNlKzCVOiRO', 'type': 'tool_call'}], usage_metadata={'input_tokens': 55, 'output_tokens': 14, 'total_tokens': 69, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]}}------{'input': 'what is the value of magic_function(3)?', 'output': 'Agent stopped due to a step timeout.'}\n```\n\nThe other way to set a single max timeout for an entire run is to directly use the python stdlib asyncio library.\n\n```\nimport asynciofrom langgraph.prebuilt import create_react_agentlanggraph_agent_executor = create_react_agent(model, tools=tools)async def stream(langgraph_agent_executor, inputs):    async for chunk in langgraph_agent_executor.astream(        {\"messages\": [(\"human\", query)]}    ):        print(chunk)        print(\"------\")try:    task = asyncio.create_task(        stream(langgraph_agent_executor, {\"messages\": [(\"human\", query)]})    )    await asyncio.wait_for(task, timeout=3)except asyncio.TimeoutError:    print(\"Task Cancelled.\")\n```\n\n```\n{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_km17xvoY7wJ5yNnXhb5V9D3I', 'function': {'arguments': '{\"input\":\"3\"}', 'name': 'magic_function'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 55, 'total_tokens': 69, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_45c6de4934', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-b44a04e5-9b68-4020-be36-98de1593eefc-0', tool_calls=[{'name': 'magic_function', 'args': {'input': '3'}, 'id': 'call_km17xvoY7wJ5yNnXhb5V9D3I', 'type': 'tool_call'}], usage_metadata={'input_tokens': 55, 'output_tokens': 14, 'total_tokens': 69, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]}}------Task Cancelled.\n```", "url_final": "https://python.langchain.com/docs/how_to/migrate_agent/", "url_citable": "https://python.langchain.com/docs/how_to/migrate_agent/#in-langgraph-4", "title": "How to migrate from legacy LangChain agents to LangGraph | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "In LangGraph‚Äã", "section_anchor": "in-langgraph-4", "fetched_at": "2025-10-06T21:55:55.200740", "has_code": true}
{"id": "1be278c1563ca3811445b9a56b4a6b4150bd4643", "text": "early_stopping_method‚Äã\n\nIn LangChain‚Äã\n\nWith LangChain's AgentExecutor, you could configure an early_stopping_method to either return a string saying \"Agent stopped due to iteration limit or time limit.\" (\"force\") or prompt the LLM a final time to respond (\"generate\").\n\n```\nfrom langchain.agents import AgentExecutor, create_tool_calling_agentfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.tools import toolfrom langchain_openai import ChatOpenAImodel = ChatOpenAI(model=\"gpt-4o\")prompt = ChatPromptTemplate.from_messages(    [        (\"system\", \"You are a helpful assistant.\"),        (\"human\", \"{input}\"),        # Placeholders fill up a **list** of messages        (\"placeholder\", \"{agent_scratchpad}\"),    ])@tooldef magic_function(input: int) -> int:    \"\"\"Applies a magic function to an input.\"\"\"    return \"Sorry there was an error, please try again.\"tools = [magic_function]agent = create_tool_calling_agent(model, tools, prompt=prompt)agent_executor = AgentExecutor(    agent=agent, tools=tools, early_stopping_method=\"force\", max_iterations=1)result = agent_executor.invoke({\"input\": query})print(\"Output with early_stopping_method='force':\")print(result[\"output\"])\n```\n\n```\nOutput with early_stopping_method='force':Agent stopped due to max iterations.\n```", "url_final": "https://python.langchain.com/docs/how_to/migrate_agent/", "url_citable": "https://python.langchain.com/docs/how_to/migrate_agent/#early_stopping_method", "title": "How to migrate from legacy LangChain agents to LangGraph | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "early_stopping_method‚Äã", "section_anchor": "early_stopping_method", "fetched_at": "2025-10-06T21:55:55.200740", "has_code": true}
{"id": "5cfc48425e951e571ce29ee5291ac40418c49c04", "text": "In LangGraph‚Äã\n\nIn LangGraph, you can explicitly handle the response behavior outside the agent, since the full state can be accessed.\n\n```\nfrom langgraph.errors import GraphRecursionErrorfrom langgraph.prebuilt import create_react_agentRECURSION_LIMIT = 2 * 1 + 1langgraph_agent_executor = create_react_agent(model, tools=tools)try:    for chunk in langgraph_agent_executor.stream(        {\"messages\": [(\"human\", query)]},        {\"recursion_limit\": RECURSION_LIMIT},        stream_mode=\"values\",    ):        print(chunk[\"messages\"][-1])except GraphRecursionError:    print({\"input\": query, \"output\": \"Agent stopped due to max iterations.\"})\n```\n\n```\ncontent='what is the value of magic_function(3)?' additional_kwargs={} response_metadata={} id='81fd2e50-1e6a-4871-87aa-b7c1225913a4'content='' additional_kwargs={'tool_calls': [{'id': 'call_aaEzj3aO1RTnB0uoc9rYUIhi', 'function': {'arguments': '{\"input\":3}', 'name': 'magic_function'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 55, 'total_tokens': 69, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-476bc4b1-b7bf-4607-a31c-ddf09dc814c5-0' tool_calls=[{'name': 'magic_function', 'args': {'input': 3}, 'id': 'call_aaEzj3aO1RTnB0uoc9rYUIhi', 'type': 'tool_call'}] usage_metadata={'input_tokens': 55, 'output_tokens': 14, 'total_tokens': 69, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}content='Sorry there was an error, please try again.' name='magic_function' id='dcbe7e3e-0ed4-467d-a729-2f45916ff44f' tool_call_id='call_aaEzj3aO1RTnB0uoc9rYUIhi'content=\"It seems there was an error when trying to compute the value of `magic_function(3)`. Let's try that again.\" additional_kwargs={'tool_calls': [{'id': 'call_jr4R8uJn2pdXF5GZC2Dg3YWS', 'function': {'arguments': '{\"input\":3}', 'name': 'magic_function'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 87, 'total_tokens': 127, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-d94b8932-6e9e-4ab1-99f7-7dca89887ffe-0' tool_calls=[{'name': 'magic_function', 'args': {'input': 3}, 'id': 'call_jr4R8uJn2pdXF5GZC2Dg3YWS', 'type': 'tool_call'}] usage_metadata={'input_tokens': 87, 'output_tokens': 40, 'total_tokens': 127, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}{'input': 'what is the value of magic_function(3)?', 'output': 'Agent stopped due to max iterations.'}\n```", "url_final": "https://python.langchain.com/docs/how_to/migrate_agent/", "url_citable": "https://python.langchain.com/docs/how_to/migrate_agent/#in-langgraph-5", "title": "How to migrate from legacy LangChain agents to LangGraph | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "In LangGraph‚Äã", "section_anchor": "in-langgraph-5", "fetched_at": "2025-10-06T21:55:55.200740", "has_code": true}
{"id": "c9d73374cb4713d4882f52a8aaf7f56314fee97b", "text": "trim_intermediate_steps‚Äã\n\nIn LangChain‚Äã\n\nWith LangChain's AgentExecutor, you could trim the intermediate steps of long-running agents using trim_intermediate_steps, which is either an integer (indicating the agent should keep the last N steps) or a custom function.\n\nFor instance, we could trim the value so the agent only sees the most recent intermediate step.\n\n```\nfrom langchain.agents import AgentExecutor, create_tool_calling_agentfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.tools import toolfrom langchain_openai import ChatOpenAImodel = ChatOpenAI(model=\"gpt-4o\")prompt = ChatPromptTemplate.from_messages(    [        (\"system\", \"You are a helpful assistant.\"),        (\"human\", \"{input}\"),        # Placeholders fill up a **list** of messages        (\"placeholder\", \"{agent_scratchpad}\"),    ])magic_step_num = 1@tooldef magic_function(input: int) -> int:    \"\"\"Applies a magic function to an input.\"\"\"    global magic_step_num    print(f\"Call number: {magic_step_num}\")    magic_step_num += 1    return input + magic_step_numtools = [magic_function]agent = create_tool_calling_agent(model, tools, prompt=prompt)def trim_steps(steps: list):    # Let's give the agent amnesia    return []agent_executor = AgentExecutor(    agent=agent, tools=tools, trim_intermediate_steps=trim_steps)query = \"Call the magic function 4 times in sequence with the value 3. You cannot call it multiple times at once.\"for step in agent_executor.stream({\"input\": query}):    pass\n```\n\n```\nCall number: 1Call number: 2Call number: 3Call number: 4Call number: 5Call number: 6Call number: 7Call number: 8Call number: 9Call number: 10Call number: 11Call number: 12Call number: 13Call number: 14``````outputStopping agent prematurely due to triggering stop condition``````outputCall number: 15\n```\n\nIn LangGraph‚Äã\n\nWe can use the prompt just as before when passing in prompt templates.\n\n```\nfrom langgraph.errors import GraphRecursionErrorfrom langgraph.prebuilt import create_react_agentfrom langgraph.prebuilt.chat_agent_executor import AgentStatemagic_step_num = 1@tooldef magic_function(input: int) -> int:    \"\"\"Applies a magic function to an input.\"\"\"    global magic_step_num    print(f\"Call number: {magic_step_num}\")    magic_step_num += 1    return input + magic_step_numtools = [magic_function]def _modify_state_messages(state: AgentState):    # Give the agent amnesia, only keeping the original user query    return [(\"system\", \"You are a helpful assistant\"), state[\"messages\"][0]]langgraph_agent_executor = create_react_agent(    model, tools, prompt=_modify_state_messages)try:    for step in langgraph_agent_executor.stream(        {\"messages\": [(\"human\", query)]}, stream_mode=\"updates\"    ):        passexcept GraphRecursionError as e:    print(\"Stopping agent prematurely due to triggering stop condition\")\n```\n\n```\nCall number: 1Call number: 2Call number: 3Call number: 4Call number: 5Call number: 6Call number: 7Call number: 8Call number: 9Call number: 10Call number: 11Call number: 12Stopping agent prematurely due to triggering stop condition\n```", "url_final": "https://python.langchain.com/docs/how_to/migrate_agent/", "url_citable": "https://python.langchain.com/docs/how_to/migrate_agent/#trim_intermediate_steps", "title": "How to migrate from legacy LangChain agents to LangGraph | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "trim_intermediate_steps‚Äã", "section_anchor": "trim_intermediate_steps", "fetched_at": "2025-10-06T21:55:55.200740", "has_code": true}
{"id": "3e2fdd494c5e05339f92d34abf0f3eaca38e33e6", "text": "Next steps‚Äã\n\nYou've now learned how to migrate your LangChain agent executors to LangGraph.\n\nNext, check out other LangGraph how-to guides.\n\n- In LangChain\n- In LangGraph\n\n- In LangChain\n- In LangGraph\n\n- In LangChain\n- In LangGraph\n\n- In LangChain\n- In LangGraph\n\n- In LangChain\n- In LangGraph\n\n- In LangChain\n- In LangGraph\n\n- In LangChain\n- In LangGraph", "url_final": "https://python.langchain.com/docs/how_to/migrate_agent/", "url_citable": "https://python.langchain.com/docs/how_to/migrate_agent/#next-steps", "title": "How to migrate from legacy LangChain agents to LangGraph | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Next steps‚Äã", "section_anchor": "next-steps", "fetched_at": "2025-10-06T21:55:55.200740", "has_code": false}
{"id": "e2261749320efc5a7d98e196974954e4dbdd1733", "text": "Retain Elements‚Äã\n\nUnder the hood, Unstructured creates different \"elements\" for different chunks of text. By default we combine those together, but you can easily keep that separation by specifying mode=\"elements\".\n\n```\nloader = UnstructuredMarkdownLoader(markdown_path, mode=\"elements\")data = loader.load()print(f\"Number of documents: {len(data)}\\n\")for document in data[:2]:    print(f\"{document}\\n\")\n```\n\n```\nNumber of documents: 66page_content='ü¶úÔ∏èüîó LangChain' metadata={'source': '../../../README.md', 'category_depth': 0, 'last_modified': '2024-06-28T15:20:01', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '../../..', 'filename': 'README.md', 'category': 'Title'}page_content='‚ö° Build context-aware reasoning applications ‚ö°' metadata={'source': '../../../README.md', 'last_modified': '2024-06-28T15:20:01', 'languages': ['eng'], 'parent_id': '200b8a7d0dd03f66e4f13456566d2b3a', 'filetype': 'text/markdown', 'file_directory': '../../..', 'filename': 'README.md', 'category': 'NarrativeText'}\n```\n\nNote that in this case we recover three distinct element types:\n\n```\nprint(set(document.metadata[\"category\"] for document in data))\n```\n\n```\n{'ListItem', 'NarrativeText', 'Title'}\n```\n\n- Retain Elements", "url_final": "https://python.langchain.com/docs/how_to/document_loader_markdown/", "url_citable": "https://python.langchain.com/docs/how_to/document_loader_markdown/#retain-elements", "title": "How to load Markdown | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Retain Elements‚Äã", "section_anchor": "retain-elements", "fetched_at": "2025-10-06T21:55:24.248947", "has_code": true}
{"id": "067754acb5e9f156c46a9f886660190d4a643be3", "text": "Creating tools from functions‚Äã\n\n@tool decorator‚Äã\n\nThis @tool decorator is the simplest way to define a custom tool. The decorator uses the function name as the tool name by default, but this can be overridden by passing a string as the first argument. Additionally, the decorator will use the function's docstring as the tool's description - so a docstring MUST be provided.\n\n```\nfrom langchain_core.tools import tool@tooldef multiply(a: int, b: int) -> int:    \"\"\"Multiply two numbers.\"\"\"    return a * b# Let's inspect some of the attributes associated with the tool.print(multiply.name)print(multiply.description)print(multiply.args)\n```\n\n```\nmultiplyMultiply two numbers.{'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}\n```\n\nOr create an async implementation, like this:\n\n```\nfrom langchain_core.tools import tool@toolasync def amultiply(a: int, b: int) -> int:    \"\"\"Multiply two numbers.\"\"\"    return a * b\n```\n\nNote that @tool supports parsing of annotations, nested schemas, and other features:\n\n```\nfrom typing import Annotated, List@tooldef multiply_by_max(    a: Annotated[int, \"scale factor\"],    b: Annotated[List[int], \"list of ints over which to take maximum\"],) -> int:    \"\"\"Multiply a by the maximum of b.\"\"\"    return a * max(b)print(multiply_by_max.args_schema.model_json_schema())\n```\n\n```\n{'description': 'Multiply a by the maximum of b.', 'properties': {'a': {'description': 'scale factor',   'title': 'A',   'type': 'integer'},  'b': {'description': 'list of ints over which to take maximum',   'items': {'type': 'integer'},   'title': 'B',   'type': 'array'}}, 'required': ['a', 'b'], 'title': 'multiply_by_maxSchema', 'type': 'object'}\n```\n\nYou can also customize the tool name and JSON args by passing them into the tool decorator.\n\n```\nfrom pydantic import BaseModel, Fieldclass CalculatorInput(BaseModel):    a: int = Field(description=\"first number\")    b: int = Field(description=\"second number\")@tool(\"multiplication-tool\", args_schema=CalculatorInput, return_direct=True)def multiply(a: int, b: int) -> int:    \"\"\"Multiply two numbers.\"\"\"    return a * b# Let's inspect some of the attributes associated with the tool.print(multiply.name)print(multiply.description)print(multiply.args)print(multiply.return_direct)\n```\n\n```\nmultiplication-toolMultiply two numbers.{'a': {'description': 'first number', 'title': 'A', 'type': 'integer'}, 'b': {'description': 'second number', 'title': 'B', 'type': 'integer'}}True\n```\n\n@tool can optionally parse Google Style docstrings and associate the docstring components (such as arg descriptions) to the relevant parts of the tool schema. To toggle this behavior, specify parse_docstring:\n\n```\n@tool(parse_docstring=True)def foo(bar: str, baz: int) -> str:    \"\"\"The foo.    Args:        bar: The bar.        baz: The baz.    \"\"\"    return barprint(foo.args_schema.model_json_schema())\n```\n\n```\n{'description': 'The foo.', 'properties': {'bar': {'description': 'The bar.',   'title': 'Bar',   'type': 'string'},  'baz': {'description': 'The baz.', 'title': 'Baz', 'type': 'integer'}}, 'required': ['bar', 'baz'], 'title': 'fooSchema', 'type': 'object'}\n```\n\nBy default, @tool(parse_docstring=True) will raise ValueError if the docstring does not parse correctly. See API Reference for detail and examples.", "url_final": "https://python.langchain.com/docs/how_to/custom_tools/", "url_citable": "https://python.langchain.com/docs/how_to/custom_tools/#creating-tools-from-functions", "title": "How to create tools | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Creating tools from functions‚Äã", "section_anchor": "creating-tools-from-functions", "fetched_at": "2025-10-06T21:55:17.400477", "has_code": true}
{"id": "3f9341a0652821716ad1d06c117dc56bf2a9b979", "text": "StructuredTool‚Äã\n\nThe StructuredTool.from_function class method provides a bit more configurability than the @tool decorator, without requiring much additional code.\n\n```\nfrom langchain_core.tools import StructuredTooldef multiply(a: int, b: int) -> int:    \"\"\"Multiply two numbers.\"\"\"    return a * basync def amultiply(a: int, b: int) -> int:    \"\"\"Multiply two numbers.\"\"\"    return a * bcalculator = StructuredTool.from_function(func=multiply, coroutine=amultiply)print(calculator.invoke({\"a\": 2, \"b\": 3}))print(await calculator.ainvoke({\"a\": 2, \"b\": 5}))\n```\n\n```\n610\n```\n\nTo configure it:\n\n```\nclass CalculatorInput(BaseModel):    a: int = Field(description=\"first number\")    b: int = Field(description=\"second number\")def multiply(a: int, b: int) -> int:    \"\"\"Multiply two numbers.\"\"\"    return a * bcalculator = StructuredTool.from_function(    func=multiply,    name=\"Calculator\",    description=\"multiply numbers\",    args_schema=CalculatorInput,    return_direct=True,    # coroutine= ... <- you can specify an async method if desired as well)print(calculator.invoke({\"a\": 2, \"b\": 3}))print(calculator.name)print(calculator.description)print(calculator.args)\n```\n\n```\n6Calculatormultiply numbers{'a': {'description': 'first number', 'title': 'A', 'type': 'integer'}, 'b': {'description': 'second number', 'title': 'B', 'type': 'integer'}}\n```", "url_final": "https://python.langchain.com/docs/how_to/custom_tools/", "url_citable": "https://python.langchain.com/docs/how_to/custom_tools/#structuredtool", "title": "How to create tools | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "StructuredTool‚Äã", "section_anchor": "structuredtool", "fetched_at": "2025-10-06T21:55:17.400477", "has_code": true}
{"id": "245c1753dc7ca3c701f1a82c50ee1b988a3eef81", "text": "Creating tools from Runnables‚Äã\n\nLangChain Runnables that accept string or dict input can be converted to tools using the as_tool method, which allows for the specification of names, descriptions, and additional schema information for arguments.\n\nExample usage:\n\n```\nfrom langchain_core.language_models import GenericFakeChatModelfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplateprompt = ChatPromptTemplate.from_messages(    [(\"human\", \"Hello. Please respond in the style of {answer_style}.\")])# Placeholder LLMllm = GenericFakeChatModel(messages=iter([\"hello matey\"]))chain = prompt | llm | StrOutputParser()as_tool = chain.as_tool(    name=\"Style responder\", description=\"Description of when to use tool.\")as_tool.args\n```\n\n```\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_95770/2548361071.py:14: LangChainBetaWarning: This API is in beta and may change in the future.  as_tool = chain.as_tool(\n```\n\n```\n{'answer_style': {'title': 'Answer Style', 'type': 'string'}}\n```\n\nSee this guide for more detail.", "url_final": "https://python.langchain.com/docs/how_to/custom_tools/", "url_citable": "https://python.langchain.com/docs/how_to/custom_tools/#creating-tools-from-runnables", "title": "How to create tools | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Creating tools from Runnables‚Äã", "section_anchor": "creating-tools-from-runnables", "fetched_at": "2025-10-06T21:55:17.400477", "has_code": true}
{"id": "ba361efc36812013c26971a5c7232de900c358f9", "text": "Subclass BaseTool‚Äã\n\nYou can define a custom tool by sub-classing from BaseTool. This provides maximal control over the tool definition, but requires writing more code.\n\n```\nfrom typing import Optionalfrom langchain_core.callbacks import (    AsyncCallbackManagerForToolRun,    CallbackManagerForToolRun,)from langchain_core.tools import BaseToolfrom langchain_core.tools.base import ArgsSchemafrom pydantic import BaseModel, Fieldclass CalculatorInput(BaseModel):    a: int = Field(description=\"first number\")    b: int = Field(description=\"second number\")# Note: It's important that every field has type hints. BaseTool is a# Pydantic class and not having type hints can lead to unexpected behavior.class CustomCalculatorTool(BaseTool):    name: str = \"Calculator\"    description: str = \"useful for when you need to answer questions about math\"    args_schema: Optional[ArgsSchema] = CalculatorInput    return_direct: bool = True    def _run(        self, a: int, b: int, run_manager: Optional[CallbackManagerForToolRun] = None    ) -> int:        \"\"\"Use the tool.\"\"\"        return a * b    async def _arun(        self,        a: int,        b: int,        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,    ) -> int:        \"\"\"Use the tool asynchronously.\"\"\"        # If the calculation is cheap, you can just delegate to the sync implementation        # as shown below.        # If the sync calculation is expensive, you should delete the entire _arun method.        # LangChain will automatically provide a better implementation that will        # kick off the task in a thread to make sure it doesn't block other async code.        return self._run(a, b, run_manager=run_manager.get_sync())\n```\n\n```\nmultiply = CustomCalculatorTool()print(multiply.name)print(multiply.description)print(multiply.args)print(multiply.return_direct)print(multiply.invoke({\"a\": 2, \"b\": 3}))print(await multiply.ainvoke({\"a\": 2, \"b\": 3}))\n```\n\n```\nCalculatoruseful for when you need to answer questions about math{'a': {'description': 'first number', 'title': 'A', 'type': 'integer'}, 'b': {'description': 'second number', 'title': 'B', 'type': 'integer'}}True66\n```", "url_final": "https://python.langchain.com/docs/how_to/custom_tools/", "url_citable": "https://python.langchain.com/docs/how_to/custom_tools/#subclass-basetool", "title": "How to create tools | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Subclass BaseTool‚Äã", "section_anchor": "subclass-basetool", "fetched_at": "2025-10-06T21:55:17.400477", "has_code": true}
{"id": "dced4e00caef179fac7fd8263a80fa59f680ff7d", "text": "How to create async tools‚Äã\n\nLangChain Tools implement the Runnable interface üèÉ.\n\nAll Runnables expose the invoke and ainvoke methods (as well as other methods like batch, abatch, astream etc).\n\nSo even if you only provide an sync implementation of a tool, you could still use the ainvoke interface, but there are some important things to know:\n\n- LangChain's by default provides an async implementation that assumes that the function is expensive to compute, so it'll delegate execution to another thread.\n- If you're working in an async codebase, you should create async tools rather than sync tools, to avoid incuring a small overhead due to that thread.\n- If you need both sync and async implementations, use StructuredTool.from_function or sub-class from BaseTool.\n- If implementing both sync and async, and the sync code is fast to run, override the default LangChain async implementation and simply call the sync code.\n- You CANNOT and SHOULD NOT use the sync invoke with an async tool.\n\n```\nfrom langchain_core.tools import StructuredTooldef multiply(a: int, b: int) -> int:    \"\"\"Multiply two numbers.\"\"\"    return a * bcalculator = StructuredTool.from_function(func=multiply)print(calculator.invoke({\"a\": 2, \"b\": 3}))print(    await calculator.ainvoke({\"a\": 2, \"b\": 5}))  # Uses default LangChain async implementation incurs small overhead\n```\n\n```\n610\n```\n\n```\nfrom langchain_core.tools import StructuredTooldef multiply(a: int, b: int) -> int:    \"\"\"Multiply two numbers.\"\"\"    return a * basync def amultiply(a: int, b: int) -> int:    \"\"\"Multiply two numbers.\"\"\"    return a * bcalculator = StructuredTool.from_function(func=multiply, coroutine=amultiply)print(calculator.invoke({\"a\": 2, \"b\": 3}))print(    await calculator.ainvoke({\"a\": 2, \"b\": 5}))  # Uses use provided amultiply without additional overhead\n```\n\n```\n610\n```\n\nYou should not and cannot use .invoke when providing only an async definition.\n\n```\n@toolasync def multiply(a: int, b: int) -> int:    \"\"\"Multiply two numbers.\"\"\"    return a * btry:    multiply.invoke({\"a\": 2, \"b\": 3})except NotImplementedError:    print(\"Raised not implemented error. You should not be doing this.\")\n```\n\n```\nRaised not implemented error. You should not be doing this.\n```", "url_final": "https://python.langchain.com/docs/how_to/custom_tools/", "url_citable": "https://python.langchain.com/docs/how_to/custom_tools/#how-to-create-async-tools", "title": "How to create tools | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "How to create async tools‚Äã", "section_anchor": "how-to-create-async-tools", "fetched_at": "2025-10-06T21:55:17.400477", "has_code": true}
{"id": "b7ec7c66b7bb2450639a2b848509aa69bf220080", "text": "Handling Tool Errors‚Äã\n\nIf you're using tools with agents, you will likely need an error handling strategy, so the agent can recover from the error and continue execution.\n\nA simple strategy is to throw a ToolException from inside the tool and specify an error handler using handle_tool_errors.\n\nWhen the error handler is specified, the exception will be caught and the error handler will decide which output to return from the tool.\n\nYou can set handle_tool_errors to True, a string value, or a function. If it's a function, the function should take a ToolException as a parameter and return a value.\n\nPlease note that only raising a ToolException won't be effective. You need to first set the handle_tool_errors of the tool because its default value is False.\n\n```\nfrom langchain_core.tools import ToolExceptiondef get_weather(city: str) -> int:    \"\"\"Get weather for the given city.\"\"\"    raise ToolException(f\"Error: There is no city by the name of {city}.\")\n```\n\nHere's an example with the default handle_tool_errors=True behavior.\n\n```\nget_weather_tool = StructuredTool.from_function(    func=get_weather,    handle_tool_errors=True,)get_weather_tool.invoke({\"city\": \"foobar\"})\n```\n\n```\n'Error: There is no city by the name of foobar.'\n```\n\nWe can set handle_tool_errors to a string that will always be returned.\n\n```\nget_weather_tool = StructuredTool.from_function(    func=get_weather,    handle_tool_errors=\"There is no such city, but it's probably above 0K there!\",)get_weather_tool.invoke({\"city\": \"foobar\"})\n```\n\n```\n\"There is no such city, but it's probably above 0K there!\"\n```\n\nHandling the error using a function:\n\n```\ndef _handle_error(error: ToolException) -> str:    return f\"The following errors occurred during tool execution: `{error.args[0]}`\"get_weather_tool = StructuredTool.from_function(    func=get_weather,    handle_tool_errors=_handle_error,)get_weather_tool.invoke({\"city\": \"foobar\"})\n```\n\n```\n'The following errors occurred during tool execution: `Error: There is no city by the name of foobar.`'\n```", "url_final": "https://python.langchain.com/docs/how_to/custom_tools/", "url_citable": "https://python.langchain.com/docs/how_to/custom_tools/#handling-tool-errors", "title": "How to create tools | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Handling Tool Errors‚Äã", "section_anchor": "handling-tool-errors", "fetched_at": "2025-10-06T21:55:17.400477", "has_code": true}
{"id": "0986d42dcf540146ee222a52d64b1109aacc4a8f", "text": "Returning artifacts of Tool execution‚Äã\n\nSometimes there are artifacts of a tool's execution that we want to make accessible to downstream components in our chain or agent, but that we don't want to expose to the model itself. For example if a tool returns custom objects like Documents, we may want to pass some view or metadata about this output to the model without passing the raw output to the model. At the same time, we may want to be able to access this full output elsewhere, for example in downstream tools.\n\nThe Tool and ToolMessage interfaces make it possible to distinguish between the parts of the tool output meant for the model (this is the ToolMessage.content) and those parts which are meant for use outside the model (ToolMessage.artifact).\n\nThis functionality was added in langchain-core == 0.2.19. Please make sure your package is up to date.\n\nIf we want our tool to distinguish between message content and other artifacts, we need to specify response_format=\"content_and_artifact\" when defining our tool and make sure that we return a tuple of (content, artifact):\n\n```\nimport randomfrom typing import List, Tuplefrom langchain_core.tools import tool@tool(response_format=\"content_and_artifact\")def generate_random_ints(min: int, max: int, size: int) -> Tuple[str, List[int]]:    \"\"\"Generate size random ints in the range [min, max].\"\"\"    array = [random.randint(min, max) for _ in range(size)]    content = f\"Successfully generated array of {size} random ints in [{min}, {max}].\"    return content, array\n```\n\nIf we invoke our tool directly with the tool arguments, we'll get back just the content part of the output:\n\n```\ngenerate_random_ints.invoke({\"min\": 0, \"max\": 9, \"size\": 10})\n```\n\n```\n'Successfully generated array of 10 random ints in [0, 9].'\n```\n\nIf we invoke our tool with a ToolCall (like the ones generated by tool-calling models), we'll get back a ToolMessage that contains both the content and artifact generated by the Tool:\n\n```\ngenerate_random_ints.invoke(    {        \"name\": \"generate_random_ints\",        \"args\": {\"min\": 0, \"max\": 9, \"size\": 10},        \"id\": \"123\",  # required        \"type\": \"tool_call\",  # required    })\n```\n\n```\nToolMessage(content='Successfully generated array of 10 random ints in [0, 9].', name='generate_random_ints', tool_call_id='123', artifact=[4, 8, 2, 4, 1, 0, 9, 5, 8, 1])\n```\n\nWe can do the same when subclassing BaseTool:\n\n```\nfrom langchain_core.tools import BaseToolclass GenerateRandomFloats(BaseTool):    name: str = \"generate_random_floats\"    description: str = \"Generate size random floats in the range [min, max].\"    response_format: str = \"content_and_artifact\"    ndigits: int = 2    def _run(self, min: float, max: float, size: int) -> Tuple[str, List[float]]:        range_ = max - min        array = [            round(min + (range_ * random.random()), ndigits=self.ndigits)            for _ in range(size)        ]        content = f\"Generated {size} floats in [{min}, {max}], rounded to {self.ndigits} decimals.\"        return content, array    # Optionally define an equivalent async method    # async def _arun(self, min: float, max: float, size: int) -> Tuple[str, List[float]]:    #     ...\n```\n\n```\nrand_gen = GenerateRandomFloats(ndigits=4)rand_gen.invoke(    {        \"name\": \"generate_random_floats\",        \"args\": {\"min\": 0.1, \"max\": 3.3333, \"size\": 3},        \"id\": \"123\",        \"type\": \"tool_call\",    })\n```\n\n```\nToolMessage(content='Generated 3 floats in [0.1, 3.3333], rounded to 4 decimals.', name='generate_random_floats', tool_call_id='123', artifact=[1.5566, 0.5134, 2.7914])\n```\n\n- @tool decorator\n- StructuredTool", "url_final": "https://python.langchain.com/docs/how_to/custom_tools/", "url_citable": "https://python.langchain.com/docs/how_to/custom_tools/#returning-artifacts-of-tool-execution", "title": "How to create tools | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Returning artifacts of Tool execution‚Äã", "section_anchor": "returning-artifacts-of-tool-execution", "fetched_at": "2025-10-06T21:55:17.400477", "has_code": true}
{"id": "b0c0f68465e207008f2074471233b30a77503453", "text": "‚ö†Ô∏è Security note ‚ö†Ô∏è‚Äã\n\nConstructing knowledge graphs requires executing write access to the database. There are inherent risks in doing this. Make sure that you verify and validate data before importing it. For more on general security best practices, see here.", "url_final": "https://python.langchain.com/docs/how_to/graph_constructing/", "url_citable": "https://python.langchain.com/docs/how_to/graph_constructing/#Ô∏è-security-note-Ô∏è", "title": "How to construct knowledge graphs | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "‚ö†Ô∏è Security note ‚ö†Ô∏è‚Äã", "section_anchor": "Ô∏è-security-note-Ô∏è", "fetched_at": "2025-10-06T21:54:42.469200", "has_code": false}
{"id": "f5c9d2fb9300564037dae3570b5a3719265cdea0", "text": "Architecture‚Äã\n\nAt a high-level, the steps of constructing a knowledge graph from text are:\n\n- Extracting structured information from text: Model is used to extract structured graph information from text.\n- Storing into graph database: Storing the extracted structured graph information into a graph database enables downstream RAG applications", "url_final": "https://python.langchain.com/docs/how_to/graph_constructing/", "url_citable": "https://python.langchain.com/docs/how_to/graph_constructing/#architecture", "title": "How to construct knowledge graphs | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Architecture‚Äã", "section_anchor": "architecture", "fetched_at": "2025-10-06T21:54:42.469200", "has_code": false}
{"id": "dfbe8c33f35f6c4b4cef4a96ff3b46fda301905c", "text": "Setup‚Äã\n\nFirst, get required packages and set environment variables. In this example, we will be using Neo4j graph database.\n\n```\n%pip install --upgrade --quiet  langchain langchain-neo4j langchain-openai langchain-experimental neo4j\n```\n\n```\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0mNote: you may need to restart the kernel to use updated packages.\n```\n\nWe default to OpenAI models in this guide.\n\n```\nimport getpassimport osos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()# Uncomment the below to use LangSmith. Not required.# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n```\n\n```\n ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n```\n\nNext, we need to define Neo4j credentials and connection. Follow these installation steps to set up a Neo4j database.\n\n```\nimport osfrom langchain_neo4j import Neo4jGraphos.environ[\"NEO4J_URI\"] = \"bolt://localhost:7687\"os.environ[\"NEO4J_USERNAME\"] = \"neo4j\"os.environ[\"NEO4J_PASSWORD\"] = \"password\"graph = Neo4jGraph(refresh_schema=False)\n```", "url_final": "https://python.langchain.com/docs/how_to/graph_constructing/", "url_citable": "https://python.langchain.com/docs/how_to/graph_constructing/#setup", "title": "How to construct knowledge graphs | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Setup‚Äã", "section_anchor": "setup", "fetched_at": "2025-10-06T21:54:42.469200", "has_code": true}
{"id": "2b27b142bd7597fa92ce04dca34a64a5542ea369", "text": "LLM Graph Transformer‚Äã\n\nExtracting graph data from text enables the transformation of unstructured information into structured formats, facilitating deeper insights and more efficient navigation through complex relationships and patterns. The LLMGraphTransformer converts text documents into structured graph documents by leveraging a LLM to parse and categorize entities and their relationships. The selection of the LLM model significantly influences the output by determining the accuracy and nuance of the extracted graph data.\n\n```\nimport osfrom langchain_experimental.graph_transformers import LLMGraphTransformerfrom langchain_openai import ChatOpenAIllm = ChatOpenAI(temperature=0, model_name=\"gpt-4-turbo\")llm_transformer = LLMGraphTransformer(llm=llm)\n```\n\nNow we can pass in example text and examine the results.\n\n```\nfrom langchain_core.documents import Documenttext = \"\"\"Marie Curie, born in 1867, was a Polish and naturalised-French physicist and chemist who conducted pioneering research on radioactivity.She was the first woman to win a Nobel Prize, the first person to win a Nobel Prize twice, and the only person to win a Nobel Prize in two scientific fields.Her husband, Pierre Curie, was a co-winner of her first Nobel Prize, making them the first-ever married couple to win the Nobel Prize and launching the Curie family legacy of five Nobel Prizes.She was, in 1906, the first woman to become a professor at the University of Paris.\"\"\"documents = [Document(page_content=text)]graph_documents = await llm_transformer.aconvert_to_graph_documents(documents)print(f\"Nodes:{graph_documents[0].nodes}\")print(f\"Relationships:{graph_documents[0].relationships}\")\n```\n\n```\nNodes:[Node(id='Marie Curie', type='Person', properties={}), Node(id='Pierre Curie', type='Person', properties={}), Node(id='University Of Paris', type='Organization', properties={})]Relationships:[Relationship(source=Node(id='Marie Curie', type='Person', properties={}), target=Node(id='Pierre Curie', type='Person', properties={}), type='MARRIED', properties={}), Relationship(source=Node(id='Marie Curie', type='Person', properties={}), target=Node(id='University Of Paris', type='Organization', properties={}), type='PROFESSOR', properties={})]\n```\n\nExamine the following image to better grasp the structure of the generated knowledge graph.\n\nNote that the graph construction process is non-deterministic since we are using LLM. Therefore, you might get slightly different results on each execution.\n\nAdditionally, you have the flexibility to define specific types of nodes and relationships for extraction according to your requirements.\n\n```\nllm_transformer_filtered = LLMGraphTransformer(    llm=llm,    allowed_nodes=[\"Person\", \"Country\", \"Organization\"],    allowed_relationships=[\"NATIONALITY\", \"LOCATED_IN\", \"WORKED_AT\", \"SPOUSE\"],)graph_documents_filtered = await llm_transformer_filtered.aconvert_to_graph_documents(    documents)print(f\"Nodes:{graph_documents_filtered[0].nodes}\")print(f\"Relationships:{graph_documents_filtered[0].relationships}\")\n```\n\n```\nNodes:[Node(id='Marie Curie', type='Person', properties={}), Node(id='Pierre Curie', type='Person', properties={}), Node(id='University Of Paris', type='Organization', properties={})]Relationships:[Relationship(source=Node(id='Marie Curie', type='Person', properties={}), target=Node(id='Pierre Curie', type='Person', properties={}), type='SPOUSE', properties={}), Relationship(source=Node(id='Marie Curie', type='Person', properties={}), target=Node(id='University Of Paris', type='Organization', properties={}), type='WORKED_AT', properties={})]\n```\n\nTo define the graph schema more precisely, consider using a three-tuple approach for relationships. In this approach, each tuple consists of three elements: the source node, the relationship type, and the target node.\n\n```\nallowed_relationships = [    (\"Person\", \"SPOUSE\", \"Person\"),    (\"Person\", \"NATIONALITY\", \"Country\"),    (\"Person\", \"WORKED_AT\", \"Organization\"),]llm_transformer_tuple = LLMGraphTransformer(    llm=llm,    allowed_nodes=[\"Person\", \"Country\", \"Organization\"],    allowed_relationships=allowed_relationships,)graph_documents_filtered = await llm_transformer_tuple.aconvert_to_graph_documents(    documents)print(f\"Nodes:{graph_documents_filtered[0].nodes}\")print(f\"Relationships:{graph_documents_filtered[0].relationships}\")\n```\n\n```\nNodes:[Node(id='Marie Curie', type='Person', properties={}), Node(id='Pierre Curie', type='Person', properties={}), Node(id='University Of Paris', type='Organization', properties={})]Relationships:[Relationship(source=Node(id='Marie Curie', type='Person', properties={}), target=Node(id='Pierre Curie', type='Person', properties={}), type='SPOUSE', properties={}), Relationship(source=Node(id='Marie Curie', type='Person', properties={}), target=Node(id='University Of Paris', type='Organization', properties={}), type='WORKED_AT', properties={})]\n```\n\nFor a better understanding of the generated graph, we can again visualize it.\n\nThe node_properties parameter enables the extraction of node properties, allowing the creation of a more detailed graph. When set to True, LLM autonomously identifies and extracts relevant node properties. Conversely, if node_properties is defined as a list of strings, the LLM selectively retrieves only the specified properties from the text.\n\n```\nllm_transformer_props = LLMGraphTransformer(    llm=llm,    allowed_nodes=[\"Person\", \"Country\", \"Organization\"],    allowed_relationships=[\"NATIONALITY\", \"LOCATED_IN\", \"WORKED_AT\", \"SPOUSE\"],    node_properties=[\"born_year\"],)graph_documents_props = await llm_transformer_props.aconvert_to_graph_documents(    documents)print(f\"Nodes:{graph_documents_props[0].nodes}\")print(f\"Relationships:{graph_documents_props[0].relationships}\")\n```\n\n```\nNodes:[Node(id='Marie Curie', type='Person', properties={'born_year': '1867'}), Node(id='Pierre Curie', type='Person', properties={}), Node(id='University Of Paris', type='Organization', properties={}), Node(id='Poland', type='Country', properties={}), Node(id='France', type='Country', properties={})]Relationships:[Relationship(source=Node(id='Marie Curie', type='Person', properties={}), target=Node(id='Poland', type='Country', properties={}), type='NATIONALITY', properties={}), Relationship(source=Node(id='Marie Curie', type='Person', properties={}), target=Node(id='France', type='Country', properties={}), type='NATIONALITY', properties={}), Relationship(source=Node(id='Marie Curie', type='Person', properties={}), target=Node(id='Pierre Curie', type='Person', properties={}), type='SPOUSE', properties={}), Relationship(source=Node(id='Marie Curie', type='Person', properties={}), target=Node(id='University Of Paris', type='Organization', properties={}), type='WORKED_AT', properties={})]\n```", "url_final": "https://python.langchain.com/docs/how_to/graph_constructing/", "url_citable": "https://python.langchain.com/docs/how_to/graph_constructing/#llm-graph-transformer", "title": "How to construct knowledge graphs | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "LLM Graph Transformer‚Äã", "section_anchor": "llm-graph-transformer", "fetched_at": "2025-10-06T21:54:42.469200", "has_code": true}
{"id": "a0527abaf2e13c24baa3a6c025398ce3b892bb9e", "text": "Storing to graph database‚Äã\n\nThe generated graph documents can be stored to a graph database using the add_graph_documents method.\n\n```\ngraph.add_graph_documents(graph_documents_props)\n```\n\nMost graph databases support indexes to optimize data import and retrieval. Since we might not know all the node labels in advance, we can handle this by adding a secondary base label to each node using the baseEntityLabel parameter.\n\n```\ngraph.add_graph_documents(graph_documents, baseEntityLabel=True)\n```\n\nResults will look like:\n\nThe final option is to also import the source documents for the extracted nodes and relationships. This approach lets us track which documents each entity appeared in.\n\n```\ngraph.add_graph_documents(graph_documents, include_source=True)\n```\n\nGraph will have the following structure:\n\nIn this visualization, the source document is highlighted in blue, with all entities extracted from it connected by MENTIONS relationships.\n\n- ‚ö†Ô∏è Security note ‚ö†Ô∏è\n- Architecture\n- Setup\n- LLM Graph Transformer\n- Storing to graph database", "url_final": "https://python.langchain.com/docs/how_to/graph_constructing/", "url_citable": "https://python.langchain.com/docs/how_to/graph_constructing/#storing-to-graph-database", "title": "How to construct knowledge graphs | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Storing to graph database‚Äã", "section_anchor": "storing-to-graph-database", "fetched_at": "2025-10-06T21:54:42.469200", "has_code": true}
{"id": "1c2a98ac6aa2c10eca920082bcdb5ec20e6f6217", "text": "Setup‚Äã\n\nFirst, get required packages and set environment variables:\n\n```\n%pip install --upgrade --quiet  langchain langchain-community langchain-experimental langchain-openai\n```\n\n```\n# Uncomment the below to use LangSmith. Not required.# import os# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n```\n\nThe below example will use a SQLite connection with Chinook database. Follow these installation steps to create Chinook.db in the same directory as this notebook:\n\n- Save this file as Chinook_Sqlite.sql\n- Run sqlite3 Chinook.db\n- Run .read Chinook_Sqlite.sql\n- Test SELECT * FROM Artist LIMIT 10;\n\nNow, Chinook.db is in our directory and we can interface with it using the SQLAlchemy-driven SQLDatabase class:\n\n```\nfrom langchain_community.utilities import SQLDatabasedb = SQLDatabase.from_uri(\"sqlite:///Chinook.db\", sample_rows_in_table_info=3)print(db.dialect)print(db.get_usable_table_names())print(db.run(\"SELECT * FROM Artist LIMIT 10;\"))\n```\n\n```\nsqlite['Album', 'Artist', 'Customer', 'Employee', 'Genre', 'Invoice', 'InvoiceLine', 'MediaType', 'Playlist', 'PlaylistTrack', 'Track'][(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains'), (6, 'Ant√¥nio Carlos Jobim'), (7, 'Apocalyptica'), (8, 'Audioslave'), (9, 'BackBeat'), (10, 'Billy Cobham')]\n```", "url_final": "https://python.langchain.com/docs/how_to/sql_prompting/", "url_citable": "https://python.langchain.com/docs/how_to/sql_prompting/#setup", "title": "How to better prompt when doing SQL question-answering | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Setup‚Äã", "section_anchor": "setup", "fetched_at": "2025-10-06T21:56:26.487558", "has_code": true}
{"id": "e77d50c73bb53423145769c55279da144e6dfd26", "text": "Dialect-specific prompting‚Äã\n\nOne of the simplest things we can do is make our prompt specific to the SQL dialect we're using. When using the built-in create_sql_query_chain and SQLDatabase, this is handled for you for any of the following dialects:\n\n```\nfrom langchain.chains.sql_database.prompt import SQL_PROMPTSlist(SQL_PROMPTS)\n```\n\n```\n['crate', 'duckdb', 'googlesql', 'mssql', 'mysql', 'mariadb', 'oracle', 'postgresql', 'sqlite', 'clickhouse', 'prestodb']\n```\n\nFor example, using our current DB we can see that we'll get a SQLite-specific prompt.\n\n```\npip install -qU \"langchain[google-genai]\"\n```\n\n```\nimport getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n```\n\n```\nfrom langchain.chains import create_sql_query_chainchain = create_sql_query_chain(llm, db)chain.get_prompts()[0].pretty_print()\n```\n\n```\nYou are a SQLite expert. Given an input question, first create a syntactically correct SQLite query to run, then look at the results of the query and return the answer to the input question.Unless the user specifies in the question a specific number of examples to obtain, query for at most 5 results using the LIMIT clause as per SQLite. You can order the results to return the most informative data in the database.Never query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.Pay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.Pay attention to use date('now') function to get the current date, if the question involves \"today\".Use the following format:Question: Question hereSQLQuery: SQL Query to runSQLResult: Result of the SQLQueryAnswer: Final answer hereOnly use the following tables:\u001b[33;1m\u001b[1;3m{table_info}\u001b[0mQuestion: \u001b[33;1m\u001b[1;3m{input}\u001b[0m\n```", "url_final": "https://python.langchain.com/docs/how_to/sql_prompting/", "url_citable": "https://python.langchain.com/docs/how_to/sql_prompting/#dialect-specific-prompting", "title": "How to better prompt when doing SQL question-answering | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Dialect-specific prompting‚Äã", "section_anchor": "dialect-specific-prompting", "fetched_at": "2025-10-06T21:56:26.487558", "has_code": true}
{"id": "999aded7130d7dff9087f608925faac3c5b15155", "text": "Table definitions and example rows‚Äã\n\nIn most SQL chains, we'll need to feed the model at least part of the database schema. Without this it won't be able to write valid queries. Our database comes with some convenience methods to give us the relevant context. Specifically, we can get the table names, their schemas, and a sample of rows from each table.\n\nHere we will use SQLDatabase.get_context, which provides available tables and their schemas:\n\n```\ncontext = db.get_context()print(list(context))print(context[\"table_info\"])\n```\n\n```\n['table_info', 'table_names']CREATE TABLE \"Album\" (\t\"AlbumId\" INTEGER NOT NULL, \t\"Title\" NVARCHAR(160) NOT NULL, \t\"ArtistId\" INTEGER NOT NULL, \tPRIMARY KEY (\"AlbumId\"), \tFOREIGN KEY(\"ArtistId\") REFERENCES \"Artist\" (\"ArtistId\"))/*3 rows from Album table:AlbumId\tTitle\tArtistId1\tFor Those About To Rock We Salute You\t12\tBalls to the Wall\t23\tRestless and Wild\t2*/CREATE TABLE \"Artist\" (\t\"ArtistId\" INTEGER NOT NULL, \t\"Name\" NVARCHAR(120), \tPRIMARY KEY (\"ArtistId\"))/*3 rows from Artist table:ArtistId\tName1\tAC/DC2\tAccept3\tAerosmith*/CREATE TABLE \"Customer\" (\t\"CustomerId\" INTEGER NOT NULL, \t\"FirstName\" NVARCHAR(40) NOT NULL, \t\"LastName\" NVARCHAR(20) NOT NULL, \t\"Company\" NVARCHAR(80), \t\"Address\" NVARCHAR(70), \t\"City\" NVARCHAR(40), \t\"State\" NVARCHAR(40), \t\"Country\" NVARCHAR(40), \t\"PostalCode\" NVARCHAR(10), \t\"Phone\" NVARCHAR(24), \t\"Fax\" NVARCHAR(24), \t\"Email\" NVARCHAR(60) NOT NULL, \t\"SupportRepId\" INTEGER, \tPRIMARY KEY (\"CustomerId\"), \tFOREIGN KEY(\"SupportRepId\") REFERENCES \"Employee\" (\"EmployeeId\"))/*3 rows from Customer table:CustomerId\tFirstName\tLastName\tCompany\tAddress\tCity\tState\tCountry\tPostalCode\tPhone\tFax\tEmail\tSupportRepId1\tLu√≠s\tGon√ßalves\tEmbraer - Empresa Brasileira de Aeron√°utica S.A.\tAv. Brigadeiro Faria Lima, 2170\tS√£o Jos√© dos Campos\tSP\tBrazil\t12227-000\t+55 (12) 3923-5555\t+55 (12) 3923-5566\tluisg@embraer.com.br\t32\tLeonie\tK√∂hler\tNone\tTheodor-Heuss-Stra√üe 34\tStuttgart\tNone\tGermany\t70174\t+49 0711 2842222\tNone\tleonekohler@surfeu.de\t53\tFran√ßois\tTremblay\tNone\t1498 rue B√©langer\tMontr√©al\tQC\tCanada\tH2G 1A7\t+1 (514) 721-4711\tNone\tftremblay@gmail.com\t3*/CREATE TABLE \"Employee\" (\t\"EmployeeId\" INTEGER NOT NULL, \t\"LastName\" NVARCHAR(20) NOT NULL, \t\"FirstName\" NVARCHAR(20) NOT NULL, \t\"Title\" NVARCHAR(30), \t\"ReportsTo\" INTEGER, \t\"BirthDate\" DATETIME, \t\"HireDate\" DATETIME, \t\"Address\" NVARCHAR(70), \t\"City\" NVARCHAR(40), \t\"State\" NVARCHAR(40), \t\"Country\" NVARCHAR(40), \t\"PostalCode\" NVARCHAR(10), \t\"Phone\" NVARCHAR(24), \t\"Fax\" NVARCHAR(24), \t\"Email\" NVARCHAR(60), \tPRIMARY KEY (\"EmployeeId\"), \tFOREIGN KEY(\"ReportsTo\") REFERENCES \"Employee\" (\"EmployeeId\"))/*3 rows from Employee table:EmployeeId\tLastName\tFirstName\tTitle\tReportsTo\tBirthDate\tHireDate\tAddress\tCity\tState\tCountry\tPostalCode\tPhone\tFax\tEmail1\tAdams\tAndrew\tGeneral Manager\tNone\t1962-02-18 00:00:00\t2002-08-14 00:00:00\t11120 Jasper Ave NW\tEdmonton\tAB\tCanada\tT5K 2N1\t+1 (780) 428-9482\t+1 (780) 428-3457\tandrew@chinookcorp.com2\tEdwards\tNancy\tSales Manager\t1\t1958-12-08 00:00:00\t2002-05-01 00:00:00\t825 8 Ave SW\tCalgary\tAB\tCanada\tT2P 2T3\t+1 (403) 262-3443\t+1 (403) 262-3322\tnancy@chinookcorp.com3\tPeacock\tJane\tSales Support Agent\t2\t1973-08-29 00:00:00\t2002-04-01 00:00:00\t1111 6 Ave SW\tCalgary\tAB\tCanada\tT2P 5M5\t+1 (403) 262-3443\t+1 (403) 262-6712\tjane@chinookcorp.com*/CREATE TABLE \"Genre\" (\t\"GenreId\" INTEGER NOT NULL, \t\"Name\" NVARCHAR(120), \tPRIMARY KEY (\"GenreId\"))/*3 rows from Genre table:GenreId\tName1\tRock2\tJazz3\tMetal*/CREATE TABLE \"Invoice\" (\t\"InvoiceId\" INTEGER NOT NULL, \t\"CustomerId\" INTEGER NOT NULL, \t\"InvoiceDate\" DATETIME NOT NULL, \t\"BillingAddress\" NVARCHAR(70), \t\"BillingCity\" NVARCHAR(40), \t\"BillingState\" NVARCHAR(40), \t\"BillingCountry\" NVARCHAR(40), \t\"BillingPostalCode\" NVARCHAR(10), \t\"Total\" NUMERIC(10, 2) NOT NULL, \tPRIMARY KEY (\"InvoiceId\"), \tFOREIGN KEY(\"CustomerId\") REFERENCES \"Customer\" (\"CustomerId\"))/*3 rows from Invoice table:InvoiceId\tCustomerId\tInvoiceDate\tBillingAddress\tBillingCity\tBillingState\tBillingCountry\tBillingPostalCode\tTotal1\t2\t2021-01-01 00:00:00\tTheodor-Heuss-Stra√üe 34\tStuttgart\tNone\tGermany\t70174\t1.982\t4\t2021-01-02 00:00:00\tUllev√•lsveien 14\tOslo\tNone\tNorway\t0171\t3.963\t8\t2021-01-03 00:00:00\tGr√©trystraat 63\tBrussels\tNone\tBelgium\t1000\t5.94*/CREATE TABLE \"InvoiceLine\" (\t\"InvoiceLineId\" INTEGER NOT NULL, \t\"InvoiceId\" INTEGER NOT NULL, \t\"TrackId\" INTEGER NOT NULL, \t\"UnitPrice\" NUMERIC(10, 2) NOT NULL, \t\"Quantity\" INTEGER NOT NULL, \tPRIMARY KEY (\"InvoiceLineId\"), \tFOREIGN KEY(\"TrackId\") REFERENCES \"Track\" (\"TrackId\"), \tFOREIGN KEY(\"InvoiceId\") REFERENCES \"Invoice\" (\"InvoiceId\"))/*3 rows from InvoiceLine table:InvoiceLineId\tInvoiceId\tTrackId\tUnitPrice\tQuantity1\t1\t2\t0.99\t12\t1\t4\t0.99\t13\t2\t6\t0.99\t1*/CREATE TABLE \"MediaType\" (\t\"MediaTypeId\" INTEGER NOT NULL, \t\"Name\" NVARCHAR(120), \tPRIMARY KEY (\"MediaTypeId\"))/*3 rows from MediaType table:MediaTypeId\tName1\tMPEG audio file2\tProtected AAC audio file3\tProtected MPEG-4 video file*/CREATE TABLE \"Playlist\" (\t\"PlaylistId\" INTEGER NOT NULL, \t\"Name\" NVARCHAR(120), \tPRIMARY KEY (\"PlaylistId\"))/*3 rows from Playlist table:PlaylistId\tName1\tMusic2\tMovies3\tTV Shows*/CREATE TABLE \"PlaylistTrack\" (\t\"PlaylistId\" INTEGER NOT NULL, \t\"TrackId\" INTEGER NOT NULL, \tPRIMARY KEY (\"PlaylistId\", \"TrackId\"), \tFOREIGN KEY(\"TrackId\") REFERENCES \"Track\" (\"TrackId\"), \tFOREIGN KEY(\"PlaylistId\") REFERENCES \"Playlist\" (\"PlaylistId\"))/*3 rows from PlaylistTrack table:PlaylistId\tTrackId1\t34021\t33891\t3390*/CREATE TABLE \"Track\" (\t\"TrackId\" INTEGER NOT NULL, \t\"Name\" NVARCHAR(200) NOT NULL, \t\"AlbumId\" INTEGER, \t\"MediaTypeId\" INTEGER NOT NULL, \t\"GenreId\" INTEGER, \t\"Composer\" NVARCHAR(220), \t\"Milliseconds\" INTEGER NOT NULL, \t\"Bytes\" INTEGER, \t\"UnitPrice\" NUMERIC(10, 2) NOT NULL, \tPRIMARY KEY (\"TrackId\"), \tFOREIGN KEY(\"MediaTypeId\") REFERENCES \"MediaType\" (\"MediaTypeId\"), \tFOREIGN KEY(\"GenreId\") REFERENCES \"Genre\" (\"GenreId\"), \tFOREIGN KEY(\"AlbumId\") REFERENCES \"Album\" (\"AlbumId\"))/*3 rows from Track table:TrackId\tName\tAlbumId\tMediaTypeId\tGenreId\tComposer\tMilliseconds\tBytes\tUnitPrice1\tFor Those About To Rock (We Salute You)\t1\t1\t1\tAngus Young, Malcolm Young, Brian Johnson\t343719\t11170334\t0.992\tBalls to the Wall\t2\t2\t1\tU. Dirkschneider, W. Hoffmann, H. Frank, P. Baltes, S. Kaufmann, G. Hoffmann\t342562\t5510424\t0.993\tFast As a Shark\t3\t2\t1\tF. Baltes, S. Kaufman, U. Dirkscneider & W. Hoffman\t230619\t3990994\t0.99*/\n```\n\nWhen we don't have too many, or too wide of, tables, we can just insert the entirety of this information in our prompt:\n\n```\nprompt_with_context = chain.get_prompts()[0].partial(table_info=context[\"table_info\"])print(prompt_with_context.pretty_repr()[:1500])\n```\n\n```\nYou are a SQLite expert. Given an input question, first create a syntactically correct SQLite query to run, then look at the results of the query and return the answer to the input question.Unless the user specifies in the question a specific number of examples to obtain, query for at most 5 results using the LIMIT clause as per SQLite. You can order the results to return the most informative data in the database.Never query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.Pay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.Pay attention to use date('now') function to get the current date, if the question involves \"today\".Use the following format:Question: Question hereSQLQuery: SQL Query to runSQLResult: Result of the SQLQueryAnswer: Final answer hereOnly use the following tables:CREATE TABLE \"Album\" (\t\"AlbumId\" INTEGER NOT NULL, \t\"Title\" NVARCHAR(160) NOT NULL, \t\"ArtistId\" INTEGER NOT NULL, \tPRIMARY KEY (\"AlbumId\"), \tFOREIGN KEY(\"ArtistId\") REFERENCES \"Artist\" (\"ArtistId\"))/*3 rows from Album table:AlbumId\tTitle\tArtistId1\tFor Those About To Rock We Salute You\t12\tBalls to the Wall\t23\tRestless and Wild\t2*/CREATE TABLE \"Artist\" (\t\"ArtistId\" INTEGER NOT NULL, \t\"Name\" NVARCHAR(120)\n```\n\nWhen we do have database schemas that are too large to fit into our model's context window, we'll need to come up with ways of inserting only the relevant table definitions into the prompt based on the user input. For more on this head to the Many tables, wide tables, high-cardinality feature guide.", "url_final": "https://python.langchain.com/docs/how_to/sql_prompting/", "url_citable": "https://python.langchain.com/docs/how_to/sql_prompting/#table-definitions-and-example-rows", "title": "How to better prompt when doing SQL question-answering | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Table definitions and example rows‚Äã", "section_anchor": "table-definitions-and-example-rows", "fetched_at": "2025-10-06T21:56:26.487558", "has_code": true}
{"id": "7cb12f47596b31d3fabb3d8d703d2d1a9429e894", "text": "Few-shot examples‚Äã\n\nIncluding examples of natural language questions being converted to valid SQL queries against our database in the prompt will often improve model performance, especially for complex queries.\n\nLet's say we have the following examples:\n\n```\nexamples = [    {\"input\": \"List all artists.\", \"query\": \"SELECT * FROM Artist;\"},    {        \"input\": \"Find all albums for the artist 'AC/DC'.\",        \"query\": \"SELECT * FROM Album WHERE ArtistId = (SELECT ArtistId FROM Artist WHERE Name = 'AC/DC');\",    },    {        \"input\": \"List all tracks in the 'Rock' genre.\",        \"query\": \"SELECT * FROM Track WHERE GenreId = (SELECT GenreId FROM Genre WHERE Name = 'Rock');\",    },    {        \"input\": \"Find the total duration of all tracks.\",        \"query\": \"SELECT SUM(Milliseconds) FROM Track;\",    },    {        \"input\": \"List all customers from Canada.\",        \"query\": \"SELECT * FROM Customer WHERE Country = 'Canada';\",    },    {        \"input\": \"How many tracks are there in the album with ID 5?\",        \"query\": \"SELECT COUNT(*) FROM Track WHERE AlbumId = 5;\",    },    {        \"input\": \"Find the total number of invoices.\",        \"query\": \"SELECT COUNT(*) FROM Invoice;\",    },    {        \"input\": \"List all tracks that are longer than 5 minutes.\",        \"query\": \"SELECT * FROM Track WHERE Milliseconds > 300000;\",    },    {        \"input\": \"Who are the top 5 customers by total purchase?\",        \"query\": \"SELECT CustomerId, SUM(Total) AS TotalPurchase FROM Invoice GROUP BY CustomerId ORDER BY TotalPurchase DESC LIMIT 5;\",    },    {        \"input\": \"Which albums are from the year 2000?\",        \"query\": \"SELECT * FROM Album WHERE strftime('%Y', ReleaseDate) = '2000';\",    },    {        \"input\": \"How many employees are there\",        \"query\": 'SELECT COUNT(*) FROM \"Employee\"',    },]\n```\n\nWe can create a few-shot prompt with them like so:\n\n```\nfrom langchain_core.prompts import FewShotPromptTemplate, PromptTemplateexample_prompt = PromptTemplate.from_template(\"User input: {input}\\nSQL query: {query}\")prompt = FewShotPromptTemplate(    examples=examples[:5],    example_prompt=example_prompt,    prefix=\"You are a SQLite expert. Given an input question, create a syntactically correct SQLite query to run. Unless otherwise specificed, do not return more than {top_k} rows.\\n\\nHere is the relevant table info: {table_info}\\n\\nBelow are a number of examples of questions and their corresponding SQL queries.\",    suffix=\"User input: {input}\\nSQL query: \",    input_variables=[\"input\", \"top_k\", \"table_info\"],)\n```\n\n```\nprint(prompt.format(input=\"How many artists are there?\", top_k=3, table_info=\"foo\"))\n```\n\n```\nYou are a SQLite expert. Given an input question, create a syntactically correct SQLite query to run. Unless otherwise specificed, do not return more than 3 rows.Here is the relevant table info: fooBelow are a number of examples of questions and their corresponding SQL queries.User input: List all artists.SQL query: SELECT * FROM Artist;User input: Find all albums for the artist 'AC/DC'.SQL query: SELECT * FROM Album WHERE ArtistId = (SELECT ArtistId FROM Artist WHERE Name = 'AC/DC');User input: List all tracks in the 'Rock' genre.SQL query: SELECT * FROM Track WHERE GenreId = (SELECT GenreId FROM Genre WHERE Name = 'Rock');User input: Find the total duration of all tracks.SQL query: SELECT SUM(Milliseconds) FROM Track;User input: List all customers from Canada.SQL query: SELECT * FROM Customer WHERE Country = 'Canada';User input: How many artists are there?SQL query:\n```", "url_final": "https://python.langchain.com/docs/how_to/sql_prompting/", "url_citable": "https://python.langchain.com/docs/how_to/sql_prompting/#few-shot-examples", "title": "How to better prompt when doing SQL question-answering | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Few-shot examples‚Äã", "section_anchor": "few-shot-examples", "fetched_at": "2025-10-06T21:56:26.487558", "has_code": true}
{"id": "f55683fbee7e95433bb4e9ca9e0bb71a06cb2237", "text": "Dynamic few-shot examples‚Äã\n\nIf we have enough examples, we may want to only include the most relevant ones in the prompt, either because they don't fit in the model's context window or because the long tail of examples distracts the model. And specifically, given any input we want to include the examples most relevant to that input.\n\nWe can do just this using an ExampleSelector. In this case we'll use a SemanticSimilarityExampleSelector, which will store the examples in the vector database of our choosing. At runtime it will perform a similarity search between the input and our examples, and return the most semantically similar ones.\n\nWe default to OpenAI embeddings here, but you can swap them out for the model provider of your choice.\n\n```\nfrom langchain_community.vectorstores import FAISSfrom langchain_core.example_selectors import SemanticSimilarityExampleSelectorfrom langchain_openai import OpenAIEmbeddingsexample_selector = SemanticSimilarityExampleSelector.from_examples(    examples,    OpenAIEmbeddings(),    FAISS,    k=5,    input_keys=[\"input\"],)\n```\n\n```\nexample_selector.select_examples({\"input\": \"how many artists are there?\"})\n```\n\n```\n[{'input': 'List all artists.', 'query': 'SELECT * FROM Artist;'}, {'input': 'How many employees are there',  'query': 'SELECT COUNT(*) FROM \"Employee\"'}, {'input': 'How many tracks are there in the album with ID 5?',  'query': 'SELECT COUNT(*) FROM Track WHERE AlbumId = 5;'}, {'input': 'Which albums are from the year 2000?',  'query': \"SELECT * FROM Album WHERE strftime('%Y', ReleaseDate) = '2000';\"}, {'input': \"List all tracks in the 'Rock' genre.\",  'query': \"SELECT * FROM Track WHERE GenreId = (SELECT GenreId FROM Genre WHERE Name = 'Rock');\"}]\n```\n\nTo use it, we can pass the ExampleSelector directly in to our FewShotPromptTemplate:\n\n```\nprompt = FewShotPromptTemplate(    example_selector=example_selector,    example_prompt=example_prompt,    prefix=\"You are a SQLite expert. Given an input question, create a syntactically correct SQLite query to run. Unless otherwise specificed, do not return more than {top_k} rows.\\n\\nHere is the relevant table info: {table_info}\\n\\nBelow are a number of examples of questions and their corresponding SQL queries.\",    suffix=\"User input: {input}\\nSQL query: \",    input_variables=[\"input\", \"top_k\", \"table_info\"],)\n```\n\n```\nprint(prompt.format(input=\"how many artists are there?\", top_k=3, table_info=\"foo\"))\n```\n\n```\nYou are a SQLite expert. Given an input question, create a syntactically correct SQLite query to run. Unless otherwise specificed, do not return more than 3 rows.Here is the relevant table info: fooBelow are a number of examples of questions and their corresponding SQL queries.User input: List all artists.SQL query: SELECT * FROM Artist;User input: How many employees are thereSQL query: SELECT COUNT(*) FROM \"Employee\"User input: How many tracks are there in the album with ID 5?SQL query: SELECT COUNT(*) FROM Track WHERE AlbumId = 5;User input: Which albums are from the year 2000?SQL query: SELECT * FROM Album WHERE strftime('%Y', ReleaseDate) = '2000';User input: List all tracks in the 'Rock' genre.SQL query: SELECT * FROM Track WHERE GenreId = (SELECT GenreId FROM Genre WHERE Name = 'Rock');User input: how many artists are there?SQL query:\n```\n\nTrying it out, we see that the model identifies the relevant table:\n\n```\nchain = create_sql_query_chain(llm, db, prompt)chain.invoke({\"question\": \"how many artists are there?\"})\n```\n\n```\n'SELECT COUNT(*) FROM Artist;'\n```\n\n- Setup\n- Dialect-specific prompting\n- Table definitions and example rows\n- Few-shot examples\n- Dynamic few-shot examples", "url_final": "https://python.langchain.com/docs/how_to/sql_prompting/", "url_citable": "https://python.langchain.com/docs/how_to/sql_prompting/#dynamic-few-shot-examples", "title": "How to better prompt when doing SQL question-answering | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Dynamic few-shot examples‚Äã", "section_anchor": "dynamic-few-shot-examples", "fetched_at": "2025-10-06T21:56:26.487558", "has_code": true}
{"id": "917926707d2fba9bdd94b1df1005851922fd3122", "text": "Runnable Lambdas and Generators‚Äã\n\nThe recommended way to parse is using runnable lambdas and runnable generators!\n\nHere, we will make a simple parse that inverts the case of the output from the model.\n\nFor example, if the model outputs: \"Meow\", the parser will produce \"mEOW\".\n\n```\nfrom typing import Iterablefrom langchain_anthropic.chat_models import ChatAnthropicfrom langchain_core.messages import AIMessage, AIMessageChunkmodel = ChatAnthropic(model_name=\"claude-2.1\")def parse(ai_message: AIMessage) -> str:    \"\"\"Parse the AI message.\"\"\"    return ai_message.content.swapcase()chain = model | parsechain.invoke(\"hello\")\n```\n\n```\n'hELLO!'\n```\n\nLCEL automatically upgrades the function parse to RunnableLambda(parse) when composed using a | syntax.\n\nIf you don't like that you can manually import RunnableLambda and then runparse = RunnableLambda(parse).\n\nDoes streaming work?\n\n```\nfor chunk in chain.stream(\"tell me about yourself in one sentence\"):    print(chunk, end=\"|\", flush=True)\n```\n\n```\ni'M cLAUDE, AN ai ASSISTANT CREATED BY aNTHROPIC TO BE HELPFUL, HARMLESS, AND HONEST.|\n```\n\nNo, it doesn't because the parser aggregates the input before parsing the output.\n\nIf we want to implement a streaming parser, we can have the parser accept an iterable over the input instead and yield the results as they're available.\n\n```\nfrom langchain_core.runnables import RunnableGeneratordef streaming_parse(chunks: Iterable[AIMessageChunk]) -> Iterable[str]:    for chunk in chunks:        yield chunk.content.swapcase()streaming_parse = RunnableGenerator(streaming_parse)\n```\n\nPlease wrap the streaming parser in RunnableGenerator as we may stop automatically upgrading it with the | syntax.\n\n```\nchain = model | streaming_parsechain.invoke(\"hello\")\n```\n\n```\n'hELLO!'\n```\n\nLet's confirm that streaming works!\n\n```\nfor chunk in chain.stream(\"tell me about yourself in one sentence\"):    print(chunk, end=\"|\", flush=True)\n```\n\n```\ni|'M| cLAUDE|,| AN| ai| ASSISTANT| CREATED| BY| aN|THROP|IC| TO| BE| HELPFUL|,| HARMLESS|,| AND| HONEST|.|\n```", "url_final": "https://python.langchain.com/docs/how_to/output_parser_custom/", "url_citable": "https://python.langchain.com/docs/how_to/output_parser_custom/#runnable-lambdas-and-generators", "title": "How to create a custom Output Parser | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Runnable Lambdas and Generators‚Äã", "section_anchor": "runnable-lambdas-and-generators", "fetched_at": "2025-10-06T21:55:59.382024", "has_code": true}
{"id": "05cec43cc54a7e2de69a2216d95a39a15203c045", "text": "Inheriting from Parsing Base Classes‚Äã\n\nAnother approach to implement a parser is by inheriting from BaseOutputParser, BaseGenerationOutputParser or another one of the base parsers depending on what you need to do.\n\nIn general, we do not recommend this approach for most use cases as it results in more code to write without significant benefits.\n\nThe simplest kind of output parser extends the BaseOutputParser class and must implement the following methods:\n\n- parse: takes the string output from the model and parses it\n- (optional) _type: identifies the name of the parser.\n\nWhen the output from the chat model or LLM is malformed, the can throw an OutputParserException to indicate that parsing fails because of bad input. Using this exception allows code that utilizes the parser to handle the exceptions in a consistent manner.\n\nBecause BaseOutputParser implements the Runnable interface, any custom parser you will create this way will become valid LangChain Runnables and will benefit from automatic async support, batch interface, logging support etc.\n\nSimple Parser‚Äã\n\nHere's a simple parser that can parse a string representation of a boolean (e.g., YES or NO) and convert it into the corresponding boolean type.\n\n```\nfrom langchain_core.exceptions import OutputParserExceptionfrom langchain_core.output_parsers import BaseOutputParser# The [bool] desribes a parameterization of a generic.# It's basically indicating what the return type of parse is# in this case the return type is either True or Falseclass BooleanOutputParser(BaseOutputParser[bool]):    \"\"\"Custom boolean parser.\"\"\"    true_val: str = \"YES\"    false_val: str = \"NO\"    def parse(self, text: str) -> bool:        cleaned_text = text.strip().upper()        if cleaned_text not in (self.true_val.upper(), self.false_val.upper()):            raise OutputParserException(                f\"BooleanOutputParser expected output value to either be \"                f\"{self.true_val} or {self.false_val} (case-insensitive). \"                f\"Received {cleaned_text}.\"            )        return cleaned_text == self.true_val.upper()    @property    def _type(self) -> str:        return \"boolean_output_parser\"\n```\n\n```\nparser = BooleanOutputParser()parser.invoke(\"YES\")\n```\n\n```\nTrue\n```\n\n```\ntry:    parser.invoke(\"MEOW\")except Exception as e:    print(f\"Triggered an exception of type: {type(e)}\")\n```\n\n```\nTriggered an exception of type: <class 'langchain_core.exceptions.OutputParserException'>\n```\n\nLet's test changing the parameterization\n\n```\nparser = BooleanOutputParser(true_val=\"OKAY\")parser.invoke(\"OKAY\")\n```\n\n```\nTrue\n```\n\nLet's confirm that other LCEL methods are present\n\n```\nparser.batch([\"OKAY\", \"NO\"])\n```\n\n```\n[True, False]\n```\n\n```\nawait parser.abatch([\"OKAY\", \"NO\"])\n```\n\n```\n[True, False]\n```\n\n```\nfrom langchain_anthropic.chat_models import ChatAnthropicanthropic = ChatAnthropic(model_name=\"claude-2.1\")anthropic.invoke(\"say OKAY or NO\")\n```\n\n```\nAIMessage(content='OKAY')\n```\n\nLet's test that our parser works!\n\n```\nchain = anthropic | parserchain.invoke(\"say OKAY or NO\")\n```\n\n```\nTrue\n```\n\nThe parser will work with either the output from an LLM (a string) or the output from a chat model (an AIMessage)!", "url_final": "https://python.langchain.com/docs/how_to/output_parser_custom/", "url_citable": "https://python.langchain.com/docs/how_to/output_parser_custom/#inheriting-from-parsing-base-classes", "title": "How to create a custom Output Parser | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Inheriting from Parsing Base Classes‚Äã", "section_anchor": "inheriting-from-parsing-base-classes", "fetched_at": "2025-10-06T21:55:59.382024", "has_code": true}
{"id": "3d8281e6188fee2fa751aa709c9144fa15d75a7d", "text": "Parsing Raw Model Outputs‚Äã\n\nSometimes there is additional metadata on the model output that is important besides the raw text. One example of this is tool calling, where arguments intended to be passed to called functions are returned in a separate property. If you need this finer-grained control, you can instead subclass the BaseGenerationOutputParser class.\n\nThis class requires a single method parse_result. This method takes raw model output (e.g., list of Generation or ChatGeneration) and returns the parsed output.\n\nSupporting both Generation and ChatGeneration allows the parser to work with both regular LLMs as well as with Chat Models.\n\n```\nfrom typing import Listfrom langchain_core.exceptions import OutputParserExceptionfrom langchain_core.messages import AIMessagefrom langchain_core.output_parsers import BaseGenerationOutputParserfrom langchain_core.outputs import ChatGeneration, Generationclass StrInvertCase(BaseGenerationOutputParser[str]):    \"\"\"An example parser that inverts the case of the characters in the message.    This is an example parse shown just for demonstration purposes and to keep    the example as simple as possible.    \"\"\"    def parse_result(self, result: List[Generation], *, partial: bool = False) -> str:        \"\"\"Parse a list of model Generations into a specific format.        Args:            result: A list of Generations to be parsed. The Generations are assumed                to be different candidate outputs for a single model input.                Many parsers assume that only a single generation is passed it in.                We will assert for that            partial: Whether to allow partial results. This is used for parsers                     that support streaming        \"\"\"        if len(result) != 1:            raise NotImplementedError(                \"This output parser can only be used with a single generation.\"            )        generation = result[0]        if not isinstance(generation, ChatGeneration):            # Say that this one only works with chat generations            raise OutputParserException(                \"This output parser can only be used with a chat generation.\"            )        return generation.message.content.swapcase()chain = anthropic | StrInvertCase()\n```\n\nLet's the new parser! It should be inverting the output from the model.\n\n```\nchain.invoke(\"Tell me a short sentence about yourself\")\n```\n\n```\n'hELLO! mY NAME IS cLAUDE.'\n```\n\n- Simple Parser\n- Parsing Raw Model Outputs", "url_final": "https://python.langchain.com/docs/how_to/output_parser_custom/", "url_citable": "https://python.langchain.com/docs/how_to/output_parser_custom/#parsing-raw-model-outputs", "title": "How to create a custom Output Parser | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Parsing Raw Model Outputs‚Äã", "section_anchor": "parsing-raw-model-outputs", "fetched_at": "2025-10-06T21:55:59.382024", "has_code": true}
{"id": "2ba86990b143e9281d84aa4b15e6100879f79cc2", "text": "Next steps‚Äã\n\nYou've now learned how to create your own custom callback handlers.\n\nNext, check out the other how-to guides in this section, such as how to attach callbacks to a runnable.\n\n- Next steps", "url_final": "https://python.langchain.com/docs/how_to/custom_callbacks/", "url_citable": "https://python.langchain.com/docs/how_to/custom_callbacks/#next-steps", "title": "How to create custom callback handlers | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Next steps‚Äã", "section_anchor": "next-steps", "fetched_at": "2025-10-06T21:55:12.103077", "has_code": false}
{"id": "4a310fb3b8c8e870cb00208e8aceb584cdc0b47d", "text": "Setup‚Äã\n\nYou'll need to install a few packages, and have your OpenAI API key set as an environment variable named OPENAI_API_KEY:\n\n```\n%pip install -qU langchain langchain-openai langchain-chroma beautifulsoup4# Set env var OPENAI_API_KEY or load from a .env file:import dotenvdotenv.load_dotenv()\n```\n\n```\n\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.3.2 is available.You should consider upgrading via the '/Users/jacoblee/.pyenv/versions/3.10.5/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\u001b[0mNote: you may need to restart the kernel to use updated packages.\n```\n\n```\nTrue\n```\n\nLet's also set up a chat model that we'll use for the below examples.\n\n```\nfrom langchain_openai import ChatOpenAIchat = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n```", "url_final": "https://python.langchain.com/docs/how_to/chatbots_retrieval/", "url_citable": "https://python.langchain.com/docs/how_to/chatbots_retrieval/#setup", "title": "How to add retrieval to chatbots | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Setup‚Äã", "section_anchor": "setup", "fetched_at": "2025-10-06T21:54:24.928044", "has_code": true}
{"id": "e3a039c90106d7ae72272e8b346a31d18a091e14", "text": "Creating a retriever‚Äã\n\nWe'll use the LangSmith documentation as source material and store the content in a vector store for later retrieval. Note that this example will gloss over some of the specifics around parsing and storing a data source - you can see more in-depth documentation on creating retrieval systems here.\n\nLet's use a document loader to pull text from the docs:\n\n```\nfrom langchain_community.document_loaders import WebBaseLoaderloader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")data = loader.load()\n```\n\nNext, we split it into smaller chunks that the LLM's context window can handle and store it in a vector database:\n\n```\nfrom langchain_text_splitters import RecursiveCharacterTextSplittertext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)all_splits = text_splitter.split_documents(data)\n```\n\nThen we embed and store those chunks in a vector database:\n\n```\nfrom langchain_chroma import Chromafrom langchain_openai import OpenAIEmbeddingsvectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())\n```\n\nAnd finally, let's create a retriever from our initialized vectorstore:\n\n```\n# k is the number of chunks to retrieveretriever = vectorstore.as_retriever(k=4)docs = retriever.invoke(\"Can LangSmith help test my LLM applications?\")docs\n```\n\n```\n[Document(page_content='Skip to main contentü¶úÔ∏èüõ†Ô∏è LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ü¶úÔ∏èüõ†Ô∏è LangSmith'}), Document(page_content='LangSmith Overview and User Guide | ü¶úÔ∏èüõ†Ô∏è LangSmith', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ü¶úÔ∏èüõ†Ô∏è LangSmith'}), Document(page_content='You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ü¶úÔ∏èüõ†Ô∏è LangSmith'}), Document(page_content=\"does that affect the output?\\u200bSo you notice a bad output, and you go into LangSmith to see what's going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this\", metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ü¶úÔ∏èüõ†Ô∏è LangSmith'})]\n```\n\nWe can see that invoking the retriever above results in some parts of the LangSmith docs that contain information about testing that our chatbot can use as context when answering questions. And now we've got a retriever that can return related data from the LangSmith docs!", "url_final": "https://python.langchain.com/docs/how_to/chatbots_retrieval/", "url_citable": "https://python.langchain.com/docs/how_to/chatbots_retrieval/#creating-a-retriever", "title": "How to add retrieval to chatbots | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Creating a retriever‚Äã", "section_anchor": "creating-a-retriever", "fetched_at": "2025-10-06T21:54:24.928044", "has_code": true}
{"id": "1b81aab2db71e9dc4aca1442a6c8a5ea12401db3", "text": "Document chains‚Äã\n\nNow that we have a retriever that can return LangChain docs, let's create a chain that can use them as context to answer questions. We'll use a create_stuff_documents_chain helper function to \"stuff\" all of the input documents into the prompt. It will also handle formatting the docs as strings.\n\nIn addition to a chat model, the function also expects a prompt that has a context variables, as well as a placeholder for chat history messages named messages. We'll create an appropriate prompt and pass it as shown below:\n\n```\nfrom langchain.chains.combine_documents import create_stuff_documents_chainfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholderSYSTEM_TEMPLATE = \"\"\"Answer the user's questions based on the below context. If the context doesn't contain any relevant information to the question, don't make something up and just say \"I don't know\":<context>{context}</context>\"\"\"question_answering_prompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            SYSTEM_TEMPLATE,        ),        MessagesPlaceholder(variable_name=\"messages\"),    ])document_chain = create_stuff_documents_chain(chat, question_answering_prompt)\n```\n\nWe can invoke this document_chain by itself to answer questions. Let's use the docs we retrieved above and the same question, how can langsmith help with testing?:\n\n```\nfrom langchain_core.messages import HumanMessagedocument_chain.invoke(    {        \"context\": docs,        \"messages\": [            HumanMessage(content=\"Can LangSmith help test my LLM applications?\")        ],    })\n```\n\n```\n'Yes, LangSmith can help test and evaluate your LLM applications. It simplifies the initial setup, and you can use it to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.'\n```\n\nLooks good! For comparison, we can try it with no context docs and compare the result:\n\n```\ndocument_chain.invoke(    {        \"context\": [],        \"messages\": [            HumanMessage(content=\"Can LangSmith help test my LLM applications?\")        ],    })\n```\n\n```\n\"I don't know about LangSmith's specific capabilities for testing LLM applications. It's best to reach out to LangSmith directly to inquire about their services and how they can assist with testing your LLM applications.\"\n```\n\nWe can see that the LLM does not return any results.", "url_final": "https://python.langchain.com/docs/how_to/chatbots_retrieval/", "url_citable": "https://python.langchain.com/docs/how_to/chatbots_retrieval/#document-chains", "title": "How to add retrieval to chatbots | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Document chains‚Äã", "section_anchor": "document-chains", "fetched_at": "2025-10-06T21:54:24.928044", "has_code": true}
{"id": "cdd650654754d53b53d1c719ec85c0ae1275022b", "text": "Retrieval chains‚Äã\n\nLet's combine this document chain with the retriever. Here's one way this can look:\n\n```\nfrom typing import Dictfrom langchain_core.runnables import RunnablePassthroughdef parse_retriever_input(params: Dict):    return params[\"messages\"][-1].contentretrieval_chain = RunnablePassthrough.assign(    context=parse_retriever_input | retriever,).assign(    answer=document_chain,)\n```\n\nGiven a list of input messages, we extract the content of the last message in the list and pass that to the retriever to fetch some documents. Then, we pass those documents as context to our document chain to generate a final response.\n\nInvoking this chain combines both steps outlined above:\n\n```\nretrieval_chain.invoke(    {        \"messages\": [            HumanMessage(content=\"Can LangSmith help test my LLM applications?\")        ],    })\n```\n\n```\n{'messages': [HumanMessage(content='Can LangSmith help test my LLM applications?')], 'context': [Document(page_content='Skip to main contentü¶úÔ∏èüõ†Ô∏è LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ü¶úÔ∏èüõ†Ô∏è LangSmith'}),  Document(page_content='LangSmith Overview and User Guide | ü¶úÔ∏èüõ†Ô∏è LangSmith', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ü¶úÔ∏èüõ†Ô∏è LangSmith'}),  Document(page_content='You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ü¶úÔ∏èüõ†Ô∏è LangSmith'}),  Document(page_content=\"does that affect the output?\\u200bSo you notice a bad output, and you go into LangSmith to see what's going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this\", metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ü¶úÔ∏èüõ†Ô∏è LangSmith'})], 'answer': 'Yes, LangSmith can help test and evaluate your LLM applications. It simplifies the initial setup, and you can use it to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.'}\n```\n\nLooks good!", "url_final": "https://python.langchain.com/docs/how_to/chatbots_retrieval/", "url_citable": "https://python.langchain.com/docs/how_to/chatbots_retrieval/#retrieval-chains", "title": "How to add retrieval to chatbots | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Retrieval chains‚Äã", "section_anchor": "retrieval-chains", "fetched_at": "2025-10-06T21:54:24.928044", "has_code": true}
{"id": "dfe4009c90e2a72ef953252453e8a2353018c5b6", "text": "Query transformation‚Äã\n\nOur retrieval chain is capable of answering questions about LangSmith, but there's a problem - chatbots interact with users conversationally, and therefore have to deal with followup questions.\n\nThe chain in its current form will struggle with this. Consider a followup question to our original question like Tell me more!. If we invoke our retriever with that query directly, we get documents irrelevant to LLM application testing:\n\n```\nretriever.invoke(\"Tell me more!\")\n```\n\n```\n[Document(page_content='You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ü¶úÔ∏èüõ†Ô∏è LangSmith'}), Document(page_content='playground. Here, you can modify the prompt and re-run it to observe the resulting changes to the output - as many times as needed!Currently, this feature supports only OpenAI and Anthropic models and works for LLM and Chat Model calls. We plan to extend its functionality to more LLM types, chains, agents, and retrievers in the future.What is the exact sequence of events?\\u200bIn complicated chains and agents, it can often be hard to understand what is going on under the hood. What calls are being', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ü¶úÔ∏èüõ†Ô∏è LangSmith'}), Document(page_content='however, there is still no complete substitute for human review to get the utmost quality and reliability from your application.', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ü¶úÔ∏èüõ†Ô∏è LangSmith'}), Document(page_content='Skip to main contentü¶úÔ∏èüõ†Ô∏è LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ü¶úÔ∏èüõ†Ô∏è LangSmith'})]\n```\n\nThis is because the retriever has no innate concept of state, and will only pull documents most similar to the query given. To solve this, we can transform the query into a standalone query without any external references an LLM.\n\nHere's an example:\n\n```\nfrom langchain_core.messages import AIMessage, HumanMessagequery_transform_prompt = ChatPromptTemplate.from_messages(    [        MessagesPlaceholder(variable_name=\"messages\"),        (            \"user\",            \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation. Only respond with the query, nothing else.\",        ),    ])query_transformation_chain = query_transform_prompt | chatquery_transformation_chain.invoke(    {        \"messages\": [            HumanMessage(content=\"Can LangSmith help test my LLM applications?\"),            AIMessage(                content=\"Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.\"            ),            HumanMessage(content=\"Tell me more!\"),        ],    })\n```\n\n```\nAIMessage(content='\"LangSmith LLM application testing and evaluation\"')\n```\n\nAwesome! That transformed query would pull up context documents related to LLM application testing.\n\nLet's add this to our retrieval chain. We can wrap our retriever as follows:\n\n```\nfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.runnables import RunnableBranchquery_transforming_retriever_chain = RunnableBranch(    (        lambda x: len(x.get(\"messages\", [])) == 1,        # If only one message, then we just pass that message's content to retriever        (lambda x: x[\"messages\"][-1].content) | retriever,    ),    # If messages, then we pass inputs to LLM chain to transform the query, then pass to retriever    query_transform_prompt | chat | StrOutputParser() | retriever,).with_config(run_name=\"chat_retriever_chain\")\n```\n\nThen, we can use this query transformation chain to make our retrieval chain better able to handle such followup questions:\n\n```\nSYSTEM_TEMPLATE = \"\"\"Answer the user's questions based on the below context. If the context doesn't contain any relevant information to the question, don't make something up and just say \"I don't know\":<context>{context}</context>\"\"\"question_answering_prompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            SYSTEM_TEMPLATE,        ),        MessagesPlaceholder(variable_name=\"messages\"),    ])document_chain = create_stuff_documents_chain(chat, question_answering_prompt)conversational_retrieval_chain = RunnablePassthrough.assign(    context=query_transforming_retriever_chain,).assign(    answer=document_chain,)\n```\n\nAwesome! Let's invoke this new chain with the same inputs as earlier:\n\n```\nconversational_retrieval_chain.invoke(    {        \"messages\": [            HumanMessage(content=\"Can LangSmith help test my LLM applications?\"),        ]    })\n```\n\n```\n{'messages': [HumanMessage(content='Can LangSmith help test my LLM applications?')], 'context': [Document(page_content='Skip to main contentü¶úÔ∏èüõ†Ô∏è LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ü¶úÔ∏èüõ†Ô∏è LangSmith'}),  Document(page_content='LangSmith Overview and User Guide | ü¶úÔ∏èüõ†Ô∏è LangSmith', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ü¶úÔ∏èüõ†Ô∏è LangSmith'}),  Document(page_content='You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ü¶úÔ∏èüõ†Ô∏è LangSmith'}),  Document(page_content=\"does that affect the output?\\u200bSo you notice a bad output, and you go into LangSmith to see what's going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this\", metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ü¶úÔ∏èüõ†Ô∏è LangSmith'})], 'answer': 'Yes, LangSmith can help test and evaluate LLM (Language Model) applications. It simplifies the initial setup, and you can use it to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.'}\n```\n\n```\nconversational_retrieval_chain.invoke(    {        \"messages\": [            HumanMessage(content=\"Can LangSmith help test my LLM applications?\"),            AIMessage(                content=\"Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.\"            ),            HumanMessage(content=\"Tell me more!\"),        ],    })\n```\n\n```\n{'messages': [HumanMessage(content='Can LangSmith help test my LLM applications?'),  AIMessage(content='Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.'),  HumanMessage(content='Tell me more!')], 'context': [Document(page_content='LangSmith Overview and User Guide | ü¶úÔ∏èüõ†Ô∏è LangSmith', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ü¶úÔ∏èüõ†Ô∏è LangSmith'}),  Document(page_content='You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ü¶úÔ∏èüõ†Ô∏è LangSmith'}),  Document(page_content='Skip to main contentü¶úÔ∏èüõ†Ô∏è LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ü¶úÔ∏èüõ†Ô∏è LangSmith'}),  Document(page_content='LangSmith makes it easy to manually review and annotate runs through annotation queues.These queues allow you to select any runs based on criteria like model type or automatic evaluation scores, and queue them up for human review. As a reviewer, you can then quickly step through the runs, viewing the input, output, and any existing tags before adding your own feedback.We often use this for a couple of reasons:To assess subjective qualities that automatic evaluators struggle with, like', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ü¶úÔ∏èüõ†Ô∏è LangSmith'})], 'answer': 'LangSmith simplifies the initial setup for building reliable LLM applications, but it acknowledges that there is still work needed to bring the performance of prompts, chains, and agents up to the level where they are reliable enough to be used in production. It also provides the capability to manually review and annotate runs through annotation queues, allowing you to select runs based on criteria like model type or automatic evaluation scores for human review. This feature is particularly useful for assessing subjective qualities that automatic evaluators struggle with.'}\n```\n\nYou can check out this LangSmith trace to see the internal query transformation step for yourself.", "url_final": "https://python.langchain.com/docs/how_to/chatbots_retrieval/", "url_citable": "https://python.langchain.com/docs/how_to/chatbots_retrieval/#query-transformation", "title": "How to add retrieval to chatbots | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Query transformation‚Äã", "section_anchor": "query-transformation", "fetched_at": "2025-10-06T21:54:24.928044", "has_code": true}
{"id": "673c00fe83729cc49acfdd22e7104e26d9101a6e", "text": "Streaming‚Äã\n\nBecause this chain is constructed with LCEL, you can use familiar methods like .stream() with it:\n\n```\nstream = conversational_retrieval_chain.stream(    {        \"messages\": [            HumanMessage(content=\"Can LangSmith help test my LLM applications?\"),            AIMessage(                content=\"Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.\"            ),            HumanMessage(content=\"Tell me more!\"),        ],    })for chunk in stream:    print(chunk)\n```\n\n```\n{'messages': [HumanMessage(content='Can LangSmith help test my LLM applications?'), AIMessage(content='Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.'), HumanMessage(content='Tell me more!')]}{'context': [Document(page_content='LangSmith Overview and User Guide | ü¶úÔ∏èüõ†Ô∏è LangSmith', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ü¶úÔ∏èüõ†Ô∏è LangSmith'}), Document(page_content='You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ü¶úÔ∏èüõ†Ô∏è LangSmith'}), Document(page_content='Skip to main contentü¶úÔ∏èüõ†Ô∏è LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ü¶úÔ∏èüõ†Ô∏è LangSmith'}), Document(page_content='LangSmith makes it easy to manually review and annotate runs through annotation queues.These queues allow you to select any runs based on criteria like model type or automatic evaluation scores, and queue them up for human review. As a reviewer, you can then quickly step through the runs, viewing the input, output, and any existing tags before adding your own feedback.We often use this for a couple of reasons:To assess subjective qualities that automatic evaluators struggle with, like', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ü¶úÔ∏èüõ†Ô∏è LangSmith'})]}{'answer': ''}{'answer': 'Lang'}{'answer': 'Smith'}{'answer': ' simpl'}{'answer': 'ifies'}{'answer': ' the'}{'answer': ' initial'}{'answer': ' setup'}{'answer': ' for'}{'answer': ' building'}{'answer': ' reliable'}{'answer': ' L'}{'answer': 'LM'}{'answer': ' applications'}{'answer': '.'}{'answer': ' It'}{'answer': ' provides'}{'answer': ' features'}{'answer': ' for'}{'answer': ' manually'}{'answer': ' reviewing'}{'answer': ' and'}{'answer': ' annot'}{'answer': 'ating'}{'answer': ' runs'}{'answer': ' through'}{'answer': ' annotation'}{'answer': ' queues'}{'answer': ','}{'answer': ' allowing'}{'answer': ' you'}{'answer': ' to'}{'answer': ' select'}{'answer': ' runs'}{'answer': ' based'}{'answer': ' on'}{'answer': ' criteria'}{'answer': ' like'}{'answer': ' model'}{'answer': ' type'}{'answer': ' or'}{'answer': ' automatic'}{'answer': ' evaluation'}{'answer': ' scores'}{'answer': ','}{'answer': ' and'}{'answer': ' queue'}{'answer': ' them'}{'answer': ' up'}{'answer': ' for'}{'answer': ' human'}{'answer': ' review'}{'answer': '.'}{'answer': ' As'}{'answer': ' a'}{'answer': ' reviewer'}{'answer': ','}{'answer': ' you'}{'answer': ' can'}{'answer': ' quickly'}{'answer': ' step'}{'answer': ' through'}{'answer': ' the'}{'answer': ' runs'}{'answer': ','}{'answer': ' view'}{'answer': ' the'}{'answer': ' input'}{'answer': ','}{'answer': ' output'}{'answer': ','}{'answer': ' and'}{'answer': ' any'}{'answer': ' existing'}{'answer': ' tags'}{'answer': ' before'}{'answer': ' adding'}{'answer': ' your'}{'answer': ' own'}{'answer': ' feedback'}{'answer': '.'}{'answer': ' This'}{'answer': ' can'}{'answer': ' be'}{'answer': ' particularly'}{'answer': ' useful'}{'answer': ' for'}{'answer': ' assessing'}{'answer': ' subjective'}{'answer': ' qualities'}{'answer': ' that'}{'answer': ' automatic'}{'answer': ' evalu'}{'answer': 'ators'}{'answer': ' struggle'}{'answer': ' with'}{'answer': '.'}{'answer': ''}\n```", "url_final": "https://python.langchain.com/docs/how_to/chatbots_retrieval/", "url_citable": "https://python.langchain.com/docs/how_to/chatbots_retrieval/#streaming", "title": "How to add retrieval to chatbots | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Streaming‚Äã", "section_anchor": "streaming", "fetched_at": "2025-10-06T21:54:24.928044", "has_code": true}
{"id": "13a534599338b7274ba98ff691290c9728e504ff", "text": "Further reading‚Äã\n\nThis guide only scratches the surface of retrieval techniques. For more on different ways of ingesting, preparing, and retrieving the most relevant data, check out the relevant how-to guides here.\n\n- Setup\n- Creating a retriever\n- Document chains\n- Retrieval chains\n- Query transformation\n- Streaming\n- Further reading", "url_final": "https://python.langchain.com/docs/how_to/chatbots_retrieval/", "url_citable": "https://python.langchain.com/docs/how_to/chatbots_retrieval/#further-reading", "title": "How to add retrieval to chatbots | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Further reading‚Äã", "section_anchor": "further-reading", "fetched_at": "2025-10-06T21:54:24.928044", "has_code": false}
{"id": "9256cdee5dfe2e3bd0527939e51d290141c74515", "text": "Concepts‚Äã\n\nConcepts we will cover are:\n\n- Stuff, which simply concatenates documents into a prompt;\n- Map-reduce, for larger sets of documents. This splits documents into batches, summarizes those, and then summarizes the summaries.\n\nShorter, targeted guides on these strategies and others, including iterative refinement, can be found in the how-to guides.", "url_final": "https://python.langchain.com/docs/tutorials/summarization/", "url_citable": "https://python.langchain.com/docs/tutorials/summarization/#concepts", "title": "Summarize Text | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Concepts‚Äã", "section_anchor": "concepts", "fetched_at": "2025-10-06T21:54:14.442530", "has_code": false}
{"id": "f05725c5e319d4bdcd994d22465142933ca63d17", "text": "Setup‚Äã\n\nJupyter Notebook‚Äã\n\nThis guide (and most of the other guides in the documentation) uses Jupyter notebooks and assumes the reader is as well. Jupyter notebooks are perfect for learning how to work with LLM systems because oftentimes things can go wrong (unexpected output, API down, etc) and going through guides in an interactive environment is a great way to better understand them.\n\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebook. See here for instructions on how to install.\n\nInstallation‚Äã\n\nTo install LangChain run:\n\n- Pip\n- Conda\n\n```\npip install langchain\n```\n\n```\nconda install langchain -c conda-forge\n```\n\nFor more details, see our Installation guide.\n\nLangSmith‚Äã\n\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\n\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\n\n```\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"\n```\n\nOr, if in a notebook, you can set them with:\n\n```\nimport getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n```", "url_final": "https://python.langchain.com/docs/tutorials/summarization/", "url_citable": "https://python.langchain.com/docs/tutorials/summarization/#setup", "title": "Summarize Text | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Setup‚Äã", "section_anchor": "setup", "fetched_at": "2025-10-06T21:54:14.442530", "has_code": true}
{"id": "501d5a95b1108172b5c6e08b4c220139695d3670", "text": "Overview‚Äã\n\nA central question for building a summarizer is how to pass your documents into the LLM's context window. Two common approaches for this are:\n\n- Stuff: Simply \"stuff\" all your documents into a single prompt. This is the simplest approach (see here for more on the create_stuff_documents_chain constructor, which is used for this method).\n- Map-reduce: Summarize each document on its own in a \"map\" step and then \"reduce\" the summaries into a final summary (see here for more on the MapReduceDocumentsChain, which is used for this method).\n\nNote that map-reduce is especially effective when understanding of a sub-document does not rely on preceding context. For example, when summarizing a corpus of many, shorter documents. In other cases, such as summarizing a novel or body of text with an inherent sequence, iterative refinement may be more effective.", "url_final": "https://python.langchain.com/docs/tutorials/summarization/", "url_citable": "https://python.langchain.com/docs/tutorials/summarization/#overview", "title": "Summarize Text | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Overview‚Äã", "section_anchor": "overview", "fetched_at": "2025-10-06T21:54:14.442530", "has_code": false}
{"id": "3254592b598fe373418ae1892272a92dba4bf892", "text": "Setup‚Äã\n\nFirst set environment variables and install packages:\n\n```\n%pip install --upgrade --quiet tiktoken langchain langgraph beautifulsoup4 langchain-community# Set env var OPENAI_API_KEY or load from a .env file# import dotenv# dotenv.load_dotenv()\n```\n\n```\nimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"\n```\n\nFirst we load in our documents. We will use WebBaseLoader to load a blog post:\n\n```\nfrom langchain_community.document_loaders import WebBaseLoaderloader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")docs = loader.load()\n```\n\nLet's next select a LLM:\n\n```\npip install -qU \"langchain[google-genai]\"\n```\n\n```\nimport getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n```", "url_final": "https://python.langchain.com/docs/tutorials/summarization/", "url_citable": "https://python.langchain.com/docs/tutorials/summarization/#setup-1", "title": "Summarize Text | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Setup‚Äã", "section_anchor": "setup-1", "fetched_at": "2025-10-06T21:54:14.442530", "has_code": true}
{"id": "54a4a0f7721ea26a0a40e32d654beb4641f3afa0", "text": "Stuff: summarize in a single LLM call‚Äã\n\nWe can use create_stuff_documents_chain, especially if using larger context window models such as:\n\n- 128k token OpenAI gpt-4o\n- 200k token Anthropic claude-3-5-sonnet-latest\n\nThe chain will take a list of documents, insert them all into a prompt, and pass that prompt to an LLM:\n\n```\nfrom langchain.chains.combine_documents import create_stuff_documents_chainfrom langchain.chains.llm import LLMChainfrom langchain_core.prompts import ChatPromptTemplate# Define promptprompt = ChatPromptTemplate.from_messages(    [(\"system\", \"Write a concise summary of the following:\\\\n\\\\n{context}\")])# Instantiate chainchain = create_stuff_documents_chain(llm, prompt)# Invoke chainresult = chain.invoke({\"context\": docs})print(result)\n```\n\n```\nThe article \"LLM Powered Autonomous Agents\" by Lilian Weng discusses the development and capabilities of autonomous agents powered by large language models (LLMs). It outlines a system architecture that includes three main components: Planning, Memory, and Tool Use. 1. **Planning** involves task decomposition, where complex tasks are broken down into manageable subgoals, and self-reflection, allowing agents to learn from past actions to improve future performance. Techniques like Chain of Thought (CoT) and Tree of Thoughts (ToT) are highlighted for enhancing reasoning and planning.2. **Memory** is categorized into short-term and long-term memory, with mechanisms for fast retrieval using Maximum Inner Product Search (MIPS) algorithms. This allows agents to retain and recall information effectively.3. **Tool Use** enables agents to interact with external APIs and tools, enhancing their capabilities beyond the limitations of their training data. Examples include MRKL systems and frameworks like HuggingGPT, which facilitate task planning and execution.The article also addresses challenges such as finite context length, difficulties in long-term planning, and the reliability of natural language interfaces. It concludes with case studies demonstrating the practical applications of these concepts in scientific discovery and interactive simulations. Overall, the article emphasizes the potential of LLMs as powerful problem solvers in autonomous agent systems.\n```", "url_final": "https://python.langchain.com/docs/tutorials/summarization/", "url_citable": "https://python.langchain.com/docs/tutorials/summarization/#stuff", "title": "Summarize Text | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Stuff: summarize in a single LLM call‚Äã", "section_anchor": "stuff", "fetched_at": "2025-10-06T21:54:14.442530", "has_code": true}
{"id": "a2a595f6018f94f8b2b8a30362fbf1b45844e436", "text": "Streaming‚Äã\n\nNote that we can also stream the result token-by-token:\n\n```\nfor token in chain.stream({\"context\": docs}):    print(token, end=\"|\")\n```\n\n```\n|The| article| \"|LL|M| Powered| Autonomous| Agents|\"| by| Lil|ian| W|eng| discusses| the| development| and| capabilities| of| autonomous| agents| powered| by| large| language| models| (|LL|Ms|).| It| outlines| a| system| architecture| that| includes| three| main| components|:| Planning|,| Memory|,| and| Tool| Use|.| |1|.| **|Planning|**| involves| task| decomposition|,| where| complex| tasks| are| broken| down| into| manageable| sub|go|als|,| and| self|-ref|lection|,| allowing| agents| to| learn| from| past| actions| to| improve| future| performance|.| Techniques| like| Chain| of| Thought| (|Co|T|)| and| Tree| of| Thoughts| (|To|T|)| are| highlighted| for| enhancing| reasoning| and| planning|.|2|.| **|Memory|**| is| categorized| into| short|-term| and| long|-term| memory|,| with| mechanisms| for| fast| retrieval| using| Maximum| Inner| Product| Search| (|M|IPS|)| algorithms|.| This| allows| agents| to| retain| and| recall| information| effectively|.|3|.| **|Tool| Use|**| emphasizes| the| integration| of| external| APIs| and| tools| to| extend| the| capabilities| of| L|LM|s|,| enabling| them| to| perform| tasks| beyond| their| inherent| limitations|.| Examples| include| MR|KL| systems| and| frameworks| like| Hug|ging|GPT|,| which| facilitate| task| planning| and| execution|.|The| article| also| addresses| challenges| such| as| finite| context| length|,| difficulties| in| long|-term| planning|,| and| the| reliability| of| natural| language| interfaces|.| It| concludes| with| case| studies| demonstrating| the| practical| applications| of| L|LM|-powered| agents| in| scientific| discovery| and| interactive| simulations|.| Overall|,| the| piece| illustrates| the| potential| of| L|LM|s| as| general| problem| sol|vers| and| their| evolving| role| in| autonomous| systems|.||\n```", "url_final": "https://python.langchain.com/docs/tutorials/summarization/", "url_citable": "https://python.langchain.com/docs/tutorials/summarization/#streaming", "title": "Summarize Text | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Streaming‚Äã", "section_anchor": "streaming", "fetched_at": "2025-10-06T21:54:14.442530", "has_code": true}
{"id": "e05ec3b3a2e2d5a5642c601a95729f012f4f4254", "text": "Go deeper‚Äã\n\n- You can easily customize the prompt.\n- You can easily try different LLMs, (e.g., Claude) via the llm parameter.", "url_final": "https://python.langchain.com/docs/tutorials/summarization/", "url_citable": "https://python.langchain.com/docs/tutorials/summarization/#go-deeper", "title": "Summarize Text | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Go deeper‚Äã", "section_anchor": "go-deeper", "fetched_at": "2025-10-06T21:54:14.442530", "has_code": false}
{"id": "989c4f5808e188640395eba4f59014135ba898a5", "text": "Map-Reduce: summarize long texts via parallelization‚Äã\n\nLet's unpack the map reduce approach. For this, we'll first map each document to an individual summary using an LLM. Then we'll reduce or consolidate those summaries into a single global summary.\n\nNote that the map step is typically parallelized over the input documents.\n\nLangGraph, built on top of langchain-core, supports map-reduce workflows and is well-suited to this problem:\n\n- LangGraph allows for individual steps (such as successive summarizations) to be streamed, allowing for greater control of execution;\n- LangGraph's checkpointing supports error recovery, extending with human-in-the-loop workflows, and easier incorporation into conversational applications.\n- The LangGraph implementation is straightforward to modify and extend, as we will see below.\n\nMap‚Äã\n\nLet's first define the prompt associated with the map step. We can use the same summarization prompt as in the stuff approach, above:\n\n```\nfrom langchain_core.prompts import ChatPromptTemplatemap_prompt = ChatPromptTemplate.from_messages(    [(\"system\", \"Write a concise summary of the following:\\\\n\\\\n{context}\")])\n```\n\nWe can also use the Prompt Hub to store and fetch prompts.\n\nThis will work with your LangSmith API key.\n\nFor example, see the map prompt here.\n\n```\nfrom langchain import hubmap_prompt = hub.pull(\"rlm/map-prompt\")\n```\n\nReduce‚Äã\n\nWe also define a prompt that takes the document mapping results and reduces them into a single output.\n\n```\n# Also available via the hub: `hub.pull(\"rlm/reduce-prompt\")`reduce_template = \"\"\"The following is a set of summaries:{docs}Take these and distill it into a final, consolidated summaryof the main themes.\"\"\"reduce_prompt = ChatPromptTemplate([(\"human\", reduce_template)])\n```", "url_final": "https://python.langchain.com/docs/tutorials/summarization/", "url_citable": "https://python.langchain.com/docs/tutorials/summarization/#map-reduce", "title": "Summarize Text | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Map-Reduce: summarize long texts via parallelization‚Äã", "section_anchor": "map-reduce", "fetched_at": "2025-10-06T21:54:14.442530", "has_code": true}
{"id": "c13e9f7a0331b661d1f31fc507b434fa66e82f02", "text": "Orchestration via LangGraph‚Äã\n\nBelow we implement a simple application that maps the summarization step on a list of documents, then reduces them using the above prompts.\n\nMap-reduce flows are particularly useful when texts are long compared to the context window of a LLM. For long texts, we need a mechanism that ensures that the context to be summarized in the reduce step does not exceed a model's context window size. Here we implement a recursive \"collapsing\" of the summaries: the inputs are partitioned based on a token limit, and summaries are generated of the partitions. This step is repeated until the total length of the summaries is within a desired limit, allowing for the summarization of arbitrary-length text.\n\nFirst we chunk the blog post into smaller \"sub documents\" to be mapped:\n\n```\nfrom langchain_text_splitters import CharacterTextSplittertext_splitter = CharacterTextSplitter.from_tiktoken_encoder(    chunk_size=1000, chunk_overlap=0)split_docs = text_splitter.split_documents(docs)print(f\"Generated {len(split_docs)} documents.\")\n```\n\n```\nCreated a chunk of size 1003, which is longer than the specified 1000``````outputGenerated 14 documents.\n```\n\nNext, we define our graph. Note that we define an artificially low maximum token length of 1,000 tokens to illustrate the \"collapsing\" step.\n\n```\nimport operatorfrom typing import Annotated, List, Literal, TypedDictfrom langchain.chains.combine_documents.reduce import (    acollapse_docs,    split_list_of_docs,)from langchain_core.documents import Documentfrom langgraph.constants import Sendfrom langgraph.graph import END, START, StateGraphtoken_max = 1000def length_function(documents: List[Document]) -> int:    \"\"\"Get number of tokens for input contents.\"\"\"    return sum(llm.get_num_tokens(doc.page_content) for doc in documents)# This will be the overall state of the main graph.# It will contain the input document contents, corresponding# summaries, and a final summary.class OverallState(TypedDict):    # Notice here we use the operator.add    # This is because we want combine all the summaries we generate    # from individual nodes back into one list - this is essentially    # the \"reduce\" part    contents: List[str]    summaries: Annotated[list, operator.add]    collapsed_summaries: List[Document]    final_summary: str# This will be the state of the node that we will \"map\" all# documents to in order to generate summariesclass SummaryState(TypedDict):    content: str# Here we generate a summary, given a documentasync def generate_summary(state: SummaryState):    prompt = map_prompt.invoke(state[\"content\"])    response = await llm.ainvoke(prompt)    return {\"summaries\": [response.content]}# Here we define the logic to map out over the documents# We will use this an edge in the graphdef map_summaries(state: OverallState):    # We will return a list of `Send` objects    # Each `Send` object consists of the name of a node in the graph    # as well as the state to send to that node    return [        Send(\"generate_summary\", {\"content\": content}) for content in state[\"contents\"]    ]def collect_summaries(state: OverallState):    return {        \"collapsed_summaries\": [Document(summary) for summary in state[\"summaries\"]]    }async def _reduce(input: dict) -> str:    prompt = reduce_prompt.invoke(input)    response = await llm.ainvoke(prompt)    return response.content# Add node to collapse summariesasync def collapse_summaries(state: OverallState):    doc_lists = split_list_of_docs(        state[\"collapsed_summaries\"], length_function, token_max    )    results = []    for doc_list in doc_lists:        results.append(await acollapse_docs(doc_list, _reduce))    return {\"collapsed_summaries\": results}# This represents a conditional edge in the graph that determines# if we should collapse the summaries or notdef should_collapse(    state: OverallState,) -> Literal[\"collapse_summaries\", \"generate_final_summary\"]:    num_tokens = length_function(state[\"collapsed_summaries\"])    if num_tokens > token_max:        return \"collapse_summaries\"    else:        return \"generate_final_summary\"# Here we will generate the final summaryasync def generate_final_summary(state: OverallState):    response = await _reduce(state[\"collapsed_summaries\"])    return {\"final_summary\": response}# Construct the graph# Nodes:graph = StateGraph(OverallState)graph.add_node(\"generate_summary\", generate_summary)  # same as beforegraph.add_node(\"collect_summaries\", collect_summaries)graph.add_node(\"collapse_summaries\", collapse_summaries)graph.add_node(\"generate_final_summary\", generate_final_summary)# Edges:graph.add_conditional_edges(START, map_summaries, [\"generate_summary\"])graph.add_edge(\"generate_summary\", \"collect_summaries\")graph.add_conditional_edges(\"collect_summaries\", should_collapse)graph.add_conditional_edges(\"collapse_summaries\", should_collapse)graph.add_edge(\"generate_final_summary\", END)app = graph.compile()\n```\n\nLangGraph allows the graph structure to be plotted to help visualize its function:\n\n```\nfrom IPython.display import ImageImage(app.get_graph().draw_mermaid_png())\n```\n\nWhen running the application, we can stream the graph to observe its sequence of steps. Below, we will simply print out the name of the step.\n\nNote that because we have a loop in the graph, it can be helpful to specify a recursion_limit on its execution. This will raise a specific error when the specified limit is exceeded.\n\n```\nasync for step in app.astream(    {\"contents\": [doc.page_content for doc in split_docs]},    {\"recursion_limit\": 10},):    print(list(step.keys()))\n```\n\n```\n['generate_summary']['generate_summary']['generate_summary']['generate_summary']['generate_summary']['generate_summary']['generate_summary']['generate_summary']['generate_summary']['generate_summary']['generate_summary']['generate_summary']['generate_summary']['generate_summary']['collect_summaries']['collapse_summaries']['collapse_summaries']['generate_final_summary']\n```\n\n```\nprint(step)\n```\n\n```\n{'generate_final_summary': {'final_summary': 'The consolidated summary of the main themes from the provided documents is as follows:\\n\\n1. **Integration of Large Language Models (LLMs) in Autonomous Agents**: The documents explore the evolving role of LLMs in autonomous systems, emphasizing their enhanced reasoning and acting capabilities through methodologies that incorporate structured planning, memory systems, and tool use.\\n\\n2. **Core Components of Autonomous Agents**:\\n   - **Planning**: Techniques like task decomposition (e.g., Chain of Thought) and external classical planners are utilized to facilitate long-term planning by breaking down complex tasks.\\n   - **Memory**: The memory system is divided into short-term (in-context learning) and long-term memory, with parallels drawn between human memory and machine learning to improve agent performance.\\n   - **Tool Use**: Agents utilize external APIs and algorithms to enhance problem-solving abilities, exemplified by frameworks like HuggingGPT that manage task workflows.\\n\\n3. **Neuro-Symbolic Architectures**: The integration of MRKL (Modular Reasoning, Knowledge, and Language) systems combines neural and symbolic expert modules with LLMs, addressing challenges in tasks such as verbal math problem-solving.\\n\\n4. **Specialized Applications**: Case studies, such as ChemCrow and projects in anticancer drug discovery, demonstrate the advantages of LLMs augmented with expert tools in specialized domains.\\n\\n5. **Challenges and Limitations**: The documents highlight challenges such as hallucination in model outputs and the finite context length of LLMs, which affects their ability to incorporate historical information and perform self-reflection. Techniques like Chain of Hindsight and Algorithm Distillation are discussed to enhance model performance through iterative learning.\\n\\n6. **Structured Software Development**: A systematic approach to creating Python software projects is emphasized, focusing on defining core components, managing dependencies, and adhering to best practices for documentation.\\n\\nOverall, the integration of structured planning, memory systems, and advanced tool use aims to enhance the capabilities of LLM-powered autonomous agents while addressing the challenges and limitations these technologies face in real-world applications.'}}\n```\n\nIn the corresponding LangSmith trace we can see the individual LLM calls, grouped under their respective nodes.", "url_final": "https://python.langchain.com/docs/tutorials/summarization/", "url_citable": "https://python.langchain.com/docs/tutorials/summarization/#orchestration-via-langgraph", "title": "Summarize Text | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Orchestration via LangGraph‚Äã", "section_anchor": "orchestration-via-langgraph", "fetched_at": "2025-10-06T21:54:14.442530", "has_code": true}
{"id": "c3bfb8f46093aeec3fa20a3477d408d449f49f81", "text": "Go deeper‚Äã\n\nCustomization\n\n- As shown above, you can customize the LLMs and prompts for map and reduce stages.\n\nReal-world use-case\n\n- See this blog post case-study on analyzing user interactions (questions about LangChain documentation)!\n- The blog post and associated repo also introduce clustering as a means of summarization.\n- This opens up another path beyond the stuff or map-reduce approaches that is worth considering.", "url_final": "https://python.langchain.com/docs/tutorials/summarization/", "url_citable": "https://python.langchain.com/docs/tutorials/summarization/#go-deeper-1", "title": "Summarize Text | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Go deeper‚Äã", "section_anchor": "go-deeper-1", "fetched_at": "2025-10-06T21:54:14.442530", "has_code": false}
{"id": "d943c970d04fc43867fee469971c26824c5c5b35", "text": "Next steps‚Äã\n\nWe encourage you to check out the how-to guides for more detail on:\n\n- Other summarization strategies, such as iterative refinement\n- Built-in document loaders and text-splitters\n- Integrating various combine-document chains into a RAG application\n- Incorporating retrieval into a chatbot\n\nand other concepts.\n\n- Jupyter Notebook\n- Installation\n- LangSmith\n\n- Streaming\n- Go deeper\n\n- Map\n- Reduce\n- Orchestration via LangGraph\n- Go deeper", "url_final": "https://python.langchain.com/docs/tutorials/summarization/", "url_citable": "https://python.langchain.com/docs/tutorials/summarization/#next-steps", "title": "Summarize Text | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Next steps‚Äã", "section_anchor": "next-steps", "fetched_at": "2025-10-06T21:54:14.442530", "has_code": false}
{"id": "91e895e6af20cfdad159d4d9e12d072d8c2d9f44", "text": "Defining tool schemas‚Äã\n\nFor a model to be able to call tools, we need to pass in tool schemas that describe what the tool does and what its arguments are. Chat models that support tool calling features implement a .bind_tools() method for passing tool schemas to the model. Tool schemas can be passed in as Python functions (with typehints and docstrings), Pydantic models, TypedDict classes, or LangChain Tool objects. Subsequent invocations of the model will pass in these tool schemas along with the prompt.\n\nPython functions‚Äã\n\nOur tool schemas can be Python functions:\n\n```\n# The function name, type hints, and docstring are all part of the tool# schema that's passed to the model. Defining good, descriptive schemas# is an extension of prompt engineering and is an important part of# getting models to perform well.def add(a: int, b: int) -> int:    \"\"\"Add two integers.    Args:        a: First integer        b: Second integer    \"\"\"    return a + bdef multiply(a: int, b: int) -> int:    \"\"\"Multiply two integers.    Args:        a: First integer        b: Second integer    \"\"\"    return a * b\n```\n\nLangChain Tool‚Äã\n\nLangChain also implements a @tool decorator that allows for further control of the tool schema, such as tool names and argument descriptions. See the how-to guide here for details.\n\nPydantic class‚Äã\n\nYou can equivalently define the schemas without the accompanying functions using Pydantic.\n\nNote that all fields are required unless provided a default value.\n\n```\nfrom pydantic import BaseModel, Fieldclass add(BaseModel):    \"\"\"Add two integers.\"\"\"    a: int = Field(..., description=\"First integer\")    b: int = Field(..., description=\"Second integer\")class multiply(BaseModel):    \"\"\"Multiply two integers.\"\"\"    a: int = Field(..., description=\"First integer\")    b: int = Field(..., description=\"Second integer\")\n```", "url_final": "https://python.langchain.com/docs/how_to/tool_calling/", "url_citable": "https://python.langchain.com/docs/how_to/tool_calling/#defining-tool-schemas", "title": "How to use chat models to call tools | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Defining tool schemas‚Äã", "section_anchor": "defining-tool-schemas", "fetched_at": "2025-10-06T21:56:33.539439", "has_code": true}
{"id": "0a51298282f81980cfe0aaae9cb4ec2215bbc05a", "text": "TypedDict class‚Äã\n\nOr using TypedDicts and annotations:\n\n```\nfrom typing_extensions import Annotated, TypedDictclass add(TypedDict):    \"\"\"Add two integers.\"\"\"    # Annotations must have the type and can optionally include a default value and description (in that order).    a: Annotated[int, ..., \"First integer\"]    b: Annotated[int, ..., \"Second integer\"]class multiply(TypedDict):    \"\"\"Multiply two integers.\"\"\"    a: Annotated[int, ..., \"First integer\"]    b: Annotated[int, ..., \"Second integer\"]tools = [add, multiply]\n```\n\nTo actually bind those schemas to a chat model, we'll use the .bind_tools() method. This handles converting the add and multiply schemas to the proper format for the model. The tool schema will then be passed it in each time the model is invoked.\n\n```\npip install -qU \"langchain[google-genai]\"\n```\n\n```\nimport getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n```\n\n```\nllm_with_tools = llm.bind_tools(tools)query = \"What is 3 * 12?\"llm_with_tools.invoke(query)\n```\n\n```\nAIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_iXj4DiW1p7WLjTAQMRO0jxMs', 'function': {'arguments': '{\"a\":3,\"b\":12}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 80, 'total_tokens': 97}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_483d39d857', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-0b620986-3f62-4df7-9ba3-4595089f9ad4-0', tool_calls=[{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_iXj4DiW1p7WLjTAQMRO0jxMs', 'type': 'tool_call'}], usage_metadata={'input_tokens': 80, 'output_tokens': 17, 'total_tokens': 97})\n```\n\nAs we can see our LLM generated arguments to a tool! You can look at the docs for bind_tools() to learn about all the ways to customize how your LLM selects tools, as well as this guide on how to force the LLM to call a tool rather than letting it decide.", "url_final": "https://python.langchain.com/docs/how_to/tool_calling/", "url_citable": "https://python.langchain.com/docs/how_to/tool_calling/#typeddict-class", "title": "How to use chat models to call tools | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "TypedDict class‚Äã", "section_anchor": "typeddict-class", "fetched_at": "2025-10-06T21:56:33.539439", "has_code": true}
{"id": "aa30c0671302970947318752d1d0536ec7423d13", "text": "Tool calls‚Äã\n\nIf tool calls are included in a LLM response, they are attached to the corresponding message or message chunk as a list of tool call objects in the .tool_calls attribute.\n\nNote that chat models can call multiple tools at once.\n\nA ToolCall is a typed dict that includes a tool name, dict of argument values, and (optionally) an identifier. Messages with no tool calls default to an empty list for this attribute.\n\n```\nquery = \"What is 3 * 12? Also, what is 11 + 49?\"llm_with_tools.invoke(query).tool_calls\n```\n\n```\n[{'name': 'multiply',  'args': {'a': 3, 'b': 12},  'id': 'call_1fyhJAbJHuKQe6n0PacubGsL',  'type': 'tool_call'}, {'name': 'add',  'args': {'a': 11, 'b': 49},  'id': 'call_fc2jVkKzwuPWyU7kS9qn1hyG',  'type': 'tool_call'}]\n```\n\nThe .tool_calls attribute should contain valid tool calls. Note that on occasion, model providers may output malformed tool calls (e.g., arguments that are not valid JSON). When parsing fails in these cases, instances of InvalidToolCall are populated in the .invalid_tool_calls attribute. An InvalidToolCall can have a name, string arguments, identifier, and error message.", "url_final": "https://python.langchain.com/docs/how_to/tool_calling/", "url_citable": "https://python.langchain.com/docs/how_to/tool_calling/#tool-calls", "title": "How to use chat models to call tools | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Tool calls‚Äã", "section_anchor": "tool-calls", "fetched_at": "2025-10-06T21:56:33.539439", "has_code": true}
{"id": "c242a2d3f526e6ecf063eb86d3dd78b6c8d422b4", "text": "Parsing‚Äã\n\nIf desired, output parsers can further process the output. For example, we can convert existing values populated on the .tool_calls to Pydantic objects using the PydanticToolsParser:\n\n```\nfrom langchain_core.output_parsers import PydanticToolsParserfrom pydantic import BaseModel, Fieldclass add(BaseModel):    \"\"\"Add two integers.\"\"\"    a: int = Field(..., description=\"First integer\")    b: int = Field(..., description=\"Second integer\")class multiply(BaseModel):    \"\"\"Multiply two integers.\"\"\"    a: int = Field(..., description=\"First integer\")    b: int = Field(..., description=\"Second integer\")chain = llm_with_tools | PydanticToolsParser(tools=[add, multiply])chain.invoke(query)\n```\n\n```\n[multiply(a=3, b=12), add(a=11, b=49)]\n```", "url_final": "https://python.langchain.com/docs/how_to/tool_calling/", "url_citable": "https://python.langchain.com/docs/how_to/tool_calling/#parsing", "title": "How to use chat models to call tools | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Parsing‚Äã", "section_anchor": "parsing", "fetched_at": "2025-10-06T21:56:33.539439", "has_code": true}
{"id": "a5fab6e76097a41b088372cda137a0586081ab10", "text": "Next steps‚Äã\n\nNow you've learned how to bind tool schemas to a chat model and have the model call the tool.\n\nNext, check out this guide on actually using the tool by invoking the function and passing the results back to the model:\n\n- Pass tool results back to model\n\nYou can also check out some more specific uses of tool calling:\n\n- Getting structured outputs from models\n- Few shot prompting with tools\n- Stream tool calls\n- Pass runtime values to tools\n\n- Python functions\n- LangChain Tool\n- Pydantic class\n- TypedDict class", "url_final": "https://python.langchain.com/docs/how_to/tool_calling/", "url_citable": "https://python.langchain.com/docs/how_to/tool_calling/#next-steps", "title": "How to use chat models to call tools | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Next steps‚Äã", "section_anchor": "next-steps", "fetched_at": "2025-10-06T21:56:33.539439", "has_code": false}
{"id": "1363a3c2acf236f8855d43216ff5270a850dab74", "text": "OpenAI‚Äã\n\nInstall the LangChain x OpenAI package and set your API key\n\n```\n%pip install -qU langchain-openai\n```\n\n```\nimport getpassimport osif \"OPENAI_API_KEY\" not in os.environ:    os.environ[\"OPENAI_API_KEY\"] = getpass()\n```\n\nFor the OpenAI API to return log probabilities we need to configure the logprobs=True param. Then, the logprobs are included on each output AIMessage as part of the response_metadata:\n\n```\nfrom langchain_openai import ChatOpenAIllm = ChatOpenAI(model=\"gpt-4o-mini\").bind(logprobs=True)msg = llm.invoke((\"human\", \"how are you today\"))msg.response_metadata[\"logprobs\"][\"content\"][:5]\n```\n\n```\n[{'token': 'I', 'bytes': [73], 'logprob': -0.26341408, 'top_logprobs': []}, {'token': \"'m\",  'bytes': [39, 109],  'logprob': -0.48584133,  'top_logprobs': []}, {'token': ' just',  'bytes': [32, 106, 117, 115, 116],  'logprob': -0.23484154,  'top_logprobs': []}, {'token': ' a',  'bytes': [32, 97],  'logprob': -0.0018291725,  'top_logprobs': []}, {'token': ' computer',  'bytes': [32, 99, 111, 109, 112, 117, 116, 101, 114],  'logprob': -0.052299336,  'top_logprobs': []}]\n```\n\nAnd are part of streamed Message chunks as well:\n\n```\nct = 0full = Nonefor chunk in llm.stream((\"human\", \"how are you today\")):    if ct < 5:        full = chunk if full is None else full + chunk        if \"logprobs\" in full.response_metadata:            print(full.response_metadata[\"logprobs\"][\"content\"])    else:        break    ct += 1\n```\n\n```\n[][{'token': 'I', 'bytes': [73], 'logprob': -0.26593843, 'top_logprobs': []}][{'token': 'I', 'bytes': [73], 'logprob': -0.26593843, 'top_logprobs': []}, {'token': \"'m\", 'bytes': [39, 109], 'logprob': -0.3238896, 'top_logprobs': []}][{'token': 'I', 'bytes': [73], 'logprob': -0.26593843, 'top_logprobs': []}, {'token': \"'m\", 'bytes': [39, 109], 'logprob': -0.3238896, 'top_logprobs': []}, {'token': ' just', 'bytes': [32, 106, 117, 115, 116], 'logprob': -0.23778509, 'top_logprobs': []}][{'token': 'I', 'bytes': [73], 'logprob': -0.26593843, 'top_logprobs': []}, {'token': \"'m\", 'bytes': [39, 109], 'logprob': -0.3238896, 'top_logprobs': []}, {'token': ' just', 'bytes': [32, 106, 117, 115, 116], 'logprob': -0.23778509, 'top_logprobs': []}, {'token': ' a', 'bytes': [32, 97], 'logprob': -0.0022134194, 'top_logprobs': []}]\n```", "url_final": "https://python.langchain.com/docs/how_to/logprobs/", "url_citable": "https://python.langchain.com/docs/how_to/logprobs/#openai", "title": "How to get log probabilities | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "OpenAI‚Äã", "section_anchor": "openai", "fetched_at": "2025-10-06T21:55:49.981824", "has_code": true}
{"id": "c4ef8f680f0cafb8bb70687afc936481d424f322", "text": "Next steps‚Äã\n\nYou've now learned how to get logprobs from OpenAI models in LangChain.\n\nNext, check out the other how-to guides chat models in this section, like how to get a model to return structured output or how to track token usage.\n\n- OpenAI\n- Next steps", "url_final": "https://python.langchain.com/docs/how_to/logprobs/", "url_citable": "https://python.langchain.com/docs/how_to/logprobs/#next-steps", "title": "How to get log probabilities | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Next steps‚Äã", "section_anchor": "next-steps", "fetched_at": "2025-10-06T21:55:49.981824", "has_code": false}
{"id": "692db39ac92e9e224408c1881686f204c2ce6154", "text": "Splitting text from languages without word boundaries‚Äã\n\nSome writing systems do not have word boundaries, for example Chinese, Japanese, and Thai. Splitting text with the default separator list of [\"\\n\\n\", \"\\n\", \" \", \"\"] can cause words to be split between chunks. To keep words together, you can override the list of separators to include additional punctuation:\n\n- Add ASCII full-stop \".\", Unicode fullwidth full stop \"Ôºé\" (used in Chinese text), and ideographic full stop \"„ÄÇ\" (used in Japanese and Chinese)\n- Add Zero-width space used in Thai, Myanmar, Kmer, and Japanese.\n- Add ASCII comma \",\", Unicode fullwidth comma \"Ôºå\", and Unicode ideographic comma \"„ÄÅ\"\n\n```\ntext_splitter = RecursiveCharacterTextSplitter(    separators=[        \"\\n\\n\",        \"\\n\",        \" \",        \".\",        \",\",        \"\\u200b\",  # Zero-width space        \"\\uff0c\",  # Fullwidth comma        \"\\u3001\",  # Ideographic comma        \"\\uff0e\",  # Fullwidth full stop        \"\\u3002\",  # Ideographic full stop        \"\",    ],    # Existing args)\n```\n\n- Splitting text from languages without word boundaries", "url_final": "https://python.langchain.com/docs/how_to/recursive_text_splitter/", "url_citable": "https://python.langchain.com/docs/how_to/recursive_text_splitter/#splitting-text-from-languages-without-word-boundaries", "title": "How to recursively split text by characters | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Splitting text from languages without word boundaries‚Äã", "section_anchor": "splitting-text-from-languages-without-word-boundaries", "fetched_at": "2025-10-06T21:56:15.050691", "has_code": true}
{"id": "8dd6d9b59900da46fce9025743d0b66294bdbfa1", "text": "Get a graph‚Äã\n\nYou can use the get_graph() method to get a graph representation of the runnable:\n\n```\nchain.get_graph()\n```", "url_final": "https://python.langchain.com/docs/how_to/inspect/", "url_citable": "https://python.langchain.com/docs/how_to/inspect/#get-a-graph", "title": "How to inspect runnables | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Get a graph‚Äã", "section_anchor": "get-a-graph", "fetched_at": "2025-10-06T21:55:44.837537", "has_code": true}
{"id": "7cce0a98fb673f2c709827ca83bfb05d14385fb4", "text": "Print a graph‚Äã\n\nWhile that is not super legible, you can use the print_ascii() method to show that graph in a way that's easier to understand:\n\n```\nchain.get_graph().print_ascii()\n```\n\n```\n           +---------------------------------+                    | Parallel<context,question>Input |                    +---------------------------------+                             **               **                                 ***                   ***                            **                         **           +----------------------+              +-------------+  | VectorStoreRetriever |              | Passthrough |  +----------------------+              +-------------+                      **               **                                      ***         ***                                           **     **                                +----------------------------------+                   | Parallel<context,question>Output |                   +----------------------------------+                                     *                                                      *                                                      *                                           +--------------------+                                 | ChatPromptTemplate |                                 +--------------------+                                            *                                                      *                                                      *                                               +------------+                                         | ChatOpenAI |                                         +------------+                                                *                                                      *                                                      *                                            +-----------------+                                    | StrOutputParser |                                    +-----------------+                                              *                                                      *                                                      *                                         +-----------------------+                              | StrOutputParserOutput |                              +-----------------------+\n```", "url_final": "https://python.langchain.com/docs/how_to/inspect/", "url_citable": "https://python.langchain.com/docs/how_to/inspect/#print-a-graph", "title": "How to inspect runnables | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Print a graph‚Äã", "section_anchor": "print-a-graph", "fetched_at": "2025-10-06T21:55:44.837537", "has_code": true}
{"id": "9a7e0db07e0420b9484014159ba86f758f974bd1", "text": "Get the prompts‚Äã\n\nYou may want to see just the prompts that are used in a chain with the get_prompts() method:\n\n```\nchain.get_prompts()\n```\n\n```\n[ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'))])]\n```", "url_final": "https://python.langchain.com/docs/how_to/inspect/", "url_citable": "https://python.langchain.com/docs/how_to/inspect/#get-the-prompts", "title": "How to inspect runnables | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Get the prompts‚Äã", "section_anchor": "get-the-prompts", "fetched_at": "2025-10-06T21:55:44.837537", "has_code": true}
{"id": "2a37aabf204f1be27a2e3e8d23bfe9e104c72612", "text": "Next steps‚Äã\n\nYou've now learned how to introspect your composed LCEL chains.\n\nNext, check out the other how-to guides on runnables in this section, or the related how-to guide on debugging your chains.\n\n- Get a graph\n- Print a graph\n- Get the prompts\n- Next steps", "url_final": "https://python.langchain.com/docs/how_to/inspect/", "url_citable": "https://python.langchain.com/docs/how_to/inspect/#next-steps", "title": "How to inspect runnables | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Next steps‚Äã", "section_anchor": "next-steps", "fetched_at": "2025-10-06T21:55:44.837537", "has_code": false}
{"id": "7f41c46445af2a4022645875e96d089c4d94ee98", "text": "How to use the MultiQueryRetriever | ü¶úÔ∏èüîó LangChain\n\nDistance-based vector database retrieval embeds (represents) queries in high-dimensional space and finds similar embedded documents based on a distance metric. But, retrieval may produce different results with subtle changes in query wording, or if the embeddings do not capture the semantics of the data well. Prompt engineering / tuning is sometimes done to manually address these problems, but can be tedious.\n\nThe MultiQueryRetriever automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query. For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents. By generating multiple perspectives on the same question, the MultiQueryRetriever can mitigate some of the limitations of the distance-based retrieval and get a richer set of results.\n\nLet's build a vectorstore using the LLM Powered Autonomous Agents blog post by Lilian Weng from the RAG tutorial:\n\n```\n# Build a sample vectorDBfrom langchain_chroma import Chromafrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_openai import OpenAIEmbeddingsfrom langchain_text_splitters import RecursiveCharacterTextSplitter# Load blog postloader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")data = loader.load()# Splittext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)splits = text_splitter.split_documents(data)# VectorDBembedding = OpenAIEmbeddings()vectordb = Chroma.from_documents(documents=splits, embedding=embedding)\n```\n\n```\nUSER_AGENT environment variable not set, consider setting it to identify your requests.\n```\n\nSpecify the LLM to use for query generation, and the retriever will do the rest.\n\n```\nfrom langchain.retrievers.multi_query import MultiQueryRetrieverfrom langchain_openai import ChatOpenAIquestion = \"What are the approaches to Task Decomposition?\"llm = ChatOpenAI(temperature=0)retriever_from_llm = MultiQueryRetriever.from_llm(    retriever=vectordb.as_retriever(), llm=llm)\n```\n\n```\n# Set logging for the queriesimport logginglogging.basicConfig()logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n```\n\n```\nunique_docs = retriever_from_llm.invoke(question)len(unique_docs)\n```\n\n```\nINFO:langchain.retrievers.multi_query:Generated queries: ['1. How can Task Decomposition be achieved through different methods?', '2. What strategies are commonly used for Task Decomposition?', '3. What are the various ways to break down tasks in Task Decomposition?']\n```\n\n```\n5\n```\n\nNote that the underlying queries generated by the retriever are logged at the INFO level.\n\nUnder the hood, MultiQueryRetriever generates queries using a specific prompt. To customize this prompt:\n\n- Make a PromptTemplate with an input variable for the question;\n- Implement an output parser like the one below to split the result into a list of queries.\n\nThe prompt and output parser together must support the generation of a list of queries.\n\n```\nfrom typing import Listfrom langchain_core.output_parsers import BaseOutputParserfrom langchain_core.prompts import PromptTemplatefrom pydantic import BaseModel, Field# Output parser will split the LLM result into a list of queriesclass LineListOutputParser(BaseOutputParser[List[str]]):    \"\"\"Output parser for a list of lines.\"\"\"    def parse(self, text: str) -> List[str]:        lines = text.strip().split(\"\\n\")        return list(filter(None, lines))  # Remove empty linesoutput_parser = LineListOutputParser()QUERY_PROMPT = PromptTemplate(    input_variables=[\"question\"],    template=\"\"\"You are an AI language model assistant. Your task is to generate five     different versions of the given user question to retrieve relevant documents from a vector     database. By generating multiple perspectives on the user question, your goal is to help    the user overcome some of the limitations of the distance-based similarity search.     Provide these alternative questions separated by newlines.    Original question: {question}\"\"\",)llm = ChatOpenAI(temperature=0)# Chainllm_chain = QUERY_PROMPT | llm | output_parser# Other inputsquestion = \"What are the approaches to Task Decomposition?\"\n```\n\n```\n# Runretriever = MultiQueryRetriever(    retriever=vectordb.as_retriever(), llm_chain=llm_chain, parser_key=\"lines\")  # \"lines\" is the key (attribute name) of the parsed output# Resultsunique_docs = retriever.invoke(\"What does the course say about regression?\")len(unique_docs)\n```\n\n```\nINFO:langchain.retrievers.multi_query:Generated queries: ['1. Can you provide insights on regression from the course material?', '2. How is regression discussed in the course content?', '3. What information does the course offer regarding regression?', '4. In what way is regression covered in the course?', \"5. What are the course's teachings on regression?\"]\n```\n\n```\n9\n```", "url_final": "https://python.langchain.com/docs/how_to/MultiQueryRetriever/", "url_citable": "https://python.langchain.com/docs/how_to/MultiQueryRetriever/", "title": "How to use the MultiQueryRetriever | ü¶úÔ∏èüîó LangChain", "section_level": 1, "section_heading": "How to use the MultiQueryRetriever | ü¶úÔ∏èüîó LangChain", "section_anchor": null, "fetched_at": "2025-10-06T21:54:54.768602", "has_code": true}
{"id": "a692e80a83aec715f6b391d61f4aaeea96f7d6db", "text": "Load chat model‚Äã\n\nLet's first load a chat model:\n\n```\npip install -qU \"langchain[google-genai]\"\n```\n\n```\nimport getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n```", "url_final": "https://python.langchain.com/docs/how_to/summarize_refine/", "url_citable": "https://python.langchain.com/docs/how_to/summarize_refine/#load-chat-model", "title": "How to summarize text through iterative refinement | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Load chat model‚Äã", "section_anchor": "load-chat-model", "fetched_at": "2025-10-06T21:54:37.307865", "has_code": true}
{"id": "bfc2931869386c425cfb464c6a711b0515204863", "text": "Load documents‚Äã\n\nNext, we need some documents to summarize. Below, we generate some toy documents for illustrative purposes. See the document loader how-to guides and integration pages for additional sources of data. The summarization tutorial also includes an example summarizing a blog post.\n\n```\nfrom langchain_core.documents import Documentdocuments = [    Document(page_content=\"Apples are red\", metadata={\"title\": \"apple_book\"}),    Document(page_content=\"Blueberries are blue\", metadata={\"title\": \"blueberry_book\"}),    Document(page_content=\"Bananas are yelow\", metadata={\"title\": \"banana_book\"}),]\n```", "url_final": "https://python.langchain.com/docs/how_to/summarize_refine/", "url_citable": "https://python.langchain.com/docs/how_to/summarize_refine/#load-documents", "title": "How to summarize text through iterative refinement | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Load documents‚Äã", "section_anchor": "load-documents", "fetched_at": "2025-10-06T21:54:37.307865", "has_code": true}
{"id": "673a6baee47671d7950fc65625dfa6a659f25f5d", "text": "Create graph‚Äã\n\nBelow we show a LangGraph implementation of this process:\n\n- We generate a simple chain for the initial summary that plucks out the first document, formats it into a prompt and runs inference with our LLM.\n- We generate a second refine_summary_chain that operates on each successive document, refining the initial summary.\n\nWe will need to install langgraph:\n\n```\npip install -qU langgraph\n```\n\n```\nimport operatorfrom typing import List, Literal, TypedDictfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnableConfigfrom langgraph.constants import Sendfrom langgraph.graph import END, START, StateGraph# Initial summarysummarize_prompt = ChatPromptTemplate(    [        (\"human\", \"Write a concise summary of the following: {context}\"),    ])initial_summary_chain = summarize_prompt | llm | StrOutputParser()# Refining the summary with new docsrefine_template = \"\"\"Produce a final summary.Existing summary up to this point:{existing_answer}New context:------------{context}------------Given the new context, refine the original summary.\"\"\"refine_prompt = ChatPromptTemplate([(\"human\", refine_template)])refine_summary_chain = refine_prompt | llm | StrOutputParser()# We will define the state of the graph to hold the document# contents and summary. We also include an index to keep track# of our position in the sequence of documents.class State(TypedDict):    contents: List[str]    index: int    summary: str# We define functions for each node, including a node that generates# the initial summary:async def generate_initial_summary(state: State, config: RunnableConfig):    summary = await initial_summary_chain.ainvoke(        state[\"contents\"][0],        config,    )    return {\"summary\": summary, \"index\": 1}# And a node that refines the summary based on the next documentasync def refine_summary(state: State, config: RunnableConfig):    content = state[\"contents\"][state[\"index\"]]    summary = await refine_summary_chain.ainvoke(        {\"existing_answer\": state[\"summary\"], \"context\": content},        config,    )    return {\"summary\": summary, \"index\": state[\"index\"] + 1}# Here we implement logic to either exit the application or refine# the summary.def should_refine(state: State) -> Literal[\"refine_summary\", END]:    if state[\"index\"] >= len(state[\"contents\"]):        return END    else:        return \"refine_summary\"graph = StateGraph(State)graph.add_node(\"generate_initial_summary\", generate_initial_summary)graph.add_node(\"refine_summary\", refine_summary)graph.add_edge(START, \"generate_initial_summary\")graph.add_conditional_edges(\"generate_initial_summary\", should_refine)graph.add_conditional_edges(\"refine_summary\", should_refine)app = graph.compile()\n```\n\nLangGraph allows the graph structure to be plotted to help visualize its function:\n\n```\nfrom IPython.display import ImageImage(app.get_graph().draw_mermaid_png())\n```", "url_final": "https://python.langchain.com/docs/how_to/summarize_refine/", "url_citable": "https://python.langchain.com/docs/how_to/summarize_refine/#create-graph", "title": "How to summarize text through iterative refinement | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Create graph‚Äã", "section_anchor": "create-graph", "fetched_at": "2025-10-06T21:54:37.307865", "has_code": true}
{"id": "d8e4894c783b6ce70f6bffefc481fb64976ded9f", "text": "Invoke graph‚Äã\n\nWe can step through the execution as follows, printing out the summary as it is refined:\n\n```\nasync for step in app.astream(    {\"contents\": [doc.page_content for doc in documents]},    stream_mode=\"values\",):    if summary := step.get(\"summary\"):        print(summary)\n```\n\n```\nApples are characterized by their red color.Apples are characterized by their red color, while blueberries are known for their blue hue.Apples are characterized by their red color, blueberries are known for their blue hue, and bananas are recognized for their yellow color.\n```\n\nThe final step contains the summary as synthesized from the entire set of documents.", "url_final": "https://python.langchain.com/docs/how_to/summarize_refine/", "url_citable": "https://python.langchain.com/docs/how_to/summarize_refine/#invoke-graph", "title": "How to summarize text through iterative refinement | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Invoke graph‚Äã", "section_anchor": "invoke-graph", "fetched_at": "2025-10-06T21:54:37.307865", "has_code": true}
{"id": "ad8b134292636d1bbc1d28934424507bf151b3cb", "text": "Next steps‚Äã\n\nCheck out the summarization how-to guides for additional summarization strategies, including those designed for larger volumes of text.\n\nSee this tutorial for more detail on summarization.\n\nSee also the LangGraph documentation for detail on building with LangGraph.\n\n- Load chat model\n- Load documents\n- Create graph\n- Invoke graph\n- Next steps", "url_final": "https://python.langchain.com/docs/how_to/summarize_refine/", "url_citable": "https://python.langchain.com/docs/how_to/summarize_refine/#next-steps", "title": "How to summarize text through iterative refinement | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Next steps‚Äã", "section_anchor": "next-steps", "fetched_at": "2025-10-06T21:54:37.307865", "has_code": false}
{"id": "76e952ac7f84b3dde7629af1eb068abb9884ef08", "text": "Setup‚Äã\n\nFor this guide, we'll be using a tool calling agent with a single tool for searching the web. The default will be powered by Tavily, but you can switch it out for any similar tool. The rest of this section will assume you're using Tavily.\n\nYou'll need to sign up for an account on the Tavily website, and install the following packages:\n\n```\n%pip install --upgrade --quiet langchain-openai tavily-python langgraphimport getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"):    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")if not os.environ.get(\"TAVILY_API_KEY\"):    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Tavily API Key:\")\n```\n\nYou will also need your OpenAI key set as OPENAI_API_KEY and your Tavily API key set as TAVILY_API_KEY.", "url_final": "https://python.langchain.com/docs/how_to/chatbots_tools/", "url_citable": "https://python.langchain.com/docs/how_to/chatbots_tools/#setup", "title": "How to add tools to chatbots | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Setup‚Äã", "section_anchor": "setup", "fetched_at": "2025-10-06T21:55:08.044333", "has_code": true}
{"id": "2a87155f46b0b082ebdcaea2905ebab7c74ad314", "text": "Creating an agent‚Äã\n\nOur end goal is to create an agent that can respond conversationally to user questions while looking up information as needed.\n\nFirst, let's initialize Tavily and an OpenAI chat model capable of tool calling:\n\n```\nfrom langchain_openai import ChatOpenAIfrom langchain_tavily import TavilySearchtools = [TavilySearch(max_results=10, topic=\"general\")]# Choose the LLM that will drive the agent# Only certain models support thismodel = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nTo make our agent conversational, we can also specify a prompt. Here's an example:\n\n```\nprompt = (    \"You are a helpful assistant. \"    \"You may not need to use tools for every query - the user may just want to chat!\")\n```\n\nGreat! Now let's assemble our agent using LangGraph's prebuilt create_react_agent, which allows you to create a tool-calling agent:\n\n```\nfrom langgraph.prebuilt import create_react_agent# prompt allows you to preprocess the inputs to the model inside ReAct agent# in this case, since we're passing a prompt string, we'll just always add a SystemMessage# with this prompt string before any other messages sent to the modelagent = create_react_agent(model, tools, prompt=prompt)\n```", "url_final": "https://python.langchain.com/docs/how_to/chatbots_tools/", "url_citable": "https://python.langchain.com/docs/how_to/chatbots_tools/#creating-an-agent", "title": "How to add tools to chatbots | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Creating an agent‚Äã", "section_anchor": "creating-an-agent", "fetched_at": "2025-10-06T21:55:08.044333", "has_code": true}
{"id": "fb76115731c77df1674a288dfbf8e0dc05bc0dce", "text": "Running the agent‚Äã\n\nNow that we've set up our agent, let's try interacting with it! It can handle both trivial queries that require no lookup:\n\n```\nfrom langchain_core.messages import HumanMessageagent.invoke({\"messages\": [HumanMessage(content=\"I'm Nemo!\")]})\n```\n\n```\n{'messages': [HumanMessage(content=\"I'm Nemo!\", additional_kwargs={}, response_metadata={}, id='40b60204-1af1-40d4-b6a7-b845a2281dd6'),  AIMessage(content='Hi Nemo! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 795, 'total_tokens': 806, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-BjsUwqprT2mVdjqu1aaSm1jVVWYVz', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--08282ec6-6d3e-4495-b004-b3b08f3879c3-0', usage_metadata={'input_tokens': 795, 'output_tokens': 11, 'total_tokens': 806, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n```\n\nOr, it can use of the passed search tool to get up to date information if needed:\n\n```\nagent.invoke(    {        \"messages\": [            HumanMessage(                content=\"What is the current conservation status of the Great Barrier Reef?\"            )        ],    })\n```\n\n```\n{'messages': [HumanMessage(content='What is the current conservation status of the Great Barrier Reef?', additional_kwargs={}, response_metadata={}, id='5240955c-d842-408d-af3d-4ee74db29dbd'),  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_W37BFkNuZlJu9US1Tl71xpiX', 'function': {'arguments': '{\"query\":\"current conservation status of the Great Barrier Reef\",\"time_range\":\"year\",\"topic\":\"general\"}', 'name': 'tavily_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 804, 'total_tokens': 836, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-BjsV6EJ7F1vDipoG4dpEiBRZvuTLo', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--5f5b32d7-fb80-4913-a7ec-ca9c5acaa101-0', tool_calls=[{'name': 'tavily_search', 'args': {'query': 'current conservation status of the Great Barrier Reef', 'time_range': 'year', 'topic': 'general'}, 'id': 'call_W37BFkNuZlJu9US1Tl71xpiX', 'type': 'tool_call'}], usage_metadata={'input_tokens': 804, 'output_tokens': 32, 'total_tokens': 836, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),  ToolMessage(content='{\"query\": \"current conservation status of the Great Barrier Reef\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"The Great Barrier Reef: Current Conservation Efforts and Future Outlook\", \"url\": \"https://discoverwildscience.com/the-great-barrier-reef-current-conservation-efforts-and-future-outlook-1-279446/\", \"content\": \"The Great Barrier Reef, a mesmerizing marvel of nature, stretches over 2,300 kilometers along the northeast coast of Australia. As the largest coral reef system in the world, it is home to an incredible diversity of marine life, including more than 1,500 species of fish and 411 types of hard coral.\", \"score\": 0.6353361, \"raw_content\": null}, {\"title\": \"Monitoring progress - Protecting the Great Barrier Reef\", \"url\": \"https://www.detsi.qld.gov.au/great-barrier-reef/monitoring-progress\", \"content\": \"Stay informed about the current state of the Great Barrier Reef through comprehensive monitoring reports and reef report cards. Delve into the scientific research and advancements contributing to reef conservation. Learn about ongoing efforts to track progress and ensure the reef\\'s long-term health.\", \"score\": 0.6347929, \"raw_content\": null}, {\"title\": \"Great Barrier Reef Outlook Report shows that the reef is in serious ...\", \"url\": \"https://biodiversitycouncil.org.au/news/great-barrier-reef-outlook-report-shows-that-the-reef-is-in-serious-trouble\", \"content\": \"The Great Barrier Reef is in very serious trouble. Climate change is the biggest threat to the reef. Catchment restoration activities that reduce sediment flowing to the reef will aid the health of the reef but cannot match the scale of destruction occurring due to marine heatwaves caused by climate change.\", \"score\": 0.5183761, \"raw_content\": null}, {\"title\": \"Water pollution threatens Great Barrier Reef\\'s survival: new report ...\", \"url\": \"https://www.marineconservation.org.au/water-pollution-threatens-great-barrier-reefs-survival-new-report-highlights-funding-need/\", \"content\": \"While this investment has supported critical work across the Great Barrier Reef catchments, more funding is needed. At current rates, the target to cut fine sediment by 25% on 2009 levels will not be met until 2047, while the target to reduce dissolved inorganic nitrogen by 60% is not expected to be achieved until 2114.\", \"score\": 0.51383984, \"raw_content\": null}, {\"title\": \"What is the state of the Great Barrier Reef? - Tangaroa Blue\", \"url\": \"https://tangaroablue.org/the-state-of-the-great-barrier-reef/\", \"content\": \"The Great Barrier Reef Outlook Report 2024, prepared every five years by the Great Barrier Reef Marine Park Authority, summarises the Reef\\'s long-term outlook based on its use, management, and risks.This year\\'s report uses data from the Australian Marine Debris Initiative Database to analyse the risks and impacts of marine debris on the Great Barrier Reef and help identify areas for\", \"score\": 0.47489962, \"raw_content\": null}, {\"title\": \"New report on Great Barrier Reef shows coral cover increases before ...\", \"url\": \"https://www.aims.gov.au/information-centre/news-and-stories/new-report-great-barrier-reef-shows-coral-cover-increases-onset-serious-bleaching-cyclones\", \"content\": \"Coral cover has increased in all three regions on the Great Barrier Reef and is at regional highs in two of the three regions. But the results come with a note of caution. ... trained scientists during manta tow surveys and is a metric which allows AIMS scientists to provide an overview of the Great Barrier Reef\\'s status and keep policy\", \"score\": 0.40330887, \"raw_content\": null}, {\"title\": \"Cycle of coral bleaching on the Great Barrier Reef now at \\'catastrophic ...\", \"url\": \"https://www.sydney.edu.au/news-opinion/news/2025/01/21/coral-bleaching-2024-great-barrier-reef-one-tree-island.html\", \"content\": \"As the Great Barrier Reef faces increasing threats from climate change, the study calls for a collaborative approach to conservation that involves local communities, scientists and policymakers. Dr Shawna Foo , a Sydney Horizon Fellow and co-author of the study, said: \\\\\"Seeing the impacts on a reef that has largely avoided mass bleaching until\", \"score\": 0.3759361, \"raw_content\": null}, {\"title\": \"Great Barrier Reef Outlook Report 2024: An ecosystem under pressure\", \"url\": \"https://icriforum.org/gbr-outlook-report-2024/\", \"content\": \"The 2024 Great Barrier Reef Outlook Report is the fourth in a series of comprehensive five-yearly reports on the Reef\\'s health, pressures, management, and potential future. It found climate-driven threats such as warming oceans and severe cyclones have been compounding other impacts from crown-of-thorns starfish outbreaks, poor water quality\", \"score\": 0.34634283, \"raw_content\": null}, {\"title\": \"UNESCO expresses \\'utmost concern\\' at the state of the Great Barrier Reef\", \"url\": \"https://theconversation.com/unesco-expresses-utmost-concern-at-the-state-of-the-great-barrier-reef-257638\", \"content\": \"This 2017 photo from Ribbon Reef, near Cairns, shows what a healthy reef looks like. J Summerling/AP Poor water quality persists. Poor water quality is a major issue on the Great Barrier Reef.\", \"score\": 0.31069487, \"raw_content\": null}, {\"title\": \"Reef health updates | Reef Authority - gbrmpa\", \"url\": \"https://www2.gbrmpa.gov.au/learn/reef-health/reef-health-updates\", \"content\": \"As the lead managers of the Great Barrier Reef, the Reef Authority keeps an eye on the Reef year-round ‚Äî with efforts stepped up over summer, a typically high-risk period from extreme weather. The Reef Authority releases updates on the health of Reef which includes; sea surface temperatures, rainfall and floods, cyclones, crown-of-thorns\", \"score\": 0.18051112, \"raw_content\": null}], \"response_time\": 2.07}', name='tavily_search', id='cbf7ae84-1df7-4ead-b00d-f8fba2152720', tool_call_id='call_W37BFkNuZlJu9US1Tl71xpiX'),  AIMessage(content='The current conservation status of the Great Barrier Reef is concerning. The reef is facing significant threats primarily due to climate change, which is causing marine heatwaves and coral bleaching. A report highlights that while there have been some local efforts in conservation, such as catchment restoration to reduce sediment flow, these cannot keep pace with the destruction caused by climate impacts. Recent findings from the 2024 Great Barrier Reef Outlook Report indicate that climate-driven phenomena like warming oceans and severe cyclones are exacerbating other pressures, such as crown-of-thorns starfish outbreaks and poor water quality.\\n\\nSome reports have indicated that coral cover has increased in certain regions of the reef, but overall, the health of the reef remains in serious decline. There‚Äôs an urgent call for more funding and collaborative efforts between local communities, scientists, and policymakers to enhance conservation measures.\\n\\nFor more detailed information, you can refer to these articles:\\n- [The Great Barrier Reef: Current Conservation Efforts and Future Outlook](https://discoverwildscience.com/the-great-barrier-reef-current-conservation-efforts-and-future-outlook-1-279446/)\\n- [Great Barrier Reef Outlook Report 2024: An ecosystem under pressure](https://icriforum.org/gbr-outlook-report-2024/)', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 255, 'prompt_tokens': 2208, 'total_tokens': 2463, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-BjsVAxeGL7PKGVkb2DieFPE0ZPgor', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--71441b27-81a0-427f-8784-b2ea674bebd4-0', usage_metadata={'input_tokens': 2208, 'output_tokens': 255, 'total_tokens': 2463, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n```", "url_final": "https://python.langchain.com/docs/how_to/chatbots_tools/", "url_citable": "https://python.langchain.com/docs/how_to/chatbots_tools/#running-the-agent", "title": "How to add tools to chatbots | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Running the agent‚Äã", "section_anchor": "running-the-agent", "fetched_at": "2025-10-06T21:55:08.044333", "has_code": true}
{"id": "653f8eab6ee60d652ff81706bda8fdbd4aec7419", "text": "Conversational responses‚Äã\n\nBecause our prompt contains a placeholder for chat history messages, our agent can also take previous interactions into account and respond conversationally like a standard chatbot:\n\n```\nfrom langchain_core.messages import AIMessage, HumanMessageagent.invoke(    {        \"messages\": [            HumanMessage(content=\"I'm Nemo!\"),            AIMessage(content=\"Hello Nemo! How can I assist you today?\"),            HumanMessage(content=\"What is my name?\"),        ],    })\n```\n\n```\n{'messages': [HumanMessage(content=\"I'm Nemo!\", additional_kwargs={}, response_metadata={}, id='8a67dea0-acd8-40f9-8c28-292c5f81c05f'),  AIMessage(content='Hello Nemo! How can I assist you today?', additional_kwargs={}, response_metadata={}, id='92a2533e-5c62-4cbe-80f1-302f5f1caf28'),  HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}, id='efa8c3d3-86d7-428f-985e-a3aadd6504bc'),  AIMessage(content='Your name is Nemo!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 818, 'total_tokens': 824, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-BjsVIf5MX5jXUEjYCorT5bWYzc7iu', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--a1a32c7d-8066-4954-86f9-3a8f43fcb48d-0', usage_metadata={'input_tokens': 818, 'output_tokens': 6, 'total_tokens': 824, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n```\n\nIf preferred, you can also add memory to the LangGraph agent to manage the history of messages. Let's redeclare it this way:\n\n```\nfrom langgraph.checkpoint.memory import MemorySavermemory = MemorySaver()agent = create_react_agent(model, tools, prompt=prompt, checkpointer=memory)\n```\n\n```\nagent.invoke(    {\"messages\": [HumanMessage(\"I'm Nemo!\")]},    config={\"configurable\": {\"thread_id\": \"1\"}},)\n```\n\n```\n{'messages': [HumanMessage(content=\"I'm Nemo!\", additional_kwargs={}, response_metadata={}, id='31c2249a-13eb-4040-b56d-0c8746fa158e'),  AIMessage(content='Hello, Nemo! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 795, 'total_tokens': 807, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-BjsVRB0FItvtPawTTIAjNwgmlQFFw', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--a9703ca1-de4c-4f76-b622-9683d86ca777-0', usage_metadata={'input_tokens': 795, 'output_tokens': 12, 'total_tokens': 807, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n```\n\nAnd then if we rerun our wrapped agent executor:\n\n```\nagent.invoke(    {\"messages\": [HumanMessage(\"What is my name?\")]},    config={\"configurable\": {\"thread_id\": \"1\"}},)\n```\n\n```\n{'messages': [HumanMessage(content=\"I'm Nemo!\", additional_kwargs={}, response_metadata={}, id='31c2249a-13eb-4040-b56d-0c8746fa158e'),  AIMessage(content='Hello, Nemo! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 795, 'total_tokens': 807, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-BjsVRB0FItvtPawTTIAjNwgmlQFFw', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--a9703ca1-de4c-4f76-b622-9683d86ca777-0', usage_metadata={'input_tokens': 795, 'output_tokens': 12, 'total_tokens': 807, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),  HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}, id='0cde6457-8d4d-45d5-b175-ad846018c4d2'),  AIMessage(content='Your name is Nemo! How can I help you today, Nemo?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 819, 'total_tokens': 834, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-BjsVTa1plxGPNitbOcw7YVTFdmz1e', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--1d742bc1-5839-4837-b6f4-9a6b92fa6897-0', usage_metadata={'input_tokens': 819, 'output_tokens': 15, 'total_tokens': 834, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n```\n\nThis LangSmith trace shows what's going on under the hood.", "url_final": "https://python.langchain.com/docs/how_to/chatbots_tools/", "url_citable": "https://python.langchain.com/docs/how_to/chatbots_tools/#conversational-responses", "title": "How to add tools to chatbots | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Conversational responses‚Äã", "section_anchor": "conversational-responses", "fetched_at": "2025-10-06T21:55:08.044333", "has_code": true}
{"id": "24f54cc7278c9b8dd33a1ebe210ad8b06afe1615", "text": "Further reading‚Äã\n\nFor more on how to build agents, check these LangGraph guides:\n\n- agents conceptual guide\n- agents tutorials\n- create_react_agent\n\nFor more on tool usage, you can also check out this use case section.\n\n- Setup\n- Creating an agent\n- Running the agent\n- Conversational responses\n- Further reading", "url_final": "https://python.langchain.com/docs/how_to/chatbots_tools/", "url_citable": "https://python.langchain.com/docs/how_to/chatbots_tools/#further-reading", "title": "How to add tools to chatbots | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Further reading‚Äã", "section_anchor": "further-reading", "fetched_at": "2025-10-06T21:55:08.044333", "has_code": false}
{"id": "8431288c611092bff3c26b8f66724a63c920305e", "text": "Get started‚Äã\n\nBelow we go over the main type of output parser, the PydanticOutputParser.\n\n```\nfrom langchain_core.output_parsers import PydanticOutputParserfrom langchain_core.prompts import PromptTemplatefrom langchain_openai import OpenAIfrom pydantic import BaseModel, Field, model_validatormodel = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0.0)# Define your desired data structure.class Joke(BaseModel):    setup: str = Field(description=\"question to set up a joke\")    punchline: str = Field(description=\"answer to resolve the joke\")    # You can add custom validation logic easily with Pydantic.    @model_validator(mode=\"before\")    @classmethod    def question_ends_with_question_mark(cls, values: dict) -> dict:        setup = values.get(\"setup\")        if setup and setup[-1] != \"?\":            raise ValueError(\"Badly formed question!\")        return values# Set up a parser + inject instructions into the prompt template.parser = PydanticOutputParser(pydantic_object=Joke)prompt = PromptTemplate(    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",    input_variables=[\"query\"],    partial_variables={\"format_instructions\": parser.get_format_instructions()},)# And a query intended to prompt a language model to populate the data structure.prompt_and_model = prompt | modeloutput = prompt_and_model.invoke({\"query\": \"Tell me a joke.\"})parser.invoke(output)\n```\n\n```\nJoke(setup='Why did the tomato turn red?', punchline='Because it saw the salad dressing!')\n```", "url_final": "https://python.langchain.com/docs/how_to/output_parser_structured/", "url_citable": "https://python.langchain.com/docs/how_to/output_parser_structured/#get-started", "title": "How to use output parsers to parse an LLM response into structured format | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Get started‚Äã", "section_anchor": "get-started", "fetched_at": "2025-10-06T21:54:32.263095", "has_code": true}
{"id": "3271b3d6ca322421085e776896fa769cfcfc7d8e", "text": "LCEL‚Äã\n\nOutput parsers implement the Runnable interface, the basic building block of the LangChain Expression Language (LCEL). This means they support invoke, ainvoke, stream, astream, batch, abatch, astream_log calls.\n\nOutput parsers accept a string or BaseMessage as input and can return an arbitrary type.\n\n```\nparser.invoke(output)\n```\n\n```\nJoke(setup='Why did the tomato turn red?', punchline='Because it saw the salad dressing!')\n```\n\nInstead of manually invoking the parser, we also could've just added it to our Runnable sequence:\n\n```\nchain = prompt | model | parserchain.invoke({\"query\": \"Tell me a joke.\"})\n```\n\n```\nJoke(setup='Why did the tomato turn red?', punchline='Because it saw the salad dressing!')\n```\n\nWhile all parsers support the streaming interface, only certain parsers can stream through partially parsed objects, since this is highly dependent on the output type. Parsers which cannot construct partial objects will simply yield the fully parsed output.\n\nThe SimpleJsonOutputParser for example can stream through partial outputs:\n\n```\nfrom langchain.output_parsers.json import SimpleJsonOutputParserjson_prompt = PromptTemplate.from_template(    \"Return a JSON object with an `answer` key that answers the following question: {question}\")json_parser = SimpleJsonOutputParser()json_chain = json_prompt | model | json_parser\n```\n\n```\nlist(json_chain.stream({\"question\": \"Who invented the microscope?\"}))\n```\n\n```\n[{}, {'answer': ''}, {'answer': 'Ant'}, {'answer': 'Anton'}, {'answer': 'Antonie'}, {'answer': 'Antonie van'}, {'answer': 'Antonie van Lee'}, {'answer': 'Antonie van Leeu'}, {'answer': 'Antonie van Leeuwen'}, {'answer': 'Antonie van Leeuwenho'}, {'answer': 'Antonie van Leeuwenhoek'}]\n```\n\nSimilarly,for PydanticOutputParser:\n\n```\nlist(chain.stream({\"query\": \"Tell me a joke.\"}))\n```\n\n```\n[Joke(setup='Why did the tomato turn red?', punchline=''), Joke(setup='Why did the tomato turn red?', punchline='Because'), Joke(setup='Why did the tomato turn red?', punchline='Because it'), Joke(setup='Why did the tomato turn red?', punchline='Because it saw'), Joke(setup='Why did the tomato turn red?', punchline='Because it saw the'), Joke(setup='Why did the tomato turn red?', punchline='Because it saw the salad'), Joke(setup='Why did the tomato turn red?', punchline='Because it saw the salad dressing'), Joke(setup='Why did the tomato turn red?', punchline='Because it saw the salad dressing!')]\n```\n\n- Get started\n- LCEL", "url_final": "https://python.langchain.com/docs/how_to/output_parser_structured/", "url_citable": "https://python.langchain.com/docs/how_to/output_parser_structured/#lcel", "title": "How to use output parsers to parse an LLM response into structured format | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "LCEL‚Äã", "section_anchor": "lcel", "fetched_at": "2025-10-06T21:54:32.263095", "has_code": true}
{"id": "405d5cd069dcb0cf4322de69fbc1bb3b3260dc0a", "text": "How to pass runtime secrets to runnables | ü¶úÔ∏èüîó LangChain\n\nWe can pass in secrets to our runnables at runtime using the RunnableConfig. Specifically we can pass in secrets with a __ prefix to the configurable field. This will ensure that these secrets aren't traced as part of the invocation:\n\n```\nfrom langchain_core.runnables import RunnableConfigfrom langchain_core.tools import tool@tooldef foo(x: int, config: RunnableConfig) -> int:    \"\"\"Sum x and a secret int\"\"\"    return x + config[\"configurable\"][\"__top_secret_int\"]foo.invoke({\"x\": 5}, {\"configurable\": {\"__top_secret_int\": 2, \"traced_key\": \"bar\"}})\n```\n\n```\n7\n```\n\nLooking at the LangSmith trace for this run, we can see that \"traced_key\" was recorded (as part of Metadata) while our secret int was not: https://smith.langchain.com/public/aa7e3289-49ca-422d-a408-f6b927210170/r", "url_final": "https://python.langchain.com/docs/how_to/runnable_runtime_secrets/", "url_citable": "https://python.langchain.com/docs/how_to/runnable_runtime_secrets/", "title": "How to pass runtime secrets to runnables | ü¶úÔ∏èüîó LangChain", "section_level": 1, "section_heading": "How to pass runtime secrets to runnables | ü¶úÔ∏èüîó LangChain", "section_anchor": null, "fetched_at": "2025-10-06T21:56:17.049594", "has_code": true}
{"id": "17b6e9aeb85ea1c3f01d13ab4db84ccd9edcb86d", "text": "Install Dependencies‚Äã\n\n```\n!pip install --quiet langchain_experimental langchain_openai\n```", "url_final": "https://python.langchain.com/docs/how_to/semantic-chunker/", "url_citable": "https://python.langchain.com/docs/how_to/semantic-chunker/#install-dependencies", "title": "How to split text based on semantic similarity | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Install Dependencies‚Äã", "section_anchor": "install-dependencies", "fetched_at": "2025-10-06T21:56:19.078733", "has_code": true}
{"id": "ad5c0fdcfb45e808f155631d91d6bf1b84d34c90", "text": "Load Example Data‚Äã\n\n```\n# This is a long document we can split up.with open(\"state_of_the_union.txt\") as f:    state_of_the_union = f.read()\n```", "url_final": "https://python.langchain.com/docs/how_to/semantic-chunker/", "url_citable": "https://python.langchain.com/docs/how_to/semantic-chunker/#load-example-data", "title": "How to split text based on semantic similarity | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Load Example Data‚Äã", "section_anchor": "load-example-data", "fetched_at": "2025-10-06T21:56:19.078733", "has_code": true}
{"id": "dd519e81e144a363f9668edbe5212cef0c7f3330", "text": "Create Text Splitter‚Äã\n\nTo instantiate a SemanticChunker, we must specify an embedding model. Below we will use OpenAIEmbeddings.\n\n```\nfrom langchain_experimental.text_splitter import SemanticChunkerfrom langchain_openai.embeddings import OpenAIEmbeddingstext_splitter = SemanticChunker(OpenAIEmbeddings())\n```", "url_final": "https://python.langchain.com/docs/how_to/semantic-chunker/", "url_citable": "https://python.langchain.com/docs/how_to/semantic-chunker/#create-text-splitter", "title": "How to split text based on semantic similarity | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Create Text Splitter‚Äã", "section_anchor": "create-text-splitter", "fetched_at": "2025-10-06T21:56:19.078733", "has_code": true}
{"id": "05790cd6a9183ba0d06b09c1ca6706a73e3573c5", "text": "Split Text‚Äã\n\nWe split text in the usual way, e.g., by invoking .create_documents to create LangChain Document objects:\n\n```\ndocs = text_splitter.create_documents([state_of_the_union])print(docs[0].page_content)\n```\n\n```\nMadam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans. Last year COVID-19 kept us apart. This year we are finally together again. Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. With a duty to one another to the American people to the Constitution. And with an unwavering resolve that freedom will always triumph over tyranny. Six days ago, Russia‚Äôs Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. He thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. He met the Ukrainian people. From President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. Groups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland. In this struggle as President Zelenskyy said in his speech to the European Parliament ‚ÄúLight will win over darkness.‚Äù The Ukrainian Ambassador to the United States is here tonight. Let each of us here tonight in this Chamber send an unmistakable signal to Ukraine and to the world. Please rise if you are able and show that, Yes, we the United States of America stand with the Ukrainian people. Throughout our history we‚Äôve learned this lesson when dictators do not pay a price for their aggression they cause more chaos. They keep moving.\n```", "url_final": "https://python.langchain.com/docs/how_to/semantic-chunker/", "url_citable": "https://python.langchain.com/docs/how_to/semantic-chunker/#split-text", "title": "How to split text based on semantic similarity | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Split Text‚Äã", "section_anchor": "split-text", "fetched_at": "2025-10-06T21:56:19.078733", "has_code": true}
{"id": "5ddb4336e5cdf0f35b2ac94dfc30e12489deebdc", "text": "Breakpoints‚Äã\n\nThis chunker works by determining when to \"break\" apart sentences. This is done by looking for differences in embeddings between any two sentences. When that difference is past some threshold, then they are split.\n\nThere are a few ways to determine what that threshold is, which are controlled by the breakpoint_threshold_type kwarg.\n\nNote: if the resulting chunk sizes are too small/big, the additional kwargs breakpoint_threshold_amount and min_chunk_size can be used for adjustments.\n\nPercentile‚Äã\n\nThe default way to split is based on percentile. In this method, all differences between sentences are calculated, and then any difference greater than the X percentile is split. The default value for X is 95.0 and can be adjusted by the keyword argument breakpoint_threshold_amount which expects a number between 0.0 and 100.0.\n\n```\ntext_splitter = SemanticChunker(    OpenAIEmbeddings(), breakpoint_threshold_type=\"percentile\")\n```\n\n```\ndocs = text_splitter.create_documents([state_of_the_union])print(docs[0].page_content)\n```\n\n```\nMadam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans. Last year COVID-19 kept us apart. This year we are finally together again. Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. With a duty to one another to the American people to the Constitution. And with an unwavering resolve that freedom will always triumph over tyranny. Six days ago, Russia‚Äôs Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. He thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. He met the Ukrainian people. From President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. Groups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland. In this struggle as President Zelenskyy said in his speech to the European Parliament ‚ÄúLight will win over darkness.‚Äù The Ukrainian Ambassador to the United States is here tonight. Let each of us here tonight in this Chamber send an unmistakable signal to Ukraine and to the world. Please rise if you are able and show that, Yes, we the United States of America stand with the Ukrainian people. Throughout our history we‚Äôve learned this lesson when dictators do not pay a price for their aggression they cause more chaos. They keep moving.\n```\n\n```\nprint(len(docs))\n```\n\n```\n26\n```", "url_final": "https://python.langchain.com/docs/how_to/semantic-chunker/", "url_citable": "https://python.langchain.com/docs/how_to/semantic-chunker/#breakpoints", "title": "How to split text based on semantic similarity | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Breakpoints‚Äã", "section_anchor": "breakpoints", "fetched_at": "2025-10-06T21:56:19.078733", "has_code": true}
{"id": "fe0e5f36afe6a3c88af5f6435b5c8252d382b1ca", "text": "Standard Deviation‚Äã\n\nIn this method, any difference greater than X standard deviations is split. The default value for X is 3.0 and can be adjusted by the keyword argument breakpoint_threshold_amount.\n\n```\ntext_splitter = SemanticChunker(    OpenAIEmbeddings(), breakpoint_threshold_type=\"standard_deviation\")\n```\n\n```\ndocs = text_splitter.create_documents([state_of_the_union])print(docs[0].page_content)\n```\n\n```\nMadam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans. Last year COVID-19 kept us apart. This year we are finally together again. Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. With a duty to one another to the American people to the Constitution. And with an unwavering resolve that freedom will always triumph over tyranny. Six days ago, Russia‚Äôs Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. He thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. He met the Ukrainian people. From President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. Groups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland. In this struggle as President Zelenskyy said in his speech to the European Parliament ‚ÄúLight will win over darkness.‚Äù The Ukrainian Ambassador to the United States is here tonight. Let each of us here tonight in this Chamber send an unmistakable signal to Ukraine and to the world. Please rise if you are able and show that, Yes, we the United States of America stand with the Ukrainian people. Throughout our history we‚Äôve learned this lesson when dictators do not pay a price for their aggression they cause more chaos. They keep moving. And the costs and the threats to America and the world keep rising. That‚Äôs why the NATO Alliance was created to secure peace and stability in Europe after World War 2. The United States is a member along with 29 other nations. It matters. American diplomacy matters. American resolve matters. Putin‚Äôs latest attack on Ukraine was premeditated and unprovoked. He rejected repeated efforts at diplomacy. He thought the West and NATO wouldn‚Äôt respond. And he thought he could divide us at home. Putin was wrong. We were ready. Here is what we did. We prepared extensively and carefully. We spent months building a coalition of other freedom-loving nations from Europe and the Americas to Asia and Africa to confront Putin. I spent countless hours unifying our European allies. We shared with the world in advance what we knew Putin was planning and precisely how he would try to falsely justify his aggression. We countered Russia‚Äôs lies with truth. And now that he has acted the free world is holding him accountable. Along with twenty-seven members of the European Union including France, Germany, Italy, as well as countries like the United Kingdom, Canada, Japan, Korea, Australia, New Zealand, and many others, even Switzerland. We are inflicting pain on Russia and supporting the people of Ukraine. Putin is now isolated from the world more than ever. Together with our allies ‚Äìwe are right now enforcing powerful economic sanctions. We are cutting off Russia‚Äôs largest banks from the international financial system. Preventing Russia‚Äôs central bank from defending the Russian Ruble making Putin‚Äôs $630 Billion ‚Äúwar fund‚Äù worthless. We are choking off Russia‚Äôs access to technology that will sap its economic strength and weaken its military for years to come. Tonight I say to the Russian oligarchs and corrupt leaders who have bilked billions of dollars off this violent regime no more. The U.S. Department of Justice is assembling a dedicated task force to go after the crimes of Russian oligarchs. We are joining with our European allies to find and seize your yachts your luxury apartments your private jets. We are coming for your ill-begotten gains. And tonight I am announcing that we will join our allies in closing off American air space to all Russian flights ‚Äì further isolating Russia ‚Äì and adding an additional squeeze ‚Äìon their economy. The Ruble has lost 30% of its value. The Russian stock market has lost 40% of its value and trading remains suspended. Russia‚Äôs economy is reeling and Putin alone is to blame. Together with our allies we are providing support to the Ukrainians in their fight for freedom. Military assistance. Economic assistance. Humanitarian assistance. We are giving more than $1 Billion in direct assistance to Ukraine. And we will continue to aid the Ukrainian people as they defend their country and to help ease their suffering. Let me be clear, our forces are not engaged and will not engage in conflict with Russian forces in Ukraine. Our forces are not going to Europe to fight in Ukraine, but to defend our NATO Allies ‚Äì in the event that Putin decides to keep moving west. For that purpose we‚Äôve mobilized American ground forces, air squadrons, and ship deployments to protect NATO countries including Poland, Romania, Latvia, Lithuania, and Estonia. As I have made crystal clear the United States and our Allies will defend every inch of territory of NATO countries with the full force of our collective power. And we remain clear-eyed. The Ukrainians are fighting back with pure courage. But the next few days weeks, months, will be hard on them. Putin has unleashed violence and chaos. But while he may make gains on the battlefield ‚Äì he will pay a continuing high price over the long run. And a proud Ukrainian people, who have known 30 years  of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards. To all Americans, I will be honest with you, as I‚Äôve always promised. A Russian dictator, invading a foreign country, has costs around the world. And I‚Äôm taking robust action to make sure the pain of our sanctions  is targeted at Russia‚Äôs economy. And I will use every tool at our disposal to protect American businesses and consumers. Tonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world. America will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, unified with our allies. These steps will help blunt gas prices here at home. And I know the news about what‚Äôs happening can seem alarming.\n```\n\n```\nprint(len(docs))\n```\n\n```\n4\n```", "url_final": "https://python.langchain.com/docs/how_to/semantic-chunker/", "url_citable": "https://python.langchain.com/docs/how_to/semantic-chunker/#standard-deviation", "title": "How to split text based on semantic similarity | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Standard Deviation‚Äã", "section_anchor": "standard-deviation", "fetched_at": "2025-10-06T21:56:19.078733", "has_code": true}
{"id": "f3b44e07e51724e16b4c5f38b51d557f63f30b3c", "text": "Interquartile‚Äã\n\nIn this method, the interquartile distance is used to split chunks. The interquartile range can be scaled by the keyword argument breakpoint_threshold_amount, the default value is 1.5.\n\n```\ntext_splitter = SemanticChunker(    OpenAIEmbeddings(), breakpoint_threshold_type=\"interquartile\")\n```\n\n```\ndocs = text_splitter.create_documents([state_of_the_union])print(docs[0].page_content)\n```\n\n```\nMadam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans. Last year COVID-19 kept us apart. This year we are finally together again. Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. With a duty to one another to the American people to the Constitution. And with an unwavering resolve that freedom will always triumph over tyranny. Six days ago, Russia‚Äôs Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. He thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. He met the Ukrainian people. From President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. Groups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland. In this struggle as President Zelenskyy said in his speech to the European Parliament ‚ÄúLight will win over darkness.‚Äù The Ukrainian Ambassador to the United States is here tonight. Let each of us here tonight in this Chamber send an unmistakable signal to Ukraine and to the world. Please rise if you are able and show that, Yes, we the United States of America stand with the Ukrainian people. Throughout our history we‚Äôve learned this lesson when dictators do not pay a price for their aggression they cause more chaos. They keep moving.\n```\n\n```\nprint(len(docs))\n```\n\n```\n25\n```", "url_final": "https://python.langchain.com/docs/how_to/semantic-chunker/", "url_citable": "https://python.langchain.com/docs/how_to/semantic-chunker/#interquartile", "title": "How to split text based on semantic similarity | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Interquartile‚Äã", "section_anchor": "interquartile", "fetched_at": "2025-10-06T21:56:19.078733", "has_code": true}
{"id": "677cfbda61fa55fd788d898428ecf2ae666588ae", "text": "Gradient‚Äã\n\nIn this method, the gradient of distance is used to split chunks along with the percentile method. This method is useful when chunks are highly correlated with each other or specific to a domain e.g. legal or medical. The idea is to apply anomaly detection on gradient array so that the distribution become wider and easy to identify boundaries in highly semantic data. Similar to the percentile method, the split can be adjusted by the keyword argument breakpoint_threshold_amount which expects a number between 0.0 and 100.0, the default value is 95.0.\n\n```\ntext_splitter = SemanticChunker(    OpenAIEmbeddings(), breakpoint_threshold_type=\"gradient\")\n```\n\n```\ndocs = text_splitter.create_documents([state_of_the_union])print(docs[0].page_content)\n```\n\n```\nMadam Speaker, Madam Vice President, our First Lady and Second Gentleman.\n```\n\n```\nprint(len(docs))\n```\n\n```\n26\n```\n\n- Percentile\n- Standard Deviation\n- Interquartile\n- Gradient", "url_final": "https://python.langchain.com/docs/how_to/semantic-chunker/", "url_citable": "https://python.langchain.com/docs/how_to/semantic-chunker/#gradient", "title": "How to split text based on semantic similarity | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Gradient‚Äã", "section_anchor": "gradient", "fetched_at": "2025-10-06T21:56:19.078733", "has_code": true}
{"id": "615d6338dbc35e509e3bde9cb497418c9b8c946c", "text": "Next steps‚Äã\n\nYou've now learned how to create your own custom callback handlers.\n\nNext, check out the other how-to guides in this section, such as how to attach callbacks to a runnable.\n\n- Next steps", "url_final": "https://python.langchain.com/docs/how_to/callbacks_async/", "url_citable": "https://python.langchain.com/docs/how_to/callbacks_async/#next-steps", "title": "How to use callbacks in async environments | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Next steps‚Äã", "section_anchor": "next-steps", "fetched_at": "2025-10-06T21:54:57.832143", "has_code": false}
{"id": "f61f823171020bfec1fcd24a19008a8122d6b60b", "text": "Astream Events API‚Äã\n\nThe most useful way to consume custom events is via the Astream Events API.\n\nWe can use the async adispatch_custom_event API to emit custom events in an async setting.\n\nTo see custom events via the astream events API, you need to use the newer v2 API of astream_events.\n\n```\nfrom langchain_core.callbacks.manager import (    adispatch_custom_event,)from langchain_core.runnables import RunnableLambdafrom langchain_core.runnables.config import RunnableConfig@RunnableLambdaasync def foo(x: str) -> str:    await adispatch_custom_event(\"event1\", {\"x\": x})    await adispatch_custom_event(\"event2\", 5)    return xasync for event in foo.astream_events(\"hello world\", version=\"v2\"):    print(event)\n```\n\n```\n{'event': 'on_chain_start', 'data': {'input': 'hello world'}, 'name': 'foo', 'tags': [], 'run_id': 'f354ffe8-4c22-4881-890a-c1cad038a9a6', 'metadata': {}, 'parent_ids': []}{'event': 'on_custom_event', 'run_id': 'f354ffe8-4c22-4881-890a-c1cad038a9a6', 'name': 'event1', 'tags': [], 'metadata': {}, 'data': {'x': 'hello world'}, 'parent_ids': []}{'event': 'on_custom_event', 'run_id': 'f354ffe8-4c22-4881-890a-c1cad038a9a6', 'name': 'event2', 'tags': [], 'metadata': {}, 'data': 5, 'parent_ids': []}{'event': 'on_chain_stream', 'run_id': 'f354ffe8-4c22-4881-890a-c1cad038a9a6', 'name': 'foo', 'tags': [], 'metadata': {}, 'data': {'chunk': 'hello world'}, 'parent_ids': []}{'event': 'on_chain_end', 'data': {'output': 'hello world'}, 'run_id': 'f354ffe8-4c22-4881-890a-c1cad038a9a6', 'name': 'foo', 'tags': [], 'metadata': {}, 'parent_ids': []}\n```\n\nIn python <= 3.10, you must propagate the config manually!\n\n```\nfrom langchain_core.callbacks.manager import (    adispatch_custom_event,)from langchain_core.runnables import RunnableLambdafrom langchain_core.runnables.config import RunnableConfig@RunnableLambdaasync def bar(x: str, config: RunnableConfig) -> str:    \"\"\"An example that shows how to manually propagate config.    You must do this if you're running python<=3.10.    \"\"\"    await adispatch_custom_event(\"event1\", {\"x\": x}, config=config)    await adispatch_custom_event(\"event2\", 5, config=config)    return xasync for event in bar.astream_events(\"hello world\", version=\"v2\"):    print(event)\n```\n\n```\n{'event': 'on_chain_start', 'data': {'input': 'hello world'}, 'name': 'bar', 'tags': [], 'run_id': 'c787b09d-698a-41b9-8290-92aaa656f3e7', 'metadata': {}, 'parent_ids': []}{'event': 'on_custom_event', 'run_id': 'c787b09d-698a-41b9-8290-92aaa656f3e7', 'name': 'event1', 'tags': [], 'metadata': {}, 'data': {'x': 'hello world'}, 'parent_ids': []}{'event': 'on_custom_event', 'run_id': 'c787b09d-698a-41b9-8290-92aaa656f3e7', 'name': 'event2', 'tags': [], 'metadata': {}, 'data': 5, 'parent_ids': []}{'event': 'on_chain_stream', 'run_id': 'c787b09d-698a-41b9-8290-92aaa656f3e7', 'name': 'bar', 'tags': [], 'metadata': {}, 'data': {'chunk': 'hello world'}, 'parent_ids': []}{'event': 'on_chain_end', 'data': {'output': 'hello world'}, 'run_id': 'c787b09d-698a-41b9-8290-92aaa656f3e7', 'name': 'bar', 'tags': [], 'metadata': {}, 'parent_ids': []}\n```", "url_final": "https://python.langchain.com/docs/how_to/callbacks_custom_events/", "url_citable": "https://python.langchain.com/docs/how_to/callbacks_custom_events/#astream-events-api", "title": "How to dispatch custom callback events | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Astream Events API‚Äã", "section_anchor": "astream-events-api", "fetched_at": "2025-10-06T21:55:00.967143", "has_code": true}
{"id": "d00f1526309144e6d6485e3270e5e6ced46eeed2", "text": "Async Callback Handler‚Äã\n\nYou can also consume the dispatched event via an async callback handler.\n\n```\nfrom typing import Any, Dict, List, Optionalfrom uuid import UUIDfrom langchain_core.callbacks import AsyncCallbackHandlerfrom langchain_core.callbacks.manager import (    adispatch_custom_event,)from langchain_core.runnables import RunnableLambdafrom langchain_core.runnables.config import RunnableConfigclass AsyncCustomCallbackHandler(AsyncCallbackHandler):    async def on_custom_event(        self,        name: str,        data: Any,        *,        run_id: UUID,        tags: Optional[List[str]] = None,        metadata: Optional[Dict[str, Any]] = None,        **kwargs: Any,    ) -> None:        print(            f\"Received event {name} with data: {data}, with tags: {tags}, with metadata: {metadata} and run_id: {run_id}\"        )@RunnableLambdaasync def bar(x: str, config: RunnableConfig) -> str:    \"\"\"An example that shows how to manually propagate config.    You must do this if you're running python<=3.10.    \"\"\"    await adispatch_custom_event(\"event1\", {\"x\": x}, config=config)    await adispatch_custom_event(\"event2\", 5, config=config)    return xasync_handler = AsyncCustomCallbackHandler()await foo.ainvoke(1, {\"callbacks\": [async_handler], \"tags\": [\"foo\", \"bar\"]})\n```\n\n```\nReceived event event1 with data: {'x': 1}, with tags: ['foo', 'bar'], with metadata: {} and run_id: a62b84be-7afd-4829-9947-7165df1f37d9Received event event2 with data: 5, with tags: ['foo', 'bar'], with metadata: {} and run_id: a62b84be-7afd-4829-9947-7165df1f37d9\n```\n\n```\n1\n```", "url_final": "https://python.langchain.com/docs/how_to/callbacks_custom_events/", "url_citable": "https://python.langchain.com/docs/how_to/callbacks_custom_events/#async-callback-handler", "title": "How to dispatch custom callback events | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Async Callback Handler‚Äã", "section_anchor": "async-callback-handler", "fetched_at": "2025-10-06T21:55:00.967143", "has_code": true}
{"id": "93895927128fc3d37e6aea114d43152bbe1bae09", "text": "Sync Callback Handler‚Äã\n\nLet's see how to emit custom events in a sync environment using dispatch_custom_event.\n\nYou must call dispatch_custom_event from within an existing Runnable.\n\n```\nfrom typing import Any, Dict, List, Optionalfrom uuid import UUIDfrom langchain_core.callbacks import BaseCallbackHandlerfrom langchain_core.callbacks.manager import (    dispatch_custom_event,)from langchain_core.runnables import RunnableLambdafrom langchain_core.runnables.config import RunnableConfigclass CustomHandler(BaseCallbackHandler):    def on_custom_event(        self,        name: str,        data: Any,        *,        run_id: UUID,        tags: Optional[List[str]] = None,        metadata: Optional[Dict[str, Any]] = None,        **kwargs: Any,    ) -> None:        print(            f\"Received event {name} with data: {data}, with tags: {tags}, with metadata: {metadata} and run_id: {run_id}\"        )@RunnableLambdadef foo(x: int, config: RunnableConfig) -> int:    dispatch_custom_event(\"event1\", {\"x\": x})    dispatch_custom_event(\"event2\", {\"x\": x})    return xhandler = CustomHandler()foo.invoke(1, {\"callbacks\": [handler], \"tags\": [\"foo\", \"bar\"]})\n```\n\n```\nReceived event event1 with data: {'x': 1}, with tags: ['foo', 'bar'], with metadata: {} and run_id: 27b5ce33-dc26-4b34-92dd-08a89cb22268Received event event2 with data: {'x': 1}, with tags: ['foo', 'bar'], with metadata: {} and run_id: 27b5ce33-dc26-4b34-92dd-08a89cb22268\n```\n\n```\n1\n```", "url_final": "https://python.langchain.com/docs/how_to/callbacks_custom_events/", "url_citable": "https://python.langchain.com/docs/how_to/callbacks_custom_events/#sync-callback-handler", "title": "How to dispatch custom callback events | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Sync Callback Handler‚Äã", "section_anchor": "sync-callback-handler", "fetched_at": "2025-10-06T21:55:00.967143", "has_code": true}
{"id": "a63244e055876ada2ff9db6e562ca7275d8f0471", "text": "Next steps‚Äã\n\nYou've seen how to emit custom events, you can check out the more in depth guide for astream events which is the easiest way to leverage custom events.\n\n- Astream Events API\n- Async Callback Handler\n- Sync Callback Handler\n- Next steps", "url_final": "https://python.langchain.com/docs/how_to/callbacks_custom_events/", "url_citable": "https://python.langchain.com/docs/how_to/callbacks_custom_events/#next-steps", "title": "How to dispatch custom callback events | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Next steps‚Äã", "section_anchor": "next-steps", "fetched_at": "2025-10-06T21:55:00.967143", "has_code": false}
{"id": "cc723dc4517b27f87cd05bfd30b568c1d845fe44", "text": "Get started‚Äã\n\nSetup‚Äã\n\n```\npip install -qU langchain-openai\n```\n\n```\nimport getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"):  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")from langchain_openai import OpenAIEmbeddingsembeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n```\n\nembed_documents‚Äã\n\nUse .embed_documents to embed a list of strings, recovering a list of embeddings:\n\n```\nembeddings = embeddings_model.embed_documents(    [        \"Hi there!\",        \"Oh, hello!\",        \"What's your name?\",        \"My friends call me World\",        \"Hello World!\"    ])len(embeddings), len(embeddings[0])\n```\n\n```\n(5, 1536)\n```\n\nembed_query‚Äã\n\nUse .embed_query to embed a single piece of text (e.g., for the purpose of comparing to other embedded pieces of texts).\n\n```\nembedded_query = embeddings_model.embed_query(\"What was the name mentioned in the conversation?\")embedded_query[:5]\n```\n\n```\n[0.0053587136790156364, -0.0004999046213924885, 0.038883671164512634, -0.003001077566295862, -0.00900818221271038]\n```\n\n- Setup\n- embed_documents\n- embed_query", "url_final": "https://python.langchain.com/docs/how_to/embed_text/", "url_citable": "https://python.langchain.com/docs/how_to/embed_text/#get-started", "title": "Text embedding models | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Get started‚Äã", "section_anchor": "get-started", "fetched_at": "2025-10-06T21:55:29.911481", "has_code": true}
{"id": "f9583ded81767af08ee3b23d532e293702155ee8", "text": "Setup‚Äã\n\nFirst, get required packages and set environment variables:\n\n```\n%pip install --upgrade --quiet  langchain langchain-community langchain-openai\n```\n\n```\n# Uncomment the below to use LangSmith. Not required.# import os# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n```\n\nThe below example will use a SQLite connection with Chinook database. Follow these installation steps to create Chinook.db in the same directory as this notebook:\n\n- Save this file as Chinook_Sqlite.sql\n- Run sqlite3 Chinook.db\n- Run .read Chinook_Sqlite.sql\n- Test SELECT * FROM Artist LIMIT 10;\n\nNow, Chinook.db is in our directory and we can interface with it using the SQLAlchemy-driven SQLDatabase class:\n\n```\nfrom langchain_community.utilities import SQLDatabasedb = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")print(db.dialect)print(db.get_usable_table_names())print(db.run(\"SELECT * FROM Artist LIMIT 10;\"))\n```\n\n```\nsqlite['Album', 'Artist', 'Customer', 'Employee', 'Genre', 'Invoice', 'InvoiceLine', 'MediaType', 'Playlist', 'PlaylistTrack', 'Track'][(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains'), (6, 'Ant√¥nio Carlos Jobim'), (7, 'Apocalyptica'), (8, 'Audioslave'), (9, 'BackBeat'), (10, 'Billy Cobham')]\n```", "url_final": "https://python.langchain.com/docs/how_to/sql_large_db/", "url_citable": "https://python.langchain.com/docs/how_to/sql_large_db/#setup", "title": "How to deal with large databases when doing SQL question-answering | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Setup‚Äã", "section_anchor": "setup", "fetched_at": "2025-10-06T21:56:25.487561", "has_code": true}
{"id": "a24c04839e0b6ceac239e37bf481ec810b648d58", "text": "Many tables‚Äã\n\nOne of the main pieces of information we need to include in our prompt is the schemas of the relevant tables. When we have very many tables, we can't fit all of the schemas in a single prompt. What we can do in such cases is first extract the names of the tables related to the user input, and then include only their schemas.\n\nOne easy and reliable way to do this is using tool-calling. Below, we show how we can use this feature to obtain output conforming to a desired format (in this case, a list of table names). We use the chat model's .bind_tools method to bind a tool in Pydantic format, and feed this into an output parser to reconstruct the object from the model's response.\n\n```\npip install -qU \"langchain[google-genai]\"\n```\n\n```\nimport getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n```\n\n```\nfrom langchain_core.output_parsers.openai_tools import PydanticToolsParserfrom langchain_core.prompts import ChatPromptTemplatefrom pydantic import BaseModel, Fieldclass Table(BaseModel):    \"\"\"Table in SQL database.\"\"\"    name: str = Field(description=\"Name of table in SQL database.\")table_names = \"\\n\".join(db.get_usable_table_names())system = f\"\"\"Return the names of ALL the SQL tables that MIGHT be relevant to the user question. \\The tables are:{table_names}Remember to include ALL POTENTIALLY RELEVANT tables, even if you're not sure that they're needed.\"\"\"prompt = ChatPromptTemplate.from_messages(    [        (\"system\", system),        (\"human\", \"{input}\"),    ])llm_with_tools = llm.bind_tools([Table])output_parser = PydanticToolsParser(tools=[Table])table_chain = prompt | llm_with_tools | output_parsertable_chain.invoke({\"input\": \"What are all the genres of Alanis Morissette songs\"})\n```\n\n```\n[Table(name='Genre')]\n```\n\nThis works pretty well! Except, as we'll see below, we actually need a few other tables as well. This would be pretty difficult for the model to know based just on the user question. In this case, we might think to simplify our model's job by grouping the tables together. We'll just ask the model to choose between categories \"Music\" and \"Business\", and then take care of selecting all the relevant tables from there:\n\n```\nsystem = \"\"\"Return the names of any SQL tables that are relevant to the user question.The tables are:MusicBusiness\"\"\"prompt = ChatPromptTemplate.from_messages(    [        (\"system\", system),        (\"human\", \"{input}\"),    ])category_chain = prompt | llm_with_tools | output_parsercategory_chain.invoke({\"input\": \"What are all the genres of Alanis Morissette songs\"})\n```\n\n```\n[Table(name='Music'), Table(name='Business')]\n```\n\n```\nfrom typing import Listdef get_tables(categories: List[Table]) -> List[str]:    tables = []    for category in categories:        if category.name == \"Music\":            tables.extend(                [                    \"Album\",                    \"Artist\",                    \"Genre\",                    \"MediaType\",                    \"Playlist\",                    \"PlaylistTrack\",                    \"Track\",                ]            )        elif category.name == \"Business\":            tables.extend([\"Customer\", \"Employee\", \"Invoice\", \"InvoiceLine\"])    return tablestable_chain = category_chain | get_tablestable_chain.invoke({\"input\": \"What are all the genres of Alanis Morissette songs\"})\n```\n\n```\n['Album', 'Artist', 'Genre', 'MediaType', 'Playlist', 'PlaylistTrack', 'Track', 'Customer', 'Employee', 'Invoice', 'InvoiceLine']\n```\n\nNow that we've got a chain that can output the relevant tables for any query we can combine this with our create_sql_query_chain, which can accept a list of table_names_to_use to determine which table schemas are included in the prompt:\n\n```\nfrom operator import itemgetterfrom langchain.chains import create_sql_query_chainfrom langchain_core.runnables import RunnablePassthroughquery_chain = create_sql_query_chain(llm, db)# Convert \"question\" key to the \"input\" key expected by current table_chain.table_chain = {\"input\": itemgetter(\"question\")} | table_chain# Set table_names_to_use using table_chain.full_chain = RunnablePassthrough.assign(table_names_to_use=table_chain) | query_chain\n```\n\n```\nquery = full_chain.invoke(    {\"question\": \"What are all the genres of Alanis Morissette songs\"})print(query)\n```\n\n```\nSELECT DISTINCT \"g\".\"Name\"FROM \"Genre\" gJOIN \"Track\" t ON \"g\".\"GenreId\" = \"t\".\"GenreId\"JOIN \"Album\" a ON \"t\".\"AlbumId\" = \"a\".\"AlbumId\"JOIN \"Artist\" ar ON \"a\".\"ArtistId\" = \"ar\".\"ArtistId\"WHERE \"ar\".\"Name\" = 'Alanis Morissette'LIMIT 5;\n```\n\n```\ndb.run(query)\n```\n\n```\n\"[('Rock',)]\"\n```\n\nWe can see the LangSmith trace for this run here.\n\nWe've seen how to dynamically include a subset of table schemas in a prompt within a chain. Another possible approach to this problem is to let an Agent decide for itself when to look up tables by giving it a Tool to do so. You can see an example of this in the SQL: Agents guide.", "url_final": "https://python.langchain.com/docs/how_to/sql_large_db/", "url_citable": "https://python.langchain.com/docs/how_to/sql_large_db/#many-tables", "title": "How to deal with large databases when doing SQL question-answering | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Many tables‚Äã", "section_anchor": "many-tables", "fetched_at": "2025-10-06T21:56:25.487561", "has_code": true}
{"id": "34c8d201339a91fad005a44425f20da8edc6e349", "text": "High-cardinality columns‚Äã\n\nIn order to filter columns that contain proper nouns such as addresses, song names or artists, we first need to double-check the spelling in order to filter the data correctly.\n\nOne naive strategy it to create a vector store with all the distinct proper nouns that exist in the database. We can then query that vector store each user input and inject the most relevant proper nouns into the prompt.\n\nFirst we need the unique values for each entity we want, for which we define a function that parses the result into a list of elements:\n\n```\nimport astimport redef query_as_list(db, query):    res = db.run(query)    res = [el for sub in ast.literal_eval(res) for el in sub if el]    res = [re.sub(r\"\\b\\d+\\b\", \"\", string).strip() for string in res]    return resproper_nouns = query_as_list(db, \"SELECT Name FROM Artist\")proper_nouns += query_as_list(db, \"SELECT Title FROM Album\")proper_nouns += query_as_list(db, \"SELECT Name FROM Genre\")len(proper_nouns)proper_nouns[:5]\n```\n\n```\n['AC/DC', 'Accept', 'Aerosmith', 'Alanis Morissette', 'Alice In Chains']\n```\n\nNow we can embed and store all of our values in a vector database:\n\n```\nfrom langchain_community.vectorstores import FAISSfrom langchain_openai import OpenAIEmbeddingsvector_db = FAISS.from_texts(proper_nouns, OpenAIEmbeddings())retriever = vector_db.as_retriever(search_kwargs={\"k\": 15})\n```\n\nAnd put together a query construction chain that first retrieves values from the database and inserts them into the prompt:\n\n```\nfrom operator import itemgetterfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnablePassthroughsystem = \"\"\"You are a SQLite expert. Given an input question, create a syntacticallycorrect SQLite query to run. Unless otherwise specificed, do not return more than{top_k} rows.Only return the SQL query with no markup or explanation.Here is the relevant table info: {table_info}Here is a non-exhaustive list of possible feature values. If filtering on a featurevalue make sure to check its spelling against this list first:{proper_nouns}\"\"\"prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", \"{input}\")])query_chain = create_sql_query_chain(llm, db, prompt=prompt)retriever_chain = (    itemgetter(\"question\")    | retriever    | (lambda docs: \"\\n\".join(doc.page_content for doc in docs)))chain = RunnablePassthrough.assign(proper_nouns=retriever_chain) | query_chain\n```\n\nTo try out our chain, let's see what happens when we try filtering on \"elenis moriset\", a misspelling of Alanis Morissette, without and with retrieval:\n\n```\n# Without retrievalquery = query_chain.invoke(    {\"question\": \"What are all the genres of elenis moriset songs\", \"proper_nouns\": \"\"})print(query)db.run(query)\n```\n\n```\nSELECT DISTINCT g.Name FROM Track tJOIN Album a ON t.AlbumId = a.AlbumIdJOIN Artist ar ON a.ArtistId = ar.ArtistIdJOIN Genre g ON t.GenreId = g.GenreIdWHERE ar.Name = 'Elenis Moriset';\n```\n\n```\n''\n```\n\n```\n# With retrievalquery = chain.invoke({\"question\": \"What are all the genres of elenis moriset songs\"})print(query)db.run(query)\n```\n\n```\nSELECT DISTINCT g.NameFROM Genre gJOIN Track t ON g.GenreId = t.GenreIdJOIN Album a ON t.AlbumId = a.AlbumIdJOIN Artist ar ON a.ArtistId = ar.ArtistIdWHERE ar.Name = 'Alanis Morissette';\n```\n\n```\n\"[('Rock',)]\"\n```\n\nWe can see that with retrieval we're able to correct the spelling from \"Elenis Moriset\" to \"Alanis Morissette\" and get back a valid result.\n\nAnother possible approach to this problem is to let an Agent decide for itself when to look up proper nouns. You can see an example of this in the SQL: Agents guide.\n\n- Setup\n- Many tables\n- High-cardinality columns", "url_final": "https://python.langchain.com/docs/how_to/sql_large_db/", "url_citable": "https://python.langchain.com/docs/how_to/sql_large_db/#high-cardinality-columns", "title": "How to deal with large databases when doing SQL question-answering | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "High-cardinality columns‚Äã", "section_anchor": "high-cardinality-columns", "fetched_at": "2025-10-06T21:56:25.487561", "has_code": true}
{"id": "6a031b9a0803ece74770e32c9bdd4344101cd581", "text": "Setup‚Äã\n\nWe'll need to install the following packages:\n\n```\n%pip install --upgrade --quiet langchain\n```\n\nAnd set these environment variables:\n\n```\nimport getpassimport os# If you'd like to use LangSmith, uncomment the below:# os.environ[\"LANGSMITH_TRACING\"] = \"true\"# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n```", "url_final": "https://python.langchain.com/docs/how_to/tools_human/", "url_citable": "https://python.langchain.com/docs/how_to/tools_human/#setup", "title": "How to add a human-in-the-loop for tools | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Setup‚Äã", "section_anchor": "setup", "fetched_at": "2025-10-06T21:56:44.995554", "has_code": true}
{"id": "90240d1ac197a4cd80b8b563d037a8b77ee12d02", "text": "Chain‚Äã\n\nLet's create a few simple (dummy) tools and a tool-calling chain:\n\n```\npip install -qU \"langchain[google-genai]\"\n```\n\n```\nimport getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n```\n\n```\nfrom typing import Dict, Listfrom langchain_core.messages import AIMessagefrom langchain_core.runnables import Runnable, RunnablePassthroughfrom langchain_core.tools import tool@tooldef count_emails(last_n_days: int) -> int:    \"\"\"Dummy function to count number of e-mails. Returns 2 * last_n_days.\"\"\"    return last_n_days * 2@tooldef send_email(message: str, recipient: str) -> str:    \"\"\"Dummy function for sending an e-mail.\"\"\"    return f\"Successfully sent email to {recipient}.\"tools = [count_emails, send_email]llm_with_tools = llm.bind_tools(tools)def call_tools(msg: AIMessage) -> List[Dict]:    \"\"\"Simple sequential tool calling helper.\"\"\"    tool_map = {tool.name: tool for tool in tools}    tool_calls = msg.tool_calls.copy()    for tool_call in tool_calls:        tool_call[\"output\"] = tool_map[tool_call[\"name\"]].invoke(tool_call[\"args\"])    return tool_callschain = llm_with_tools | call_toolschain.invoke(\"how many emails did i get in the last 5 days?\")\n```\n\n```\n[{'name': 'count_emails',  'args': {'last_n_days': 5},  'id': 'toolu_01XrE4AU9QLo4imbriDDkmXm',  'type': 'tool_call',  'output': 10}]\n```", "url_final": "https://python.langchain.com/docs/how_to/tools_human/", "url_citable": "https://python.langchain.com/docs/how_to/tools_human/#chain", "title": "How to add a human-in-the-loop for tools | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Chain‚Äã", "section_anchor": "chain", "fetched_at": "2025-10-06T21:56:44.995554", "has_code": true}
{"id": "e752dc6f34ea51a5fdc2dd1ed38939dc6967ab2c", "text": "Adding human approval‚Äã\n\nLet's add a step in the chain that will ask a person to approve or reject the tool call request.\n\nOn rejection, the step will raise an exception which will stop execution of the rest of the chain.\n\n```\nimport jsonclass NotApproved(Exception):    \"\"\"Custom exception.\"\"\"def human_approval(msg: AIMessage) -> AIMessage:    \"\"\"Responsible for passing through its input or raising an exception.    Args:        msg: output from the chat model    Returns:        msg: original output from the msg    \"\"\"    tool_strs = \"\\n\\n\".join(        json.dumps(tool_call, indent=2) for tool_call in msg.tool_calls    )    input_msg = (        f\"Do you approve of the following tool invocations\\n\\n{tool_strs}\\n\\n\"        \"Anything except 'Y'/'Yes' (case-insensitive) will be treated as a no.\\n >>>\"    )    resp = input(input_msg)    if resp.lower() not in (\"yes\", \"y\"):        raise NotApproved(f\"Tool invocations not approved:\\n\\n{tool_strs}\")    return msg\n```\n\n```\nchain = llm_with_tools | human_approval | call_toolschain.invoke(\"how many emails did i get in the last 5 days?\")\n```\n\n```\nDo you approve of the following tool invocations{  \"name\": \"count_emails\",  \"args\": {    \"last_n_days\": 5  },  \"id\": \"toolu_01WbD8XeMoQaRFtsZezfsHor\"}Anything except 'Y'/'Yes' (case-insensitive) will be treated as a no. >>> yes\n```\n\n```\n[{'name': 'count_emails',  'args': {'last_n_days': 5},  'id': 'toolu_01WbD8XeMoQaRFtsZezfsHor',  'output': 10}]\n```\n\n```\ntry:    chain.invoke(\"Send sally@gmail.com an email saying 'What's up homie'\")except NotApproved as e:    print()    print(e)\n```\n\n```\nDo you approve of the following tool invocations{  \"name\": \"send_email\",  \"args\": {    \"recipient\": \"sally@gmail.com\",    \"message\": \"What's up homie\"  },  \"id\": \"toolu_014XccHFzBiVcc9GV1harV9U\"}Anything except 'Y'/'Yes' (case-insensitive) will be treated as a no. >>> no``````outputTool invocations not approved:{  \"name\": \"send_email\",  \"args\": {    \"recipient\": \"sally@gmail.com\",    \"message\": \"What's up homie\"  },  \"id\": \"toolu_014XccHFzBiVcc9GV1harV9U\"}\n```\n\n- Setup\n- Chain\n- Adding human approval", "url_final": "https://python.langchain.com/docs/how_to/tools_human/", "url_citable": "https://python.langchain.com/docs/how_to/tools_human/#adding-human-approval", "title": "How to add a human-in-the-loop for tools | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Adding human approval‚Äã", "section_anchor": "adding-human-approval", "fetched_at": "2025-10-06T21:56:44.995554", "has_code": true}
{"id": "cdf4172e41edc23e756f93f1b555091c5dd57ac2", "text": "Example Setup‚Äã\n\nFirst, let's create a chain that will identify incoming questions as being about LangChain, Anthropic, or Other:\n\n```\nfrom langchain_anthropic import ChatAnthropicfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import PromptTemplatechain = (    PromptTemplate.from_template(        \"\"\"Given the user question below, classify it as either being about `LangChain`, `Anthropic`, or `Other`.Do not respond with more than one word.<question>{question}</question>Classification:\"\"\"    )    | ChatAnthropic(model_name=\"claude-3-haiku-20240307\")    | StrOutputParser())chain.invoke({\"question\": \"how do I call Anthropic?\"})\n```\n\n```\n'Anthropic'\n```\n\nNow, let's create three sub chains:\n\n```\nlangchain_chain = PromptTemplate.from_template(    \"\"\"You are an expert in langchain. \\Always answer questions starting with \"As Harrison Chase told me\". \\Respond to the following question:Question: {question}Answer:\"\"\") | ChatAnthropic(model_name=\"claude-3-haiku-20240307\")anthropic_chain = PromptTemplate.from_template(    \"\"\"You are an expert in anthropic. \\Always answer questions starting with \"As Dario Amodei told me\". \\Respond to the following question:Question: {question}Answer:\"\"\") | ChatAnthropic(model_name=\"claude-3-haiku-20240307\")general_chain = PromptTemplate.from_template(    \"\"\"Respond to the following question:Question: {question}Answer:\"\"\") | ChatAnthropic(model_name=\"claude-3-haiku-20240307\")\n```", "url_final": "https://python.langchain.com/docs/how_to/routing/", "url_citable": "https://python.langchain.com/docs/how_to/routing/#example-setup", "title": "How to route between sub-chains | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Example Setup‚Äã", "section_anchor": "example-setup", "fetched_at": "2025-10-06T21:54:34.268194", "has_code": true}
{"id": "8b6452cd7911904eccfdb9967ae8d2333da9a108", "text": "Using a custom function (Recommended)‚Äã\n\nYou can also use a custom function to route between different outputs. Here's an example:\n\n```\ndef route(info):    if \"anthropic\" in info[\"topic\"].lower():        return anthropic_chain    elif \"langchain\" in info[\"topic\"].lower():        return langchain_chain    else:        return general_chain\n```\n\n```\nfrom langchain_core.runnables import RunnableLambdafull_chain = {\"topic\": chain, \"question\": lambda x: x[\"question\"]} | RunnableLambda(    route)\n```\n\n```\nfull_chain.invoke({\"question\": \"how do I use Anthropic?\"})\n```\n\n```\nAIMessage(content=\"As Dario Amodei told me, to use Anthropic, you can start by exploring the company's website and learning about their mission, values, and the different services and products they offer. Anthropic is focused on developing safe and ethical AI systems, so they have a strong emphasis on transparency and responsible AI development. \\n\\nDepending on your specific needs, you can look into Anthropic's AI research and development services, which cover areas like natural language processing, computer vision, and reinforcement learning. They also offer consulting and advisory services to help organizations navigate the challenges and opportunities of AI integration.\\n\\nAdditionally, Anthropic has released some open-source AI models and tools that you can explore and experiment with. These can be a great way to get hands-on experience with Anthropic's approach to AI development.\\n\\nOverall, Anthropic aims to be a reliable and trustworthy partner in the AI space, so I'd encourage you to reach out to them directly to discuss how they can best support your specific requirements.\", response_metadata={'id': 'msg_01CtLFgFSwvTaJomrihE87Ra', 'content': [ContentBlock(text=\"As Dario Amodei told me, to use Anthropic, you can start by exploring the company's website and learning about their mission, values, and the different services and products they offer. Anthropic is focused on developing safe and ethical AI systems, so they have a strong emphasis on transparency and responsible AI development. \\n\\nDepending on your specific needs, you can look into Anthropic's AI research and development services, which cover areas like natural language processing, computer vision, and reinforcement learning. They also offer consulting and advisory services to help organizations navigate the challenges and opportunities of AI integration.\\n\\nAdditionally, Anthropic has released some open-source AI models and tools that you can explore and experiment with. These can be a great way to get hands-on experience with Anthropic's approach to AI development.\\n\\nOverall, Anthropic aims to be a reliable and trustworthy partner in the AI space, so I'd encourage you to reach out to them directly to discuss how they can best support your specific requirements.\", type='text')], 'model': 'claude-3-haiku-20240307', 'role': 'assistant', 'stop_reason': 'end_turn', 'stop_sequence': None, 'type': 'message', 'usage': Usage(input_tokens=53, output_tokens=219)})\n```\n\n```\nfull_chain.invoke({\"question\": \"how do I use LangChain?\"})\n```\n\n```\nAIMessage(content=\"As Harrison Chase told me, using LangChain involves a few key steps:\\n\\n1. **Set up your environment**: Install the necessary Python packages, including the LangChain library itself, as well as any other dependencies your application might require, such as language models or other integrations.\\n\\n2. **Understand the core concepts**: LangChain revolves around a few core concepts, like Agents, Chains, and Tools. Familiarize yourself with these concepts and how they work together to build powerful language-based applications.\\n\\n3. **Identify your use case**: Determine what kind of task or application you want to build using LangChain, such as a chatbot, a question-answering system, or a document summarization tool.\\n\\n4. **Choose the appropriate components**: Based on your use case, select the right LangChain components, such as agents, chains, and tools, to build your application.\\n\\n5. **Integrate with language models**: LangChain is designed to work seamlessly with various language models, such as OpenAI's GPT-3 or Anthropic's models. Connect your chosen language model to your LangChain application.\\n\\n6. **Implement your application logic**: Use LangChain's building blocks to implement the specific functionality of your application, such as prompting the language model, processing the response, and integrating with other services or data sources.\\n\\n7. **Test and iterate**: Thoroughly test your application, gather feedback, and iterate on your design and implementation to improve its performance and user experience.\\n\\nAs Harrison Chase emphasized, LangChain provides a flexible and powerful framework for building language-based applications, making it easier to leverage the capabilities of modern language models. By following these steps, you can get started with LangChain and create innovative solutions tailored to your specific needs.\", response_metadata={'id': 'msg_01H3UXAAHG4TwxJLpxwuuVU7', 'content': [ContentBlock(text=\"As Harrison Chase told me, using LangChain involves a few key steps:\\n\\n1. **Set up your environment**: Install the necessary Python packages, including the LangChain library itself, as well as any other dependencies your application might require, such as language models or other integrations.\\n\\n2. **Understand the core concepts**: LangChain revolves around a few core concepts, like Agents, Chains, and Tools. Familiarize yourself with these concepts and how they work together to build powerful language-based applications.\\n\\n3. **Identify your use case**: Determine what kind of task or application you want to build using LangChain, such as a chatbot, a question-answering system, or a document summarization tool.\\n\\n4. **Choose the appropriate components**: Based on your use case, select the right LangChain components, such as agents, chains, and tools, to build your application.\\n\\n5. **Integrate with language models**: LangChain is designed to work seamlessly with various language models, such as OpenAI's GPT-3 or Anthropic's models. Connect your chosen language model to your LangChain application.\\n\\n6. **Implement your application logic**: Use LangChain's building blocks to implement the specific functionality of your application, such as prompting the language model, processing the response, and integrating with other services or data sources.\\n\\n7. **Test and iterate**: Thoroughly test your application, gather feedback, and iterate on your design and implementation to improve its performance and user experience.\\n\\nAs Harrison Chase emphasized, LangChain provides a flexible and powerful framework for building language-based applications, making it easier to leverage the capabilities of modern language models. By following these steps, you can get started with LangChain and create innovative solutions tailored to your specific needs.\", type='text')], 'model': 'claude-3-haiku-20240307', 'role': 'assistant', 'stop_reason': 'end_turn', 'stop_sequence': None, 'type': 'message', 'usage': Usage(input_tokens=50, output_tokens=400)})\n```\n\n```\nfull_chain.invoke({\"question\": \"whats 2 + 2\"})\n```\n\n```\nAIMessage(content='4', response_metadata={'id': 'msg_01UAKP81jTZu9fyiyFYhsbHc', 'content': [ContentBlock(text='4', type='text')], 'model': 'claude-3-haiku-20240307', 'role': 'assistant', 'stop_reason': 'end_turn', 'stop_sequence': None, 'type': 'message', 'usage': Usage(input_tokens=28, output_tokens=5)})\n```", "url_final": "https://python.langchain.com/docs/how_to/routing/", "url_citable": "https://python.langchain.com/docs/how_to/routing/#using-a-custom-function-recommended", "title": "How to route between sub-chains | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Using a custom function (Recommended)‚Äã", "section_anchor": "using-a-custom-function-recommended", "fetched_at": "2025-10-06T21:54:34.268194", "has_code": true}
{"id": "fa35fcd01fe3bd411c9e60080d9737884b25a64a", "text": "Using a RunnableBranch‚Äã\n\nA RunnableBranch is a special type of runnable that allows you to define a set of conditions and runnables to execute based on the input. It does not offer anything that you can't achieve in a custom function as described above, so we recommend using a custom function instead.\n\nA RunnableBranch is initialized with a list of (condition, runnable) pairs and a default runnable. It selects which branch by passing each condition the input it's invoked with. It selects the first condition to evaluate to True, and runs the corresponding runnable to that condition with the input.\n\nIf no provided conditions match, it runs the default runnable.\n\nHere's an example of what it looks like in action:\n\n```\nfrom langchain_core.runnables import RunnableBranchbranch = RunnableBranch(    (lambda x: \"anthropic\" in x[\"topic\"].lower(), anthropic_chain),    (lambda x: \"langchain\" in x[\"topic\"].lower(), langchain_chain),    general_chain,)full_chain = {\"topic\": chain, \"question\": lambda x: x[\"question\"]} | branchfull_chain.invoke({\"question\": \"how do I use Anthropic?\"})\n```\n\n```\nAIMessage(content=\"As Dario Amodei told me, to use Anthropic, you should first familiarize yourself with our mission and principles. Anthropic is committed to developing safe and beneficial artificial intelligence that can help solve important problems facing humanity. \\n\\nTo get started, I recommend exploring the resources on our website, which cover our research, products, and approach to AI development. You can also reach out to our team to learn more about how Anthropic's technology and services can support your specific needs.\\n\\nThe key is to engage with us in a way that aligns with our values of transparency, ethical AI, and a commitment to the wellbeing of humanity. We're here to collaborate and help you harness the power of advanced AI responsibly.\", response_metadata={'id': 'msg_0187BVnpniPDJnVvwf3M1LdY', 'content': [ContentBlock(text=\"As Dario Amodei told me, to use Anthropic, you should first familiarize yourself with our mission and principles. Anthropic is committed to developing safe and beneficial artificial intelligence that can help solve important problems facing humanity. \\n\\nTo get started, I recommend exploring the resources on our website, which cover our research, products, and approach to AI development. You can also reach out to our team to learn more about how Anthropic's technology and services can support your specific needs.\\n\\nThe key is to engage with us in a way that aligns with our values of transparency, ethical AI, and a commitment to the wellbeing of humanity. We're here to collaborate and help you harness the power of advanced AI responsibly.\", type='text')], 'model': 'claude-3-haiku-20240307', 'role': 'assistant', 'stop_reason': 'end_turn', 'stop_sequence': None, 'type': 'message', 'usage': Usage(input_tokens=53, output_tokens=160)})\n```\n\n```\nfull_chain.invoke({\"question\": \"how do I use LangChain?\"})\n```\n\n```\nAIMessage(content=\"As Harrison Chase told me, using LangChain involves several key steps. First, you'll need to install the LangChain library and import the necessary modules. Then, you'll want to define your language model, any data sources you plan to use, and the specific tasks you want to accomplish, such as question answering, text generation, or agent-based reasoning. \\n\\nLangChain provides a flexible framework for building applications that leverage large language models. It includes abstractions for things like retrievers, prompts, and chains, which allow you to compose different components together to create powerful workflows. \\n\\nThe documentation on the LangChain website is excellent and covers many common use cases in detail. I'd recommend starting there to get a solid understanding of the core concepts and how to apply them to your specific needs. And of course, feel free to reach out if you have any other questions - I'm always happy to share more insights from my conversations with Harrison.\", response_metadata={'id': 'msg_01T1naS99wGPkEAP4LME8iAv', 'content': [ContentBlock(text=\"As Harrison Chase told me, using LangChain involves several key steps. First, you'll need to install the LangChain library and import the necessary modules. Then, you'll want to define your language model, any data sources you plan to use, and the specific tasks you want to accomplish, such as question answering, text generation, or agent-based reasoning. \\n\\nLangChain provides a flexible framework for building applications that leverage large language models. It includes abstractions for things like retrievers, prompts, and chains, which allow you to compose different components together to create powerful workflows. \\n\\nThe documentation on the LangChain website is excellent and covers many common use cases in detail. I'd recommend starting there to get a solid understanding of the core concepts and how to apply them to your specific needs. And of course, feel free to reach out if you have any other questions - I'm always happy to share more insights from my conversations with Harrison.\", type='text')], 'model': 'claude-3-haiku-20240307', 'role': 'assistant', 'stop_reason': 'end_turn', 'stop_sequence': None, 'type': 'message', 'usage': Usage(input_tokens=50, output_tokens=205)})\n```\n\n```\nfull_chain.invoke({\"question\": \"whats 2 + 2\"})\n```\n\n```\nAIMessage(content='4', response_metadata={'id': 'msg_01T6T3TS6hRCtU8JayN93QEi', 'content': [ContentBlock(text='4', type='text')], 'model': 'claude-3-haiku-20240307', 'role': 'assistant', 'stop_reason': 'end_turn', 'stop_sequence': None, 'type': 'message', 'usage': Usage(input_tokens=28, output_tokens=5)})\n```", "url_final": "https://python.langchain.com/docs/how_to/routing/", "url_citable": "https://python.langchain.com/docs/how_to/routing/#using-a-runnablebranch", "title": "How to route between sub-chains | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Using a RunnableBranch‚Äã", "section_anchor": "using-a-runnablebranch", "fetched_at": "2025-10-06T21:54:34.268194", "has_code": true}
{"id": "095152e1706b2b3b5d3b7e954e5b0b8d3f43ba4b", "text": "Routing by semantic similarity‚Äã\n\nOne especially useful technique is to use embeddings to route a query to the most relevant prompt. Here's an example.\n\n```\nfrom langchain_community.utils.math import cosine_similarityfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import PromptTemplatefrom langchain_core.runnables import RunnableLambda, RunnablePassthroughfrom langchain_openai import OpenAIEmbeddingsphysics_template = \"\"\"You are a very smart physics professor. \\You are great at answering questions about physics in a concise and easy to understand manner. \\When you don't know the answer to a question you admit that you don't know.Here is a question:{query}\"\"\"math_template = \"\"\"You are a very good mathematician. You are great at answering math questions. \\You are so good because you are able to break down hard problems into their component parts, \\answer the component parts, and then put them together to answer the broader question.Here is a question:{query}\"\"\"embeddings = OpenAIEmbeddings()prompt_templates = [physics_template, math_template]prompt_embeddings = embeddings.embed_documents(prompt_templates)def prompt_router(input):    query_embedding = embeddings.embed_query(input[\"query\"])    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]    most_similar = prompt_templates[similarity.argmax()]    print(\"Using MATH\" if most_similar == math_template else \"Using PHYSICS\")    return PromptTemplate.from_template(most_similar)chain = (    {\"query\": RunnablePassthrough()}    | RunnableLambda(prompt_router)    | ChatAnthropic(model=\"claude-3-haiku-20240307\")    | StrOutputParser())\n```\n\n```\nprint(chain.invoke(\"What's a black hole\"))\n```\n\n```\nUsing PHYSICSAs a physics professor, I would be happy to provide a concise and easy-to-understand explanation of what a black hole is.A black hole is an incredibly dense region of space-time where the gravitational pull is so strong that nothing, not even light, can escape from it. This means that if you were to get too close to a black hole, you would be pulled in and crushed by the intense gravitational forces.The formation of a black hole occurs when a massive star, much larger than our Sun, reaches the end of its life and collapses in on itself. This collapse causes the matter to become extremely dense, and the gravitational force becomes so strong that it creates a point of no return, known as the event horizon.Beyond the event horizon, the laws of physics as we know them break down, and the intense gravitational forces create a singularity, which is a point of infinite density and curvature in space-time.Black holes are fascinating and mysterious objects, and there is still much to be learned about their properties and behavior. If I were unsure about any specific details or aspects of black holes, I would readily admit that I do not have a complete understanding and would encourage further research and investigation.\n```\n\n```\nprint(chain.invoke(\"What's a path integral\"))\n```\n\n```\nUsing MATHA path integral is a powerful mathematical concept in physics, particularly in the field of quantum mechanics. It was developed by the renowned physicist Richard Feynman as an alternative formulation of quantum mechanics.In a path integral, instead of considering a single, definite path that a particle might take from one point to another, as in classical mechanics, the particle is considered to take all possible paths simultaneously. Each path is assigned a complex-valued weight, and the total probability amplitude for the particle to go from one point to another is calculated by summing (integrating) over all possible paths.The key ideas behind the path integral formulation are:1. Superposition principle: In quantum mechanics, particles can exist in a superposition of multiple states or paths simultaneously.2. Probability amplitude: The probability amplitude for a particle to go from one point to another is calculated by summing the complex-valued weights of all possible paths.3. Weighting of paths: Each path is assigned a weight based on the action (the time integral of the Lagrangian) along that path. Paths with lower action have a greater weight.4. Feynman's approach: Feynman developed the path integral formulation as an alternative to the traditional wave function approach in quantum mechanics, providing a more intuitive and conceptual understanding of quantum phenomena.The path integral approach is particularly useful in quantum field theory, where it provides a powerful framework for calculating transition probabilities and understanding the behavior of quantum systems. It has also found applications in various areas of physics, such as condensed matter, statistical mechanics, and even in finance (the path integral approach to option pricing).The mathematical construction of the path integral involves the use of advanced concepts from functional analysis and measure theory, making it a powerful and sophisticated tool in the physicist's arsenal.\n```", "url_final": "https://python.langchain.com/docs/how_to/routing/", "url_citable": "https://python.langchain.com/docs/how_to/routing/#routing-by-semantic-similarity", "title": "How to route between sub-chains | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Routing by semantic similarity‚Äã", "section_anchor": "routing-by-semantic-similarity", "fetched_at": "2025-10-06T21:54:34.268194", "has_code": true}
{"id": "ede25252f15260b7a43f8d1d8afede3ccf8aeeeb", "text": "Next steps‚Äã\n\nYou've now learned how to add routing to your composed LCEL chains.\n\nNext, check out the other how-to guides on runnables in this section.\n\n- Example Setup\n- Using a custom function (Recommended)\n- Using a RunnableBranch\n- Routing by semantic similarity\n- Next steps", "url_final": "https://python.langchain.com/docs/how_to/routing/", "url_citable": "https://python.langchain.com/docs/how_to/routing/#next-steps", "title": "How to route between sub-chains | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Next steps‚Äã", "section_anchor": "next-steps", "fetched_at": "2025-10-06T21:54:34.268194", "has_code": false}
{"id": "fb4216008125b95268e7eb9c3a792cb8eaac233a", "text": "Installation‚Äã\n\n- How to: install LangChain packages\n- How to: use LangChain with different Pydantic versions", "url_final": "https://python.langchain.com/docs/how_to/", "url_citable": "https://python.langchain.com/docs/how_to/#installation", "title": "How-to guides | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Installation‚Äã", "section_anchor": "installation", "fetched_at": "2025-10-06T21:54:15.440556", "has_code": false}
{"id": "ed7c9be2e8a109c599c23e8e196d78ea711c2a84", "text": "Key features‚Äã\n\nThis highlights functionality that is core to using LangChain.\n\n- How to: return structured data from a model\n- How to: use a model to call tools\n- How to: stream runnables\n- How to: debug your LLM apps", "url_final": "https://python.langchain.com/docs/how_to/", "url_citable": "https://python.langchain.com/docs/how_to/#key-features", "title": "How-to guides | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Key features‚Äã", "section_anchor": "key-features", "fetched_at": "2025-10-06T21:54:15.440556", "has_code": false}
{"id": "4290a58fb6036c3b8f7f7f7cd0fe7ab4aeea1a7e", "text": "Components‚Äã\n\nThese are the core building blocks you can use when building applications.\n\nChat models‚Äã\n\nChat Models are newer forms of language models that take messages in and output a message. See supported integrations for details on getting started with chat models from a specific provider.\n\n- How to: initialize any model in one line\n- How to: work with local models\n- How to: do function/tool calling\n- How to: get models to return structured output\n- How to: cache model responses\n- How to: get log probabilities\n- How to: create a custom chat model class\n- How to: stream a response back\n- How to: track token usage\n- How to: track response metadata across providers\n- How to: use chat model to call tools\n- How to: stream tool calls\n- How to: handle rate limits\n- How to: few-shot prompt tool behavior\n- How to: bind model-specific formatted tools\n- How to: force a specific tool call\n- How to: pass multimodal data directly to models\n\nMessages‚Äã\n\nMessages are the input and output of chat models. They have some content and a role, which describes the source of the message.\n\n- How to: trim messages\n- How to: filter messages\n- How to: merge consecutive messages of the same type\n\nPrompt templates‚Äã\n\nPrompt Templates are responsible for formatting user input into a format that can be passed to a language model.\n\n- How to: use few-shot examples\n- How to: use few-shot examples in chat models\n- How to: partially format prompt templates\n- How to: compose prompts together\n- How to: use multimodal prompts\n\nExample selectors‚Äã\n\nExample Selectors are responsible for selecting the correct few-shot examples to pass to the prompt.\n\n- How to: use example selectors\n- How to: select examples by length\n- How to: select examples by semantic similarity\n- How to: select examples by semantic ngram overlap\n- How to: select examples by maximal marginal relevance\n- How to: select examples from LangSmith few-shot datasets\n\nLLMs‚Äã\n\nWhat LangChain calls LLMs are older forms of language models that take a string in and output a string.\n\n- How to: cache model responses\n- How to: create a custom LLM class\n- How to: stream a response back\n- How to: track token usage\n- How to: work with local models\n\nOutput parsers‚Äã\n\nOutput Parsers are responsible for taking the output of an LLM and parsing into more structured format.\n\n- How to: parse text from message objects\n- How to: use output parsers to parse an LLM response into structured format\n- How to: parse JSON output\n- How to: parse XML output\n- How to: parse YAML output\n- How to: retry when output parsing errors occur\n- How to: try to fix errors in output parsing\n- How to: write a custom output parser class\n\nDocument loaders‚Äã\n\nDocument Loaders are responsible for loading documents from a variety of sources.\n\n- How to: load PDF files\n- How to: load web pages\n- How to: load CSV data\n- How to: load data from a directory\n- How to: load HTML data\n- How to: load JSON data\n- How to: load Markdown data\n- How to: load Microsoft Office data\n- How to: write a custom document loader\n\nText splitters‚Äã\n\nText Splitters take a document and split into chunks that can be used for retrieval.\n\n- How to: recursively split text\n- How to: split HTML\n- How to: split by character\n- How to: split code\n- How to: split Markdown by headers\n- How to: recursively split JSON\n- How to: split text into semantic chunks\n- How to: split by tokens\n\nEmbedding models‚Äã\n\nEmbedding Models take a piece of text and create a numerical representation of it. See supported integrations for details on getting started with embedding models from a specific provider.\n\n- How to: embed text data\n- How to: cache embedding results\n- How to: create a custom embeddings class\n\nVector stores‚Äã\n\nVector stores are databases that can efficiently store and retrieve embeddings. See supported integrations for details on getting started with vector stores from a specific provider.\n\n- How to: use a vector store to retrieve data", "url_final": "https://python.langchain.com/docs/how_to/", "url_citable": "https://python.langchain.com/docs/how_to/#components", "title": "How-to guides | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Components‚Äã", "section_anchor": "components", "fetched_at": "2025-10-06T21:54:15.440556", "has_code": false}
{"id": "7eb28326f729d92a7c4b8e1283910895746eff83", "text": "Retrievers‚Äã\n\nRetrievers are responsible for taking a query and returning relevant documents.\n\n- How to: use a vector store to retrieve data\n- How to: generate multiple queries to retrieve data for\n- How to: use contextual compression to compress the data retrieved\n- How to: write a custom retriever class\n- How to: add similarity scores to retriever results\n- How to: combine the results from multiple retrievers\n- How to: reorder retrieved results to mitigate the \"lost in the middle\" effect\n- How to: generate multiple embeddings per document\n- How to: retrieve the whole document for a chunk\n- How to: generate metadata filters\n- How to: create a time-weighted retriever\n- How to: use hybrid vector and keyword retrieval", "url_final": "https://python.langchain.com/docs/how_to/", "url_citable": "https://python.langchain.com/docs/how_to/#retrievers", "title": "How-to guides | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Retrievers‚Äã", "section_anchor": "retrievers", "fetched_at": "2025-10-06T21:54:15.440556", "has_code": false}
{"id": "b41a7c24987069185133772a111f70cd06617630", "text": "Indexing‚Äã\n\nIndexing is the process of keeping your vectorstore in-sync with the underlying data source.\n\n- How to: reindex data to keep your vectorstore in-sync with the underlying data source", "url_final": "https://python.langchain.com/docs/how_to/", "url_citable": "https://python.langchain.com/docs/how_to/#indexing", "title": "How-to guides | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Indexing‚Äã", "section_anchor": "indexing", "fetched_at": "2025-10-06T21:54:15.440556", "has_code": false}
{"id": "52a3583af0a5a8878dfb9f67d7ec9aa80f3aa977", "text": "Tools‚Äã\n\nLangChain Tools contain a description of the tool (to pass to the language model) as well as the implementation of the function to call. Refer here for a list of pre-built tools.\n\n- How to: create tools\n- How to: use built-in tools and toolkits\n- How to: use chat models to call tools\n- How to: pass tool outputs to chat models\n- How to: pass runtime values to tools\n- How to: add a human-in-the-loop for tools\n- How to: handle tool errors\n- How to: force models to call a tool\n- How to: disable parallel tool calling\n- How to: access the RunnableConfig from a tool\n- How to: stream events from a tool\n- How to: return artifacts from a tool\n- How to: convert Runnables to tools\n- How to: add ad-hoc tool calling capability to models\n- How to: pass in runtime secrets", "url_final": "https://python.langchain.com/docs/how_to/", "url_citable": "https://python.langchain.com/docs/how_to/#tools", "title": "How-to guides | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Tools‚Äã", "section_anchor": "tools", "fetched_at": "2025-10-06T21:54:15.440556", "has_code": false}
{"id": "b11973b397f3e6e55fe1806b61e8d6ad67c327e9", "text": "Multimodal‚Äã\n\n- How to: pass multimodal data directly to models\n- How to: use multimodal prompts", "url_final": "https://python.langchain.com/docs/how_to/", "url_citable": "https://python.langchain.com/docs/how_to/#multimodal", "title": "How-to guides | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Multimodal‚Äã", "section_anchor": "multimodal", "fetched_at": "2025-10-06T21:54:15.440556", "has_code": false}
{"id": "246d9b7a98a5725f95af21682416e09fd0327950", "text": "Agents‚Äã\n\nFor in depth how-to guides for agents, please check out LangGraph documentation.\n\n- How to: use legacy LangChain Agents (AgentExecutor)\n- How to: migrate from legacy LangChain agents to LangGraph", "url_final": "https://python.langchain.com/docs/how_to/", "url_citable": "https://python.langchain.com/docs/how_to/#agents", "title": "How-to guides | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Agents‚Äã", "section_anchor": "agents", "fetched_at": "2025-10-06T21:54:15.440556", "has_code": false}
{"id": "d24a97cdd48dbd6af59a11370bfde0374033149f", "text": "Callbacks‚Äã\n\nCallbacks allow you to hook into the various stages of your LLM application's execution.\n\n- How to: pass in callbacks at runtime\n- How to: attach callbacks to a module\n- How to: pass callbacks into a module constructor\n- How to: create custom callback handlers\n- How to: use callbacks in async environments\n- How to: dispatch custom callback events", "url_final": "https://python.langchain.com/docs/how_to/", "url_citable": "https://python.langchain.com/docs/how_to/#callbacks", "title": "How-to guides | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Callbacks‚Äã", "section_anchor": "callbacks", "fetched_at": "2025-10-06T21:54:15.440556", "has_code": false}
{"id": "aaae646405ba20bfdf0e9438000e7d4426aa7644", "text": "Custom‚Äã\n\nAll of LangChain components can easily be extended to support your own versions.\n\n- How to: create a custom chat model class\n- How to: create a custom LLM class\n- How to: create a custom embeddings class\n- How to: write a custom retriever class\n- How to: write a custom document loader\n- How to: write a custom output parser class\n- How to: create custom callback handlers\n- How to: define a custom tool\n- How to: dispatch custom callback events", "url_final": "https://python.langchain.com/docs/how_to/", "url_citable": "https://python.langchain.com/docs/how_to/#custom", "title": "How-to guides | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Custom‚Äã", "section_anchor": "custom", "fetched_at": "2025-10-06T21:54:15.440556", "has_code": false}
{"id": "f0d2163d0288d4b8755ddb156c3fd7760ceaf6d7", "text": "Serialization‚Äã\n\n- How to: save and load LangChain objects", "url_final": "https://python.langchain.com/docs/how_to/", "url_citable": "https://python.langchain.com/docs/how_to/#serialization", "title": "How-to guides | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Serialization‚Äã", "section_anchor": "serialization", "fetched_at": "2025-10-06T21:54:15.440556", "has_code": false}
{"id": "a7efb717d4be2637e84179eb047cde8dcc12428f", "text": "Use cases‚Äã\n\nThese guides cover use-case specific details.\n\nQ&A with RAG‚Äã\n\nRetrieval Augmented Generation (RAG) is a way to connect LLMs to external sources of data. For a high-level tutorial on RAG, check out this guide.\n\n- How to: add chat history\n- How to: stream\n- How to: return sources\n- How to: return citations\n- How to: do per-user retrieval\n\nExtraction‚Äã\n\nExtraction is when you use LLMs to extract structured information from unstructured text. For a high level tutorial on extraction, check out this guide.\n\n- How to: use reference examples\n- How to: handle long text\n- How to: do extraction without using function calling\n\nChatbots‚Äã\n\nChatbots involve using an LLM to have a conversation. For a high-level tutorial on building chatbots, check out this guide.\n\n- How to: manage memory\n- How to: do retrieval\n- How to: use tools\n- How to: manage large chat history\n\nQuery analysis‚Äã\n\nQuery Analysis is the task of using an LLM to generate a query to send to a retriever. For a high-level tutorial on query analysis, check out this guide.\n\n- How to: add examples to the prompt\n- How to: handle cases where no queries are generated\n- How to: handle multiple queries\n- How to: handle multiple retrievers\n- How to: construct filters\n- How to: deal with high cardinality categorical variables\n\nQ&A over SQL + CSV‚Äã\n\nYou can use LLMs to do question answering over tabular data. For a high-level tutorial, check out this guide.\n\n- How to: use prompting to improve results\n- How to: do query validation\n- How to: deal with large databases\n- How to: deal with CSV files\n\nQ&A over graph databases‚Äã\n\nYou can use an LLM to do question answering over graph databases. For a high-level tutorial, check out this guide.\n\n- How to: add a semantic layer over a database\n- How to: construct knowledge graphs\n\nSummarization‚Äã\n\nLLMs can summarize and otherwise distill desired information from text, including large volumes of text. For a high-level tutorial, check out this guide.\n\n- How to: summarize text in a single LLM call\n- How to: summarize text through parallelization\n- How to: summarize text through iterative refinement", "url_final": "https://python.langchain.com/docs/how_to/", "url_citable": "https://python.langchain.com/docs/how_to/#use-cases", "title": "How-to guides | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Use cases‚Äã", "section_anchor": "use-cases", "fetched_at": "2025-10-06T21:54:15.440556", "has_code": false}
{"id": "ee286af3967b0d985f95a57226e0e5a55add75f1", "text": "LangChain Expression Language (LCEL)‚Äã\n\nLCEL is an orchestration solution. See our concepts page for recommendations on when to use LCEL.\n\nLangChain Expression Language is a way to create arbitrary custom chains. It is built on the Runnable protocol.\n\nLCEL cheatsheet: For a quick overview of how to use the main LCEL primitives.\n\nMigration guide: For migrating legacy chain abstractions to LCEL.\n\n- How to: chain runnables\n- How to: stream runnables\n- How to: invoke runnables in parallel\n- How to: add default invocation args to runnables\n- How to: turn any function into a runnable\n- How to: pass through inputs from one chain step to the next\n- How to: configure runnable behavior at runtime\n- How to: add message history (memory) to a chain\n- How to: route between sub-chains\n- How to: create a dynamic (self-constructing) chain\n- How to: inspect runnables\n- How to: add fallbacks to a runnable\n- How to: pass runtime secrets to a runnable", "url_final": "https://python.langchain.com/docs/how_to/", "url_citable": "https://python.langchain.com/docs/how_to/#langchain-expression-language-lcel", "title": "How-to guides | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "LangChain Expression Language (LCEL)‚Äã", "section_anchor": "langchain-expression-language-lcel", "fetched_at": "2025-10-06T21:54:15.440556", "has_code": false}
{"id": "bdfa8c403e67ed4a59ab737a03975509f2a5c007", "text": "LangGraph‚Äã\n\nLangGraph is an extension of LangChain aimed at building robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.\n\nLangGraph documentation is currently hosted on a separate site. You can find the LangGraph guides here.", "url_final": "https://python.langchain.com/docs/how_to/", "url_citable": "https://python.langchain.com/docs/how_to/#langgraph", "title": "How-to guides | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "LangGraph‚Äã", "section_anchor": "langgraph", "fetched_at": "2025-10-06T21:54:15.440556", "has_code": false}
{"id": "7038d2f774ef5cea450e9a86b51e30eae7b3c64f", "text": "LangSmith‚Äã\n\nLangSmith allows you to closely trace, monitor and evaluate your LLM application. It seamlessly integrates with LangChain and LangGraph, and you can use it to inspect and debug individual steps of your chains and agents as you build.\n\nLangSmith documentation is hosted on a separate site. You can peruse LangSmith how-to guides here, but we'll highlight a few sections that are particularly relevant to LangChain below:\n\nEvaluation‚Äã\n\nEvaluating performance is a vital part of building LLM-powered applications. LangSmith helps with every step of the process from creating a dataset to defining metrics to running evaluators.\n\nTo learn more, check out the LangSmith evaluation how-to guides.\n\nTracing‚Äã\n\nTracing gives you observability inside your chains and agents, and is vital in diagnosing issues.\n\n- How to: trace with LangChain\n- How to: add metadata and tags to traces\n\nYou can see general tracing-related how-tos in this section of the LangSmith docs.\n\n- Chat models\n- Messages\n- Prompt templates\n- Example selectors\n- LLMs\n- Output parsers\n- Document loaders\n- Text splitters\n- Embedding models\n- Vector stores\n- Retrievers\n- Indexing\n- Tools\n- Multimodal\n- Agents\n- Callbacks\n- Custom\n- Serialization\n\n- Q&A with RAG\n- Extraction\n- Chatbots\n- Query analysis\n- Q&A over SQL + CSV\n- Q&A over graph databases\n- Summarization\n\n- Evaluation\n- Tracing", "url_final": "https://python.langchain.com/docs/how_to/", "url_citable": "https://python.langchain.com/docs/how_to/#langsmith", "title": "How-to guides | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "LangSmith‚Äã", "section_anchor": "langsmith", "fetched_at": "2025-10-06T21:54:15.440556", "has_code": false}
{"id": "91493c305f9f47794c51a5176db40e48894464b0", "text": "Simple and fast text extraction‚Äã\n\nIf you are looking for a simple string representation of text that is embedded in a PDF, the method below is appropriate. It will return a list of Document objects-- one per page-- containing a single string of the page's text in the Document's page_content attribute. It will not parse text in images or scanned PDF pages. Under the hood it uses the pypdf Python library.\n\nLangChain document loaders implement lazy_load and its async variant, alazy_load, which return iterators of Document objects. We will use these below.\n\n```\n%pip install -qU pypdf\n```\n\n```\nfrom langchain_community.document_loaders import PyPDFLoaderloader = PyPDFLoader(file_path)pages = []async for page in loader.alazy_load():    pages.append(page)\n```\n\n```\nprint(f\"{pages[0].metadata}\\n\")print(pages[0].page_content)\n```\n\n```\n{'source': '../../docs/integrations/document_loaders/example_data/layout-parser-paper.pdf', 'page': 0}LayoutParser : A UniÔ¨Åed Toolkit for DeepLearning Based Document Image AnalysisZejiang Shen1( ÔøΩ), Ruochen Zhang2, Melissa Dell3, Benjamin Charles GermainLee4, Jacob Carlson3, and Weining Li51Allen Institute for AIshannons@allenai.org2Brown Universityruochen zhang@brown.edu3Harvard University{melissadell,jacob carlson }@fas.harvard.edu4University of Washingtonbcgl@cs.washington.edu5University of Waterloow422li@uwaterloo.caAbstract. Recent advances in document image analysis (DIA) have beenprimarily driven by the application of neural networks. Ideally, researchoutcomes could be easily deployed in production and extended for furtherinvestigation. However, various factors like loosely organized codebasesand sophisticated model conÔ¨Ågurations complicate the easy reuse of im-portant innovations by a wide audience. Though there have been on-goingeÔ¨Äorts to improve reusability and simplify deep learning (DL) modeldevelopment in disciplines like natural language processing and computervision, none of them are optimized for challenges in the domain of DIA.This represents a major gap in the existing toolkit, as DIA is central toacademic research across a wide range of disciplines in the social sciencesand humanities. This paper introduces LayoutParser , an open-sourcelibrary for streamlining the usage of DL in DIA research and applica-tions. The core LayoutParser library comes with a set of simple andintuitive interfaces for applying and customizing DL models for layout de-tection, character recognition, and many other document processing tasks.To promote extensibility, LayoutParser also incorporates a communityplatform for sharing both pre-trained models and full document digiti-zation pipelines. We demonstrate that LayoutParser is helpful for bothlightweight and large-scale digitization pipelines in real-word use cases.The library is publicly available at https://layout-parser.github.io .Keywords: Document Image Analysis ¬∑Deep Learning ¬∑Layout Analysis¬∑Character Recognition ¬∑Open Source library ¬∑Toolkit.1 IntroductionDeep Learning(DL)-based approaches are the state-of-the-art for a wide range ofdocument image analysis (DIA) tasks including document image classiÔ¨Åcation [ 11,arXiv:2103.15348v2  [cs.CV]  21 Jun 2021\n```\n\nNote that the metadata of each document stores the corresponding page number.", "url_final": "https://python.langchain.com/docs/how_to/document_loader_pdf/", "url_citable": "https://python.langchain.com/docs/how_to/document_loader_pdf/#simple-and-fast-text-extraction", "title": "How to load PDFs | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Simple and fast text extraction‚Äã", "section_anchor": "simple-and-fast-text-extraction", "fetched_at": "2025-10-06T21:55:26.902847", "has_code": true}
{"id": "e2416637d70eb09f307e2643266392f3bd5b1e58", "text": "Vector search over PDFs‚Äã\n\nOnce we have loaded PDFs into LangChain Document objects, we can index them (e.g., a RAG application) in the usual way. Below we use OpenAI embeddings, although any LangChain embeddings model will suffice.\n\n```\n%pip install -qU langchain-openai\n```\n\n```\nimport getpassimport osif \"OPENAI_API_KEY\" not in os.environ:    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n```\n\n```\nfrom langchain_core.vectorstores import InMemoryVectorStorefrom langchain_openai import OpenAIEmbeddingsvector_store = InMemoryVectorStore.from_documents(pages, OpenAIEmbeddings())docs = vector_store.similarity_search(\"What is LayoutParser?\", k=2)for doc in docs:    print(f\"Page {doc.metadata['page']}: {doc.page_content[:300]}\\n\")\n```\n\n```\nPage 13: 14 Z. Shen et al.6 ConclusionLayoutParser provides a comprehensive toolkit for deep learning-based documentimage analysis. The oÔ¨Ä-the-shelf library is easy to install, and can be used tobuild Ô¨Çexible and accurate pipelines for processing documents with complicatedstructures. It also supports hiPage 0: LayoutParser : A UniÔ¨Åed Toolkit for DeepLearning Based Document Image AnalysisZejiang Shen1( ÔøΩ), Ruochen Zhang2, Melissa Dell3, Benjamin Charles GermainLee4, Jacob Carlson3, and Weining Li51Allen Institute for AIshannons@allenai.org2Brown Universityruochen zhang@brown.edu3Harvard University\n```", "url_final": "https://python.langchain.com/docs/how_to/document_loader_pdf/", "url_citable": "https://python.langchain.com/docs/how_to/document_loader_pdf/#vector-search-over-pdfs", "title": "How to load PDFs | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Vector search over PDFs‚Äã", "section_anchor": "vector-search-over-pdfs", "fetched_at": "2025-10-06T21:55:26.902847", "has_code": true}
{"id": "004b6f66929382151eb9f3998b888ec3fd7ec3ed", "text": "Layout analysis and extraction of text from images‚Äã\n\nIf you require a more granular segmentation of text (e.g., into distinct paragraphs, titles, tables, or other structures) or require extraction of text from images, the method below is appropriate. It will return a list of Document objects, where each object represents a structure on the page. The Document's metadata stores the page number and other information related to the object (e.g., it might store table rows and columns in the case of a table object).\n\nUnder the hood it uses the langchain-unstructured library. See the integration docs for more information about using Unstructured with LangChain.\n\nUnstructured supports multiple parameters for PDF parsing:\n\n- strategy (e.g., \"fast\" or \"hi-res\")\n- API or local processing. You will need an API key to use the API.\n\nThe hi-res strategy provides support for document layout analysis and OCR. We demonstrate it below via the API. See local parsing section below for considerations when running locally.\n\n```\n%pip install -qU langchain-unstructured\n```\n\n```\nimport getpassimport osif \"UNSTRUCTURED_API_KEY\" not in os.environ:    os.environ[\"UNSTRUCTURED_API_KEY\"] = getpass.getpass(\"Unstructured API Key:\")\n```\n\n```\nUnstructured API Key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n```\n\nAs before, we initialize a loader and load documents lazily:\n\n```\nfrom langchain_unstructured import UnstructuredLoaderloader = UnstructuredLoader(    file_path=file_path,    strategy=\"hi_res\",    partition_via_api=True,    coordinates=True,)docs = []for doc in loader.lazy_load():    docs.append(doc)\n```\n\n```\nINFO: Preparing to split document for partition.INFO: Starting page number set to 1INFO: Allow failed set to 0INFO: Concurrency level set to 5INFO: Splitting pages 1 to 16 (16 total)INFO: Determined optimal split size of 4 pages.INFO: Partitioning 4 files with 4 page(s) each.INFO: Partitioning set #1 (pages 1-4).INFO: Partitioning set #2 (pages 5-8).INFO: Partitioning set #3 (pages 9-12).INFO: Partitioning set #4 (pages 13-16).INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 200 OK\"INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 200 OK\"INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 200 OK\"INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 200 OK\"INFO: Successfully partitioned set #1, elements added to the final result.INFO: Successfully partitioned set #2, elements added to the final result.INFO: Successfully partitioned set #3, elements added to the final result.INFO: Successfully partitioned set #4, elements added to the final result.\n```\n\nHere we recover 171 distinct structures over the 16 page document:\n\n```\nprint(len(docs))\n```\n\n```\n171\n```\n\nWe can use the document metadata to recover content from a single page:\n\n```\nfirst_page_docs = [doc for doc in docs if doc.metadata.get(\"page_number\") == 1]for doc in first_page_docs:    print(doc.page_content)\n```\n\n```\nLayoutParser: A UniÔ¨Åed Toolkit for Deep Learning Based Document Image Analysis1 2 0 2 n u J 1 2 ] V C . s c [ 2 v 8 4 3 5 1 . 3 0 1 2 : v i X r aZejiang Shen¬Æ (<), Ruochen Zhang?, Melissa Dell¬Æ, Benjamin Charles Germain Lee?, Jacob Carlson¬Æ, and Weining Li¬Æ1 Allen Institute for AI shannons@allenai.org 2 Brown University ruochen zhang@brown.edu 3 Harvard University {melissadell,jacob carlson}@fas.harvard.edu 4 University of Washington bcgl@cs.washington.edu 5 University of Waterloo w422li@uwaterloo.caAbstract. Recent advances in document image analysis (DIA) have been primarily driven by the application of neural networks. Ideally, research outcomes could be easily deployed in production and extended for further investigation. However, various factors like loosely organized codebases and sophisticated model conÔ¨Ågurations complicate the easy reuse of im- portant innovations by a wide audience. Though there have been on-going eÔ¨Äorts to improve reusability and simplify deep learning (DL) model development in disciplines like natural language processing and computer vision, none of them are optimized for challenges in the domain of DIA. This represents a major gap in the existing toolkit, as DIA is central to academic research across a wide range of disciplines in the social sciences and humanities. This paper introduces LayoutParser, an open-source library for streamlining the usage of DL in DIA research and applica- tions. The core LayoutParser library comes with a set of simple and intuitive interfaces for applying and customizing DL models for layout de- tection, character recognition, and many other document processing tasks. To promote extensibility, LayoutParser also incorporates a community platform for sharing both pre-trained models and full document digiti- zation pipelines. We demonstrate that LayoutParser is helpful for both lightweight and large-scale digitization pipelines in real-word use cases. The library is publicly available at https://layout-parser.github.io.Keywords: Document Image Analysis ¬∑ Deep Learning ¬∑ Layout Analysis ¬∑ Character Recognition ¬∑ Open Source library ¬∑ Toolkit.1 IntroductionDeep Learning(DL)-based approaches are the state-of-the-art for a wide range of document image analysis (DIA) tasks including document image classiÔ¨Åcation [11,\n```", "url_final": "https://python.langchain.com/docs/how_to/document_loader_pdf/", "url_citable": "https://python.langchain.com/docs/how_to/document_loader_pdf/#layout-analysis-and-extraction-of-text-from-images", "title": "How to load PDFs | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Layout analysis and extraction of text from images‚Äã", "section_anchor": "layout-analysis-and-extraction-of-text-from-images", "fetched_at": "2025-10-06T21:55:26.902847", "has_code": true}
{"id": "47cd4f4bc1f42e82b78a91e156d19364a0fc1058", "text": "Extracting tables and other structures‚Äã\n\nEach Document we load represents a structure, like a title, paragraph, or table.\n\nSome structures may be of special interest for indexing or question-answering tasks. These structures may be:\n\n- Classified for easy identification;\n- Parsed into a more structured representation.\n\nBelow, we identify and extract a table:\n\n%pip install -qU matplotlib PyMuPDF pillow\n\n```\nimport fitzimport matplotlib.patches as patchesimport matplotlib.pyplot as pltfrom PIL import Imagedef plot_pdf_with_boxes(pdf_page, segments):    pix = pdf_page.get_pixmap()    pil_image = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)    fig, ax = plt.subplots(1, figsize=(10, 10))    ax.imshow(pil_image)    categories = set()    category_to_color = {        \"Title\": \"orchid\",        \"Image\": \"forestgreen\",        \"Table\": \"tomato\",    }    for segment in segments:        points = segment[\"coordinates\"][\"points\"]        layout_width = segment[\"coordinates\"][\"layout_width\"]        layout_height = segment[\"coordinates\"][\"layout_height\"]        scaled_points = [            (x * pix.width / layout_width, y * pix.height / layout_height)            for x, y in points        ]        box_color = category_to_color.get(segment[\"category\"], \"deepskyblue\")        categories.add(segment[\"category\"])        rect = patches.Polygon(            scaled_points, linewidth=1, edgecolor=box_color, facecolor=\"none\"        )        ax.add_patch(rect)    # Make legend    legend_handles = [patches.Patch(color=\"deepskyblue\", label=\"Text\")]    for category in [\"Title\", \"Image\", \"Table\"]:        if category in categories:            legend_handles.append(                patches.Patch(color=category_to_color[category], label=category)            )    ax.axis(\"off\")    ax.legend(handles=legend_handles, loc=\"upper right\")    plt.tight_layout()    plt.show()def render_page(doc_list: list, page_number: int, print_text=True) -> None:    pdf_page = fitz.open(file_path).load_page(page_number - 1)    page_docs = [        doc for doc in doc_list if doc.metadata.get(\"page_number\") == page_number    ]    segments = [doc.metadata for doc in page_docs]    plot_pdf_with_boxes(pdf_page, segments)    if print_text:        for doc in page_docs:            print(f\"{doc.page_content}\\n\")\n```\n\n```\nrender_page(docs, 5)\n```\n\n```\nLayoutParser: A UniÔ¨Åed Toolkit for DL-Based DIA5Table 1: Current layout detection models in the LayoutParser model zooDataset Base Model1 Large Model Notes PubLayNet [38] PRImA [3] Newspaper [17] TableBank [18] HJDataset [31] F / M M F F F / M M - - F - Layouts of modern scientiÔ¨Åc documents Layouts of scanned modern magazines and scientiÔ¨Åc reports Layouts of scanned US newspapers from the 20th century Table region on modern scientiÔ¨Åc and business document Layouts of history Japanese documents1 For each dataset, we train several models of diÔ¨Äerent sizes for diÔ¨Äerent needs (the trade-oÔ¨Ä between accuracy vs. computational cost). For ‚Äúbase model‚Äù and ‚Äúlarge model‚Äù, we refer to using the ResNet 50 or ResNet 101 backbones [13], respectively. One can train models of diÔ¨Äerent architectures, like Faster R-CNN [28] (F) and Mask R-CNN [12] (M). For example, an F in the Large Model column indicates it has a Faster R-CNN model trained using the ResNet 101 backbone. The platform is maintained and a number of additions will be made to the model zoo in coming months.layout data structures, which are optimized for eÔ¨Éciency and versatility. 3) When necessary, users can employ existing or customized OCR models via the uniÔ¨Åed API provided in the OCR module. 4) LayoutParser comes with a set of utility functions for the visualization and storage of the layout data. 5) LayoutParser is also highly customizable, via its integration with functions for layout data annotation and model training. We now provide detailed descriptions for each component.3.1 Layout Detection ModelsIn LayoutParser, a layout model takes a document image as an input and generates a list of rectangular boxes for the target content regions. DiÔ¨Äerent from traditional methods, it relies on deep convolutional neural networks rather than manually curated rules to identify content regions. It is formulated as an object detection problem and state-of-the-art models like Faster R-CNN [28] and Mask R-CNN [12] are used. This yields prediction results of high accuracy and makes it possible to build a concise, generalized interface for layout detection. LayoutParser, built upon Detectron2 [35], provides a minimal API that can perform layout detection with only four lines of code in Python:1 import layoutparser as lp 2 image = cv2 . imread ( \" image_file \" ) # load images 3 model = lp . De t e c tro n2 Lay outM odel ( \" lp :// PubLayNet / f as t er _ r c nn _ R _ 50 _ F P N_ 3 x / config \" ) 4 5 layout = model . detect ( image )LayoutParser provides a wealth of pre-trained model weights using various datasets covering diÔ¨Äerent languages, time periods, and document types. Due to domain shift [7], the prediction performance can notably drop when models are ap- plied to target samples that are signiÔ¨Åcantly diÔ¨Äerent from the training dataset. As document structures and layouts vary greatly in diÔ¨Äerent domains, it is important to select models trained on a dataset similar to the test samples. A semantic syntax is used for initializing the model weights in LayoutParser, using both the dataset name and model name lp://<dataset-name>/<model-architecture-name>.\n```\n\nNote that although the table text is collapsed into a single string in the document's content, the metadata contains a representation of its rows and columns:\n\n```\nfrom IPython.display import HTML, displaysegments = [    doc.metadata    for doc in docs    if doc.metadata.get(\"page_number\") == 5 and doc.metadata.get(\"category\") == \"Table\"]display(HTML(segments[0][\"text_as_html\"]))\n```", "url_final": "https://python.langchain.com/docs/how_to/document_loader_pdf/", "url_citable": "https://python.langchain.com/docs/how_to/document_loader_pdf/#extracting-tables-and-other-structures", "title": "How to load PDFs | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Extracting tables and other structures‚Äã", "section_anchor": "extracting-tables-and-other-structures", "fetched_at": "2025-10-06T21:55:26.902847", "has_code": true}
{"id": "dcc459f6ca8ba501fc63f5fbb7598e47f02838ca", "text": "Extracting text from specific sections‚Äã\n\nStructures may have parent-child relationships -- for example, a paragraph might belong to a section with a title. If a section is of particular interest (e.g., for indexing) we can isolate the corresponding Document objects.\n\nBelow, we extract all text associated with the document's \"Conclusion\" section:\n\n```\nrender_page(docs, 14, print_text=False)\n```\n\n```\nconclusion_docs = []parent_id = -1for doc in docs:    if doc.metadata[\"category\"] == \"Title\" and \"Conclusion\" in doc.page_content:        parent_id = doc.metadata[\"element_id\"]    if doc.metadata.get(\"parent_id\") == parent_id:        conclusion_docs.append(doc)for doc in conclusion_docs:    print(doc.page_content)\n```\n\n```\nLayoutParser provides a comprehensive toolkit for deep learning-based document image analysis. The oÔ¨Ä-the-shelf library is easy to install, and can be used to build Ô¨Çexible and accurate pipelines for processing documents with complicated structures. It also supports high-level customization and enables easy labeling and training of DL models on unique document image datasets. The LayoutParser community platform facilitates sharing DL models and DIA pipelines, inviting discussion and promoting code reproducibility and reusability. The LayoutParser team is committed to keeping the library updated continuously and bringing the most recent advances in DL-based DIA, such as multi-modal document modeling [37, 36, 9] (an upcoming priority), to a diverse audience of end-users.Acknowledgements We thank the anonymous reviewers for their comments and suggestions. This project is supported in part by NSF Grant OIA-2033558 and funding from the Harvard Data Science Initiative and Harvard Catalyst. Zejiang Shen thanks Doug Downey for suggestions.\n```", "url_final": "https://python.langchain.com/docs/how_to/document_loader_pdf/", "url_citable": "https://python.langchain.com/docs/how_to/document_loader_pdf/#extracting-text-from-specific-sections", "title": "How to load PDFs | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Extracting text from specific sections‚Äã", "section_anchor": "extracting-text-from-specific-sections", "fetched_at": "2025-10-06T21:55:26.902847", "has_code": true}
{"id": "50c0a98b9f19586461c894f6c667af54892c8a6b", "text": "Extracting text from images‚Äã\n\nOCR is run on images, enabling the extraction of text therein:\n\n```\nrender_page(docs, 11)\n```\n\n```\nLayoutParser: A UniÔ¨Åed Toolkit for DL-Based DIAfocuses on precision, eÔ¨Éciency, and robustness. The target documents may have complicated structures, and may require training multiple layout detection models to achieve the optimal accuracy. Light-weight pipelines are built for relatively simple documents, with an emphasis on development ease, speed and Ô¨Çexibility. Ideally one only needs to use existing resources, and model training should be avoided. Through two exemplar projects, we show how practitioners in both academia and industry can easily build such pipelines using LayoutParser and extract high-quality structured document data for their downstream tasks. The source code for these projects will be publicly available in the LayoutParser community hub.115.1 A Comprehensive Historical Document Digitization PipelineThe digitization of historical documents can unlock valuable data that can shed light on many important social, economic, and historical questions. Yet due to scan noises, page wearing, and the prevalence of complicated layout structures, ob- taining a structured representation of historical document scans is often extremely complicated. In this example, LayoutParser was used to develop a comprehensive pipeline, shown in Figure 5, to gener- ate high-quality structured data from historical Japanese Ô¨Årm Ô¨Ånancial ta- bles with complicated layouts. The pipeline applies two layout models to identify diÔ¨Äerent levels of document structures and two customized OCR engines for optimized character recog- nition accuracy.‚ÄòActive Learning Layout Annotate Layout Dataset | +‚Äî‚Äî Annotation Toolkit A4 Deep Learning Layout Layout Detection Model Training & Inference, A Post-processing ‚Äî Handy Data Structures & \\ Lo orajport 7 ) Al Pls for Layout Data A4 Default and Customized Text Recognition 0CR Models ¬• Visualization & Export Layout Structure Visualization & Storage The Japanese Document Helpful LayoutParser Modules Digitization PipelineAs shown in Figure 4 (a), the document contains columns of text written vertically 15, a common style in Japanese. Due to scanning noise and archaic printing technology, the columns can be skewed or have vari- able widths, and hence cannot be eas- ily identiÔ¨Åed via rule-based methods. Within each column, words are sepa- rated by white spaces of variable size, and the vertical positions of objects can be an indicator of their layout type.Fig. 5: Illustration of how LayoutParser helps with the historical document digi- tization pipeline.15 A document page consists of eight rows like this. For simplicity we skip the row segmentation discussion and refer readers to the source code when available.\n```\n\nNote that the text from the figure on the right is extracted and incorporated into the content of the Document.", "url_final": "https://python.langchain.com/docs/how_to/document_loader_pdf/", "url_citable": "https://python.langchain.com/docs/how_to/document_loader_pdf/#extracting-text-from-images", "title": "How to load PDFs | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Extracting text from images‚Äã", "section_anchor": "extracting-text-from-images", "fetched_at": "2025-10-06T21:55:26.902847", "has_code": true}
{"id": "40db63767d3c28151ec8221a6595bb31441fefd3", "text": "Local parsing‚Äã\n\nParsing locally requires the installation of additional dependencies.\n\nPoppler (PDF analysis)\n\n- Linux: apt-get install poppler-utils\n- Mac: brew install poppler\n- Windows: https://github.com/oschwartz10612/poppler-windows\n\nTesseract (OCR)\n\n- Linux: apt-get install tesseract-ocr\n- Mac: brew install tesseract\n- Windows: https://github.com/UB-Mannheim/tesseract/wiki#tesseract-installer-for-windows\n\nWe will also need to install the unstructured PDF extras:\n\n```\n%pip install -qU \"unstructured[pdf]\"\n```\n\nWe can then use the UnstructuredLoader much the same way, forgoing the API key and partition_via_api setting:\n\n```\nloader_local = UnstructuredLoader(    file_path=file_path,    strategy=\"hi_res\",)docs_local = []for doc in loader_local.lazy_load():    docs_local.append(doc)\n```\n\n```\nWARNING: This function will be deprecated in a future release and `unstructured` will simply use the DEFAULT_MODEL from `unstructured_inference.model.base` to set default model nameINFO: Reading PDF for file: /Users/chestercurme/repos/langchain/libs/community/tests/integration_tests/examples/layout-parser-paper.pdf ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: padding image by 20 for structure detectionINFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: padding image by 20 for structure detectionINFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...\n```\n\nThe list of documents can then be processed similarly to those obtained from the API.", "url_final": "https://python.langchain.com/docs/how_to/document_loader_pdf/", "url_citable": "https://python.langchain.com/docs/how_to/document_loader_pdf/#local-parsing", "title": "How to load PDFs | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Local parsing‚Äã", "section_anchor": "local-parsing", "fetched_at": "2025-10-06T21:55:26.902847", "has_code": true}
{"id": "13129613ab5b631662645e7f55af54ac06f89780", "text": "Use of multimodal models‚Äã\n\nMany modern LLMs support inference over multimodal inputs (e.g., images). In some applications-- such as question-answering over PDFs with complex layouts, diagrams, or scans-- it may be advantageous to skip the PDF parsing, instead casting a PDF page to an image and passing it to a model directly. This allows a model to reason over the two dimensional content on the page, instead of a \"one-dimensional\" string representation.\n\nIn principle we can use any LangChain chat model that supports multimodal inputs. A list of these models is documented here. Below we use OpenAI's gpt-4o-mini.\n\nFirst we define a short utility function to convert a PDF page to a base64-encoded image:\n\n```\n%pip install -qU PyMuPDF pillow langchain-openai\n```\n\n```\nimport base64import ioimport fitzfrom PIL import Imagedef pdf_page_to_base64(pdf_path: str, page_number: int):    pdf_document = fitz.open(pdf_path)    page = pdf_document.load_page(page_number - 1)  # input is one-indexed    pix = page.get_pixmap()    img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)    buffer = io.BytesIO()    img.save(buffer, format=\"PNG\")    return base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n```\n\n```\nfrom IPython.display import Image as IPImagefrom IPython.display import displaybase64_image = pdf_page_to_base64(file_path, 11)display(IPImage(data=base64.b64decode(base64_image)))\n```\n\nWe can then query the model in the usual way. Below we ask it a question on related to the diagram on the page.\n\n```\nfrom langchain_openai import ChatOpenAIllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\n```\nfrom langchain_core.messages import HumanMessagequery = \"What is the name of the first step in the pipeline?\"message = HumanMessage(    content=[        {\"type\": \"text\", \"text\": query},        {            \"type\": \"image_url\",            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},        },    ],)response = llm.invoke([message])print(response.content)\n```\n\n```\nINFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"``````outputThe first step in the pipeline is \"Annotate Layout Dataset.\"\n```", "url_final": "https://python.langchain.com/docs/how_to/document_loader_pdf/", "url_citable": "https://python.langchain.com/docs/how_to/document_loader_pdf/#use-of-multimodal-models", "title": "How to load PDFs | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Use of multimodal models‚Äã", "section_anchor": "use-of-multimodal-models", "fetched_at": "2025-10-06T21:55:26.902847", "has_code": true}
{"id": "5aa3ac6febec5c726089fc214490bf2999583499", "text": "Other PDF loaders‚Äã\n\nFor a list of available LangChain PDF loaders, please see this table.\n\n- Vector search over PDFs\n\n- Extracting tables and other structures\n- Extracting text from specific sections\n- Extracting text from images\n- Local parsing", "url_final": "https://python.langchain.com/docs/how_to/document_loader_pdf/", "url_citable": "https://python.langchain.com/docs/how_to/document_loader_pdf/#other-pdf-loaders", "title": "How to load PDFs | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Other PDF loaders‚Äã", "section_anchor": "other-pdf-loaders", "fetched_at": "2025-10-06T21:55:26.902847", "has_code": false}
{"id": "013cc908c69037b165acebaaa3a1add7c48582e0", "text": "Images‚Äã\n\nMany providers will accept images passed in-line as base64 data. Some will additionally accept an image from a URL directly.\n\nImages from base64 data‚Äã\n\nTo pass images in-line, format them as content blocks of the following form:\n\n```\n{    \"type\": \"image\",    \"source_type\": \"base64\",    \"mime_type\": \"image/jpeg\",  # or image/png, etc.    \"data\": \"<base64 data string>\",}\n```\n\nExample:\n\n```\nimport base64import httpxfrom langchain.chat_models import init_chat_model# Fetch image dataimage_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"image_data = base64.b64encode(httpx.get(image_url).content).decode(\"utf-8\")# Pass to LLMllm = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\")message = {    \"role\": \"user\",    \"content\": [        {            \"type\": \"text\",            \"text\": \"Describe the weather in this image:\",        },        {            \"type\": \"image\",            \"source_type\": \"base64\",            \"data\": image_data,            \"mime_type\": \"image/jpeg\",        },    ],}response = llm.invoke([message])print(response.text())\n```\n\n```\nThe image shows a beautiful clear day with bright blue skies and wispy cirrus clouds stretching across the horizon. The clouds are thin and streaky, creating elegant patterns against the blue backdrop. The lighting suggests it's during the day, possibly late afternoon given the warm, golden quality of the light on the grass. The weather appears calm with no signs of wind (the grass looks relatively still) and no indication of rain. It's the kind of perfect, mild weather that's ideal for walking along the wooden boardwalk through the marsh grass.\n```\n\nSee LangSmith trace for more detail.\n\nImages from a URL‚Äã\n\nSome providers (including OpenAI, Anthropic, and Google Gemini) will also accept images from URLs directly.\n\nTo pass images as URLs, format them as content blocks of the following form:\n\n```\n{    \"type\": \"image\",    \"source_type\": \"url\",    \"url\": \"https://...\",}\n```\n\nExample:\n\n```\nmessage = {    \"role\": \"user\",    \"content\": [        {            \"type\": \"text\",            \"text\": \"Describe the weather in this image:\",        },        {            \"type\": \"image\",            \"source_type\": \"url\",            \"url\": image_url,        },    ],}response = llm.invoke([message])print(response.text())\n```\n\n```\nThe weather in this image appears to be pleasant and clear. The sky is mostly blue with a few scattered, light clouds, and there is bright sunlight illuminating the green grass and plants. There are no signs of rain or stormy conditions, suggesting it is a calm, likely warm day‚Äîtypical of spring or summer.\n```\n\nWe can also pass in multiple images:\n\n```\nmessage = {    \"role\": \"user\",    \"content\": [        {\"type\": \"text\", \"text\": \"Are these two images the same?\"},        {\"type\": \"image\", \"source_type\": \"url\", \"url\": image_url},        {\"type\": \"image\", \"source_type\": \"url\", \"url\": image_url},    ],}response = llm.invoke([message])print(response.text())\n```\n\n```\nYes, these two images are the same. They depict a wooden boardwalk going through a grassy field under a blue sky with some clouds. The colors, composition, and elements in both images are identical.\n```", "url_final": "https://python.langchain.com/docs/how_to/multimodal_inputs/", "url_citable": "https://python.langchain.com/docs/how_to/multimodal_inputs/#images", "title": "How to pass multimodal data to models | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Images‚Äã", "section_anchor": "images", "fetched_at": "2025-10-06T21:55:57.346436", "has_code": true}
{"id": "bc5b939c168782326fab114858d11f79ed7e769e", "text": "Documents (PDF)‚Äã\n\nSome providers (including OpenAI, Anthropic, and Google Gemini) will accept PDF documents.\n\nOpenAI requires file-names be specified for PDF inputs. When using LangChain's format, include the filename key. See example below.\n\nDocuments from base64 data‚Äã\n\nTo pass documents in-line, format them as content blocks of the following form:\n\n```\n{    \"type\": \"file\",    \"source_type\": \"base64\",    \"mime_type\": \"application/pdf\",    \"data\": \"<base64 data string>\",}\n```\n\nExample:\n\n```\nimport base64import httpxfrom langchain.chat_models import init_chat_model# Fetch PDF datapdf_url = \"https://pdfobject.com/pdf/sample.pdf\"pdf_data = base64.b64encode(httpx.get(pdf_url).content).decode(\"utf-8\")# Pass to LLMllm = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\")message = {    \"role\": \"user\",    \"content\": [        {            \"type\": \"text\",            \"text\": \"Describe the document:\",        },        {            \"type\": \"file\",            \"source_type\": \"base64\",            \"data\": pdf_data,            \"mime_type\": \"application/pdf\",        },    ],}response = llm.invoke([message])print(response.text())\n```\n\n```\nThis document appears to be a sample PDF file that contains Lorem ipsum placeholder text. It begins with a title \"Sample PDF\" followed by the subtitle \"This is a simple PDF file. Fun fun fun.\"The rest of the document consists of several paragraphs of Lorem ipsum text, which is a commonly used placeholder text in design and publishing. The text is formatted in a clean, readable layout with consistent paragraph spacing. The document appears to be a single page containing four main paragraphs of this placeholder text.The Lorem ipsum text, while appearing to be Latin, is actually scrambled Latin-like text that is used primarily to demonstrate the visual form of a document or typeface without the distraction of meaningful content. It's commonly used in publishing and graphic design when the actual content is not yet available but the layout needs to be demonstrated.The document has a professional, simple layout with generous margins and clear paragraph separation, making it an effective example of basic PDF formatting and structure.\n```\n\nDocuments from a URL‚Äã\n\nSome providers (specifically Anthropic) will also accept documents from URLs directly.\n\nTo pass documents as URLs, format them as content blocks of the following form:\n\n```\n{    \"type\": \"file\",    \"source_type\": \"url\",    \"url\": \"https://...\",}\n```\n\nExample:\n\n```\nmessage = {    \"role\": \"user\",    \"content\": [        {            \"type\": \"text\",            \"text\": \"Describe the document:\",        },        {            \"type\": \"file\",            \"source_type\": \"url\",            \"url\": pdf_url,        },    ],}response = llm.invoke([message])print(response.text())\n```\n\n```\nThis document appears to be a sample PDF file with both text and an image. It begins with a title \"Sample PDF\" followed by the text \"This is a simple PDF file. Fun fun fun.\" The rest of the document contains Lorem ipsum placeholder text arranged in several paragraphs. The content is shown both as text and as an image of the formatted PDF, with the same content displayed in a clean, formatted layout with consistent spacing and typography. The document consists of a single page containing this sample text.\n```", "url_final": "https://python.langchain.com/docs/how_to/multimodal_inputs/", "url_citable": "https://python.langchain.com/docs/how_to/multimodal_inputs/#documents-pdf", "title": "How to pass multimodal data to models | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Documents (PDF)‚Äã", "section_anchor": "documents-pdf", "fetched_at": "2025-10-06T21:55:57.346436", "has_code": true}
{"id": "75743e0513ce59bbb9e20e8c55b90be2d9e095da", "text": "Audio‚Äã\n\nSome providers (including OpenAI and Google Gemini) will accept audio inputs.\n\nAudio from base64 data‚Äã\n\nTo pass audio in-line, format them as content blocks of the following form:\n\n```\n{    \"type\": \"audio\",    \"source_type\": \"base64\",    \"mime_type\": \"audio/wav\",  # or appropriate mime-type    \"data\": \"<base64 data string>\",}\n```\n\nExample:\n\n```\nimport base64import httpxfrom langchain.chat_models import init_chat_model# Fetch audio dataaudio_url = \"https://upload.wikimedia.org/wikipedia/commons/3/3d/Alcal%C3%A1_de_Henares_%28RPS_13-04-2024%29_canto_de_ruise%C3%B1or_%28Luscinia_megarhynchos%29_en_el_Soto_del_Henares.wav\"audio_data = base64.b64encode(httpx.get(audio_url).content).decode(\"utf-8\")# Pass to LLMllm = init_chat_model(\"google_genai:gemini-2.5-flash\")message = {    \"role\": \"user\",    \"content\": [        {            \"type\": \"text\",            \"text\": \"Describe this audio:\",        },        {            \"type\": \"audio\",            \"source_type\": \"base64\",            \"data\": audio_data,            \"mime_type\": \"audio/wav\",        },    ],}response = llm.invoke([message])print(response.text())\n```\n\n```\nThe audio appears to consist primarily of bird sounds, specifically bird vocalizations like chirping and possibly other bird songs.\n```", "url_final": "https://python.langchain.com/docs/how_to/multimodal_inputs/", "url_citable": "https://python.langchain.com/docs/how_to/multimodal_inputs/#audio", "title": "How to pass multimodal data to models | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Audio‚Äã", "section_anchor": "audio", "fetched_at": "2025-10-06T21:55:57.346436", "has_code": true}
{"id": "a3c70f94920bab607d2c0418615022c481e972b6", "text": "Provider-specific parameters‚Äã\n\nSome providers will support or require additional fields on content blocks containing multimodal data. For example, Anthropic lets you specify caching of specific content to reduce token consumption.\n\nTo use these fields, you can:\n\n- Store them on directly on the content block; or\n- Use the native format supported by each provider (see chat model integrations for detail).\n\nWe show three examples below.\n\nExample: Anthropic prompt caching‚Äã\n\n```\nllm = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\")message = {    \"role\": \"user\",    \"content\": [        {            \"type\": \"text\",            \"text\": \"Describe the weather in this image:\",        },        {            \"type\": \"image\",            \"source_type\": \"url\",            \"url\": image_url,            \"cache_control\": {\"type\": \"ephemeral\"},        },    ],}response = llm.invoke([message])print(response.text())response.usage_metadata\n```\n\n```\nThe image shows a beautiful, clear day with partly cloudy skies. The sky is a vibrant blue with wispy, white cirrus clouds stretching across it. The lighting suggests it's during daylight hours, possibly late afternoon or early evening given the warm, golden quality of the light on the grass. The weather appears calm with no signs of wind (the grass looks relatively still) and no threatening weather conditions. It's the kind of perfect weather you'd want for a walk along this wooden boardwalk through the marshland or grassland area.\n```\n\n```\n{'input_tokens': 1586, 'output_tokens': 117, 'total_tokens': 1703, 'input_token_details': {'cache_read': 0, 'cache_creation': 1582}}\n```\n\n```\nnext_message = {    \"role\": \"user\",    \"content\": [        {            \"type\": \"text\",            \"text\": \"Summarize that in 5 words.\",        }    ],}response = llm.invoke([message, response, next_message])print(response.text())response.usage_metadata\n```\n\n```\nClear blue skies, wispy clouds.\n```\n\n```\n{'input_tokens': 1716, 'output_tokens': 12, 'total_tokens': 1728, 'input_token_details': {'cache_read': 1582, 'cache_creation': 0}}\n```\n\nExample: Anthropic citations‚Äã\n\n```\nmessage = {    \"role\": \"user\",    \"content\": [        {            \"type\": \"text\",            \"text\": \"Generate a 5 word summary of this document.\",        },        {            \"type\": \"file\",            \"source_type\": \"base64\",            \"data\": pdf_data,            \"mime_type\": \"application/pdf\",            \"citations\": {\"enabled\": True},        },    ],}response = llm.invoke([message])response.content\n```\n\n```\n[{'citations': [{'cited_text': 'Sample PDF\\r\\nThis is a simple PDF file. Fun fun fun.\\r\\n',    'document_index': 0,    'document_title': None,    'end_page_number': 2,    'start_page_number': 1,    'type': 'page_location'}],  'text': 'Simple PDF file: fun fun',  'type': 'text'}]\n```", "url_final": "https://python.langchain.com/docs/how_to/multimodal_inputs/", "url_citable": "https://python.langchain.com/docs/how_to/multimodal_inputs/#provider-specific-parameters", "title": "How to pass multimodal data to models | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Provider-specific parameters‚Äã", "section_anchor": "provider-specific-parameters", "fetched_at": "2025-10-06T21:55:57.346436", "has_code": true}
{"id": "8589752c3448eff2fdc8cca372b0745acb04dedc", "text": "Example: OpenAI file names‚Äã\n\nOpenAI requires that PDF documents be associated with file names:\n\n```\nllm = init_chat_model(\"openai:gpt-4.1\")message = {    \"role\": \"user\",    \"content\": [        {            \"type\": \"text\",            \"text\": \"Describe the document:\",        },        {            \"type\": \"file\",            \"source_type\": \"base64\",            \"data\": pdf_data,            \"mime_type\": \"application/pdf\",            \"filename\": \"my-file\",        },    ],}response = llm.invoke([message])print(response.text())\n```\n\n```\nThe document is a sample PDF file containing placeholder text. It consists of one page, titled \"Sample PDF\". The content is a mixture of English and the commonly used filler text \"Lorem ipsum dolor sit amet...\" and its extensions, which are often used in publishing and web design as generic text to demonstrate font, layout, and other visual elements.**Key points about the document:**- Length: 1 page- Purpose: Demonstrative/sample content- Content: No substantive or meaningful information, just demonstration text in paragraph form- Language: English (with the Latin-like \"Lorem Ipsum\" text used for layout purposes)There are no charts, tables, diagrams, or images on the page‚Äîonly plain text. The document serves as an example of what a PDF file looks like rather than providing actual, useful content.\n```", "url_final": "https://python.langchain.com/docs/how_to/multimodal_inputs/", "url_citable": "https://python.langchain.com/docs/how_to/multimodal_inputs/#example-openai-file-names", "title": "How to pass multimodal data to models | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Example: OpenAI file names‚Äã", "section_anchor": "example-openai-file-names", "fetched_at": "2025-10-06T21:55:57.346436", "has_code": true}
{"id": "eaec060504ada7921cbc6edf0b74db6fd3558b68", "text": "Tool calls‚Äã\n\nSome multimodal models support tool calling features as well. To call tools using such models, simply bind tools to them in the usual way, and invoke the model using content blocks of the desired type (e.g., containing image data).\n\n```\nfrom typing import Literalfrom langchain_core.tools import tool@tooldef weather_tool(weather: Literal[\"sunny\", \"cloudy\", \"rainy\"]) -> None:    \"\"\"Describe the weather\"\"\"    passllm_with_tools = llm.bind_tools([weather_tool])message = {    \"role\": \"user\",    \"content\": [        {\"type\": \"text\", \"text\": \"Describe the weather in this image:\"},        {\"type\": \"image\", \"source_type\": \"url\", \"url\": image_url},    ],}response = llm_with_tools.invoke([message])response.tool_calls\n```\n\n```\n[{'name': 'weather_tool',  'args': {'weather': 'sunny'},  'id': 'toolu_01G6JgdkhwggKcQKfhXZQPjf',  'type': 'tool_call'}]\n```\n\n- Images from base64 data\n- Images from a URL\n\n- Documents from base64 data\n- Documents from a URL\n\n- Audio from base64 data\n\n- Example: Anthropic prompt caching\n- Example: Anthropic citations\n- Example: OpenAI file names", "url_final": "https://python.langchain.com/docs/how_to/multimodal_inputs/", "url_citable": "https://python.langchain.com/docs/how_to/multimodal_inputs/#tool-calls", "title": "How to pass multimodal data to models | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Tool calls‚Äã", "section_anchor": "tool-calls", "fetched_at": "2025-10-06T21:55:57.346436", "has_code": true}
{"id": "b3c30218236070f847803be282647606001acf5a", "text": "Using JSONLoader‚Äã\n\nSuppose we are interested in extracting the values under the content field within the messages key of the JSON data. This can easily be done through the JSONLoader as shown below.\n\nJSON file‚Äã\n\n```\nloader = JSONLoader(    file_path='./example_data/facebook_chat.json',    jq_schema='.messages[].content',    text_content=False)data = loader.load()\n```\n\n```\npprint(data)\n```\n\n```\n    [Document(page_content='Bye!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 1}),     Document(page_content='Oh no worries! Bye', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 2}),     Document(page_content='No Im sorry it was my mistake, the blue one is not for sale', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 3}),     Document(page_content='I thought you were selling the blue one!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 4}),     Document(page_content='Im not interested in this bag. Im interested in the blue one!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 5}),     Document(page_content='Here is $129', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 6}),     Document(page_content='', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 7}),     Document(page_content='Online is at least $100', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 8}),     Document(page_content='How much do you want?', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 9}),     Document(page_content='Goodmorning! $50 is too low.', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 10}),     Document(page_content='Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 11})]\n```", "url_final": "https://python.langchain.com/docs/how_to/document_loader_json/", "url_citable": "https://python.langchain.com/docs/how_to/document_loader_json/#using-jsonloader", "title": "How to load JSON | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Using JSONLoader‚Äã", "section_anchor": "using-jsonloader", "fetched_at": "2025-10-06T21:55:23.089447", "has_code": true}
{"id": "9f954cec487e784ec14b8a4f5c740915881b4b5c", "text": "JSON Lines file‚Äã\n\nIf you want to load documents from a JSON Lines file, you pass json_lines=True and specify jq_schema to extract page_content from a single JSON object.\n\n```\nfile_path = './example_data/facebook_chat_messages.jsonl'pprint(Path(file_path).read_text())\n```\n\n```\n    ('{\"sender_name\": \"User 2\", \"timestamp_ms\": 1675597571851, \"content\": \"Bye!\"}\\n'     '{\"sender_name\": \"User 1\", \"timestamp_ms\": 1675597435669, \"content\": \"Oh no '     'worries! Bye\"}\\n'     '{\"sender_name\": \"User 2\", \"timestamp_ms\": 1675596277579, \"content\": \"No Im '     'sorry it was my mistake, the blue one is not for sale\"}\\n')\n```\n\n```\nloader = JSONLoader(    file_path='./example_data/facebook_chat_messages.jsonl',    jq_schema='.content',    text_content=False,    json_lines=True)data = loader.load()\n```\n\n```\npprint(data)\n```\n\n```\n    [Document(page_content='Bye!', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl', 'seq_num': 1}),     Document(page_content='Oh no worries! Bye', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl', 'seq_num': 2}),     Document(page_content='No Im sorry it was my mistake, the blue one is not for sale', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl', 'seq_num': 3})]\n```\n\nAnother option is to set jq_schema='.' and provide content_key:\n\n```\nloader = JSONLoader(    file_path='./example_data/facebook_chat_messages.jsonl',    jq_schema='.',    content_key='sender_name',    json_lines=True)data = loader.load()\n```\n\n```\npprint(data)\n```\n\n```\n    [Document(page_content='User 2', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl', 'seq_num': 1}),     Document(page_content='User 1', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl', 'seq_num': 2}),     Document(page_content='User 2', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl', 'seq_num': 3})]\n```", "url_final": "https://python.langchain.com/docs/how_to/document_loader_json/", "url_citable": "https://python.langchain.com/docs/how_to/document_loader_json/#json-lines-file", "title": "How to load JSON | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "JSON Lines file‚Äã", "section_anchor": "json-lines-file", "fetched_at": "2025-10-06T21:55:23.089447", "has_code": true}
{"id": "9b6c6f27eb999429d133cd14a76254585efed238", "text": "JSON file with jq schema content_key‚Äã\n\nTo load documents from a JSON file using the content_key within the jq schema, set is_content_key_jq_parsable=True. Ensure that content_key is compatible and can be parsed using the jq schema.\n\n```\nfile_path = './sample.json'pprint(Path(file_path).read_text())\n```\n\n```\n    {\"data\": [        {\"attributes\": {            \"message\": \"message1\",            \"tags\": [            \"tag1\"]},        \"id\": \"1\"},        {\"attributes\": {            \"message\": \"message2\",            \"tags\": [            \"tag2\"]},        \"id\": \"2\"}]}\n```\n\n```\nloader = JSONLoader(    file_path=file_path,    jq_schema=\".data[]\",    content_key=\".attributes.message\",    is_content_key_jq_parsable=True,)data = loader.load()\n```\n\n```\npprint(data)\n```\n\n```\n    [Document(page_content='message1', metadata={'source': '/path/to/sample.json', 'seq_num': 1}),     Document(page_content='message2', metadata={'source': '/path/to/sample.json', 'seq_num': 2})]\n```", "url_final": "https://python.langchain.com/docs/how_to/document_loader_json/", "url_citable": "https://python.langchain.com/docs/how_to/document_loader_json/#json-file-with-jq-schema-content_key", "title": "How to load JSON | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "JSON file with jq schema content_key‚Äã", "section_anchor": "json-file-with-jq-schema-content_key", "fetched_at": "2025-10-06T21:55:23.089447", "has_code": true}
{"id": "44a8542621fb0e361ff605e72c0bd9e4af2dc9b5", "text": "Extracting metadata‚Äã\n\nGenerally, we want to include metadata available in the JSON file into the documents that we create from the content.\n\nThe following demonstrates how metadata can be extracted using the JSONLoader.\n\nThere are some key changes to be noted. In the previous example where we didn't collect the metadata, we managed to directly specify in the schema where the value for the page_content can be extracted from.\n\n```\n.messages[].content\n```\n\nIn the current example, we have to tell the loader to iterate over the records in the messages field. The jq_schema then has to be:\n\n```\n.messages[]\n```\n\nThis allows us to pass the records (dict) into the metadata_func that has to be implemented. The metadata_func is responsible for identifying which pieces of information in the record should be included in the metadata stored in the final Document object.\n\nAdditionally, we now have to explicitly specify in the loader, via the content_key argument, the key from the record where the value for the page_content needs to be extracted from.\n\n```\n# Define the metadata extraction function.def metadata_func(record: dict, metadata: dict) -> dict:    metadata[\"sender_name\"] = record.get(\"sender_name\")    metadata[\"timestamp_ms\"] = record.get(\"timestamp_ms\")    return metadataloader = JSONLoader(    file_path='./example_data/facebook_chat.json',    jq_schema='.messages[]',    content_key=\"content\",    metadata_func=metadata_func)data = loader.load()\n```\n\n```\npprint(data)\n```\n\n```\n    [Document(page_content='Bye!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 1, 'sender_name': 'User 2', 'timestamp_ms': 1675597571851}),     Document(page_content='Oh no worries! Bye', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 2, 'sender_name': 'User 1', 'timestamp_ms': 1675597435669}),     Document(page_content='No Im sorry it was my mistake, the blue one is not for sale', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 3, 'sender_name': 'User 2', 'timestamp_ms': 1675596277579}),     Document(page_content='I thought you were selling the blue one!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 4, 'sender_name': 'User 1', 'timestamp_ms': 1675595140251}),     Document(page_content='Im not interested in this bag. Im interested in the blue one!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 5, 'sender_name': 'User 1', 'timestamp_ms': 1675595109305}),     Document(page_content='Here is $129', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 6, 'sender_name': 'User 2', 'timestamp_ms': 1675595068468}),     Document(page_content='', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 7, 'sender_name': 'User 2', 'timestamp_ms': 1675595060730}),     Document(page_content='Online is at least $100', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 8, 'sender_name': 'User 2', 'timestamp_ms': 1675595045152}),     Document(page_content='How much do you want?', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 9, 'sender_name': 'User 1', 'timestamp_ms': 1675594799696}),     Document(page_content='Goodmorning! $50 is too low.', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 10, 'sender_name': 'User 2', 'timestamp_ms': 1675577876645}),     Document(page_content='Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 11, 'sender_name': 'User 1', 'timestamp_ms': 1675549022673})]\n```\n\nNow, you will see that the documents contain the metadata associated with the content we extracted.", "url_final": "https://python.langchain.com/docs/how_to/document_loader_json/", "url_citable": "https://python.langchain.com/docs/how_to/document_loader_json/#extracting-metadata", "title": "How to load JSON | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Extracting metadata‚Äã", "section_anchor": "extracting-metadata", "fetched_at": "2025-10-06T21:55:23.089447", "has_code": true}
{"id": "347b33228be75c53a0ed78b53471f0ff2cfebd30", "text": "The metadata_func‚Äã\n\nAs shown above, the metadata_func accepts the default metadata generated by the JSONLoader. This allows full control to the user with respect to how the metadata is formatted.\n\nFor example, the default metadata contains the source and the seq_num keys. However, it is possible that the JSON data contain these keys as well. The user can then exploit the metadata_func to rename the default keys and use the ones from the JSON data.\n\nThe example below shows how we can modify the source to only contain information of the file source relative to the langchain directory.\n\n```\n# Define the metadata extraction function.def metadata_func(record: dict, metadata: dict) -> dict:    metadata[\"sender_name\"] = record.get(\"sender_name\")    metadata[\"timestamp_ms\"] = record.get(\"timestamp_ms\")    if \"source\" in metadata:        source = metadata[\"source\"].split(\"/\")        source = source[source.index(\"langchain\"):]        metadata[\"source\"] = \"/\".join(source)    return metadataloader = JSONLoader(    file_path='./example_data/facebook_chat.json',    jq_schema='.messages[]',    content_key=\"content\",    metadata_func=metadata_func)data = loader.load()\n```\n\n```\npprint(data)\n```\n\n```\n    [Document(page_content='Bye!', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 1, 'sender_name': 'User 2', 'timestamp_ms': 1675597571851}),     Document(page_content='Oh no worries! Bye', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 2, 'sender_name': 'User 1', 'timestamp_ms': 1675597435669}),     Document(page_content='No Im sorry it was my mistake, the blue one is not for sale', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 3, 'sender_name': 'User 2', 'timestamp_ms': 1675596277579}),     Document(page_content='I thought you were selling the blue one!', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 4, 'sender_name': 'User 1', 'timestamp_ms': 1675595140251}),     Document(page_content='Im not interested in this bag. Im interested in the blue one!', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 5, 'sender_name': 'User 1', 'timestamp_ms': 1675595109305}),     Document(page_content='Here is $129', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 6, 'sender_name': 'User 2', 'timestamp_ms': 1675595068468}),     Document(page_content='', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 7, 'sender_name': 'User 2', 'timestamp_ms': 1675595060730}),     Document(page_content='Online is at least $100', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 8, 'sender_name': 'User 2', 'timestamp_ms': 1675595045152}),     Document(page_content='How much do you want?', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 9, 'sender_name': 'User 1', 'timestamp_ms': 1675594799696}),     Document(page_content='Goodmorning! $50 is too low.', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 10, 'sender_name': 'User 2', 'timestamp_ms': 1675577876645}),     Document(page_content='Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 11, 'sender_name': 'User 1', 'timestamp_ms': 1675549022673})]\n```", "url_final": "https://python.langchain.com/docs/how_to/document_loader_json/", "url_citable": "https://python.langchain.com/docs/how_to/document_loader_json/#the-metadata_func", "title": "How to load JSON | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "The metadata_func‚Äã", "section_anchor": "the-metadata_func", "fetched_at": "2025-10-06T21:55:23.089447", "has_code": true}
{"id": "502e1d52d08472a6e16293c08c8512811737fbdf", "text": "Common JSON structures with jq schema‚Äã\n\nThe list below provides a reference to the possible jq_schema the user can use to extract content from the JSON data depending on the structure.\n\n```\nJSON        -> [{\"text\": ...}, {\"text\": ...}, {\"text\": ...}]jq_schema   -> \".[].text\"JSON        -> {\"key\": [{\"text\": ...}, {\"text\": ...}, {\"text\": ...}]}jq_schema   -> \".key[].text\"JSON        -> [\"...\", \"...\", \"...\"]jq_schema   -> \".[]\"\n```\n\n- JSON file\n- JSON Lines file\n- JSON file with jq schema content_key", "url_final": "https://python.langchain.com/docs/how_to/document_loader_json/", "url_citable": "https://python.langchain.com/docs/how_to/document_loader_json/#common-json-structures-with-jq-schema", "title": "How to load JSON | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Common JSON structures with jq schema‚Äã", "section_anchor": "common-json-structures-with-jq-schema", "fetched_at": "2025-10-06T21:55:23.089447", "has_code": true}
{"id": "ca0f69ee6e778b8d25a4dca06b364cab1860107a", "text": "How to use multimodal prompts | ü¶úÔ∏èüîó LangChain\n\nHere we demonstrate how to use prompt templates to format multimodal inputs to models.\n\nTo use prompt templates in the context of multimodal data, we can templatize elements of the corresponding content block. For example, below we define a prompt that takes a URL for an image as a parameter:\n\n```\nfrom langchain_core.prompts import ChatPromptTemplate# Define promptprompt = ChatPromptTemplate(    [        {            \"role\": \"system\",            \"content\": \"Describe the image provided.\",        },        {            \"role\": \"user\",            \"content\": [                {                    \"type\": \"image\",                    \"source_type\": \"url\",                    \"url\": \"{image_url}\",                },            ],        },    ])\n```\n\nLet's use this prompt to pass an image to a chat model:\n\n```\nfrom langchain.chat_models import init_chat_modelllm = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\")url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"chain = prompt | llmresponse = chain.invoke({\"image_url\": url})print(response.text())\n```\n\n```\nThis image shows a beautiful wooden boardwalk cutting through a lush green wetland or marsh area. The boardwalk extends straight ahead toward the horizon, creating a strong leading line through the composition. On either side, tall green grasses sway in what appears to be a summer or late spring setting. The sky is particularly striking, with wispy cirrus clouds streaking across a vibrant blue background. In the distance, you can see a tree line bordering the wetland area. The lighting suggests this may be during \"golden hour\" - either early morning or late afternoon - as there's a warm, gentle quality to the light that's illuminating the scene. The wooden planks of the boardwalk appear well-maintained and provide safe passage through what would otherwise be difficult terrain to traverse. It's the kind of scene you might find in a nature preserve or wildlife refuge designed to give visitors access to observe wetland ecosystems while protecting the natural environment.\n```\n\nNote that we can templatize arbitrary elements of the content block:\n\n```\nprompt = ChatPromptTemplate(    [        {            \"role\": \"system\",            \"content\": \"Describe the image provided.\",        },        {            \"role\": \"user\",            \"content\": [                {                    \"type\": \"image\",                    \"source_type\": \"base64\",                    \"mime_type\": \"{image_mime_type}\",                    \"data\": \"{image_data}\",                    \"cache_control\": {\"type\": \"{cache_type}\"},                },            ],        },    ])\n```\n\n```\nimport base64import httpximage_data = base64.b64encode(httpx.get(url).content).decode(\"utf-8\")chain = prompt | llmresponse = chain.invoke(    {        \"image_data\": image_data,        \"image_mime_type\": \"image/jpeg\",        \"cache_type\": \"ephemeral\",    })print(response.text())\n```\n\n```\nThis image shows a beautiful wooden boardwalk cutting through a lush green marsh or wetland area. The boardwalk extends straight ahead toward the horizon, creating a strong leading line in the composition. The surrounding vegetation consists of tall grass and reeds in vibrant green hues, with some bushes and trees visible in the background. The sky is particularly striking, featuring a bright blue color with wispy white clouds streaked across it. The lighting suggests this photo was taken during the \"golden hour\" - either early morning or late afternoon - giving the scene a warm, peaceful quality. The raised wooden path provides accessible access through what would otherwise be difficult terrain to traverse, allowing visitors to experience and appreciate this natural environment.\n```", "url_final": "https://python.langchain.com/docs/how_to/multimodal_prompts/", "url_citable": "https://python.langchain.com/docs/how_to/multimodal_prompts/", "title": "How to use multimodal prompts | ü¶úÔ∏èüîó LangChain", "section_level": 1, "section_heading": "How to use multimodal prompts | ü¶úÔ∏èüîó LangChain", "section_anchor": null, "fetched_at": "2025-10-06T21:55:58.344907", "has_code": true}
{"id": "2674e6867d453d3be0fb112df2d0fb7d51954650", "text": "Creating a retriever from a vectorstore‚Äã\n\nYou can build a retriever from a vectorstore using its .as_retriever method. Let's walk through an example.\n\nFirst we instantiate a vectorstore. We will use an in-memory FAISS vectorstore:\n\n```\nfrom langchain_community.document_loaders import TextLoaderfrom langchain_community.vectorstores import FAISSfrom langchain_openai import OpenAIEmbeddingsfrom langchain_text_splitters import CharacterTextSplitterloader = TextLoader(\"state_of_the_union.txt\")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)texts = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings()vectorstore = FAISS.from_documents(texts, embeddings)\n```\n\nWe can then instantiate a retriever:\n\n```\nretriever = vectorstore.as_retriever()\n```\n\nThis creates a retriever (specifically a VectorStoreRetriever), which we can use in the usual way:\n\n```\ndocs = retriever.invoke(\"what did the president say about ketanji brown jackson?\")\n```", "url_final": "https://python.langchain.com/docs/how_to/vectorstore_retriever/", "url_citable": "https://python.langchain.com/docs/how_to/vectorstore_retriever/#creating-a-retriever-from-a-vectorstore", "title": "How to use a vectorstore as a retriever | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Creating a retriever from a vectorstore‚Äã", "section_anchor": "creating-a-retriever-from-a-vectorstore", "fetched_at": "2025-10-06T21:54:17.563182", "has_code": true}
{"id": "f54fdca2eb44a79320ec8dccc53bb6764facfa86", "text": "Maximum marginal relevance retrieval‚Äã\n\nBy default, the vector store retriever uses similarity search. If the underlying vector store supports maximum marginal relevance search, you can specify that as the search type.\n\nThis effectively specifies what method on the underlying vectorstore is used (e.g., similarity_search, max_marginal_relevance_search, etc.).\n\n```\nretriever = vectorstore.as_retriever(search_type=\"mmr\")\n```\n\n```\ndocs = retriever.invoke(\"what did the president say about ketanji brown jackson?\")\n```", "url_final": "https://python.langchain.com/docs/how_to/vectorstore_retriever/", "url_citable": "https://python.langchain.com/docs/how_to/vectorstore_retriever/#maximum-marginal-relevance-retrieval", "title": "How to use a vectorstore as a retriever | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Maximum marginal relevance retrieval‚Äã", "section_anchor": "maximum-marginal-relevance-retrieval", "fetched_at": "2025-10-06T21:54:17.563182", "has_code": true}
{"id": "6092c7d88e3bbbca71bfd9e3bec0d7fe1e6f2b2e", "text": "Passing search parameters‚Äã\n\nWe can pass parameters to the underlying vectorstore's search methods using search_kwargs.\n\nSimilarity score threshold retrieval‚Äã\n\nFor example, we can set a similarity score threshold and only return documents with a score above that threshold.\n\n```\nretriever = vectorstore.as_retriever(    search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.5})\n```\n\n```\ndocs = retriever.invoke(\"what did the president say about ketanji brown jackson?\")\n```\n\nSpecifying top k‚Äã\n\nWe can also limit the number of documents k returned by the retriever.\n\n```\nretriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n```\n\n```\ndocs = retriever.invoke(\"what did the president say about ketanji brown jackson?\")len(docs)\n```\n\n```\n1\n```\n\n- Similarity score threshold retrieval\n- Specifying top k", "url_final": "https://python.langchain.com/docs/how_to/vectorstore_retriever/", "url_citable": "https://python.langchain.com/docs/how_to/vectorstore_retriever/#passing-search-parameters", "title": "How to use a vectorstore as a retriever | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Passing search parameters‚Äã", "section_anchor": "passing-search-parameters", "fetched_at": "2025-10-06T21:54:17.563182", "has_code": true}
{"id": "e4582a261c52ee21fee1367a7023fa2c6d82d001", "text": "Streaming‚Äã\n\nAs mentioned above, a key difference between the JsonOutputParser and the PydanticOutputParser is that the JsonOutputParser output parser supports streaming partial chunks. Here's what that looks like:\n\n```\nfor s in chain.stream({\"query\": joke_query}):    print(s)\n```\n\n```\n{}{'setup': ''}{'setup': 'Why'}{'setup': 'Why couldn'}{'setup': \"Why couldn't\"}{'setup': \"Why couldn't the\"}{'setup': \"Why couldn't the bicycle\"}{'setup': \"Why couldn't the bicycle stand\"}{'setup': \"Why couldn't the bicycle stand up\"}{'setup': \"Why couldn't the bicycle stand up by\"}{'setup': \"Why couldn't the bicycle stand up by itself\"}{'setup': \"Why couldn't the bicycle stand up by itself?\"}{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': ''}{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because'}{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it'}{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was'}{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was two'}{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was two tired'}{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was two tired!'}\n```", "url_final": "https://python.langchain.com/docs/how_to/output_parser_json/", "url_citable": "https://python.langchain.com/docs/how_to/output_parser_json/#streaming", "title": "How to parse JSON output | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Streaming‚Äã", "section_anchor": "streaming", "fetched_at": "2025-10-06T21:56:01.392056", "has_code": true}
{"id": "6c42d0498cefec7a76131d22939593880f157f13", "text": "Without Pydantic‚Äã\n\nYou can also use the JsonOutputParser without Pydantic. This will prompt the model to return JSON, but doesn't provide specifics about what the schema should be.\n\n```\njoke_query = \"Tell me a joke.\"parser = JsonOutputParser()prompt = PromptTemplate(    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",    input_variables=[\"query\"],    partial_variables={\"format_instructions\": parser.get_format_instructions()},)chain = prompt | model | parserchain.invoke({\"query\": joke_query})\n```\n\n```\n{'response': \"Sure! Here's a joke for you: Why couldn't the bicycle stand up by itself? Because it was two tired!\"}\n```", "url_final": "https://python.langchain.com/docs/how_to/output_parser_json/", "url_citable": "https://python.langchain.com/docs/how_to/output_parser_json/#without-pydantic", "title": "How to parse JSON output | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Without Pydantic‚Äã", "section_anchor": "without-pydantic", "fetched_at": "2025-10-06T21:56:01.392056", "has_code": true}
{"id": "35c2f0450b66f020a61196fccd4faaa2ab47e1b6", "text": "Next steps‚Äã\n\nYou've now learned one way to prompt a model to return structured JSON. Next, check out the broader guide on obtaining structured output for other techniques.\n\n- Streaming\n- Without Pydantic\n- Next steps", "url_final": "https://python.langchain.com/docs/how_to/output_parser_json/", "url_citable": "https://python.langchain.com/docs/how_to/output_parser_json/#next-steps", "title": "How to parse JSON output | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Next steps‚Äã", "section_anchor": "next-steps", "fetched_at": "2025-10-06T21:56:01.392056", "has_code": false}
{"id": "1d80468dbda2c92230ba163ff24407f65cc1803c", "text": "Setup‚Äã\n\nJupyter Notebook‚Äã\n\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebooks. Going through guides in an interactive environment is a great way to better understand them. See here for instructions on how to install.\n\nInstallation‚Äã\n\nTo install LangChain run:\n\n- Pip\n- Conda\n\n```\npip install --upgrade langchain-core\n```\n\n```\nconda install langchain-core -c conda-forge\n```\n\nFor more details, see our Installation guide.\n\nLangSmith‚Äã\n\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\n\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\n\n```\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"\n```\n\nOr, if in a notebook, you can set them with:\n\n```\nimport getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n```", "url_final": "https://python.langchain.com/docs/tutorials/extraction/", "url_citable": "https://python.langchain.com/docs/tutorials/extraction/#setup", "title": "Build an Extraction Chain | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Setup‚Äã", "section_anchor": "setup", "fetched_at": "2025-10-06T21:54:08.443518", "has_code": true}
{"id": "e32fff5ca95d905c1269f2a8157d4a41cd03a3e2", "text": "The Schema‚Äã\n\nFirst, we need to describe what information we want to extract from the text.\n\nWe'll use Pydantic to define an example schema to extract personal information.\n\n```\nfrom typing import Optionalfrom pydantic import BaseModel, Fieldclass Person(BaseModel):    \"\"\"Information about a person.\"\"\"    # ^ Doc-string for the entity Person.    # This doc-string is sent to the LLM as the description of the schema Person,    # and it can help to improve extraction results.    # Note that:    # 1. Each field is an `optional` -- this allows the model to decline to extract it!    # 2. Each field has a `description` -- this description is used by the LLM.    # Having a good description can help improve extraction results.    name: Optional[str] = Field(default=None, description=\"The name of the person\")    hair_color: Optional[str] = Field(        default=None, description=\"The color of the person's hair if known\"    )    height_in_meters: Optional[str] = Field(        default=None, description=\"Height measured in meters\"    )\n```\n\nThere are two best practices when defining schema:\n\n- Document the attributes and the schema itself: This information is sent to the LLM and is used to improve the quality of information extraction.\n- Do not force the LLM to make up information! Above we used Optional for the attributes allowing the LLM to output None if it doesn't know the answer.\n\nFor best performance, document the schema well and make sure the model isn't forced to return results if there's no information to be extracted in the text.", "url_final": "https://python.langchain.com/docs/tutorials/extraction/", "url_citable": "https://python.langchain.com/docs/tutorials/extraction/#the-schema", "title": "Build an Extraction Chain | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "The Schema‚Äã", "section_anchor": "the-schema", "fetched_at": "2025-10-06T21:54:08.443518", "has_code": true}
{"id": "0993f2696c21cc484149c60d85edc6db1cffe042", "text": "The Extractor‚Äã\n\nLet's create an information extractor using the schema we defined above.\n\n```\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder# Define a custom prompt to provide instructions and any additional context.# 1) You can add examples into the prompt template to improve extraction quality# 2) Introduce additional parameters to take context into account (e.g., include metadata#    about the document from which the text was extracted.)prompt_template = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"You are an expert extraction algorithm. \"            \"Only extract relevant information from the text. \"            \"If you do not know the value of an attribute asked to extract, \"            \"return null for the attribute's value.\",        ),        # Please see the how-to about improving performance with        # reference examples.        # MessagesPlaceholder('examples'),        (\"human\", \"{text}\"),    ])\n```\n\nWe need to use a model that supports function/tool calling.\n\nPlease review the documentation for all models that can be used with this API.\n\n```\npip install -qU \"langchain[google-genai]\"\n```\n\n```\nimport getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n```\n\n```\nstructured_llm = llm.with_structured_output(schema=Person)\n```\n\nLet's test it out:\n\n```\ntext = \"Alan Smith is 6 feet tall and has blond hair.\"prompt = prompt_template.invoke({\"text\": text})structured_llm.invoke(prompt)\n```\n\n```\nPerson(name='Alan Smith', hair_color='blond', height_in_meters='1.83')\n```\n\nExtraction is Generative ü§Ø\n\nLLMs are generative models, so they can do some pretty cool things like correctly extract the height of the person in meters even though it was provided in feet!\n\nWe can see the LangSmith trace here. Note that the chat model portion of the trace reveals the exact sequence of messages sent to the model, tools invoked, and other metadata.", "url_final": "https://python.langchain.com/docs/tutorials/extraction/", "url_citable": "https://python.langchain.com/docs/tutorials/extraction/#the-extractor", "title": "Build an Extraction Chain | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "The Extractor‚Äã", "section_anchor": "the-extractor", "fetched_at": "2025-10-06T21:54:08.443518", "has_code": true}
{"id": "fbe753436a178824dae1ec80eb8c78b1e8e16455", "text": "Multiple Entities‚Äã\n\nIn most cases, you should be extracting a list of entities rather than a single entity.\n\nThis can be easily achieved using pydantic by nesting models inside one another.\n\n```\nfrom typing import List, Optionalfrom pydantic import BaseModel, Fieldclass Person(BaseModel):    \"\"\"Information about a person.\"\"\"    # ^ Doc-string for the entity Person.    # This doc-string is sent to the LLM as the description of the schema Person,    # and it can help to improve extraction results.    # Note that:    # 1. Each field is an `optional` -- this allows the model to decline to extract it!    # 2. Each field has a `description` -- this description is used by the LLM.    # Having a good description can help improve extraction results.    name: Optional[str] = Field(default=None, description=\"The name of the person\")    hair_color: Optional[str] = Field(        default=None, description=\"The color of the person's hair if known\"    )    height_in_meters: Optional[str] = Field(        default=None, description=\"Height measured in meters\"    )class Data(BaseModel):    \"\"\"Extracted data about people.\"\"\"    # Creates a model so that we can extract multiple entities.    people: List[Person]\n```\n\nExtraction results might not be perfect here. Read on to see how to use Reference Examples to improve the quality of extraction, and check out our extraction how-to guides for more detail.\n\n```\nstructured_llm = llm.with_structured_output(schema=Data)text = \"My name is Jeff, my hair is black and i am 6 feet tall. Anna has the same color hair as me.\"prompt = prompt_template.invoke({\"text\": text})structured_llm.invoke(prompt)\n```\n\n```\nData(people=[Person(name='Jeff', hair_color='black', height_in_meters='1.83'), Person(name='Anna', hair_color='black', height_in_meters=None)])\n```\n\nWhen the schema accommodates the extraction of multiple entities, it also allows the model to extract no entities if no relevant information is in the text by providing an empty list.\n\nThis is usually a good thing! It allows specifying required attributes on an entity without necessarily forcing the model to detect this entity.\n\nWe can see the LangSmith trace here.", "url_final": "https://python.langchain.com/docs/tutorials/extraction/", "url_citable": "https://python.langchain.com/docs/tutorials/extraction/#multiple-entities", "title": "Build an Extraction Chain | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Multiple Entities‚Äã", "section_anchor": "multiple-entities", "fetched_at": "2025-10-06T21:54:08.443518", "has_code": true}
{"id": "549850b9aa1785581375303fb278b7ec32659d81", "text": "Reference examples‚Äã\n\nThe behavior of LLM applications can be steered using few-shot prompting. For chat models, this can take the form of a sequence of pairs of input and response messages demonstrating desired behaviors.\n\nFor example, we can convey the meaning of a symbol with alternating user and assistant messages:\n\n```\nmessages = [    {\"role\": \"user\", \"content\": \"2 ü¶ú 2\"},    {\"role\": \"assistant\", \"content\": \"4\"},    {\"role\": \"user\", \"content\": \"2 ü¶ú 3\"},    {\"role\": \"assistant\", \"content\": \"5\"},    {\"role\": \"user\", \"content\": \"3 ü¶ú 4\"},]response = llm.invoke(messages)print(response.content)\n```\n\n```\n7\n```\n\nStructured output often uses tool calling under-the-hood. This typically involves the generation of AI messages containing tool calls, as well as tool messages containing the results of tool calls. What should a sequence of messages look like in this case?\n\nDifferent chat model providers impose different requirements for valid message sequences. Some will accept a (repeating) message sequence of the form:\n\n- User message\n- AI message with tool call\n- Tool message with result\n\nOthers require a final AI message containing some sort of response.\n\nLangChain includes a utility function tool_example_to_messages that will generate a valid sequence for most model providers. It simplifies the generation of structured few-shot examples by just requiring Pydantic representations of the corresponding tool calls.\n\nLet's try this out. We can convert pairs of input strings and desired Pydantic objects to a sequence of messages that can be provided to a chat model. Under the hood, LangChain will format the tool calls to each provider's required format.\n\nNote: this version of tool_example_to_messages requires langchain-core>=0.3.20.\n\n```\nfrom langchain_core.utils.function_calling import tool_example_to_messagesexamples = [    (        \"The ocean is vast and blue. It's more than 20,000 feet deep.\",        Data(people=[]),    ),    (        \"Fiona traveled far from France to Spain.\",        Data(people=[Person(name=\"Fiona\", height_in_meters=None, hair_color=None)]),    ),]messages = []for txt, tool_call in examples:    if tool_call.people:        # This final message is optional for some providers        ai_response = \"Detected people.\"    else:        ai_response = \"Detected no people.\"    messages.extend(tool_example_to_messages(txt, [tool_call], ai_response=ai_response))\n```\n\nInspecting the result, we see these two example pairs generated eight messages:\n\n```\nfor message in messages:    message.pretty_print()\n```\n\n```\n================================\u001b[1m Human Message \u001b[0m=================================The ocean is vast and blue. It's more than 20,000 feet deep.==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls:  Data (d8f2e054-7fb9-417f-b28f-0447a775b2c3) Call ID: d8f2e054-7fb9-417f-b28f-0447a775b2c3  Args:    people: []=================================\u001b[1m Tool Message \u001b[0m=================================You have correctly called this tool.==================================\u001b[1m Ai Message \u001b[0m==================================Detected no people.================================\u001b[1m Human Message \u001b[0m=================================Fiona traveled far from France to Spain.==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls:  Data (0178939e-a4b1-4d2a-a93e-b87f665cdfd6) Call ID: 0178939e-a4b1-4d2a-a93e-b87f665cdfd6  Args:    people: [{'name': 'Fiona', 'hair_color': None, 'height_in_meters': None}]=================================\u001b[1m Tool Message \u001b[0m=================================You have correctly called this tool.==================================\u001b[1m Ai Message \u001b[0m==================================Detected people.\n```\n\nLet's compare performance with and without these messages. For example, let's pass a message for which we intend no people to be extracted:\n\n```\nmessage_no_extraction = {    \"role\": \"user\",    \"content\": \"The solar system is large, but earth has only 1 moon.\",}structured_llm = llm.with_structured_output(schema=Data)structured_llm.invoke([message_no_extraction])\n```\n\n```\nData(people=[Person(name='Earth', hair_color='None', height_in_meters='0.00')])\n```\n\nIn this example, the model is liable to erroneously generate records of people.\n\nBecause our few-shot examples contain examples of \"negatives\", we encourage the model to behave correctly in this case:\n\n```\nstructured_llm.invoke(messages + [message_no_extraction])\n```\n\n```\nData(people=[])\n```\n\nThe LangSmith trace for the run reveals the exact sequence of messages sent to the chat model, tool calls generated, latency, token counts, and other metadata.\n\nSee this guide for more detail on extraction workflows with reference examples, including how to incorporate prompt templates and customize the generation of example messages.", "url_final": "https://python.langchain.com/docs/tutorials/extraction/", "url_citable": "https://python.langchain.com/docs/tutorials/extraction/#reference-examples", "title": "Build an Extraction Chain | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Reference examples‚Äã", "section_anchor": "reference-examples", "fetched_at": "2025-10-06T21:54:08.443518", "has_code": true}
{"id": "4a2d3020c3fc6a9dc6a6ed1f8b001ebe30292416", "text": "Next steps‚Äã\n\nNow that you understand the basics of extraction with LangChain, you're ready to proceed to the rest of the how-to guides:\n\n- Add Examples: More detail on using reference examples to improve performance.\n- Handle Long Text: What should you do if the text does not fit into the context window of the LLM?\n- Use a Parsing Approach: Use a prompt based approach to extract with models that do not support tool/function calling.\n\n- Jupyter Notebook\n- Installation\n- LangSmith", "url_final": "https://python.langchain.com/docs/tutorials/extraction/", "url_citable": "https://python.langchain.com/docs/tutorials/extraction/#next-steps", "title": "Build an Extraction Chain | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Next steps‚Äã", "section_anchor": "next-steps", "fetched_at": "2025-10-06T21:54:08.443518", "has_code": false}
{"id": "eae524aeb66790386bee8ee7d12d4b36d5c355df", "text": "Configurable Fields‚Äã\n\nLet's walk through an example that configures chat model fields like temperature at runtime:\n\n```\n%pip install --upgrade --quiet langchain langchain-openaiimport osfrom getpass import getpassif \"OPENAI_API_KEY\" not in os.environ:    os.environ[\"OPENAI_API_KEY\"] = getpass()\n```\n\nConfiguring fields on a chat model‚Äã\n\nIf using init_chat_model to create a chat model, you can specify configurable fields in the constructor:\n\n```\nfrom langchain.chat_models import init_chat_modelllm = init_chat_model(    \"openai:gpt-4o-mini\",    configurable_fields=(\"temperature\",),)\n```\n\nYou can then set the parameter at runtime using .with_config:\n\n```\nresponse = llm.with_config({\"temperature\": 0}).invoke(\"Hello\")print(response.content)\n```\n\n```\nHello! How can I assist you today?\n```\n\nIn addition to invocation parameters like temperature, configuring fields this way extends to clients and other attributes.\n\nThis method is applicable when binding tools as well:\n\n```\nfrom langchain_core.tools import tool@tooldef get_weather(location: str):    \"\"\"Get the weather.\"\"\"    return \"It's sunny.\"llm_with_tools = llm.bind_tools([get_weather])response = llm_with_tools.with_config({\"temperature\": 0}).invoke(    \"What's the weather in SF?\")response.tool_calls\n```\n\n```\n[{'name': 'get_weather',  'args': {'location': 'San Francisco'},  'id': 'call_B93EttzlGyYUhzbIIiMcl3bE',  'type': 'tool_call'}]\n```\n\nIn addition to .with_config, we can now include the parameter when passing a configuration directly. See example below, where we allow the underlying model temperature to be configurable inside of a langgraph agent:\n\n```\n! pip install --upgrade langgraph\n```\n\n```\nfrom langgraph.prebuilt import create_react_agentagent = create_react_agent(llm, [get_weather])response = agent.invoke(    {\"messages\": \"What's the weather in Boston?\"},    {\"configurable\": {\"temperature\": 0}},)\n```", "url_final": "https://python.langchain.com/docs/how_to/configure/", "url_citable": "https://python.langchain.com/docs/how_to/configure/#configurable-fields", "title": "How to configure runtime chain internals | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Configurable Fields‚Äã", "section_anchor": "configurable-fields", "fetched_at": "2025-10-06T21:54:51.726137", "has_code": true}
{"id": "42883f5d39c751b541f544a0d3f45aee74aed2a8", "text": "Configuring fields on arbitrary Runnables‚Äã\n\nYou can also use the .configurable_fields method on arbitrary Runnables, as shown below:\n\n```\nfrom langchain_core.prompts import PromptTemplatefrom langchain_core.runnables import ConfigurableFieldfrom langchain_openai import ChatOpenAImodel = ChatOpenAI(temperature=0).configurable_fields(    temperature=ConfigurableField(        id=\"llm_temperature\",        name=\"LLM Temperature\",        description=\"The temperature of the LLM\",    ))model.invoke(\"pick a random number\")\n```\n\n```\nAIMessage(content='17', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 11, 'total_tokens': 12}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-ba26a0da-0a69-4533-ab7f-21178a73d303-0')\n```\n\nAbove, we defined temperature as a ConfigurableField that we can set at runtime. To do so, we use the with_config method like this:\n\n```\nmodel.with_config(configurable={\"llm_temperature\": 0.9}).invoke(\"pick a random number\")\n```\n\n```\nAIMessage(content='12', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 11, 'total_tokens': 12}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-ba8422ad-be77-4cb1-ac45-ad0aae74e3d9-0')\n```\n\nNote that the passed llm_temperature entry in the dict has the same key as the id of the ConfigurableField.\n\nWe can also do this to affect just one step that's part of a chain:\n\n```\nprompt = PromptTemplate.from_template(\"Pick a random number above {x}\")chain = prompt | modelchain.invoke({\"x\": 0})\n```\n\n```\nAIMessage(content='27', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 14, 'total_tokens': 15}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-ecd4cadd-1b72-4f92-b9a0-15e08091f537-0')\n```\n\n```\nchain.with_config(configurable={\"llm_temperature\": 0.9}).invoke({\"x\": 0})\n```\n\n```\nAIMessage(content='35', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 14, 'total_tokens': 15}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-a916602b-3460-46d3-a4a8-7c926ec747c0-0')\n```", "url_final": "https://python.langchain.com/docs/how_to/configure/", "url_citable": "https://python.langchain.com/docs/how_to/configure/#configuring-fields-on-arbitrary-runnables", "title": "How to configure runtime chain internals | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Configuring fields on arbitrary Runnables‚Äã", "section_anchor": "configuring-fields-on-arbitrary-runnables", "fetched_at": "2025-10-06T21:54:51.726137", "has_code": true}
{"id": "ed573eb31aad7b88c706300195f30528d6c24af8", "text": "Configurable Alternatives‚Äã\n\nThe configurable_alternatives() method allows us to swap out steps in a chain with an alternative. Below, we swap out one chat model for another:\n\n```\n%pip install --upgrade --quiet langchain-anthropicimport osfrom getpass import getpassif \"ANTHROPIC_API_KEY\" not in os.environ:    os.environ[\"ANTHROPIC_API_KEY\"] = getpass()\n```\n\n```\n\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.You should consider upgrading via the '/Users/jacoblee/.pyenv/versions/3.10.5/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\u001b[0mNote: you may need to restart the kernel to use updated packages.\n```\n\n```\nfrom langchain_anthropic import ChatAnthropicfrom langchain_core.prompts import PromptTemplatefrom langchain_core.runnables import ConfigurableFieldfrom langchain_openai import ChatOpenAIllm = ChatAnthropic(    model=\"claude-3-haiku-20240307\", temperature=0).configurable_alternatives(    # This gives this field an id    # When configuring the end runnable, we can then use this id to configure this field    ConfigurableField(id=\"llm\"),    # This sets a default_key.    # If we specify this key, the default LLM (ChatAnthropic initialized above) will be used    default_key=\"anthropic\",    # This adds a new option, with name `openai` that is equal to `ChatOpenAI()`    openai=ChatOpenAI(),    # This adds a new option, with name `gpt4` that is equal to `ChatOpenAI(model=\"gpt-4\")`    gpt4=ChatOpenAI(model=\"gpt-4\"),    # You can add more configuration options here)prompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\")chain = prompt | llm# By default it will call Anthropicchain.invoke({\"topic\": \"bears\"})\n```\n\n```\nAIMessage(content=\"Here's a bear joke for you:\\n\\nWhy don't bears wear socks? \\nBecause they have bear feet!\\n\\nHow's that? I tried to come up with a simple, silly pun-based joke about bears. Puns and wordplay are a common way to create humorous bear jokes. Let me know if you'd like to hear another one!\", response_metadata={'id': 'msg_018edUHh5fUbWdiimhrC3dZD', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 13, 'output_tokens': 80}}, id='run-775bc58c-28d7-4e6b-a268-48fa6661f02f-0')\n```\n\n```\n# We can use `.with_config(configurable={\"llm\": \"openai\"})` to specify an llm to usechain.with_config(configurable={\"llm\": \"openai\"}).invoke({\"topic\": \"bears\"})\n```\n\n```\nAIMessage(content=\"Why don't bears like fast food?\\n\\nBecause they can't catch it!\", response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 13, 'total_tokens': 28}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-7bdaa992-19c9-4f0d-9a0c-1f326bc992d4-0')\n```\n\n```\n# If we use the `default_key` then it uses the defaultchain.with_config(configurable={\"llm\": \"anthropic\"}).invoke({\"topic\": \"bears\"})\n```\n\n```\nAIMessage(content=\"Here's a bear joke for you:\\n\\nWhy don't bears wear socks? \\nBecause they have bear feet!\\n\\nHow's that? I tried to come up with a simple, silly pun-based joke about bears. Puns and wordplay are a common way to create humorous bear jokes. Let me know if you'd like to hear another one!\", response_metadata={'id': 'msg_01BZvbmnEPGBtcxRWETCHkct', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 13, 'output_tokens': 80}}, id='run-59b6ee44-a1cd-41b8-a026-28ee67cdd718-0')\n```", "url_final": "https://python.langchain.com/docs/how_to/configure/", "url_citable": "https://python.langchain.com/docs/how_to/configure/#configurable-alternatives", "title": "How to configure runtime chain internals | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Configurable Alternatives‚Äã", "section_anchor": "configurable-alternatives", "fetched_at": "2025-10-06T21:54:51.726137", "has_code": true}
{"id": "1e83507d6e1e10e81e660eca26e2dd051ef5dd9e", "text": "With Prompts‚Äã\n\nWe can do a similar thing, but alternate between prompts\n\n```\nllm = ChatAnthropic(model=\"claude-3-haiku-20240307\", temperature=0)prompt = PromptTemplate.from_template(    \"Tell me a joke about {topic}\").configurable_alternatives(    # This gives this field an id    # When configuring the end runnable, we can then use this id to configure this field    ConfigurableField(id=\"prompt\"),    # This sets a default_key.    # If we specify this key, the default prompt (asking for a joke, as initialized above) will be used    default_key=\"joke\",    # This adds a new option, with name `poem`    poem=PromptTemplate.from_template(\"Write a short poem about {topic}\"),    # You can add more configuration options here)chain = prompt | llm# By default it will write a jokechain.invoke({\"topic\": \"bears\"})\n```\n\n```\nAIMessage(content=\"Here's a bear joke for you:\\n\\nWhy don't bears wear socks? \\nBecause they have bear feet!\", response_metadata={'id': 'msg_01DtM1cssjNFZYgeS3gMZ49H', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 13, 'output_tokens': 28}}, id='run-8199af7d-ea31-443d-b064-483693f2e0a1-0')\n```\n\n```\n# We can configure it write a poemchain.with_config(configurable={\"prompt\": \"poem\"}).invoke({\"topic\": \"bears\"})\n```\n\n```\nAIMessage(content=\"Here is a short poem about bears:\\n\\nMajestic bears, strong and true,\\nRoaming the forests, wild and free.\\nPowerful paws, fur soft and brown,\\nCommanding respect, nature's crown.\\n\\nForaging for berries, fishing streams,\\nProtecting their young, fierce and keen.\\nMighty bears, a sight to behold,\\nGuardians of the wilderness, untold.\\n\\nIn the wild they reign supreme,\\nEmbodying nature's grand theme.\\nBears, a symbol of strength and grace,\\nCaptivating all who see their face.\", response_metadata={'id': 'msg_01Wck3qPxrjURtutvtodaJFn', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 13, 'output_tokens': 134}}, id='run-69414a1e-51d7-4bec-a307-b34b7d61025e-0')\n```", "url_final": "https://python.langchain.com/docs/how_to/configure/", "url_citable": "https://python.langchain.com/docs/how_to/configure/#with-prompts", "title": "How to configure runtime chain internals | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "With Prompts‚Äã", "section_anchor": "with-prompts", "fetched_at": "2025-10-06T21:54:51.726137", "has_code": true}
{"id": "f91ec4e396af9cf95b56e7d48c6aca4e3132a8b4", "text": "With Prompts and LLMs‚Äã\n\nWe can also have multiple things configurable! Here's an example doing that with both prompts and LLMs.\n\n```\nllm = ChatAnthropic(    model=\"claude-3-haiku-20240307\", temperature=0).configurable_alternatives(    # This gives this field an id    # When configuring the end runnable, we can then use this id to configure this field    ConfigurableField(id=\"llm\"),    # This sets a default_key.    # If we specify this key, the default LLM (ChatAnthropic initialized above) will be used    default_key=\"anthropic\",    # This adds a new option, with name `openai` that is equal to `ChatOpenAI()`    openai=ChatOpenAI(),    # This adds a new option, with name `gpt4` that is equal to `ChatOpenAI(model=\"gpt-4\")`    gpt4=ChatOpenAI(model=\"gpt-4\"),    # You can add more configuration options here)prompt = PromptTemplate.from_template(    \"Tell me a joke about {topic}\").configurable_alternatives(    # This gives this field an id    # When configuring the end runnable, we can then use this id to configure this field    ConfigurableField(id=\"prompt\"),    # This sets a default_key.    # If we specify this key, the default prompt (asking for a joke, as initialized above) will be used    default_key=\"joke\",    # This adds a new option, with name `poem`    poem=PromptTemplate.from_template(\"Write a short poem about {topic}\"),    # You can add more configuration options here)chain = prompt | llm# We can configure it write a poem with OpenAIchain.with_config(configurable={\"prompt\": \"poem\", \"llm\": \"openai\"}).invoke(    {\"topic\": \"bears\"})\n```\n\n```\nAIMessage(content=\"In the forest deep and wide,\\nBears roam with grace and pride.\\nWith fur as dark as night,\\nThey rule the land with all their might.\\n\\nIn winter's chill, they hibernate,\\nIn spring they emerge, hungry and great.\\nWith claws sharp and eyes so keen,\\nThey hunt for food, fierce and lean.\\n\\nBut beneath their tough exterior,\\nLies a gentle heart, warm and superior.\\nThey love their cubs with all their might,\\nProtecting them through day and night.\\n\\nSo let us admire these majestic creatures,\\nIn awe of their strength and features.\\nFor in the wild, they reign supreme,\\nThe mighty bears, a timeless dream.\", response_metadata={'token_usage': {'completion_tokens': 133, 'prompt_tokens': 13, 'total_tokens': 146}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-5eec0b96-d580-49fd-ac4e-e32a0803b49b-0')\n```\n\n```\n# We can always just configure only one if we wantchain.with_config(configurable={\"llm\": \"openai\"}).invoke({\"topic\": \"bears\"})\n```\n\n```\nAIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\", response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 13, 'total_tokens': 26}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-c1b14c9c-4988-49b8-9363-15bfd479973a-0')\n```", "url_final": "https://python.langchain.com/docs/how_to/configure/", "url_citable": "https://python.langchain.com/docs/how_to/configure/#with-prompts-and-llms", "title": "How to configure runtime chain internals | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "With Prompts and LLMs‚Äã", "section_anchor": "with-prompts-and-llms", "fetched_at": "2025-10-06T21:54:51.726137", "has_code": true}
{"id": "34ce32aca4afc54319d1eefd671a0b36cd832dea", "text": "Saving configurations‚Äã\n\nWe can also easily save configured chains as their own objects\n\n```\nopenai_joke = chain.with_config(configurable={\"llm\": \"openai\"})openai_joke.invoke({\"topic\": \"bears\"})\n```\n\n```\nAIMessage(content=\"Why did the bear break up with his girlfriend? \\nBecause he couldn't bear the relationship anymore!\", response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 13, 'total_tokens': 33}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-391ebd55-9137-458b-9a11-97acaff6a892-0')\n```", "url_final": "https://python.langchain.com/docs/how_to/configure/", "url_citable": "https://python.langchain.com/docs/how_to/configure/#saving-configurations", "title": "How to configure runtime chain internals | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Saving configurations‚Äã", "section_anchor": "saving-configurations", "fetched_at": "2025-10-06T21:54:51.726137", "has_code": true}
{"id": "97ea14f035a305cfb445f4daf0e55633d50ef26f", "text": "Next steps‚Äã\n\nYou now know how to configure a chain's internal steps at runtime.\n\nTo learn more, see the other how-to guides on runnables in this section, including:\n\n- Using .bind() as a simpler way to set a runnable's runtime parameters\n\n- Configuring fields on a chat model\n- Configuring fields on arbitrary Runnables\n\n- With Prompts\n- With Prompts and LLMs\n- Saving configurations", "url_final": "https://python.langchain.com/docs/how_to/configure/", "url_citable": "https://python.langchain.com/docs/how_to/configure/#next-steps", "title": "How to configure runtime chain internals | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Next steps‚Äã", "section_anchor": "next-steps", "fetched_at": "2025-10-06T21:54:51.726137", "has_code": false}
{"id": "d2e3e12794b697fab6c80027aa050d46874f929f", "text": "How to select examples by maximal marginal relevance (MMR) | ü¶úÔ∏èüîó LangChain\n\nThe MaxMarginalRelevanceExampleSelector selects examples based on a combination of which examples are most similar to the inputs, while also optimizing for diversity. It does this by finding the examples with the embeddings that have the greatest cosine similarity with the inputs, and then iteratively adding them while penalizing them for closeness to already selected examples.\n\n```\nfrom langchain_community.vectorstores import FAISSfrom langchain_core.example_selectors import (    MaxMarginalRelevanceExampleSelector,    SemanticSimilarityExampleSelector,)from langchain_core.prompts import FewShotPromptTemplate, PromptTemplatefrom langchain_openai import OpenAIEmbeddingsexample_prompt = PromptTemplate(    input_variables=[\"input\", \"output\"],    template=\"Input: {input}\\nOutput: {output}\",)# Examples of a pretend task of creating antonyms.examples = [    {\"input\": \"happy\", \"output\": \"sad\"},    {\"input\": \"tall\", \"output\": \"short\"},    {\"input\": \"energetic\", \"output\": \"lethargic\"},    {\"input\": \"sunny\", \"output\": \"gloomy\"},    {\"input\": \"windy\", \"output\": \"calm\"},]\n```\n\n```\nexample_selector = MaxMarginalRelevanceExampleSelector.from_examples(    # The list of examples available to select from.    examples,    # The embedding class used to produce embeddings which are used to measure semantic similarity.    OpenAIEmbeddings(),    # The VectorStore class that is used to store the embeddings and do a similarity search over.    FAISS,    # The number of examples to produce.    k=2,)mmr_prompt = FewShotPromptTemplate(    # We provide an ExampleSelector instead of examples.    example_selector=example_selector,    example_prompt=example_prompt,    prefix=\"Give the antonym of every input\",    suffix=\"Input: {adjective}\\nOutput:\",    input_variables=[\"adjective\"],)\n```\n\n```\n# Input is a feeling, so should select the happy/sad example as the first oneprint(mmr_prompt.format(adjective=\"worried\"))\n```\n\n```\nGive the antonym of every inputInput: happyOutput: sadInput: windyOutput: calmInput: worriedOutput:\n```\n\n```\n# Let's compare this to what we would just get if we went solely off of similarity,# by using SemanticSimilarityExampleSelector instead of MaxMarginalRelevanceExampleSelector.example_selector = SemanticSimilarityExampleSelector.from_examples(    # The list of examples available to select from.    examples,    # The embedding class used to produce embeddings which are used to measure semantic similarity.    OpenAIEmbeddings(),    # The VectorStore class that is used to store the embeddings and do a similarity search over.    FAISS,    # The number of examples to produce.    k=2,)similar_prompt = FewShotPromptTemplate(    # We provide an ExampleSelector instead of examples.    example_selector=example_selector,    example_prompt=example_prompt,    prefix=\"Give the antonym of every input\",    suffix=\"Input: {adjective}\\nOutput:\",    input_variables=[\"adjective\"],)print(similar_prompt.format(adjective=\"worried\"))\n```\n\n```\nGive the antonym of every inputInput: happyOutput: sadInput: sunnyOutput: gloomyInput: worriedOutput:\n```", "url_final": "https://python.langchain.com/docs/how_to/example_selectors_mmr/", "url_citable": "https://python.langchain.com/docs/how_to/example_selectors_mmr/", "title": "How to select examples by maximal marginal relevance (MMR) | ü¶úÔ∏èüîó LangChain", "section_level": 1, "section_heading": "How to select examples by maximal marginal relevance (MMR) | ü¶úÔ∏èüîó LangChain", "section_anchor": null, "fetched_at": "2025-10-06T21:55:34.497605", "has_code": true}
{"id": "74bd91e778958bfa2be0fbaedc7cf8b82d055dc6", "text": "Saving objects‚Äã\n\nTo json‚Äã\n\n```\nstring_representation = dumps(chain, pretty=True)print(string_representation[:500])\n```\n\n```\n{  \"lc\": 1,  \"type\": \"constructor\",  \"id\": [    \"langchain\",    \"schema\",    \"runnable\",    \"RunnableSequence\"  ],  \"kwargs\": {    \"first\": {      \"lc\": 1,      \"type\": \"constructor\",      \"id\": [        \"langchain\",        \"prompts\",        \"chat\",        \"ChatPromptTemplate\"      ],      \"kwargs\": {        \"input_variables\": [          \"language\",          \"text\"        ],        \"messages\": [          {            \"lc\": 1,            \"type\": \"constructor\",\n```\n\nTo a json-serializable Python dict‚Äã\n\n```\ndict_representation = dumpd(chain)print(type(dict_representation))\n```\n\n```\n<class 'dict'>\n```\n\nTo disk‚Äã\n\n```\nimport jsonwith open(\"/tmp/chain.json\", \"w\") as fp:    json.dump(string_representation, fp)\n```\n\nNote that the API key is withheld from the serialized representations. Parameters that are considered secret are specified by the .lc_secrets attribute of the LangChain object:\n\n```\nchain.last.lc_secrets\n```\n\n```\n{'openai_api_key': 'OPENAI_API_KEY'}\n```", "url_final": "https://python.langchain.com/docs/how_to/serialization/", "url_citable": "https://python.langchain.com/docs/how_to/serialization/#saving-objects", "title": "How to save and load LangChain objects | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Saving objects‚Äã", "section_anchor": "saving-objects", "fetched_at": "2025-10-06T21:56:21.097586", "has_code": true}
{"id": "940884a1823d0d79533d4db746a12c876ccc1c0a", "text": "Loading objects‚Äã\n\nSpecifying secrets_map in load and loads will load the corresponding secrets onto the de-serialized LangChain object.\n\nFrom string‚Äã\n\n```\nchain = loads(string_representation, secrets_map={\"OPENAI_API_KEY\": \"llm-api-key\"})\n```\n\nFrom dict‚Äã\n\n```\nchain = load(dict_representation, secrets_map={\"OPENAI_API_KEY\": \"llm-api-key\"})\n```\n\nFrom disk‚Äã\n\n```\nwith open(\"/tmp/chain.json\", \"r\") as fp:    chain = loads(json.load(fp), secrets_map={\"OPENAI_API_KEY\": \"llm-api-key\"})\n```\n\nNote that we recover the API key specified at the start of the guide:\n\n```\nchain.last.openai_api_key.get_secret_value()\n```\n\n```\n'llm-api-key'\n```\n\n- To json\n- To a json-serializable Python dict\n- To disk\n\n- From string\n- From dict\n- From disk", "url_final": "https://python.langchain.com/docs/how_to/serialization/", "url_citable": "https://python.langchain.com/docs/how_to/serialization/#loading-objects", "title": "How to save and load LangChain objects | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Loading objects‚Äã", "section_anchor": "loading-objects", "fetched_at": "2025-10-06T21:56:21.097586", "has_code": true}
{"id": "bfa9ff2ee560cff0d1db0945abcc889243296356", "text": "Using LangSmith‚Äã\n\nYou can use LangSmith to help track token usage in your LLM application. See the LangSmith quick start guide.", "url_final": "https://python.langchain.com/docs/how_to/chat_token_usage_tracking/", "url_citable": "https://python.langchain.com/docs/how_to/chat_token_usage_tracking/#using-langsmith", "title": "How to track token usage in ChatModels | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Using LangSmith‚Äã", "section_anchor": "using-langsmith", "fetched_at": "2025-10-06T21:55:07.046146", "has_code": false}
{"id": "345314676805b6c9bda9acc96d62b33148d7d21f", "text": "Using AIMessage.usage_metadata‚Äã\n\nA number of model providers return token usage information as part of the chat generation response. When available, this information will be included on the AIMessage objects produced by the corresponding model.\n\nLangChain AIMessage objects include a usage_metadata attribute. When populated, this attribute will be a UsageMetadata dictionary with standard keys (e.g., \"input_tokens\" and \"output_tokens\"). They will also include information on cached token usage and tokens from multi-modal data.\n\nExamples:\n\nOpenAI:\n\n```\nfrom langchain.chat_models import init_chat_modelllm = init_chat_model(model=\"gpt-4o-mini\")openai_response = llm.invoke(\"hello\")openai_response.usage_metadata\n```\n\n```\n{'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17}\n```\n\nAnthropic:\n\n```\nfrom langchain_anthropic import ChatAnthropicllm = ChatAnthropic(model=\"claude-3-haiku-20240307\")anthropic_response = llm.invoke(\"hello\")anthropic_response.usage_metadata\n```\n\n```\n{'input_tokens': 8, 'output_tokens': 12, 'total_tokens': 20}\n```", "url_final": "https://python.langchain.com/docs/how_to/chat_token_usage_tracking/", "url_citable": "https://python.langchain.com/docs/how_to/chat_token_usage_tracking/#using-aimessageusage_metadata", "title": "How to track token usage in ChatModels | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Using AIMessage.usage_metadata‚Äã", "section_anchor": "using-aimessageusage_metadata", "fetched_at": "2025-10-06T21:55:07.046146", "has_code": true}
{"id": "239ea4903b2685b78083a14f1252931d349b780c", "text": "Streaming‚Äã\n\nSome providers support token count metadata in a streaming context.\n\nFor example, OpenAI will return a message chunk at the end of a stream with token usage information. This behavior is supported by langchain-openai >= 0.1.9 and can be enabled by setting stream_usage=True. This attribute can also be set when ChatOpenAI is instantiated.\n\nBy default, the last message chunk in a stream will include a \"finish_reason\" in the message's response_metadata attribute. If we include token usage in streaming mode, an additional chunk containing usage metadata will be added to the end of the stream, such that \"finish_reason\" appears on the second to last message chunk.\n\n```\nllm = init_chat_model(model=\"gpt-4o-mini\")aggregate = Nonefor chunk in llm.stream(\"hello\", stream_usage=True):    print(chunk)    aggregate = chunk if aggregate is None else aggregate + chunk\n```\n\n```\ncontent='' id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623'content='Hello' id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623'content='!' id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623'content=' How' id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623'content=' can' id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623'content=' I' id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623'content=' assist' id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623'content=' you' id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623'content=' today' id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623'content='?' id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623'content='' response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini'} id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623'content='' id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623' usage_metadata={'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17}\n```\n\nNote that the usage metadata will be included in the sum of the individual message chunks:\n\n```\nprint(aggregate.content)print(aggregate.usage_metadata)\n```\n\n```\nHello! How can I assist you today?{'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17}\n```\n\nTo disable streaming token counts for OpenAI, set stream_usage to False, or omit it from the parameters:\n\n```\naggregate = Nonefor chunk in llm.stream(\"hello\"):    print(chunk)\n```\n\n```\ncontent='' id='run-8e758550-94b0-4cca-a298-57482793c25d'content='Hello' id='run-8e758550-94b0-4cca-a298-57482793c25d'content='!' id='run-8e758550-94b0-4cca-a298-57482793c25d'content=' How' id='run-8e758550-94b0-4cca-a298-57482793c25d'content=' can' id='run-8e758550-94b0-4cca-a298-57482793c25d'content=' I' id='run-8e758550-94b0-4cca-a298-57482793c25d'content=' assist' id='run-8e758550-94b0-4cca-a298-57482793c25d'content=' you' id='run-8e758550-94b0-4cca-a298-57482793c25d'content=' today' id='run-8e758550-94b0-4cca-a298-57482793c25d'content='?' id='run-8e758550-94b0-4cca-a298-57482793c25d'content='' response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini'} id='run-8e758550-94b0-4cca-a298-57482793c25d'\n```\n\nYou can also enable streaming token usage by setting stream_usage when instantiating the chat model. This can be useful when incorporating chat models into LangChain chains: usage metadata can be monitored when streaming intermediate steps or using tracing software such as LangSmith.\n\nSee the below example, where we return output structured to a desired schema, but can still observe token usage streamed from intermediate steps.\n\n```\nfrom pydantic import BaseModel, Fieldclass Joke(BaseModel):    \"\"\"Joke to tell user.\"\"\"    setup: str = Field(description=\"question to set up a joke\")    punchline: str = Field(description=\"answer to resolve the joke\")llm = init_chat_model(    model=\"gpt-4o-mini\",    stream_usage=True,)# Under the hood, .with_structured_output binds tools to the# chat model and appends a parser.structured_llm = llm.with_structured_output(Joke)async for event in structured_llm.astream_events(\"Tell me a joke\"):    if event[\"event\"] == \"on_chat_model_end\":        print(f\"Token usage: {event['data']['output'].usage_metadata}\\n\")    elif event[\"event\"] == \"on_chain_end\" and event[\"name\"] == \"RunnableSequence\":        print(event[\"data\"][\"output\"])    else:        pass\n```\n\n```\nToken usage: {'input_tokens': 79, 'output_tokens': 23, 'total_tokens': 102}setup='Why was the math book sad?' punchline='Because it had too many problems.'\n```\n\nToken usage is also visible in the corresponding LangSmith trace in the payload from the chat model.", "url_final": "https://python.langchain.com/docs/how_to/chat_token_usage_tracking/", "url_citable": "https://python.langchain.com/docs/how_to/chat_token_usage_tracking/#streaming", "title": "How to track token usage in ChatModels | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Streaming‚Äã", "section_anchor": "streaming", "fetched_at": "2025-10-06T21:55:07.046146", "has_code": true}
{"id": "bb60b7e076df3a210f80442835e6f0d022258a0f", "text": "Using callbacks‚Äã\n\nLangChain implements a callback handler and context manager that will track token usage across calls of any chat model that returns usage_metadata.\n\nThere are also some API-specific callback context managers that maintain pricing for different models, allowing for cost estimation in real time. They are currently only implemented for the OpenAI API and Bedrock Anthropic API, and are available in langchain-community:\n\n- get_openai_callback\n- get_bedrock_anthropic_callback\n\nBelow, we demonstrate the general-purpose usage metadata callback manager. We can track token usage through configuration or as a context manager.\n\nTracking token usage through configuration‚Äã\n\nTo track token usage through configuration, instantiate a UsageMetadataCallbackHandler and pass it into the config:\n\n```\nfrom langchain.chat_models import init_chat_modelfrom langchain_core.callbacks import UsageMetadataCallbackHandlerllm_1 = init_chat_model(model=\"openai:gpt-4o-mini\")llm_2 = init_chat_model(model=\"anthropic:claude-3-5-haiku-latest\")callback = UsageMetadataCallbackHandler()result_1 = llm_1.invoke(\"Hello\", config={\"callbacks\": [callback]})result_2 = llm_2.invoke(\"Hello\", config={\"callbacks\": [callback]})callback.usage_metadata\n```\n\n```\n{'gpt-4o-mini-2024-07-18': {'input_tokens': 8,  'output_tokens': 10,  'total_tokens': 18,  'input_token_details': {'audio': 0, 'cache_read': 0},  'output_token_details': {'audio': 0, 'reasoning': 0}}, 'claude-3-5-haiku-20241022': {'input_tokens': 8,  'output_tokens': 21,  'total_tokens': 29,  'input_token_details': {'cache_read': 0, 'cache_creation': 0}}}\n```", "url_final": "https://python.langchain.com/docs/how_to/chat_token_usage_tracking/", "url_citable": "https://python.langchain.com/docs/how_to/chat_token_usage_tracking/#using-callbacks", "title": "How to track token usage in ChatModels | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Using callbacks‚Äã", "section_anchor": "using-callbacks", "fetched_at": "2025-10-06T21:55:07.046146", "has_code": true}
{"id": "d09ec13a878feedfecd92c8051cebc8ecf7cdc71", "text": "Tracking token usage using a context manager‚Äã\n\nYou can also use get_usage_metadata_callback to create a context manager and aggregate usage metadata there:\n\n```\nfrom langchain.chat_models import init_chat_modelfrom langchain_core.callbacks import get_usage_metadata_callbackllm_1 = init_chat_model(model=\"openai:gpt-4o-mini\")llm_2 = init_chat_model(model=\"anthropic:claude-3-5-haiku-latest\")with get_usage_metadata_callback() as cb:    llm_1.invoke(\"Hello\")    llm_2.invoke(\"Hello\")    print(cb.usage_metadata)\n```\n\n```\n{'gpt-4o-mini-2024-07-18': {'input_tokens': 8, 'output_tokens': 10, 'total_tokens': 18, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'claude-3-5-haiku-20241022': {'input_tokens': 8, 'output_tokens': 21, 'total_tokens': 29, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}}}\n```\n\nEither of these methods will aggregate token usage across multiple calls to each model. For example, you can use it in an agent to track token usage across repeated calls to one model:\n\n```\n%pip install -qU langgraph\n```\n\n```\nfrom langgraph.prebuilt import create_react_agent# Create a tooldef get_weather(location: str) -> str:    \"\"\"Get the weather at a location.\"\"\"    return \"It's sunny.\"callback = UsageMetadataCallbackHandler()tools = [get_weather]agent = create_react_agent(\"openai:gpt-4o-mini\", tools)for step in agent.stream(    {\"messages\": [{\"role\": \"user\", \"content\": \"What's the weather in Boston?\"}]},    stream_mode=\"values\",    config={\"callbacks\": [callback]},):    step[\"messages\"][-1].pretty_print()print(f\"\\nTotal usage: {callback.usage_metadata}\")\n```\n\n```\n================================\u001b[1m Human Message \u001b[0m=================================What's the weather in Boston?==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls:  get_weather (call_izMdhUYpp9Vhx7DTNAiybzGa) Call ID: call_izMdhUYpp9Vhx7DTNAiybzGa  Args:    location: Boston=================================\u001b[1m Tool Message \u001b[0m=================================Name: get_weatherIt's sunny.==================================\u001b[1m Ai Message \u001b[0m==================================The weather in Boston is sunny.Total usage: {'gpt-4o-mini-2024-07-18': {'input_token_details': {'audio': 0, 'cache_read': 0}, 'input_tokens': 125, 'total_tokens': 149, 'output_tokens': 24, 'output_token_details': {'audio': 0, 'reasoning': 0}}}\n```", "url_final": "https://python.langchain.com/docs/how_to/chat_token_usage_tracking/", "url_citable": "https://python.langchain.com/docs/how_to/chat_token_usage_tracking/#tracking-token-usage-using-a-context-manager", "title": "How to track token usage in ChatModels | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Tracking token usage using a context manager‚Äã", "section_anchor": "tracking-token-usage-using-a-context-manager", "fetched_at": "2025-10-06T21:55:07.046146", "has_code": true}
{"id": "85d3478725e8d90b5dd1ef2e5baa72e2ff10e994", "text": "Next steps‚Äã\n\nYou've now seen a few examples of how to track token usage for supported providers.\n\nNext, check out the other how-to guides chat models in this section, like how to get a model to return structured output or how to add caching to your chat models.\n\n- Streaming\n\n- Tracking token usage through configuration\n- Tracking token usage using a context manager", "url_final": "https://python.langchain.com/docs/how_to/chat_token_usage_tracking/", "url_citable": "https://python.langchain.com/docs/how_to/chat_token_usage_tracking/#next-steps", "title": "How to track token usage in ChatModels | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Next steps‚Äã", "section_anchor": "next-steps", "fetched_at": "2025-10-06T21:55:07.046146", "has_code": false}
{"id": "ec5b568f45cabef2e9b19a8cba25dcb238b731be", "text": "Using with a Vector Store‚Äã\n\nFirst, let's see an example that uses the local file system for storing embeddings and uses FAISS vector store for retrieval.\n\n```\n%pip install --upgrade --quiet  langchain-openai faiss-cpu\n```\n\n```\nfrom langchain.storage import LocalFileStorefrom langchain_community.document_loaders import TextLoaderfrom langchain_community.vectorstores import FAISSfrom langchain_openai import OpenAIEmbeddingsfrom langchain_text_splitters import CharacterTextSplitterunderlying_embeddings = OpenAIEmbeddings()store = LocalFileStore(\"./cache/\")cached_embedder = CacheBackedEmbeddings.from_bytes_store(    underlying_embeddings, store, namespace=underlying_embeddings.model)\n```\n\nThe cache is empty prior to embedding:\n\n```\nlist(store.yield_keys())\n```\n\n```\n[]\n```\n\nLoad the document, split it into chunks, embed each chunk and load it into the vector store.\n\n```\nraw_documents = TextLoader(\"state_of_the_union.txt\").load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)documents = text_splitter.split_documents(raw_documents)\n```\n\nCreate the vector store:\n\n```\n%%timedb = FAISS.from_documents(documents, cached_embedder)\n```\n\n```\nCPU times: user 218 ms, sys: 29.7 ms, total: 248 msWall time: 1.02 s\n```\n\nIf we try to create the vector store again, it'll be much faster since it does not need to re-compute any embeddings.\n\n```\n%%timedb2 = FAISS.from_documents(documents, cached_embedder)\n```\n\n```\nCPU times: user 15.7 ms, sys: 2.22 ms, total: 18 msWall time: 17.2 ms\n```\n\nAnd here are some of the embeddings that got created:\n\n```\nlist(store.yield_keys())[:5]\n```\n\n```\n['text-embedding-ada-00217a6727d-8916-54eb-b196-ec9c9d6ca472', 'text-embedding-ada-0025fc0d904-bd80-52da-95c9-441015bfb438', 'text-embedding-ada-002e4ad20ef-dfaa-5916-9459-f90c6d8e8159', 'text-embedding-ada-002ed199159-c1cd-5597-9757-f80498e8f17b', 'text-embedding-ada-0021297d37a-2bc1-5e19-bf13-6c950f075062']\n```", "url_final": "https://python.langchain.com/docs/how_to/caching_embeddings/", "url_citable": "https://python.langchain.com/docs/how_to/caching_embeddings/#using-with-a-vector-store", "title": "Caching | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Using with a Vector Store‚Äã", "section_anchor": "using-with-a-vector-store", "fetched_at": "2025-10-06T21:54:56.760588", "has_code": true}
{"id": "70224a60a84c45952b58a5c60f2df0215cafaebe", "text": "Get started‚Äã\n\nFor demonstration purposes we'll use a Chroma vector store. We've created a small demo set of documents that contain summaries of movies.\n\nNote: The self-query retriever requires you to have lark package installed.\n\n```\n%pip install --upgrade --quiet  lark langchain-chroma\n```\n\n```\nfrom langchain_chroma import Chromafrom langchain_core.documents import Documentfrom langchain_openai import OpenAIEmbeddingsdocs = [    Document(        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},    ),    Document(        page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},    ),    Document(        page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",        metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},    ),    Document(        page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},    ),    Document(        page_content=\"Toys come alive and have a blast doing so\",        metadata={\"year\": 1995, \"genre\": \"animated\"},    ),    Document(        page_content=\"Three men walk into the Zone, three men walk out of the Zone\",        metadata={            \"year\": 1979,            \"director\": \"Andrei Tarkovsky\",            \"genre\": \"thriller\",            \"rating\": 9.9,        },    ),]vectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())\n```\n\nCreating our self-querying retriever‚Äã\n\nNow we can instantiate our retriever. To do this we'll need to provide some information upfront about the metadata fields that our documents support and a short description of the document contents.\n\n```\nfrom langchain.chains.query_constructor.schema import AttributeInfofrom langchain.retrievers.self_query.base import SelfQueryRetrieverfrom langchain_openai import ChatOpenAImetadata_field_info = [    AttributeInfo(        name=\"genre\",        description=\"The genre of the movie. One of ['science fiction', 'comedy', 'drama', 'thriller', 'romance', 'action', 'animated']\",        type=\"string\",    ),    AttributeInfo(        name=\"year\",        description=\"The year the movie was released\",        type=\"integer\",    ),    AttributeInfo(        name=\"director\",        description=\"The name of the movie director\",        type=\"string\",    ),    AttributeInfo(        name=\"rating\", description=\"A 1-10 rating for the movie\", type=\"float\"    ),]document_content_description = \"Brief summary of a movie\"llm = ChatOpenAI(temperature=0)retriever = SelfQueryRetriever.from_llm(    llm,    vectorstore,    document_content_description,    metadata_field_info,)\n```", "url_final": "https://python.langchain.com/docs/how_to/self_query/", "url_citable": "https://python.langchain.com/docs/how_to/self_query/#get-started", "title": "How to do \"self-querying\" retrieval | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Get started‚Äã", "section_anchor": "get-started", "fetched_at": "2025-10-06T21:56:18.087289", "has_code": true}
{"id": "27b620f518bd99a992603d279502f798b7b12348", "text": "Testing it out‚Äã\n\nAnd now we can actually try using our retriever!\n\n```\n# This example only specifies a filterretriever.invoke(\"I want to watch a movie rated higher than 8.5\")\n```\n\n```\n[Document(page_content='Three men walk into the Zone, three men walk out of the Zone', metadata={'director': 'Andrei Tarkovsky', 'genre': 'thriller', 'rating': 9.9, 'year': 1979}), Document(page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', metadata={'director': 'Satoshi Kon', 'rating': 8.6, 'year': 2006})]\n```\n\n```\n# This example specifies a query and a filterretriever.invoke(\"Has Greta Gerwig directed any movies about women\")\n```\n\n```\n[Document(page_content='A bunch of normal-sized women are supremely wholesome and some men pine after them', metadata={'director': 'Greta Gerwig', 'rating': 8.3, 'year': 2019})]\n```\n\n```\n# This example specifies a composite filterretriever.invoke(\"What's a highly rated (above 8.5) science fiction film?\")\n```\n\n```\n[Document(page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', metadata={'director': 'Satoshi Kon', 'rating': 8.6, 'year': 2006}), Document(page_content='Three men walk into the Zone, three men walk out of the Zone', metadata={'director': 'Andrei Tarkovsky', 'genre': 'thriller', 'rating': 9.9, 'year': 1979})]\n```\n\n```\n# This example specifies a query and composite filterretriever.invoke(    \"What's a movie after 1990 but before 2005 that's all about toys, and preferably is animated\")\n```\n\n```\n[Document(page_content='Toys come alive and have a blast doing so', metadata={'genre': 'animated', 'year': 1995})]\n```", "url_final": "https://python.langchain.com/docs/how_to/self_query/", "url_citable": "https://python.langchain.com/docs/how_to/self_query/#testing-it-out", "title": "How to do \"self-querying\" retrieval | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Testing it out‚Äã", "section_anchor": "testing-it-out", "fetched_at": "2025-10-06T21:56:18.087289", "has_code": true}
{"id": "33cdd36fc7b8fe8d6db9ee985c5aaad0297ff1d8", "text": "Filter k‚Äã\n\nWe can also use the self query retriever to specify k: the number of documents to fetch.\n\nWe can do this by passing enable_limit=True to the constructor.\n\n```\nretriever = SelfQueryRetriever.from_llm(    llm,    vectorstore,    document_content_description,    metadata_field_info,    enable_limit=True,)# This example only specifies a relevant queryretriever.invoke(\"What are two movies about dinosaurs\")\n```\n\n```\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'genre': 'science fiction', 'rating': 7.7, 'year': 1993}), Document(page_content='Toys come alive and have a blast doing so', metadata={'genre': 'animated', 'year': 1995})]\n```", "url_final": "https://python.langchain.com/docs/how_to/self_query/", "url_citable": "https://python.langchain.com/docs/how_to/self_query/#filter-k", "title": "How to do \"self-querying\" retrieval | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Filter k‚Äã", "section_anchor": "filter-k", "fetched_at": "2025-10-06T21:56:18.087289", "has_code": true}
{"id": "84f84979745b626b122872cbddef38d5928fbe23", "text": "Constructing from scratch with LCEL‚Äã\n\nTo see what's going on under the hood, and to have more custom control, we can reconstruct our retriever from scratch.\n\nFirst, we need to create a query-construction chain. This chain will take a user query and generated a StructuredQuery object which captures the filters specified by the user. We provide some helper functions for creating a prompt and output parser. These have a number of tunable params that we'll ignore here for simplicity.\n\n```\nfrom langchain.chains.query_constructor.base import (    StructuredQueryOutputParser,    get_query_constructor_prompt,)prompt = get_query_constructor_prompt(    document_content_description,    metadata_field_info,)output_parser = StructuredQueryOutputParser.from_components()query_constructor = prompt | llm | output_parser\n```\n\nLet's look at our prompt:\n\n```\nprint(prompt.format(query=\"dummy question\"))\n```\n\n```\nYour goal is to structure the user's query to match the request schema provided below.<< Structured Request Schema >>When responding use a markdown code snippet with a JSON object formatted in the following schema:\\`\\`\\`json{    \"query\": string \\ text string to compare to document contents    \"filter\": string \\ logical condition statement for filtering documents}\\`\\`\\`The query string should contain only text that is expected to match the contents of documents. Any conditions in the filter should not be mentioned in the query as well.A logical condition statement is composed of one or more comparison and logical operation statements.A comparison statement takes the form: `comp(attr, val)`:- `comp` (eq | ne | gt | gte | lt | lte | contain | like | in | nin): comparator- `attr` (string):  name of attribute to apply the comparison to- `val` (string): is the comparison valueA logical operation statement takes the form `op(statement1, statement2, ...)`:- `op` (and | or | not): logical operator- `statement1`, `statement2`, ... (comparison statements or logical operation statements): one or more statements to apply the operation toMake sure that you only use the comparators and logical operators listed above and no others.Make sure that filters only refer to attributes that exist in the data source.Make sure that filters only use the attributed names with its function names if there are functions applied on them.Make sure that filters only use format `YYYY-MM-DD` when handling date data typed values.Make sure that filters take into account the descriptions of attributes and only make comparisons that are feasible given the type of data being stored.Make sure that filters are only used as needed. If there are no filters that should be applied return \"NO_FILTER\" for the filter value.<< Example 1. >>Data Source:\\`\\`\\`json{    \"content\": \"Lyrics of a song\",    \"attributes\": {        \"artist\": {            \"type\": \"string\",            \"description\": \"Name of the song artist\"        },        \"length\": {            \"type\": \"integer\",            \"description\": \"Length of the song in seconds\"        },        \"genre\": {            \"type\": \"string\",            \"description\": \"The song genre, one of \"pop\", \"rock\" or \"rap\"\"        }    }}\\`\\`\\`User Query:What are songs by Taylor Swift or Katy Perry about teenage romance under 3 minutes long in the dance pop genreStructured Request:\\`\\`\\`json{    \"query\": \"teenager love\",    \"filter\": \"and(or(eq(\\\"artist\\\", \\\"Taylor Swift\\\"), eq(\\\"artist\\\", \\\"Katy Perry\\\")), lt(\\\"length\\\", 180), eq(\\\"genre\\\", \\\"pop\\\"))\"}\\`\\`\\`<< Example 2. >>Data Source:\\`\\`\\`json{    \"content\": \"Lyrics of a song\",    \"attributes\": {        \"artist\": {            \"type\": \"string\",            \"description\": \"Name of the song artist\"        },        \"length\": {            \"type\": \"integer\",            \"description\": \"Length of the song in seconds\"        },        \"genre\": {            \"type\": \"string\",            \"description\": \"The song genre, one of \"pop\", \"rock\" or \"rap\"\"        }    }}\\`\\`\\`User Query:What are songs that were not published on SpotifyStructured Request:\\`\\`\\`json{    \"query\": \"\",    \"filter\": \"NO_FILTER\"}\\`\\`\\`<< Example 3. >>Data Source:\\`\\`\\`json{    \"content\": \"Brief summary of a movie\",    \"attributes\": {    \"genre\": {        \"description\": \"The genre of the movie. One of ['science fiction', 'comedy', 'drama', 'thriller', 'romance', 'action', 'animated']\",        \"type\": \"string\"    },    \"year\": {        \"description\": \"The year the movie was released\",        \"type\": \"integer\"    },    \"director\": {        \"description\": \"The name of the movie director\",        \"type\": \"string\"    },    \"rating\": {        \"description\": \"A 1-10 rating for the movie\",        \"type\": \"float\"    }}}\\`\\`\\`User Query:dummy questionStructured Request:\n```\n\nAnd what our full chain produces:\n\n```\nquery_constructor.invoke(    {        \"query\": \"What are some sci-fi movies from the 90's directed by Luc Besson about taxi drivers\"    })\n```\n\n```\nStructuredQuery(query='taxi driver', filter=Operation(operator=<Operator.AND: 'and'>, arguments=[Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='genre', value='science fiction'), Operation(operator=<Operator.AND: 'and'>, arguments=[Comparison(comparator=<Comparator.GTE: 'gte'>, attribute='year', value=1990), Comparison(comparator=<Comparator.LT: 'lt'>, attribute='year', value=2000)]), Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='director', value='Luc Besson')]), limit=None)\n```\n\nThe query constructor is the key element of the self-query retriever. To make a great retrieval system you'll need to make sure your query constructor works well. Often this requires adjusting the prompt, the examples in the prompt, the attribute descriptions, etc. For an example that walks through refining a query constructor on some hotel inventory data, check out this cookbook.\n\nThe next key element is the structured query translator. This is the object responsible for translating the generic StructuredQuery object into a metadata filter in the syntax of the vector store you're using. LangChain comes with a number of built-in translators. To see them all head to the Integrations section.\n\n```\nfrom langchain_community.query_constructors.chroma import ChromaTranslatorretriever = SelfQueryRetriever(    query_constructor=query_constructor,    vectorstore=vectorstore,    structured_query_translator=ChromaTranslator(),)\n```\n\n```\nretriever.invoke(    \"What's a movie after 1990 but before 2005 that's all about toys, and preferably is animated\")\n```\n\n```\n[Document(page_content='Toys come alive and have a blast doing so', metadata={'genre': 'animated', 'year': 1995})]\n```\n\n- Creating our self-querying retriever\n- Testing it out\n- Filter k", "url_final": "https://python.langchain.com/docs/how_to/self_query/", "url_citable": "https://python.langchain.com/docs/how_to/self_query/#constructing-from-scratch-with-lcel", "title": "How to do \"self-querying\" retrieval | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Constructing from scratch with LCEL‚Äã", "section_anchor": "constructing-from-scratch-with-lcel", "fetched_at": "2025-10-06T21:56:18.087289", "has_code": true}
{"id": "d682294d1b9d8c671ed101eaf42923915dfb82ad", "text": "How to use few-shot prompting with tool calling | ü¶úÔ∏èüîó LangChain\n\nFor more complex tool use it's very useful to add few-shot examples to the prompt. We can do this by adding AIMessages with ToolCalls and corresponding ToolMessages to our prompt.\n\nFirst let's define our tools and model.\n\n```\nfrom langchain_core.tools import tool@tooldef add(a: int, b: int) -> int:    \"\"\"Adds a and b.\"\"\"    return a + b@tooldef multiply(a: int, b: int) -> int:    \"\"\"Multiplies a and b.\"\"\"    return a * btools = [add, multiply]\n```\n\n```\nimport osfrom getpass import getpassfrom langchain_openai import ChatOpenAIif \"OPENAI_API_KEY\" not in os.environ:    os.environ[\"OPENAI_API_KEY\"] = getpass()llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)llm_with_tools = llm.bind_tools(tools)\n```\n\nLet's run our model where we can notice that even with some special instructions our model can get tripped up by order of operations.\n\n```\nllm_with_tools.invoke(    \"Whats 119 times 8 minus 20. Don't do any math yourself, only use tools for math. Respect order of operations\").tool_calls\n```\n\n```\n[{'name': 'Multiply',  'args': {'a': 119, 'b': 8},  'id': 'call_T88XN6ECucTgbXXkyDeC2CQj'}, {'name': 'Add',  'args': {'a': 952, 'b': -20},  'id': 'call_licdlmGsRqzup8rhqJSb1yZ4'}]\n```\n\nThe model shouldn't be trying to add anything yet, since it technically can't know the results of 119 * 8 yet.\n\nBy adding a prompt with some examples we can correct this behavior:\n\n```\nfrom langchain_core.messages import AIMessage, HumanMessage, ToolMessagefrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnablePassthroughexamples = [    HumanMessage(        \"What's the product of 317253 and 128472 plus four\", name=\"example_user\"    ),    AIMessage(        \"\",        name=\"example_assistant\",        tool_calls=[            {\"name\": \"Multiply\", \"args\": {\"x\": 317253, \"y\": 128472}, \"id\": \"1\"}        ],    ),    ToolMessage(\"16505054784\", tool_call_id=\"1\"),    AIMessage(        \"\",        name=\"example_assistant\",        tool_calls=[{\"name\": \"Add\", \"args\": {\"x\": 16505054784, \"y\": 4}, \"id\": \"2\"}],    ),    ToolMessage(\"16505054788\", tool_call_id=\"2\"),    AIMessage(        \"The product of 317253 and 128472 plus four is 16505054788\",        name=\"example_assistant\",    ),]system = \"\"\"You are bad at math but are an expert at using a calculator. Use past tool usage as an example of how to correctly use the tools.\"\"\"few_shot_prompt = ChatPromptTemplate.from_messages(    [        (\"system\", system),        *examples,        (\"human\", \"{query}\"),    ])chain = {\"query\": RunnablePassthrough()} | few_shot_prompt | llm_with_toolschain.invoke(\"Whats 119 times 8 minus 20\").tool_calls\n```\n\n```\n[{'name': 'Multiply',  'args': {'a': 119, 'b': 8},  'id': 'call_9MvuwQqg7dlJupJcoTWiEsDo'}]\n```\n\nAnd we get the correct output this time.\n\nHere's what the LangSmith trace looks like.", "url_final": "https://python.langchain.com/docs/how_to/tools_few_shot/", "url_citable": "https://python.langchain.com/docs/how_to/tools_few_shot/", "title": "How to use few-shot prompting with tool calling | ü¶úÔ∏èüîó LangChain", "section_level": 1, "section_heading": "How to use few-shot prompting with tool calling | ü¶úÔ∏èüîó LangChain", "section_anchor": null, "fetched_at": "2025-10-06T21:56:43.873577", "has_code": true}
{"id": "ed43c314e37cb447c2902b24fd97363434003f20", "text": "Base packages#\n\nCore\n\nlangchain-core: 0.3.76\n\nLangchain\n\nlangchain: 0.3.27\n\nText Splitters\n\nlangchain-text-splitters: 0.3.11\n\nCommunity\n\nlangchain-community: 0.3.30\n\nStandard Tests\n\nlangchain-tests: 0.3.21", "url_final": "https://python.langchain.com/api_reference/", "url_citable": "https://python.langchain.com/api_reference/", "title": "LangChain Python API Reference ‚Äî ü¶úüîó LangChain  documentation", "section_level": 2, "section_heading": "Base packages#", "section_anchor": null, "fetched_at": "2025-10-06T21:54:01.946056", "has_code": false}
{"id": "8237e1cbea7b02465ddc386a4753a18aeb145df1", "text": "Integrations#\n\nOpenAI\n\nlangchain-openai 0.3.33\n\nAnthropic\n\nlangchain-anthropic 0.3.21\n\nGoogle VertexAI\n\nlangchain-google-vertexai 2.1.2\n\nAWS\n\nlangchain-aws 0.2.33\n\nHuggingface\n\nlangchain-huggingface 0.3.1\n\nMistralAI\n\nlangchain-mistralai 0.2.12\n\nSee the full list of integrations in the Section Navigation.", "url_final": "https://python.langchain.com/api_reference/", "url_citable": "https://python.langchain.com/api_reference/", "title": "LangChain Python API Reference ‚Äî ü¶úüîó LangChain  documentation", "section_level": 2, "section_heading": "Integrations#", "section_anchor": null, "fetched_at": "2025-10-06T21:54:01.946056", "has_code": false}
{"id": "985816879e9748b6e878bd4e90bd2506136d4308", "text": "Fixed Examples‚Äã\n\nThe most basic (and common) few-shot prompting technique is to use fixed prompt examples. This way you can select a chain, evaluate it, and avoid worrying about additional moving parts in production.\n\nThe basic components of the template are:\n\n- examples: A list of dictionary examples to include in the final prompt.\n- example_prompt: converts each example into 1 or more messages through its format_messages method. A common example would be to convert each example into one human message and one AI message response, or a human message followed by a function call message.\n\nBelow is a simple demonstration. First, define the examples you'd like to include. Let's give the LLM an unfamiliar mathematical operator, denoted by the \"ü¶ú\" emoji:\n\n```\n%pip install -qU langchain langchain-openai langchain-chromaimport osfrom getpass import getpassif \"OPENAI_API_KEY\" not in os.environ:    os.environ[\"OPENAI_API_KEY\"] = getpass()\n```\n\nIf we try to ask the model what the result of this expression is, it will fail:\n\n```\nfrom langchain_openai import ChatOpenAImodel = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)model.invoke(\"What is 2 ü¶ú 9?\")\n```\n\n```\nAIMessage(content='The expression \"2 ü¶ú 9\" is not a standard mathematical operation or equation. It appears to be a combination of the number 2 and the parrot emoji ü¶ú followed by the number 9. It does not have a specific mathematical meaning.', response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 17, 'total_tokens': 71}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-aad12dda-5c47-4a1e-9949-6fe94e03242a-0', usage_metadata={'input_tokens': 17, 'output_tokens': 54, 'total_tokens': 71})\n```\n\nNow let's see what happens if we give the LLM some examples to work with. We'll define some below:\n\n```\nfrom langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplateexamples = [    {\"input\": \"2 ü¶ú 2\", \"output\": \"4\"},    {\"input\": \"2 ü¶ú 3\", \"output\": \"5\"},]\n```\n\nNext, assemble them into the few-shot prompt template.\n\n```\n# This is a prompt template used to format each individual example.example_prompt = ChatPromptTemplate.from_messages(    [        (\"human\", \"{input}\"),        (\"ai\", \"{output}\"),    ])few_shot_prompt = FewShotChatMessagePromptTemplate(    example_prompt=example_prompt,    examples=examples,)print(few_shot_prompt.invoke({}).to_messages())\n```\n\n```\n[HumanMessage(content='2 ü¶ú 2'), AIMessage(content='4'), HumanMessage(content='2 ü¶ú 3'), AIMessage(content='5')]\n```\n\nFinally, we assemble the final prompt as shown below, passing few_shot_prompt directly into the from_messages factory method, and use it with a model:\n\n```\nfinal_prompt = ChatPromptTemplate.from_messages(    [        (\"system\", \"You are a wondrous wizard of math.\"),        few_shot_prompt,        (\"human\", \"{input}\"),    ])\n```\n\nAnd now let's ask the model the initial question and see how it does:\n\n```\nfrom langchain_openai import ChatOpenAIchain = final_prompt | modelchain.invoke({\"input\": \"What is 2 ü¶ú 9?\"})\n```\n\n```\nAIMessage(content='11', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 60, 'total_tokens': 61}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5ec4e051-262f-408e-ad00-3f2ebeb561c3-0', usage_metadata={'input_tokens': 60, 'output_tokens': 1, 'total_tokens': 61})\n```\n\nAnd we can see that the model has now inferred that the parrot emoji means addition from the given few-shot examples!", "url_final": "https://python.langchain.com/docs/how_to/few_shot_examples_chat/", "url_citable": "https://python.langchain.com/docs/how_to/few_shot_examples_chat/#fixed-examples", "title": "How to use few shot examples in chat models | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Fixed Examples‚Äã", "section_anchor": "fixed-examples", "fetched_at": "2025-10-06T21:54:25.948587", "has_code": true}
{"id": "b4e0145f440948e73712fecb9c96efa22a962b09", "text": "Dynamic few-shot prompting‚Äã\n\nSometimes you may want to select only a few examples from your overall set to show based on the input. For this, you can replace the examples passed into FewShotChatMessagePromptTemplate with an example_selector. The other components remain the same as above! Our dynamic few-shot prompt template would look like:\n\n- example_selector: responsible for selecting few-shot examples (and the order in which they are returned) for a given input. These implement the BaseExampleSelector interface. A common example is the vectorstore-backed SemanticSimilarityExampleSelector\n- example_prompt: convert each example into 1 or more messages through its format_messages method. A common example would be to convert each example into one human message and one AI message response, or a human message followed by a function call message.\n\nThese once again can be composed with other messages and chat templates to assemble your final prompt.\n\nLet's walk through an example with the SemanticSimilarityExampleSelector. Since this implementation uses a vectorstore to select examples based on semantic similarity, we will want to first populate the store. Since the basic idea here is that we want to search for and return examples most similar to the text input, we embed the values of our prompt examples rather than considering the keys:\n\n```\nfrom langchain_chroma import Chromafrom langchain_core.example_selectors import SemanticSimilarityExampleSelectorfrom langchain_openai import OpenAIEmbeddingsexamples = [    {\"input\": \"2 ü¶ú 2\", \"output\": \"4\"},    {\"input\": \"2 ü¶ú 3\", \"output\": \"5\"},    {\"input\": \"2 ü¶ú 4\", \"output\": \"6\"},    {\"input\": \"What did the cow say to the moon?\", \"output\": \"nothing at all\"},    {        \"input\": \"Write me a poem about the moon\",        \"output\": \"One for the moon, and one for me, who are we to talk about the moon?\",    },]to_vectorize = [\" \".join(example.values()) for example in examples]embeddings = OpenAIEmbeddings()vectorstore = Chroma.from_texts(to_vectorize, embeddings, metadatas=examples)\n```\n\nCreate the example_selector‚Äã\n\nWith a vectorstore created, we can create the example_selector. Here we will call it in isolation, and set k on it to only fetch the two example closest to the input.\n\n```\nexample_selector = SemanticSimilarityExampleSelector(    vectorstore=vectorstore,    k=2,)# The prompt template will load examples by passing the input do the `select_examples` methodexample_selector.select_examples({\"input\": \"horse\"})\n```\n\n```\n[{'input': 'What did the cow say to the moon?', 'output': 'nothing at all'}, {'input': '2 ü¶ú 4', 'output': '6'}]\n```\n\nCreate prompt template‚Äã\n\nWe now assemble the prompt template, using the example_selector created above.\n\n```\nfrom langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate# Define the few-shot prompt.few_shot_prompt = FewShotChatMessagePromptTemplate(    # The input variables select the values to pass to the example_selector    input_variables=[\"input\"],    example_selector=example_selector,    # Define how each example will be formatted.    # In this case, each example will become 2 messages:    # 1 human, and 1 AI    example_prompt=ChatPromptTemplate.from_messages(        [(\"human\", \"{input}\"), (\"ai\", \"{output}\")]    ),)print(few_shot_prompt.invoke(input=\"What's 3 ü¶ú 3?\").to_messages())\n```\n\n```\n[HumanMessage(content='2 ü¶ú 3'), AIMessage(content='5'), HumanMessage(content='2 ü¶ú 4'), AIMessage(content='6')]\n```\n\nAnd we can pass this few-shot chat message prompt template into another chat prompt template:\n\n```\nfinal_prompt = ChatPromptTemplate.from_messages(    [        (\"system\", \"You are a wondrous wizard of math.\"),        few_shot_prompt,        (\"human\", \"{input}\"),    ])print(few_shot_prompt.invoke(input=\"What's 3 ü¶ú 3?\"))\n```\n\n```\nmessages=[HumanMessage(content='2 ü¶ú 3'), AIMessage(content='5'), HumanMessage(content='2 ü¶ú 4'), AIMessage(content='6')]\n```", "url_final": "https://python.langchain.com/docs/how_to/few_shot_examples_chat/", "url_citable": "https://python.langchain.com/docs/how_to/few_shot_examples_chat/#dynamic-few-shot-prompting", "title": "How to use few shot examples in chat models | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Dynamic few-shot prompting‚Äã", "section_anchor": "dynamic-few-shot-prompting", "fetched_at": "2025-10-06T21:54:25.948587", "has_code": true}
{"id": "b9f07158ed7de9f481f19a2f714c49e70c43cbca", "text": "Use with an chat model‚Äã\n\nFinally, you can connect your model to the few-shot prompt.\n\n```\nchain = final_prompt | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)chain.invoke({\"input\": \"What's 3 ü¶ú 3?\"})\n```\n\n```\nAIMessage(content='6', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 60, 'total_tokens': 61}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d1863e5e-17cd-4e9d-bf7a-b9f118747a65-0', usage_metadata={'input_tokens': 60, 'output_tokens': 1, 'total_tokens': 61})\n```", "url_final": "https://python.langchain.com/docs/how_to/few_shot_examples_chat/", "url_citable": "https://python.langchain.com/docs/how_to/few_shot_examples_chat/#use-with-an-chat-model", "title": "How to use few shot examples in chat models | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Use with an chat model‚Äã", "section_anchor": "use-with-an-chat-model", "fetched_at": "2025-10-06T21:54:25.948587", "has_code": true}
{"id": "dacf8cd78f0d51d8c63b54646b6fb7a38043146e", "text": "Next steps\u0000\u0000‚Äã\n\nYou've now learned how to add few-shot examples to your chat prompts.\n\nNext, check out the other how-to guides on prompt templates in this section, the related how-to guide on few shotting with text completion models, or the other example selector how-to guides.\n\n- Create the example_selector\n- Create prompt template\n- Use with an chat model", "url_final": "https://python.langchain.com/docs/how_to/few_shot_examples_chat/", "url_citable": "https://python.langchain.com/docs/how_to/few_shot_examples_chat/#next-steps", "title": "How to use few shot examples in chat models | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Next steps\u0000\u0000‚Äã", "section_anchor": "next-steps", "fetched_at": "2025-10-06T21:54:25.948587", "has_code": false}
{"id": "827efe14df2845ed19b57ecca3223cdad386706b", "text": "Setup‚Äã\n\nYou'll need to install a few packages, and have your OpenAI API key set as an environment variable named OPENAI_API_KEY:\n\n```\n%pip install --upgrade --quiet langchain langchain-openai langgraphimport getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"):    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n```\n\n```\nOpenAI API Key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n```\n\nLet's also set up a chat model that we'll use for the below examples.\n\n```\nfrom langchain_openai import ChatOpenAImodel = ChatOpenAI(model=\"gpt-4o-mini\")\n```", "url_final": "https://python.langchain.com/docs/how_to/chatbots_memory/", "url_citable": "https://python.langchain.com/docs/how_to/chatbots_memory/#setup", "title": "How to add memory to chatbots | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Setup‚Äã", "section_anchor": "setup", "fetched_at": "2025-10-06T21:54:18.658556", "has_code": true}
{"id": "f92b8624487ebedf5c911ae4270c81fa5d544d0d", "text": "Message passing‚Äã\n\nThe simplest form of memory is simply passing chat history messages into a chain. Here's an example:\n\n```\nfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessagefrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholderprompt = ChatPromptTemplate.from_messages(    [        SystemMessage(            content=\"You are a helpful assistant. Answer all questions to the best of your ability.\"        ),        MessagesPlaceholder(variable_name=\"messages\"),    ])chain = prompt | modelai_msg = chain.invoke(    {        \"messages\": [            HumanMessage(                content=\"Translate from English to French: I love programming.\"            ),            AIMessage(content=\"J'adore la programmation.\"),            HumanMessage(content=\"What did you just say?\"),        ],    })print(ai_msg.content)\n```\n\n```\nI said, \"I love programming\" in French: \"J'adore la programmation.\"\n```\n\nWe can see that by passing the previous conversation into a chain, it can use it as context to answer questions. This is the basic concept underpinning chatbot memory - the rest of the guide will demonstrate convenient techniques for passing or reformatting messages.", "url_final": "https://python.langchain.com/docs/how_to/chatbots_memory/", "url_citable": "https://python.langchain.com/docs/how_to/chatbots_memory/#message-passing", "title": "How to add memory to chatbots | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Message passing‚Äã", "section_anchor": "message-passing", "fetched_at": "2025-10-06T21:54:18.658556", "has_code": true}
{"id": "8957c1d8f6d2c4e64052109e830a6729d4164141", "text": "Automatic history management‚Äã\n\nThe previous examples pass messages to the chain (and model) explicitly. This is a completely acceptable approach, but it does require external management of new messages. LangChain also provides a way to build applications that have memory using LangGraph's persistence. You can enable persistence in LangGraph applications by providing a checkpointer when compiling the graph.\n\n```\nfrom langgraph.checkpoint.memory import MemorySaverfrom langgraph.graph import START, MessagesState, StateGraphworkflow = StateGraph(state_schema=MessagesState)# Define the function that calls the modeldef call_model(state: MessagesState):    system_prompt = (        \"You are a helpful assistant. Answer all questions to the best of your ability.\"    )    messages = [SystemMessage(content=system_prompt)] + state[\"messages\"]    response = model.invoke(messages)    return {\"messages\": response}# Define the node and edgeworkflow.add_node(\"model\", call_model)workflow.add_edge(START, \"model\")# Add simple in-memory checkpointermemory = MemorySaver()app = workflow.compile(checkpointer=memory)\n```\n\nWe'll pass the latest input to the conversation here and let LangGraph keep track of the conversation history using the checkpointer:\n\n```\napp.invoke(    {\"messages\": [HumanMessage(content=\"Translate to French: I love programming.\")]},    config={\"configurable\": {\"thread_id\": \"1\"}},)\n```\n\n```\n{'messages': [HumanMessage(content='Translate to French: I love programming.', additional_kwargs={}, response_metadata={}, id='be5e7099-3149-4293-af49-6b36c8ccd71b'),  AIMessage(content=\"J'aime programmer.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 35, 'total_tokens': 39, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e9627b5346', 'finish_reason': 'stop', 'logprobs': None}, id='run-8a753d7a-b97b-4d01-a661-626be6f41b38-0', usage_metadata={'input_tokens': 35, 'output_tokens': 4, 'total_tokens': 39})]}\n```\n\n```\napp.invoke(    {\"messages\": [HumanMessage(content=\"What did I just ask you?\")]},    config={\"configurable\": {\"thread_id\": \"1\"}},)\n```\n\n```\n{'messages': [HumanMessage(content='Translate to French: I love programming.', additional_kwargs={}, response_metadata={}, id='be5e7099-3149-4293-af49-6b36c8ccd71b'),  AIMessage(content=\"J'aime programmer.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 35, 'total_tokens': 39, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e9627b5346', 'finish_reason': 'stop', 'logprobs': None}, id='run-8a753d7a-b97b-4d01-a661-626be6f41b38-0', usage_metadata={'input_tokens': 35, 'output_tokens': 4, 'total_tokens': 39}),  HumanMessage(content='What did I just ask you?', additional_kwargs={}, response_metadata={}, id='c667529b-7c41-4cc0-9326-0af47328b816'),  AIMessage(content='You asked me to translate \"I love programming\" into French.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 54, 'total_tokens': 67, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-134a7ea0-d3a4-4923-bd58-25e5a43f6a1f-0', usage_metadata={'input_tokens': 54, 'output_tokens': 13, 'total_tokens': 67})]}\n```", "url_final": "https://python.langchain.com/docs/how_to/chatbots_memory/", "url_citable": "https://python.langchain.com/docs/how_to/chatbots_memory/#automatic-history-management", "title": "How to add memory to chatbots | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Automatic history management‚Äã", "section_anchor": "automatic-history-management", "fetched_at": "2025-10-06T21:54:18.658556", "has_code": true}
{"id": "fcce91dc44f3e005474e7b55543c30c0f721636c", "text": "Modifying chat history‚Äã\n\nModifying stored chat messages can help your chatbot handle a variety of situations. Here are some examples:", "url_final": "https://python.langchain.com/docs/how_to/chatbots_memory/", "url_citable": "https://python.langchain.com/docs/how_to/chatbots_memory/#modifying-chat-history", "title": "How to add memory to chatbots | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Modifying chat history‚Äã", "section_anchor": "modifying-chat-history", "fetched_at": "2025-10-06T21:54:18.658556", "has_code": false}
{"id": "5a6c42d4cabdb8430c7564d5e2ecabe55dfefb25", "text": "Trimming messages‚Äã\n\nLLMs and chat models have limited context windows, and even if you're not directly hitting limits, you may want to limit the amount of distraction the model has to deal with. One solution is trim the history messages before passing them to the model. Let's use an example history with the app we declared above:\n\n```\ndemo_ephemeral_chat_history = [    HumanMessage(content=\"Hey there! I'm Nemo.\"),    AIMessage(content=\"Hello!\"),    HumanMessage(content=\"How are you today?\"),    AIMessage(content=\"Fine thanks!\"),]app.invoke(    {        \"messages\": demo_ephemeral_chat_history        + [HumanMessage(content=\"What's my name?\")]    },    config={\"configurable\": {\"thread_id\": \"2\"}},)\n```\n\n```\n{'messages': [HumanMessage(content=\"Hey there! I'm Nemo.\", additional_kwargs={}, response_metadata={}, id='6b4cab70-ce18-49b0-bb06-267bde44e037'),  AIMessage(content='Hello!', additional_kwargs={}, response_metadata={}, id='ba3714f4-8876-440b-a651-efdcab2fcb4c'),  HumanMessage(content='How are you today?', additional_kwargs={}, response_metadata={}, id='08d032c0-1577-4862-a3f2-5c1b90687e21'),  AIMessage(content='Fine thanks!', additional_kwargs={}, response_metadata={}, id='21790e16-db05-4537-9a6b-ecad0fcec436'),  HumanMessage(content=\"What's my name?\", additional_kwargs={}, response_metadata={}, id='c933eca3-5fd8-4651-af16-20fe2d49c216'),  AIMessage(content='Your name is Nemo.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 63, 'total_tokens': 68, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-a0b21acc-9dbb-4fb6-a953-392020f37d88-0', usage_metadata={'input_tokens': 63, 'output_tokens': 5, 'total_tokens': 68})]}\n```\n\nWe can see the app remembers the preloaded name.\n\nBut let's say we have a very small context window, and we want to trim the number of messages passed to the model to only the 2 most recent ones. We can use the built in trim_messages util to trim messages based on their token count before they reach our prompt. In this case we'll count each message as 1 \"token\" and keep only the last two messages:\n\n```\nfrom langchain_core.messages import trim_messagesfrom langgraph.checkpoint.memory import MemorySaverfrom langgraph.graph import START, MessagesState, StateGraph# Define trimmer# count each message as 1 \"token\" (token_counter=len) and keep only the last two messagestrimmer = trim_messages(strategy=\"last\", max_tokens=2, token_counter=len)workflow = StateGraph(state_schema=MessagesState)# Define the function that calls the modeldef call_model(state: MessagesState):    trimmed_messages = trimmer.invoke(state[\"messages\"])    system_prompt = (        \"You are a helpful assistant. Answer all questions to the best of your ability.\"    )    messages = [SystemMessage(content=system_prompt)] + trimmed_messages    response = model.invoke(messages)    return {\"messages\": response}# Define the node and edgeworkflow.add_node(\"model\", call_model)workflow.add_edge(START, \"model\")# Add simple in-memory checkpointermemory = MemorySaver()app = workflow.compile(checkpointer=memory)\n```\n\nLet's call this new app and check the response\n\n```\napp.invoke(    {        \"messages\": demo_ephemeral_chat_history        + [HumanMessage(content=\"What is my name?\")]    },    config={\"configurable\": {\"thread_id\": \"3\"}},)\n```\n\n```\n{'messages': [HumanMessage(content=\"Hey there! I'm Nemo.\", additional_kwargs={}, response_metadata={}, id='6b4cab70-ce18-49b0-bb06-267bde44e037'),  AIMessage(content='Hello!', additional_kwargs={}, response_metadata={}, id='ba3714f4-8876-440b-a651-efdcab2fcb4c'),  HumanMessage(content='How are you today?', additional_kwargs={}, response_metadata={}, id='08d032c0-1577-4862-a3f2-5c1b90687e21'),  AIMessage(content='Fine thanks!', additional_kwargs={}, response_metadata={}, id='21790e16-db05-4537-9a6b-ecad0fcec436'),  HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}, id='a22ab7c5-8617-4821-b3e9-a9e7dca1ff78'),  AIMessage(content=\"I'm sorry, but I don't have access to personal information about you unless you share it with me. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 39, 'total_tokens': 66, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-f7b32d72-9f57-4705-be7e-43bf1c3d293b-0', usage_metadata={'input_tokens': 39, 'output_tokens': 27, 'total_tokens': 66})]}\n```\n\nWe can see that trim_messages was called and only the two most recent messages will be passed to the model. In this case, this means that the model forgot the name we gave it.\n\nCheck out our how to guide on trimming messages for more.", "url_final": "https://python.langchain.com/docs/how_to/chatbots_memory/", "url_citable": "https://python.langchain.com/docs/how_to/chatbots_memory/#trimming-messages", "title": "How to add memory to chatbots | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Trimming messages‚Äã", "section_anchor": "trimming-messages", "fetched_at": "2025-10-06T21:54:18.658556", "has_code": true}
{"id": "11f27399f221e75177674db020040a7312713b5a", "text": "Summary memory‚Äã\n\nWe can use this same pattern in other ways too. For example, we could use an additional LLM call to generate a summary of the conversation before calling our app. Let's recreate our chat history:\n\n```\ndemo_ephemeral_chat_history = [    HumanMessage(content=\"Hey there! I'm Nemo.\"),    AIMessage(content=\"Hello!\"),    HumanMessage(content=\"How are you today?\"),    AIMessage(content=\"Fine thanks!\"),]\n```\n\nAnd now, let's update the model-calling function to distill previous interactions into a summary:\n\n```\nfrom langchain_core.messages import HumanMessage, RemoveMessagefrom langgraph.checkpoint.memory import MemorySaverfrom langgraph.graph import START, MessagesState, StateGraphworkflow = StateGraph(state_schema=MessagesState)# Define the function that calls the modeldef call_model(state: MessagesState):    system_prompt = (        \"You are a helpful assistant. \"        \"Answer all questions to the best of your ability. \"        \"The provided chat history includes a summary of the earlier conversation.\"    )    system_message = SystemMessage(content=system_prompt)    message_history = state[\"messages\"][:-1]  # exclude the most recent user input    # Summarize the messages if the chat history reaches a certain size    if len(message_history) >= 4:        last_human_message = state[\"messages\"][-1]        # Invoke the model to generate conversation summary        summary_prompt = (            \"Distill the above chat messages into a single summary message. \"            \"Include as many specific details as you can.\"        )        summary_message = model.invoke(            message_history + [HumanMessage(content=summary_prompt)]        )        # Delete messages that we no longer want to show up        delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"]]        # Re-add user message        human_message = HumanMessage(content=last_human_message.content)        # Call the model with summary & response        response = model.invoke([system_message, summary_message, human_message])        message_updates = [summary_message, human_message, response] + delete_messages    else:        message_updates = model.invoke([system_message] + state[\"messages\"])    return {\"messages\": message_updates}# Define the node and edgeworkflow.add_node(\"model\", call_model)workflow.add_edge(START, \"model\")# Add simple in-memory checkpointermemory = MemorySaver()app = workflow.compile(checkpointer=memory)\n```\n\nLet's see if it remembers the name we gave it:\n\n```\napp.invoke(    {        \"messages\": demo_ephemeral_chat_history        + [HumanMessage(\"What did I say my name was?\")]    },    config={\"configurable\": {\"thread_id\": \"4\"}},)\n```\n\n```\n{'messages': [AIMessage(content=\"Nemo greeted me, and I responded positively, indicating that I'm doing well.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 60, 'total_tokens': 76, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-ee42f98d-907d-4bad-8f16-af2db789701d-0', usage_metadata={'input_tokens': 60, 'output_tokens': 16, 'total_tokens': 76}),  HumanMessage(content='What did I say my name was?', additional_kwargs={}, response_metadata={}, id='788555ea-5b1f-4c29-a2f2-a92f15d147be'),  AIMessage(content='You mentioned that your name is Nemo.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 67, 'total_tokens': 75, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-099a43bd-a284-4969-bb6f-0be486614cd8-0', usage_metadata={'input_tokens': 67, 'output_tokens': 8, 'total_tokens': 75})]}\n```\n\nNote that invoking the app again will keep accumulating the history until it reaches the specified number of messages (four in our case). At that point we will generate another summary generated from the initial summary plus new messages and so on.\n\n- Trimming messages\n- Summary memory", "url_final": "https://python.langchain.com/docs/how_to/chatbots_memory/", "url_citable": "https://python.langchain.com/docs/how_to/chatbots_memory/#summary-memory", "title": "How to add memory to chatbots | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Summary memory‚Äã", "section_anchor": "summary-memory", "fetched_at": "2025-10-06T21:54:18.658556", "has_code": true}
{"id": "308ce54fba52162a95d5c47fd282656579c3d69d", "text": "Python‚Äã\n\nHere's an example using the PythonTextSplitter:\n\n```\nPYTHON_CODE = \"\"\"def hello_world():    print(\"Hello, World!\")# Call the functionhello_world()\"\"\"python_splitter = RecursiveCharacterTextSplitter.from_language(    language=Language.PYTHON, chunk_size=50, chunk_overlap=0)python_docs = python_splitter.create_documents([PYTHON_CODE])python_docs\n```\n\n```\n[Document(metadata={}, page_content='def hello_world():\\n    print(\"Hello, World!\")'), Document(metadata={}, page_content='# Call the function\\nhello_world()')]\n```", "url_final": "https://python.langchain.com/docs/how_to/code_splitter/", "url_citable": "https://python.langchain.com/docs/how_to/code_splitter/#python", "title": "How to split code | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Python‚Äã", "section_anchor": "python", "fetched_at": "2025-10-06T21:55:09.099204", "has_code": true}
{"id": "de9860b9ef1fb8ef9b141c9714ccea5a4a36e28c", "text": "JS‚Äã\n\nHere's an example using the JS text splitter:\n\n```\nJS_CODE = \"\"\"function helloWorld() {  console.log(\"Hello, World!\");}// Call the functionhelloWorld();\"\"\"js_splitter = RecursiveCharacterTextSplitter.from_language(    language=Language.JS, chunk_size=60, chunk_overlap=0)js_docs = js_splitter.create_documents([JS_CODE])js_docs\n```\n\n```\n[Document(metadata={}, page_content='function helloWorld() {\\n  console.log(\"Hello, World!\");\\n}'), Document(metadata={}, page_content='// Call the function\\nhelloWorld();')]\n```", "url_final": "https://python.langchain.com/docs/how_to/code_splitter/", "url_citable": "https://python.langchain.com/docs/how_to/code_splitter/#js", "title": "How to split code | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "JS‚Äã", "section_anchor": "js", "fetched_at": "2025-10-06T21:55:09.099204", "has_code": true}
{"id": "1e5be04a7f2df5be8d144d6c500537a277a456d8", "text": "TS‚Äã\n\nHere's an example using the TS text splitter:\n\n```\nTS_CODE = \"\"\"function helloWorld(): void {  console.log(\"Hello, World!\");}// Call the functionhelloWorld();\"\"\"ts_splitter = RecursiveCharacterTextSplitter.from_language(    language=Language.TS, chunk_size=60, chunk_overlap=0)ts_docs = ts_splitter.create_documents([TS_CODE])ts_docs\n```\n\n```\n[Document(metadata={}, page_content='function helloWorld(): void {'), Document(metadata={}, page_content='console.log(\"Hello, World!\");\\n}'), Document(metadata={}, page_content='// Call the function\\nhelloWorld();')]\n```", "url_final": "https://python.langchain.com/docs/how_to/code_splitter/", "url_citable": "https://python.langchain.com/docs/how_to/code_splitter/#ts", "title": "How to split code | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "TS‚Äã", "section_anchor": "ts", "fetched_at": "2025-10-06T21:55:09.099204", "has_code": true}
{"id": "31aacf6415ff6686521fb892e5fcec21882a9de2", "text": "Markdown‚Äã\n\nHere's an example using the Markdown text splitter:\n\n```\nmarkdown_text = \"\"\"# ü¶úÔ∏èüîó LangChain‚ö° Building applications with LLMs through composability ‚ö°## What is LangChain?# Hopefully this code block isn't splitLangChain is a framework for...As an open-source project in a rapidly developing field, we are extremely open to contributions.\"\"\"\n```\n\n```\nmd_splitter = RecursiveCharacterTextSplitter.from_language(    language=Language.MARKDOWN, chunk_size=60, chunk_overlap=0)md_docs = md_splitter.create_documents([markdown_text])md_docs\n```\n\n```\n[Document(metadata={}, page_content='# ü¶úÔ∏èüîó LangChain'), Document(metadata={}, page_content='‚ö° Building applications with LLMs through composability ‚ö°'), Document(metadata={}, page_content='## What is LangChain?'), Document(metadata={}, page_content=\"# Hopefully this code block isn't split\"), Document(metadata={}, page_content='LangChain is a framework for...'), Document(metadata={}, page_content='As an open-source project in a rapidly developing field, we'), Document(metadata={}, page_content='are extremely open to contributions.')]\n```", "url_final": "https://python.langchain.com/docs/how_to/code_splitter/", "url_citable": "https://python.langchain.com/docs/how_to/code_splitter/#markdown", "title": "How to split code | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Markdown‚Äã", "section_anchor": "markdown", "fetched_at": "2025-10-06T21:55:09.099204", "has_code": true}
{"id": "8ab2e2a8d8b63255359d47df838ca7ae9153d07f", "text": "Latex‚Äã\n\nHere's an example on Latex text:\n\n```\nlatex_text = \"\"\"\\documentclass{article}\\begin{document}\\maketitle\\section{Introduction}Large language models (LLMs) are a type of machine learning model that can be trained on vast amounts of text data to generate human-like language. In recent years, LLMs have made significant advances in a variety of natural language processing tasks, including language translation, text generation, and sentiment analysis.\\subsection{History of LLMs}The earliest LLMs were developed in the 1980s and 1990s, but they were limited by the amount of data that could be processed and the computational power available at the time. In the past decade, however, advances in hardware and software have made it possible to train LLMs on massive datasets, leading to significant improvements in performance.\\subsection{Applications of LLMs}LLMs have many applications in industry, including chatbots, content creation, and virtual assistants. They can also be used in academia for research in linguistics, psychology, and computational linguistics.\\end{document}\"\"\"\n```\n\n```\nlatex_splitter = RecursiveCharacterTextSplitter.from_language(    language=Language.MARKDOWN, chunk_size=60, chunk_overlap=0)latex_docs = latex_splitter.create_documents([latex_text])latex_docs\n```\n\n```\n[Document(metadata={}, page_content='\\\\documentclass{article}\\n\\n\\x08egin{document}\\n\\n\\\\maketitle'), Document(metadata={}, page_content='\\\\section{Introduction}'), Document(metadata={}, page_content='Large language models (LLMs) are a type of machine learning'), Document(metadata={}, page_content='model that can be trained on vast amounts of text data to'), Document(metadata={}, page_content='generate human-like language. In recent years, LLMs have'), Document(metadata={}, page_content='made significant advances in a variety of natural language'), Document(metadata={}, page_content='processing tasks, including language translation, text'), Document(metadata={}, page_content='generation, and sentiment analysis.'), Document(metadata={}, page_content='\\\\subsection{History of LLMs}'), Document(metadata={}, page_content='The earliest LLMs were developed in the 1980s and 1990s,'), Document(metadata={}, page_content='but they were limited by the amount of data that could be'), Document(metadata={}, page_content='processed and the computational power available at the'), Document(metadata={}, page_content='time. In the past decade, however, advances in hardware and'), Document(metadata={}, page_content='software have made it possible to train LLMs on massive'), Document(metadata={}, page_content='datasets, leading to significant improvements in'), Document(metadata={}, page_content='performance.'), Document(metadata={}, page_content='\\\\subsection{Applications of LLMs}'), Document(metadata={}, page_content='LLMs have many applications in industry, including'), Document(metadata={}, page_content='chatbots, content creation, and virtual assistants. They'), Document(metadata={}, page_content='can also be used in academia for research in linguistics,'), Document(metadata={}, page_content='psychology, and computational linguistics.'), Document(metadata={}, page_content='\\\\end{document}')]\n```", "url_final": "https://python.langchain.com/docs/how_to/code_splitter/", "url_citable": "https://python.langchain.com/docs/how_to/code_splitter/#latex", "title": "How to split code | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Latex‚Äã", "section_anchor": "latex", "fetched_at": "2025-10-06T21:55:09.099204", "has_code": true}
{"id": "3d2d64dce9eb6b219b3aa033531a8b628b93adde", "text": "HTML‚Äã\n\nHere's an example using an HTML text splitter:\n\n```\nhtml_text = \"\"\"<!DOCTYPE html><html>    <head>        <title>ü¶úÔ∏èüîó LangChain</title>        <style>            body {                font-family: Arial, sans-serif;            }            h1 {                color: darkblue;            }        </style>    </head>    <body>        <div>            <h1>ü¶úÔ∏èüîó LangChain</h1>            <p>‚ö° Building applications with LLMs through composability ‚ö°</p>        </div>        <div>            As an open-source project in a rapidly developing field, we are extremely open to contributions.        </div>    </body></html>\"\"\"\n```\n\n```\nhtml_splitter = RecursiveCharacterTextSplitter.from_language(    language=Language.HTML, chunk_size=60, chunk_overlap=0)html_docs = html_splitter.create_documents([html_text])html_docs\n```\n\n```\n[Document(metadata={}, page_content='<!DOCTYPE html>\\n<html>'), Document(metadata={}, page_content='<head>\\n        <title>ü¶úÔ∏èüîó LangChain</title>'), Document(metadata={}, page_content='<style>\\n            body {\\n                font-family: Aria'), Document(metadata={}, page_content='l, sans-serif;\\n            }\\n            h1 {'), Document(metadata={}, page_content='color: darkblue;\\n            }\\n        </style>\\n    </head'), Document(metadata={}, page_content='>'), Document(metadata={}, page_content='<body>'), Document(metadata={}, page_content='<div>\\n            <h1>ü¶úÔ∏èüîó LangChain</h1>'), Document(metadata={}, page_content='<p>‚ö° Building applications with LLMs through composability ‚ö°'), Document(metadata={}, page_content='</p>\\n        </div>'), Document(metadata={}, page_content='<div>\\n            As an open-source project in a rapidly dev'), Document(metadata={}, page_content='eloping field, we are extremely open to contributions.'), Document(metadata={}, page_content='</div>\\n    </body>\\n</html>')]\n```", "url_final": "https://python.langchain.com/docs/how_to/code_splitter/", "url_citable": "https://python.langchain.com/docs/how_to/code_splitter/#html", "title": "How to split code | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "HTML‚Äã", "section_anchor": "html", "fetched_at": "2025-10-06T21:55:09.099204", "has_code": true}
{"id": "e51182872a9ba560c1948ef9343692e7c6d7a47e", "text": "Solidity‚Äã\n\nHere's an example using the Solidity text splitter:\n\n```\nSOL_CODE = \"\"\"pragma solidity ^0.8.20;contract HelloWorld {   function add(uint a, uint b) pure public returns(uint) {       return a + b;   }}\"\"\"sol_splitter = RecursiveCharacterTextSplitter.from_language(    language=Language.SOL, chunk_size=128, chunk_overlap=0)sol_docs = sol_splitter.create_documents([SOL_CODE])sol_docs\n```\n\n```\n[Document(metadata={}, page_content='pragma solidity ^0.8.20;'), Document(metadata={}, page_content='contract HelloWorld {\\n   function add(uint a, uint b) pure public returns(uint) {\\n       return a + b;\\n   }\\n}')]\n```", "url_final": "https://python.langchain.com/docs/how_to/code_splitter/", "url_citable": "https://python.langchain.com/docs/how_to/code_splitter/#solidity", "title": "How to split code | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Solidity‚Äã", "section_anchor": "solidity", "fetched_at": "2025-10-06T21:55:09.099204", "has_code": true}
{"id": "e71d4c67ef968aee74b0060e00a4ce478c670a78", "text": "C#‚Äã\n\nHere's an example using the C# text splitter:\n\n```\nC_CODE = \"\"\"using System;class Program{    static void Main()    {        int age = 30; // Change the age value as needed        // Categorize the age without any console output        if (age < 18)        {            // Age is under 18        }        else if (age >= 18 && age < 65)        {            // Age is an adult        }        else        {            // Age is a senior citizen        }    }}\"\"\"c_splitter = RecursiveCharacterTextSplitter.from_language(    language=Language.CSHARP, chunk_size=128, chunk_overlap=0)c_docs = c_splitter.create_documents([C_CODE])c_docs\n```\n\n```\n[Document(metadata={}, page_content='using System;'), Document(metadata={}, page_content='class Program\\n{\\n    static void Main()\\n    {\\n        int age = 30; // Change the age value as needed'), Document(metadata={}, page_content='// Categorize the age without any console output\\n        if (age < 18)\\n        {\\n            // Age is under 18'), Document(metadata={}, page_content='}\\n        else if (age >= 18 && age < 65)\\n        {\\n            // Age is an adult\\n        }\\n        else\\n        {'), Document(metadata={}, page_content='// Age is a senior citizen\\n        }\\n    }\\n}')]\n```", "url_final": "https://python.langchain.com/docs/how_to/code_splitter/", "url_citable": "https://python.langchain.com/docs/how_to/code_splitter/#c", "title": "How to split code | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "C#‚Äã", "section_anchor": "c", "fetched_at": "2025-10-06T21:55:09.099204", "has_code": true}
{"id": "a7603d7678257433a0ac025aab567d8791dd991a", "text": "Haskell‚Äã\n\nHere's an example using the Haskell text splitter:\n\n```\nHASKELL_CODE = \"\"\"main :: IO ()main = do    putStrLn \"Hello, World!\"-- Some sample functionsadd :: Int -> Int -> Intadd x y = x + y\"\"\"haskell_splitter = RecursiveCharacterTextSplitter.from_language(    language=Language.HASKELL, chunk_size=50, chunk_overlap=0)haskell_docs = haskell_splitter.create_documents([HASKELL_CODE])haskell_docs\n```\n\n```\n[Document(metadata={}, page_content='main :: IO ()'), Document(metadata={}, page_content='main = do\\n    putStrLn \"Hello, World!\"\\n-- Some'), Document(metadata={}, page_content='sample functions\\nadd :: Int -> Int -> Int\\nadd x y'), Document(metadata={}, page_content='= x + y')]\n```", "url_final": "https://python.langchain.com/docs/how_to/code_splitter/", "url_citable": "https://python.langchain.com/docs/how_to/code_splitter/#haskell", "title": "How to split code | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Haskell‚Äã", "section_anchor": "haskell", "fetched_at": "2025-10-06T21:55:09.099204", "has_code": true}
{"id": "a740535799cbd5b3651bbd1587fc38f327dfa9ec", "text": "PHP‚Äã\n\nHere's an example using the PHP text splitter:\n\n```\nPHP_CODE = \"\"\"<?phpnamespace foo;class Hello {    public function __construct() { }}function hello() {    echo \"Hello World!\";}interface Human {    public function breath();}trait Foo { }enum Color{    case Red;    case Blue;}\"\"\"php_splitter = RecursiveCharacterTextSplitter.from_language(    language=Language.PHP, chunk_size=50, chunk_overlap=0)php_docs = php_splitter.create_documents([PHP_CODE])php_docs\n```\n\n```\n[Document(metadata={}, page_content='<?php\\nnamespace foo;'), Document(metadata={}, page_content='class Hello {'), Document(metadata={}, page_content='public function __construct() { }\\n}'), Document(metadata={}, page_content='function hello() {\\n    echo \"Hello World!\";\\n}'), Document(metadata={}, page_content='interface Human {\\n    public function breath();\\n}'), Document(metadata={}, page_content='trait Foo { }\\nenum Color\\n{\\n    case Red;'), Document(metadata={}, page_content='case Blue;\\n}')]\n```", "url_final": "https://python.langchain.com/docs/how_to/code_splitter/", "url_citable": "https://python.langchain.com/docs/how_to/code_splitter/#php", "title": "How to split code | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "PHP‚Äã", "section_anchor": "php", "fetched_at": "2025-10-06T21:55:09.099204", "has_code": true}
{"id": "d69478cafbc1952f11b8145a19a97066dffb30bf", "text": "PowerShell‚Äã\n\nHere's an example using the PowerShell text splitter:\n\n```\nPOWERSHELL_CODE = \"\"\"$directoryPath = Get-Location$items = Get-ChildItem -Path $directoryPath$files = $items | Where-Object { -not $_.PSIsContainer }$sortedFiles = $files | Sort-Object LastWriteTimeforeach ($file in $sortedFiles) {    Write-Output (\"Name: \" + $file.Name + \" | Last Write Time: \" + $file.LastWriteTime)}\"\"\"powershell_splitter = RecursiveCharacterTextSplitter.from_language(    language=Language.POWERSHELL, chunk_size=100, chunk_overlap=0)powershell_docs = powershell_splitter.create_documents([POWERSHELL_CODE])powershell_docs\n```\n\n```\n[Document(metadata={}, page_content='$directoryPath = Get-Location\\n\\n$items = Get-ChildItem -Path $directoryPath'), Document(metadata={}, page_content='$files = $items | Where-Object { -not $_.PSIsContainer }'), Document(metadata={}, page_content='$sortedFiles = $files | Sort-Object LastWriteTime'), Document(metadata={}, page_content='foreach ($file in $sortedFiles) {'), Document(metadata={}, page_content='Write-Output (\"Name: \" + $file.Name + \" | Last Write Time: \" + $file.LastWriteTime)\\n}')]\n```", "url_final": "https://python.langchain.com/docs/how_to/code_splitter/", "url_citable": "https://python.langchain.com/docs/how_to/code_splitter/#powershell", "title": "How to split code | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "PowerShell‚Äã", "section_anchor": "powershell", "fetched_at": "2025-10-06T21:55:09.099204", "has_code": true}
{"id": "79c6d2b40d87f7b723cdb4bdffaec0e9f9d5c7ed", "text": "Visual Basic 6‚Äã\n\n```\nVISUALBASIC6_CODE = \"\"\"Option ExplicitPublic Sub HelloWorld()    MsgBox \"Hello, World!\"End SubPrivate Function Add(a As Integer, b As Integer) As Integer    Add = a + bEnd Function\"\"\"visualbasic6_splitter = RecursiveCharacterTextSplitter.from_language(    Language.VISUALBASIC6,    chunk_size=128,    chunk_overlap=0,)visualbasic6_docs = visualbasic6_splitter.create_documents([VISUALBASIC6_CODE])visualbasic6_docs\n```\n\n```\n[Document(metadata={}, page_content='Option Explicit'), Document(metadata={}, page_content='Public Sub HelloWorld()\\n    MsgBox \"Hello, World!\"\\nEnd Sub'), Document(metadata={}, page_content='Private Function Add(a As Integer, b As Integer) As Integer\\n    Add = a + b\\nEnd Function')]\n```\n\n- Python\n- JS\n- TS\n- Markdown\n- Latex\n- HTML\n- Solidity\n- C#\n- Haskell\n- PHP\n- PowerShell\n- Visual Basic 6", "url_final": "https://python.langchain.com/docs/how_to/code_splitter/", "url_citable": "https://python.langchain.com/docs/how_to/code_splitter/#visual-basic-6", "title": "How to split code | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Visual Basic 6‚Äã", "section_anchor": "visual-basic-6", "fetched_at": "2025-10-06T21:55:09.099204", "has_code": true}
{"id": "5a0067ff454223502bdf5d16e40c7d579f7a35b1", "text": "Setup‚Äã\n\nDependencies‚Äã\n\nWe'll use the following packages:\n\n```\n%pip install --upgrade --quiet  langchain langchain-community langchainhub beautifulsoup4\n```\n\nLangSmith‚Äã\n\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\n\nNote that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:\n\n```\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n```\n\nComponents‚Äã\n\nWe will need to select three components from LangChain's suite of integrations.\n\nA chat model:\n\n```\npip install -qU \"langchain[google-genai]\"\n```\n\n```\nimport getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n```\n\nAn embedding model:\n\n```\npip install -qU langchain-openai\n```\n\n```\nimport getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"):  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")from langchain_openai import OpenAIEmbeddingsembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n```\n\nAnd a vector store:\n\n```\npip install -qU langchain-core\n```\n\n```\nfrom langchain_core.vectorstores import InMemoryVectorStorevector_store = InMemoryVectorStore(embeddings)\n```", "url_final": "https://python.langchain.com/docs/how_to/qa_streaming/", "url_citable": "https://python.langchain.com/docs/how_to/qa_streaming/#setup", "title": "How to stream results from your RAG application | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Setup‚Äã", "section_anchor": "setup", "fetched_at": "2025-10-06T21:56:13.010176", "has_code": true}
{"id": "797af2ec00ad23b494a9a07e9a2c697273000fd6", "text": "RAG application‚Äã\n\nLet's reconstruct the Q&A app with sources we built over the LLM Powered Autonomous Agents blog post by Lilian Weng in the RAG tutorial.\n\nFirst we index our documents:\n\n```\nimport bs4from langchain import hubfrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_core.documents import Documentfrom langchain_text_splitters import RecursiveCharacterTextSplitterfrom typing_extensions import List, TypedDict# Load and chunk contents of the blogloader = WebBaseLoader(    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),    bs_kwargs=dict(        parse_only=bs4.SoupStrainer(            class_=(\"post-content\", \"post-title\", \"post-header\")        )    ),)docs = loader.load()text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)all_splits = text_splitter.split_documents(docs)\n```\n\n```\n# Index chunks_ = vector_store.add_documents(documents=all_splits)\n```\n\nNext we build the application:\n\n```\nfrom langchain import hubfrom langchain_core.documents import Documentfrom langgraph.graph import START, StateGraphfrom typing_extensions import List, TypedDict# Define prompt for question-answeringprompt = hub.pull(\"rlm/rag-prompt\")# Define state for applicationclass State(TypedDict):    question: str    context: List[Document]    answer: str# Define application stepsdef retrieve(state: State):    retrieved_docs = vector_store.similarity_search(state[\"question\"])    return {\"context\": retrieved_docs}def generate(state: State):    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})    response = llm.invoke(messages)    return {\"answer\": response.content}# Compile application and testgraph_builder = StateGraph(State).add_sequence([retrieve, generate])graph_builder.add_edge(START, \"retrieve\")graph = graph_builder.compile()\n```\n\n```\nfrom IPython.display import Image, displaydisplay(Image(graph.get_graph().draw_mermaid_png()))\n```", "url_final": "https://python.langchain.com/docs/how_to/qa_streaming/", "url_citable": "https://python.langchain.com/docs/how_to/qa_streaming/#rag-application", "title": "How to stream results from your RAG application | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "RAG application‚Äã", "section_anchor": "rag-application", "fetched_at": "2025-10-06T21:56:13.010176", "has_code": true}
{"id": "30d47daadf5d7846291f351b0182a8d08e024696", "text": "Streaming final outputs‚Äã\n\nLangGraph supports several streaming modes, which can be controlled by specifying the stream_mode parameter. Setting stream_mode=\"messages\" allows us to stream tokens from chat model invocations.\n\nIn general there can be multiple chat model invocations in an application (although here there is just one). Below, we filter to only the last step using the name of the corresponding node:\n\n```\ninput_message = \"What is Task Decomposition?\"for message, metadata in graph.stream(    {\"question\": \"What is Task Decomposition?\"},    stream_mode=\"messages\",):    if metadata[\"langgraph_node\"] == \"generate\":        print(message.content, end=\"|\")\n```\n\n```\n|Task| De|composition| is| a| technique| used| to| break| down| complex| tasks| into| smaller|,| more| manageable| steps|.| It| often| involves| prompting| models| to| \"|think| step| by| step|,\"| allowing| for| clearer| reasoning| and| better| performance| on| intricate| problems|.| This| can| be| achieved| through| various| methods|,| including| simple| prompts|,| task|-specific| instructions|,| or| human| input|.||\n```", "url_final": "https://python.langchain.com/docs/how_to/qa_streaming/", "url_citable": "https://python.langchain.com/docs/how_to/qa_streaming/#streaming-final-outputs", "title": "How to stream results from your RAG application | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Streaming final outputs‚Äã", "section_anchor": "streaming-final-outputs", "fetched_at": "2025-10-06T21:56:13.010176", "has_code": true}
{"id": "1f6323148914a4b273f95c51c41355fbaad7d1ac", "text": "Streaming intermediate steps‚Äã\n\nOther streaming modes will generally stream steps from our invocation-- i.e., state updates from individual nodes. In this case, each node is just appending a new key to the state:\n\n```\nfor step in graph.stream(    {\"question\": \"What is Task Decomposition?\"},    stream_mode=\"updates\",):    print(f\"{step}\\n\\n----------------\\n\")\n```\n\n```\n{'retrieve': {'context': [Document(id='5bf5e308-6ccb-4f09-94d2-d0c36b8c9980', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.'), Document(id='d8aed221-7943-414d-8ed7-63c2b0e7523b', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'), Document(id='bfa87007-02ef-4f81-a008-4522ecea1025', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Resources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.'), Document(id='6aff7fc0-5c21-4986-9f1e-91e89715d934', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content=\"(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\")]}}----------------{'generate': {'answer': 'Task Decomposition is the process of breaking down a complex task into smaller, manageable steps to enhance understanding and execution. Techniques like Chain of Thought (CoT) and Tree of Thoughts (ToT) guide models to think through steps systematically, allowing for better problem-solving. It can be achieved through simple prompting, task-specific instructions, or human input.'}}----------------\n```\n\nFor more on streaming with LangGraph, check out its streaming documentation. For more information on streaming individual LangChain Runnables, refer to this guide.\n\n- Dependencies\n- LangSmith\n- Components", "url_final": "https://python.langchain.com/docs/how_to/qa_streaming/", "url_citable": "https://python.langchain.com/docs/how_to/qa_streaming/#streaming-intermediate-steps", "title": "How to stream results from your RAG application | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Streaming intermediate steps‚Äã", "section_anchor": "streaming-intermediate-steps", "fetched_at": "2025-10-06T21:56:13.010176", "has_code": true}
{"id": "74f46942f657db094baeddbc38da5b922c052def", "text": "How to retry when a parsing error occurs | ü¶úÔ∏èüîó LangChain\n\nWhile in some cases it is possible to fix any parsing mistakes by only looking at the output, in other cases it isn't. An example of this is when the output is not just in the incorrect format, but is partially complete. Consider the below example.\n\n```\nfrom langchain.output_parsers import OutputFixingParserfrom langchain_core.exceptions import OutputParserExceptionfrom langchain_core.output_parsers import PydanticOutputParserfrom langchain_core.prompts import PromptTemplatefrom langchain_openai import ChatOpenAI, OpenAIfrom pydantic import BaseModel, Field\n```\n\n```\ntemplate = \"\"\"Based on the user question, provide an Action and Action Input for what step should be taken.{format_instructions}Question: {query}Response:\"\"\"class Action(BaseModel):    action: str = Field(description=\"action to take\")    action_input: str = Field(description=\"input to the action\")parser = PydanticOutputParser(pydantic_object=Action)\n```\n\n```\nprompt = PromptTemplate(    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",    input_variables=[\"query\"],    partial_variables={\"format_instructions\": parser.get_format_instructions()},)\n```\n\n```\nprompt_value = prompt.format_prompt(query=\"who is leo di caprios gf?\")\n```\n\n```\nbad_response = '{\"action\": \"search\"}'\n```\n\nIf we try to parse this response as is, we will get an error:\n\n```\ntry:    parser.parse(bad_response)except OutputParserException as e:    print(e)\n```\n\n```\nFailed to parse Action from completion {\"action\": \"search\"}. Got: 1 validation error for Actionaction_input  Field required [type=missing, input_value={'action': 'search'}, input_type=dict]    For further information visit https://errors.pydantic.dev/2.9/v/missingFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE\n```\n\nIf we try to use the OutputFixingParser to fix this error, it will be confused - namely, it doesn't know what to actually put for action input.\n\n```\nfix_parser = OutputFixingParser.from_llm(parser=parser, llm=ChatOpenAI())\n```\n\n```\nfix_parser.parse(bad_response)\n```\n\n```\nAction(action='search', action_input='input')\n```\n\nInstead, we can use the RetryOutputParser, which passes in the prompt (as well as the original output) to try again to get a better response.\n\n```\nfrom langchain.output_parsers import RetryOutputParser\n```\n\n```\nretry_parser = RetryOutputParser.from_llm(parser=parser, llm=OpenAI(temperature=0))\n```\n\n```\nretry_parser.parse_with_prompt(bad_response, prompt_value)\n```\n\n```\nAction(action='search', action_input='leo di caprio girlfriend')\n```\n\nWe can also add the RetryOutputParser easily with a custom chain which transform the raw LLM/ChatModel output into a more workable format.\n\n```\nfrom langchain_core.runnables import RunnableLambda, RunnableParallelcompletion_chain = prompt | OpenAI(temperature=0)main_chain = RunnableParallel(    completion=completion_chain, prompt_value=prompt) | RunnableLambda(lambda x: retry_parser.parse_with_prompt(**x))main_chain.invoke({\"query\": \"who is leo di caprios gf?\"})\n```\n\n```\nAction(action='search', action_input='leo di caprio girlfriend')\n```\n\nFind out api documentation for RetryOutputParser.", "url_final": "https://python.langchain.com/docs/how_to/output_parser_retry/", "url_citable": "https://python.langchain.com/docs/how_to/output_parser_retry/", "title": "How to retry when a parsing error occurs | ü¶úÔ∏èüîó LangChain", "section_level": 1, "section_heading": "How to retry when a parsing error occurs | ü¶úÔ∏èüîó LangChain", "section_anchor": null, "fetched_at": "2025-10-06T21:56:02.391773", "has_code": true}
{"id": "0f6146c52012bda9e44aa90e035be66d895cbc56", "text": "Basic usage‚Äã\n\n```\nfrom langchain_core.messages import (    AIMessage,    HumanMessage,    SystemMessage,    filter_messages,)messages = [    SystemMessage(\"you are a good assistant\", id=\"1\"),    HumanMessage(\"example input\", id=\"2\", name=\"example_user\"),    AIMessage(\"example output\", id=\"3\", name=\"example_assistant\"),    HumanMessage(\"real input\", id=\"4\", name=\"bob\"),    AIMessage(\"real output\", id=\"5\", name=\"alice\"),]filter_messages(messages, include_types=\"human\")\n```\n\n```\n[HumanMessage(content='example input', additional_kwargs={}, response_metadata={}, name='example_user', id='2'), HumanMessage(content='real input', additional_kwargs={}, response_metadata={}, name='bob', id='4')]\n```\n\n```\nfilter_messages(messages, exclude_names=[\"example_user\", \"example_assistant\"])\n```\n\n```\n[SystemMessage(content='you are a good assistant', additional_kwargs={}, response_metadata={}, id='1'), HumanMessage(content='real input', additional_kwargs={}, response_metadata={}, name='bob', id='4'), AIMessage(content='real output', additional_kwargs={}, response_metadata={}, name='alice', id='5')]\n```\n\n```\nfilter_messages(messages, include_types=[HumanMessage, AIMessage], exclude_ids=[\"3\"])\n```\n\n```\n[HumanMessage(content='example input', additional_kwargs={}, response_metadata={}, name='example_user', id='2'), HumanMessage(content='real input', additional_kwargs={}, response_metadata={}, name='bob', id='4'), AIMessage(content='real output', additional_kwargs={}, response_metadata={}, name='alice', id='5')]\n```", "url_final": "https://python.langchain.com/docs/how_to/filter_messages/", "url_citable": "https://python.langchain.com/docs/how_to/filter_messages/#basic-usage", "title": "How to filter messages | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Basic usage‚Äã", "section_anchor": "basic-usage", "fetched_at": "2025-10-06T21:55:41.775232", "has_code": true}
{"id": "98fd0be4fb325fd57323c66ea0cae06ed227dbe5", "text": "Chaining‚Äã\n\nfilter_messages can be used imperatively (like above) or declaratively, making it easy to compose with other components in a chain:\n\n```\n%pip install -qU langchain-anthropic\n```\n\n```\nfrom langchain_anthropic import ChatAnthropicllm = ChatAnthropic(model=\"claude-3-7-sonnet-20250219\", temperature=0)# Notice we don't pass in messages. This creates# a RunnableLambda that takes messages as inputfilter_ = filter_messages(exclude_names=[\"example_user\", \"example_assistant\"])chain = filter_ | llmchain.invoke(messages)\n```\n\n```\nAIMessage(content=[], additional_kwargs={}, response_metadata={'id': 'msg_01At8GtCiJ79M29yvNwCiQaB', 'model': 'claude-3-7-sonnet-20250219', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 16, 'output_tokens': 3, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-7-sonnet-20250219'}, id='run--b3db2b91-0edf-4c48-99e7-35e641b8229d-0', usage_metadata={'input_tokens': 16, 'output_tokens': 3, 'total_tokens': 19, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})\n```\n\nLooking at the LangSmith trace we can see that before the messages are passed to the model they are filtered: https://smith.langchain.com/public/f808a724-e072-438e-9991-657cc9e7e253/r\n\nLooking at just the filter_, we can see that it's a Runnable object that can be invoked like all Runnables:\n\n```\nfilter_.invoke(messages)\n```\n\n```\n[SystemMessage(content='you are a good assistant', additional_kwargs={}, response_metadata={}, id='1'), HumanMessage(content='real input', additional_kwargs={}, response_metadata={}, name='bob', id='4'), AIMessage(content='real output', additional_kwargs={}, response_metadata={}, name='alice', id='5')]\n```", "url_final": "https://python.langchain.com/docs/how_to/filter_messages/", "url_citable": "https://python.langchain.com/docs/how_to/filter_messages/#chaining", "title": "How to filter messages | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Chaining‚Äã", "section_anchor": "chaining", "fetched_at": "2025-10-06T21:55:41.775232", "has_code": true}
{"id": "6f0a1d0f1d3ee57c471fc29aa43e2097398ae161", "text": "API reference‚Äã\n\nFor a complete description of all arguments head to the API reference: https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.filter_messages.html\n\n- Basic usage\n- Chaining\n- API reference", "url_final": "https://python.langchain.com/docs/how_to/filter_messages/", "url_citable": "https://python.langchain.com/docs/how_to/filter_messages/#api-reference", "title": "How to filter messages | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "API reference‚Äã", "section_anchor": "api-reference", "fetched_at": "2025-10-06T21:55:41.775232", "has_code": false}
{"id": "8e3cc16e87c4ca0a23e715e52ea78801475c0be1", "text": "Setup‚Äã\n\n```\n%pip install -qU langchain langchain-community langchain-openai faker langchain-chroma\n```\n\n```\nNote: you may need to restart the kernel to use updated packages.\n```\n\nWe'll use OpenAI in this example:\n\n```\nimport getpassimport osif \"OPENAI_API_KEY\" not in os.environ:    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()# Optional, uncomment to trace runs with LangSmith. Sign up here: https://smith.langchain.com.# os.environ[\"LANGSMITH_TRACING\"] = \"true\"# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n```\n\nWe will generate a bunch of fake names\n\n```\nfrom faker import Fakerfake = Faker()names = [fake.name() for _ in range(10000)]\n```\n\nLet's look at some of the names\n\n```\nnames[0]\n```\n\n```\n'Jacob Adams'\n```\n\n```\nnames[567]\n```\n\n```\n'Eric Acevedo'\n```", "url_final": "https://python.langchain.com/docs/how_to/query_high_cardinality/", "url_citable": "https://python.langchain.com/docs/how_to/query_high_cardinality/#setup", "title": "How to deal with high-cardinality categoricals when doing query analysis | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Setup‚Äã", "section_anchor": "setup", "fetched_at": "2025-10-06T21:54:52.717562", "has_code": true}
{"id": "9df45575a0680b5f1f5c8a14da2d0ed10d74fb4e", "text": "Query Analysis‚Äã\n\nWe can now set up a baseline query analysis\n\n```\nfrom pydantic import BaseModel, Field, model_validator\n```\n\n```\nclass Search(BaseModel):    query: str    author: str\n```\n\n```\nfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnablePassthroughfrom langchain_openai import ChatOpenAIsystem = \"\"\"Generate a relevant search query for a library system\"\"\"prompt = ChatPromptTemplate.from_messages(    [        (\"system\", system),        (\"human\", \"{question}\"),    ])llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)structured_llm = llm.with_structured_output(Search)query_analyzer = {\"question\": RunnablePassthrough()} | prompt | structured_llm\n```\n\nWe can see that if we spell the name exactly correctly, it knows how to handle it\n\n```\nquery_analyzer.invoke(\"what are books about aliens by Jesse Knight\")\n```\n\n```\nSearch(query='aliens', author='Jesse Knight')\n```\n\nThe issue is that the values you want to filter on may NOT be spelled exactly correctly\n\n```\nquery_analyzer.invoke(\"what are books about aliens by jess knight\")\n```\n\n```\nSearch(query='aliens', author='Jess Knight')\n```\n\nAdd in all values‚Äã\n\nOne way around this is to add ALL possible values to the prompt. That will generally guide the query in the right direction\n\n```\nsystem = \"\"\"Generate a relevant search query for a library system.`author` attribute MUST be one of:{authors}Do NOT hallucinate author name!\"\"\"base_prompt = ChatPromptTemplate.from_messages(    [        (\"system\", system),        (\"human\", \"{question}\"),    ])prompt = base_prompt.partial(authors=\", \".join(names))\n```\n\n```\nquery_analyzer_all = {\"question\": RunnablePassthrough()} | prompt | structured_llm\n```\n\nHowever... if the list of categoricals is long enough, it may error!\n\n```\ntry:    res = query_analyzer_all.invoke(\"what are books about aliens by jess knight\")except Exception as e:    print(e)\n```\n\nWe can try to use a longer context window... but with so much information in there, it is not garunteed to pick it up reliably\n\n```\nllm_long = ChatOpenAI(model=\"gpt-4-turbo-preview\", temperature=0)structured_llm_long = llm_long.with_structured_output(Search)query_analyzer_all = {\"question\": RunnablePassthrough()} | prompt | structured_llm_long\n```\n\n```\nquery_analyzer_all.invoke(\"what are books about aliens by jess knight\")\n```\n\n```\nSearch(query='aliens', author='jess knight')\n```\n\nFind and all relevant values‚Äã\n\nInstead, what we can do is create an index over the relevant values and then query that for the N most relevant values,\n\n```\nfrom langchain_chroma import Chromafrom langchain_openai import OpenAIEmbeddingsembeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")vectorstore = Chroma.from_texts(names, embeddings, collection_name=\"author_names\")\n```\n\n```\ndef select_names(question):    _docs = vectorstore.similarity_search(question, k=10)    _names = [d.page_content for d in _docs]    return \", \".join(_names)\n```\n\n```\ncreate_prompt = {    \"question\": RunnablePassthrough(),    \"authors\": select_names,} | base_prompt\n```\n\n```\nquery_analyzer_select = create_prompt | structured_llm\n```\n\n```\ncreate_prompt.invoke(\"what are books by jess knight\")\n```\n\n```\nChatPromptValue(messages=[SystemMessage(content='Generate a relevant search query for a library system.\\n\\n`author` attribute MUST be one of:\\n\\nJennifer Knight, Jill Knight, John Knight, Dr. Jeffrey Knight, Christopher Knight, Andrea Knight, Brandy Knight, Jennifer Keller, Becky Chambers, Sarah Knapp\\n\\nDo NOT hallucinate author name!'), HumanMessage(content='what are books by jess knight')])\n```\n\n```\nquery_analyzer_select.invoke(\"what are books about aliens by jess knight\")\n```\n\n```\nSearch(query='books about aliens', author='Jennifer Knight')\n```", "url_final": "https://python.langchain.com/docs/how_to/query_high_cardinality/", "url_citable": "https://python.langchain.com/docs/how_to/query_high_cardinality/#query-analysis", "title": "How to deal with high-cardinality categoricals when doing query analysis | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Query Analysis‚Äã", "section_anchor": "query-analysis", "fetched_at": "2025-10-06T21:54:52.717562", "has_code": true}
{"id": "c4e340f05a1ce4e5907c61d1ce67b529191ded9c", "text": "Replace after selection‚Äã\n\nAnother method is to let the LLM fill in whatever value, but then convert that value to a valid value. This can actually be done with the Pydantic class itself!\n\n```\nclass Search(BaseModel):    query: str    author: str    @model_validator(mode=\"before\")    @classmethod    def double(cls, values: dict) -> dict:        author = values[\"author\"]        closest_valid_author = vectorstore.similarity_search(author, k=1)[            0        ].page_content        values[\"author\"] = closest_valid_author        return values\n```\n\n```\nsystem = \"\"\"Generate a relevant search query for a library system\"\"\"prompt = ChatPromptTemplate.from_messages(    [        (\"system\", system),        (\"human\", \"{question}\"),    ])corrective_structure_llm = llm.with_structured_output(Search)corrective_query_analyzer = (    {\"question\": RunnablePassthrough()} | prompt | corrective_structure_llm)\n```\n\n```\ncorrective_query_analyzer.invoke(\"what are books about aliens by jes knight\")\n```\n\n```\nSearch(query='aliens', author='John Knight')\n```\n\n```\n# TODO: show trigram similarity\n```\n\n- Add in all values\n- Find and all relevant values\n- Replace after selection", "url_final": "https://python.langchain.com/docs/how_to/query_high_cardinality/", "url_citable": "https://python.langchain.com/docs/how_to/query_high_cardinality/#replace-after-selection", "title": "How to deal with high-cardinality categoricals when doing query analysis | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Replace after selection‚Äã", "section_anchor": "replace-after-selection", "fetched_at": "2025-10-06T21:54:52.717562", "has_code": true}
{"id": "854ac88c002642e7bf723ee06650c212a9edb3ae", "text": "Use case‚Äã\n\nThe popularity of projects like llama.cpp, Ollama, GPT4All, llamafile, and others underscore the demand to run LLMs locally (on your own device).\n\nThis has at least two important benefits:\n\n- Privacy: Your data is not sent to a third party, and it is not subject to the terms of service of a commercial service\n- Cost: There is no inference fee, which is important for token-intensive applications (e.g., long-running simulations, summarization)", "url_final": "https://python.langchain.com/docs/how_to/local_llms/", "url_citable": "https://python.langchain.com/docs/how_to/local_llms/#use-case", "title": "Run models locally | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Use case‚Äã", "section_anchor": "use-case", "fetched_at": "2025-10-06T21:55:48.990460", "has_code": false}
{"id": "7290a5246eb5ff0cbe619a9442c0f0d03024b240", "text": "Overview‚Äã\n\nRunning an LLM locally requires a few things:\n\n- Open-source LLM: An open-source LLM that can be freely modified and shared\n- Inference: Ability to run this LLM on your device w/ acceptable latency\n\nOpen-source LLMs‚Äã\n\nUsers can now gain access to a rapidly growing set of open-source LLMs.\n\nThese LLMs can be assessed across at least two dimensions (see figure):\n\n- Base model: What is the base-model and how was it trained?\n- Fine-tuning approach: Was the base-model fine-tuned and, if so, what set of instructions was used?\n\nThe relative performance of these models can be assessed using several leaderboards, including:\n\n- LmSys\n- GPT4All\n- HuggingFace\n\nInference‚Äã\n\nA few frameworks for this have emerged to support inference of open-source LLMs on various devices:\n\n- llama.cpp: C++ implementation of llama inference code with weight optimization / quantization\n- gpt4all: Optimized C backend for inference\n- ollama: Bundles model weights and environment into an app that runs on device and serves the LLM\n- llamafile: Bundles model weights and everything needed to run the model in a single file, allowing you to run the LLM locally from this file without any additional installation steps\n\nIn general, these frameworks will do a few things:\n\n- Quantization: Reduce the memory footprint of the raw model weights\n- Efficient implementation for inference: Support inference on consumer hardware (e.g., CPU or laptop GPU)\n\nIn particular, see this excellent post on the importance of quantization.\n\nWith less precision, we radically decrease the memory needed to store the LLM in memory.\n\nIn addition, we can see the importance of GPU memory bandwidth sheet!\n\nA Mac M2 Max is 5-6x faster than a M1 for inference due to the larger GPU memory bandwidth.\n\nFormatting prompts‚Äã\n\nSome providers have chat model wrappers that takes care of formatting your input prompt for the specific local model you're using. However, if you are prompting local models with a text-in/text-out LLM wrapper, you may need to use a prompt tailored for your specific model.\n\nThis can require the inclusion of special tokens. Here's an example for LLaMA 2.", "url_final": "https://python.langchain.com/docs/how_to/local_llms/", "url_citable": "https://python.langchain.com/docs/how_to/local_llms/#overview", "title": "Run models locally | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Overview‚Äã", "section_anchor": "overview", "fetched_at": "2025-10-06T21:55:48.990460", "has_code": false}
{"id": "27b7cef8672e70d7c35ff1f983a1ebf4718ca1e2", "text": "Quickstart‚Äã\n\nOllama is one way to easily run inference on macOS.\n\nThe instructions here provide details, which we summarize:\n\n- Download and run the app\n- From command line, fetch a model from this list of options: e.g., ollama pull gpt-oss:20b\n- When the app is running, all models are automatically served on localhost:11434\n\n```\n%pip install -qU langchain_ollama\n```\n\n```\nfrom langchain_ollama import ChatOllamallm = ChatOllama(model=\"gpt-oss:20b\", validate_model_on_init=True)llm.invoke(\"The first man on the moon was ...\").content\n```\n\n```\n'...Neil Armstrong!\\n\\nOn July 20, 1969, Neil Armstrong became the first person to set foot on the lunar surface, famously declaring \"That\\'s one small step for man, one giant leap for mankind\" as he stepped off the lunar module Eagle onto the Moon\\'s surface.\\n\\nWould you like to know more about the Apollo 11 mission or Neil Armstrong\\'s achievements?'\n```\n\nStream tokens as they are being generated:\n\n```\nfor chunk in llm.stream(\"The first man on the moon was ...\"):    print(chunk, end=\"|\", flush=True)\n```\n\n```\n...|``````outputNeil| Armstrong|,| an| American| astronaut|.| He| stepped| out| of| the| lunar| module| Eagle| and| onto| the| surface| of| the| Moon| on| July| |20|,| |196|9|,| famously| declaring|:| \"|That|'s| one| small| step| for| man|,| one| giant| leap| for| mankind|.\"||\n```\n\nOllama also includes a chat model wrapper that handles formatting conversation turns:\n\n```\nfrom langchain_ollama import ChatOllamachat_model = ChatOllama(model=\"llama3.1:8b\")chat_model.invoke(\"Who was the first man on the moon?\")\n```\n\n```\nAIMessage(content='The answer is a historic one!\\n\\nThe first man to walk on the Moon was Neil Armstrong, an American astronaut and commander of the Apollo 11 mission. On July 20, 1969, Armstrong stepped out of the lunar module Eagle onto the surface of the Moon, famously declaring:\\n\\n\"That\\'s one small step for man, one giant leap for mankind.\"\\n\\nArmstrong was followed by fellow astronaut Edwin \"Buzz\" Aldrin, who also walked on the Moon during the mission. Michael Collins remained in orbit around the Moon in the command module Columbia.\\n\\nNeil Armstrong passed away on August 25, 2012, but his legacy as a pioneering astronaut and engineer continues to inspire people around the world!', response_metadata={'model': 'llama3.1:8b', 'created_at': '2024-08-01T00:38:29.176717Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 10681861417, 'load_duration': 34270292, 'prompt_eval_count': 19, 'prompt_eval_duration': 6209448000, 'eval_count': 141, 'eval_duration': 4432022000}, id='run-7bed57c5-7f54-4092-912c-ae49073dcd48-0', usage_metadata={'input_tokens': 19, 'output_tokens': 141, 'total_tokens': 160})\n```", "url_final": "https://python.langchain.com/docs/how_to/local_llms/", "url_citable": "https://python.langchain.com/docs/how_to/local_llms/#quickstart", "title": "Run models locally | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Quickstart‚Äã", "section_anchor": "quickstart", "fetched_at": "2025-10-06T21:55:48.990460", "has_code": true}
{"id": "132e137cffdf93407bb03004c918ad64946bdd94", "text": "Environment‚Äã\n\nInference speed is a challenge when running models locally (see above).\n\nTo minimize latency, it is desirable to run models locally on GPU, which ships with many consumer laptops e.g., Apple devices.\n\nAnd even with GPU, the available GPU memory bandwidth (as noted above) is important.\n\nRunning Apple silicon GPU‚Äã\n\nollama and llamafile will automatically utilize the GPU on Apple devices.\n\nOther frameworks require the user to set up the environment to utilize the Apple GPU.\n\nFor example, llama.cpp python bindings can be configured to use the GPU via Metal.\n\nMetal is a graphics and compute API created by Apple providing near-direct access to the GPU.\n\nSee the llama.cpp setup here to enable this.\n\nIn particular, ensure that conda is using the correct virtual environment that you created (miniforge3).\n\ne.g., for me:\n\n```\nconda activate /Users/rlm/miniforge3/envs/llama\n```\n\nWith the above confirmed, then:\n\n```\nCMAKE_ARGS=\"-DLLAMA_METAL=on\" FORCE_CMAKE=1 pip install -U llama-cpp-python --no-cache-dir\n```", "url_final": "https://python.langchain.com/docs/how_to/local_llms/", "url_citable": "https://python.langchain.com/docs/how_to/local_llms/#environment", "title": "Run models locally | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Environment‚Äã", "section_anchor": "environment", "fetched_at": "2025-10-06T21:55:48.990460", "has_code": true}
{"id": "d047539b718544135cde9d80e5fc202df2d5a96e", "text": "LLMs‚Äã\n\nThere are various ways to gain access to quantized model weights.\n\n- HuggingFace - Many quantized model are available for download and can be run with framework such as llama.cpp. You can also download models in llamafile format from HuggingFace.\n- gpt4all - The model explorer offers a leaderboard of metrics and associated quantized models available for download\n- ollama - Several models can be accessed directly via pull\n\nOllama\u0000‚Äã\n\nWith Ollama, fetch a model via ollama pull <model family>:<tag>.\n\n```\nllm = ChatOllama(model=\"gpt-oss:20b\")llm.invoke(\"The first man on the moon was ... think step by step\")\n```\n\n```\n' Sure! Here\\'s the answer, broken down step by step:\\n\\nThe first man on the moon was... Neil Armstrong.\\n\\nHere\\'s how I arrived at that answer:\\n\\n1. The first manned mission to land on the moon was Apollo 11.\\n2. The mission included three astronauts: Neil Armstrong, Edwin \"Buzz\" Aldrin, and Michael Collins.\\n3. Neil Armstrong was the mission commander and the first person to set foot on the moon.\\n4. On July 20, 1969, Armstrong stepped out of the lunar module Eagle and onto the moon\\'s surface, famously declaring \"That\\'s one small step for man, one giant leap for mankind.\"\\n\\nSo, the first man on the moon was Neil Armstrong!'\n```", "url_final": "https://python.langchain.com/docs/how_to/local_llms/", "url_citable": "https://python.langchain.com/docs/how_to/local_llms/#llms", "title": "Run models locally | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "LLMs‚Äã", "section_anchor": "llms", "fetched_at": "2025-10-06T21:55:48.990460", "has_code": true}
{"id": "73667031488747c90081064c9e5aedd7d1b609e7", "text": "Llama.cpp‚Äã\n\nLlama.cpp is compatible with a broad set of models.\n\nFor example, below we run inference on llama2-13b with 4 bit quantization downloaded from HuggingFace.\n\nAs noted above, see the API reference for the full set of parameters.\n\nFrom the llama.cpp API reference docs, a few are worth commenting on:\n\nn_gpu_layers: number of layers to be loaded into GPU memory\n\n- Value: 1\n- Meaning: Only one layer of the model will be loaded into GPU memory (1 is often sufficient).\n\nn_batch: number of tokens the model should process in parallel\n\n- Value: n_batch\n- Meaning: It's recommended to choose a value between 1 and n_ctx (which in this case is set to 2048)\n\nn_ctx: Token context window\n\n- Value: 2048\n- Meaning: The model will consider a window of 2048 tokens at a time\n\nf16_kv: whether the model should use half-precision for the key/value cache\n\n- Value: True\n- Meaning: The model will use half-precision, which can be more memory efficient; Metal only supports True.\n\n```\n%env CMAKE_ARGS=\"-DLLAMA_METAL=on\"%env FORCE_CMAKE=1%pip install --upgrade --quiet  llama-cpp-python --no-cache-dir\n```\n\n```\nfrom langchain_community.llms import LlamaCppfrom langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandlerllm = LlamaCpp(    model_path=\"/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin\",    n_gpu_layers=1,    n_batch=512,    n_ctx=2048,    f16_kv=True,    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),    verbose=True,)\n```\n\nThe console log will show the below to indicate Metal was enabled properly from steps above:\n\n```\nggml_metal_init: allocatingggml_metal_init: using MPS\n```\n\n```\nllm.invoke(\"The first man on the moon was ... Let's think step by step\")\n```\n\n```\nLlama.generate: prefix-match hit``````output and use logical reasoning to figure out who the first man on the moon was.Here are some clues:1. The first man on the moon was an American.2. He was part of the Apollo 11 mission.3. He stepped out of the lunar module and became the first person to set foot on the moon's surface.4. His last name is Armstrong.Now, let's use our reasoning skills to figure out who the first man on the moon was. Based on clue #1, we know that the first man on the moon was an American. Clue #2 tells us that he was part of the Apollo 11 mission. Clue #3 reveals that he was the first person to set foot on the moon's surface. And finally, clue #4 gives us his last name: Armstrong.Therefore, the first man on the moon was Neil Armstrong!``````outputllama_print_timings:        load time =  9623.21 msllama_print_timings:      sample time =   143.77 ms /   203 runs   (    0.71 ms per token,  1412.01 tokens per second)llama_print_timings: prompt eval time =   485.94 ms /     7 tokens (   69.42 ms per token,    14.40 tokens per second)llama_print_timings:        eval time =  6385.16 ms /   202 runs   (   31.61 ms per token,    31.64 tokens per second)llama_print_timings:       total time =  7279.28 ms\n```\n\n```\n\" and use logical reasoning to figure out who the first man on the moon was.\\n\\nHere are some clues:\\n\\n1. The first man on the moon was an American.\\n2. He was part of the Apollo 11 mission.\\n3. He stepped out of the lunar module and became the first person to set foot on the moon's surface.\\n4. His last name is Armstrong.\\n\\nNow, let's use our reasoning skills to figure out who the first man on the moon was. Based on clue #1, we know that the first man on the moon was an American. Clue #2 tells us that he was part of the Apollo 11 mission. Clue #3 reveals that he was the first person to set foot on the moon's surface. And finally, clue #4 gives us his last name: Armstrong.\\nTherefore, the first man on the moon was Neil Armstrong!\"\n```", "url_final": "https://python.langchain.com/docs/how_to/local_llms/", "url_citable": "https://python.langchain.com/docs/how_to/local_llms/#llamacpp", "title": "Run models locally | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "Llama.cpp‚Äã", "section_anchor": "llamacpp", "fetched_at": "2025-10-06T21:55:48.990460", "has_code": true}
{"id": "87c1313e9a099f6b20af2b358f2ecf0ed43419ce", "text": "GPT4All‚Äã\n\nWe can use model weights downloaded from GPT4All model explorer.\n\nSimilar to what is shown above, we can run inference and use the API reference to set parameters of interest.\n\n```\n%pip install gpt4all\n```\n\n```\nfrom langchain_community.llms import GPT4Allllm = GPT4All(    model=\"/Users/rlm/Desktop/Code/gpt4all/models/nous-hermes-13b.ggmlv3.q4_0.bin\")\n```\n\n```\nllm.invoke(\"The first man on the moon was ... Let's think step by step\")\n```\n\n```\n\".\\n1) The United States decides to send a manned mission to the moon.2) They choose their best astronauts and train them for this specific mission.3) They build a spacecraft that can take humans to the moon, called the Lunar Module (LM).4) They also create a larger spacecraft, called the Saturn V rocket, which will launch both the LM and the Command Service Module (CSM), which will carry the astronauts into orbit.5) The mission is planned down to the smallest detail: from the trajectory of the rockets to the exact movements of the astronauts during their moon landing.6) On July 16, 1969, the Saturn V rocket launches from Kennedy Space Center in Florida, carrying the Apollo 11 mission crew into space.7) After one and a half orbits around the Earth, the LM separates from the CSM and begins its descent to the moon's surface.8) On July 20, 1969, at 2:56 pm EDT (GMT-4), Neil Armstrong becomes the first man on the moon. He speaks these\"\n```", "url_final": "https://python.langchain.com/docs/how_to/local_llms/", "url_citable": "https://python.langchain.com/docs/how_to/local_llms/#gpt4all", "title": "Run models locally | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "GPT4All‚Äã", "section_anchor": "gpt4all", "fetched_at": "2025-10-06T21:55:48.990460", "has_code": true}
{"id": "611ad378a83650c3768b2011dbfa5b29f7a544f4", "text": "llamafile‚Äã\n\nOne of the simplest ways to run an LLM locally is using a llamafile. All you need to do is:\n\n- Download a llamafile from HuggingFace\n- Make the file executable\n- Run the file\n\nllamafiles bundle model weights and a specially-compiled version of llama.cpp into a single file that can run on most computers without any additional dependencies. They also come with an embedded inference server that provides an API for interacting with your model.\n\nHere's a simple bash script that shows all 3 setup steps:\n\n```\n# Download a llamafile from HuggingFacewget https://huggingface.co/jartine/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile# Make the file executable. On Windows, instead just rename the file to end in \".exe\".chmod +x TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile# Start the model server. Listens at http://localhost:8080 by default../TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --server --nobrowser\n```\n\nAfter you run the above setup steps, you can use LangChain to interact with your model:\n\n```\nfrom langchain_community.llms.llamafile import Llamafilellm = Llamafile()llm.invoke(\"The first man on the moon was ... Let's think step by step.\")\n```\n\n```\n\"\\nFirstly, let's imagine the scene where Neil Armstrong stepped onto the moon. This happened in 1969. The first man on the moon was Neil Armstrong. We already know that.\\n2nd, let's take a step back. Neil Armstrong didn't have any special powers. He had to land his spacecraft safely on the moon without injuring anyone or causing any damage. If he failed to do this, he would have been killed along with all those people who were on board the spacecraft.\\n3rd, let's imagine that Neil Armstrong successfully landed his spacecraft on the moon and made it back to Earth safely. The next step was for him to be hailed as a hero by his people back home. It took years before Neil Armstrong became an American hero.\\n4th, let's take another step back. Let's imagine that Neil Armstrong wasn't hailed as a hero, and instead, he was just forgotten. This happened in the 1970s. Neil Armstrong wasn't recognized for his remarkable achievement on the moon until after he died.\\n5th, let's take another step back. Let's imagine that Neil Armstrong didn't die in the 1970s and instead, lived to be a hundred years old. This happened in 2036. In the year 2036, Neil Armstrong would have been a centenarian.\\nNow, let's think about the present. Neil Armstrong is still alive. He turned 95 years old on July 20th, 2018. If he were to die now, his achievement of becoming the first human being to set foot on the moon would remain an unforgettable moment in history.\\nI hope this helps you understand the significance and importance of Neil Armstrong's achievement on the moon!\"\n```", "url_final": "https://python.langchain.com/docs/how_to/local_llms/", "url_citable": "https://python.langchain.com/docs/how_to/local_llms/#llamafile", "title": "Run models locally | ü¶úÔ∏èüîó LangChain", "section_level": 3, "section_heading": "llamafile‚Äã", "section_anchor": "llamafile", "fetched_at": "2025-10-06T21:55:48.990460", "has_code": true}
{"id": "2813a755a2156fbf8375ce87a1139c9af51d6a37", "text": "Prompts‚Äã\n\nSome LLMs will benefit from specific prompts.\n\nFor example, LLaMA will use special tokens.\n\nWe can use ConditionalPromptSelector to set prompt based on the model type.\n\n```\n# Set our LLMllm = LlamaCpp(    model_path=\"/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin\",    n_gpu_layers=1,    n_batch=512,    n_ctx=2048,    f16_kv=True,    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),    verbose=True,)\n```\n\nSet the associated prompt based upon the model version.\n\n```\nfrom langchain.chains.prompt_selector import ConditionalPromptSelectorfrom langchain_core.prompts import PromptTemplateDEFAULT_LLAMA_SEARCH_PROMPT = PromptTemplate(    input_variables=[\"question\"],    template=\"\"\"<<SYS>> \\n You are an assistant tasked with improving Google search \\results. \\n <</SYS>> \\n\\n [INST] Generate THREE Google search queries that \\are similar to this question. The output should be a numbered list of questions \\and each should have a question mark at the end: \\n\\n {question} [/INST]\"\"\",)DEFAULT_SEARCH_PROMPT = PromptTemplate(    input_variables=[\"question\"],    template=\"\"\"You are an assistant tasked with improving Google search \\results. Generate THREE Google search queries that are similar to \\this question. The output should be a numbered list of questions and each \\should have a question mark at the end: {question}\"\"\",)QUESTION_PROMPT_SELECTOR = ConditionalPromptSelector(    default_prompt=DEFAULT_SEARCH_PROMPT,    conditionals=[(lambda llm: isinstance(llm, LlamaCpp), DEFAULT_LLAMA_SEARCH_PROMPT)],)prompt = QUESTION_PROMPT_SELECTOR.get_prompt(llm)prompt\n```\n\n```\nPromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='<<SYS>> \\n You are an assistant tasked with improving Google search results. \\n <</SYS>> \\n\\n [INST] Generate THREE Google search queries that are similar to this question. The output should be a numbered list of questions and each should have a question mark at the end: \\n\\n {question} [/INST]', template_format='f-string', validate_template=True)\n```\n\n```\n# Chainchain = prompt | llmquestion = \"What NFL team won the Super Bowl in the year that Justin Bieber was born?\"chain.invoke({\"question\": question})\n```\n\n```\n  Sure! Here are three similar search queries with a question mark at the end:1. Which NBA team did LeBron James lead to a championship in the year he was drafted?2. Who won the Grammy Awards for Best New Artist and Best Female Pop Vocal Performance in the same year that Lady Gaga was born?3. What MLB team did Babe Ruth play for when he hit 60 home runs in a single season?``````outputllama_print_timings:        load time = 14943.19 msllama_print_timings:      sample time =    72.93 ms /   101 runs   (    0.72 ms per token,  1384.87 tokens per second)llama_print_timings: prompt eval time = 14942.95 ms /    93 tokens (  160.68 ms per token,     6.22 tokens per second)llama_print_timings:        eval time =  3430.85 ms /   100 runs   (   34.31 ms per token,    29.15 tokens per second)llama_print_timings:       total time = 18578.26 ms\n```\n\n```\n'  Sure! Here are three similar search queries with a question mark at the end:\\n\\n1. Which NBA team did LeBron James lead to a championship in the year he was drafted?\\n2. Who won the Grammy Awards for Best New Artist and Best Female Pop Vocal Performance in the same year that Lady Gaga was born?\\n3. What MLB team did Babe Ruth play for when he hit 60 home runs in a single season?'\n```\n\nWe also can use the LangChain Prompt Hub to fetch and / or store prompts that are model specific.\n\nThis will work with your LangSmith API key.\n\nFor example, here is a prompt for RAG with LLaMA-specific tokens.", "url_final": "https://python.langchain.com/docs/how_to/local_llms/", "url_citable": "https://python.langchain.com/docs/how_to/local_llms/#prompts", "title": "Run models locally | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Prompts‚Äã", "section_anchor": "prompts", "fetched_at": "2025-10-06T21:55:48.990460", "has_code": true}
{"id": "2445be2c31b8d2d13e1aa6fbb29a4dd9393ced63", "text": "Use cases‚Äã\n\nGiven an llm created from one of the models above, you can use it for many use cases.\n\nFor example, you can implement a RAG application using the chat models demonstrated here.\n\nIn general, use cases for local LLMs can be driven by at least two factors:\n\n- Privacy: private data (e.g., journals, etc) that a user does not want to share\n- Cost: text preprocessing (extraction/tagging), summarization, and agent simulations are token-use-intensive tasks\n\nIn addition, here is an overview on fine-tuning, which can utilize open-source LLMs.\n\n- Open-source LLMs\n- Inference\n- Formatting prompts\n\n- Running Apple silicon GPU\n\n- Ollama\n- Llama.cpp\n- GPT4All\n- llamafile", "url_final": "https://python.langchain.com/docs/how_to/local_llms/", "url_citable": "https://python.langchain.com/docs/how_to/local_llms/#use-cases", "title": "Run models locally | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Use cases‚Äã", "section_anchor": "use-cases", "fetched_at": "2025-10-06T21:55:48.990460", "has_code": false}
{"id": "36caf330073d615dbb6748b632dccfe556b9be53", "text": "Define the schema‚Äã\n\nLet's re-use the person schema from the extraction tutorial.\n\n```\nfrom typing import List, Optionalfrom langchain_openai import ChatOpenAIfrom pydantic import BaseModel, Fieldclass Person(BaseModel):    \"\"\"Information about a person.\"\"\"    # ^ Doc-string for the entity Person.    # This doc-string is sent to the LLM as the description of the schema Person,    # and it can help to improve extraction results.    # Note that:    # 1. Each field is an `optional` -- this allows the model to decline to extract it!    # 2. Each field has a `description` -- this description is used by the LLM.    # Having a good description can help improve extraction results.    name: Optional[str] = Field(..., description=\"The name of the person\")    hair_color: Optional[str] = Field(        ..., description=\"The color of the person's hair if known\"    )    height_in_meters: Optional[str] = Field(..., description=\"Height in METERs\")class Data(BaseModel):    \"\"\"Extracted data about people.\"\"\"    # Creates a model so that we can extract multiple entities.    people: List[Person]\n```", "url_final": "https://python.langchain.com/docs/how_to/extraction_examples/", "url_citable": "https://python.langchain.com/docs/how_to/extraction_examples/#define-the-schema", "title": "How to use reference examples when doing extraction | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Define the schema‚Äã", "section_anchor": "define-the-schema", "fetched_at": "2025-10-06T21:55:37.599200", "has_code": true}
{"id": "0cd5e3c7aa4aa27bb8b38b2d820198743068e248", "text": "Define reference examples‚Äã\n\nExamples can be defined as a list of input-output pairs.\n\nEach example contains an example input text and an example output showing what should be extracted from the text.\n\nThis is a bit in the weeds, so feel free to skip.\n\nThe format of the example needs to match the API used (e.g., tool calling or JSON mode etc.).\n\nHere, the formatted examples will match the format expected for the tool calling API since that's what we're using.\n\n```\nimport uuidfrom typing import Dict, List, TypedDictfrom langchain_core.messages import (    AIMessage,    BaseMessage,    HumanMessage,    SystemMessage,    ToolMessage,)from pydantic import BaseModel, Fieldclass Example(TypedDict):    \"\"\"A representation of an example consisting of text input and expected tool calls.    For extraction, the tool calls are represented as instances of pydantic model.    \"\"\"    input: str  # This is the example text    tool_calls: List[BaseModel]  # Instances of pydantic model that should be extracteddef tool_example_to_messages(example: Example) -> List[BaseMessage]:    \"\"\"Convert an example into a list of messages that can be fed into an LLM.    This code is an adapter that converts our example to a list of messages    that can be fed into a chat model.    The list of messages per example corresponds to:    1) HumanMessage: contains the content from which content should be extracted.    2) AIMessage: contains the extracted information from the model    3) ToolMessage: contains confirmation to the model that the model requested a tool correctly.    The ToolMessage is required because some of the chat models are hyper-optimized for agents    rather than for an extraction use case.    \"\"\"    messages: List[BaseMessage] = [HumanMessage(content=example[\"input\"])]    tool_calls = []    for tool_call in example[\"tool_calls\"]:        tool_calls.append(            {                \"id\": str(uuid.uuid4()),                \"args\": tool_call.dict(),                # The name of the function right now corresponds                # to the name of the pydantic model                # This is implicit in the API right now,                # and will be improved over time.                \"name\": tool_call.__class__.__name__,            },        )    messages.append(AIMessage(content=\"\", tool_calls=tool_calls))    tool_outputs = example.get(\"tool_outputs\") or [        \"You have correctly called this tool.\"    ] * len(tool_calls)    for output, tool_call in zip(tool_outputs, tool_calls):        messages.append(ToolMessage(content=output, tool_call_id=tool_call[\"id\"]))    return messages\n```\n\nNext let's define our examples and then convert them into message format.\n\n```\nexamples = [    (        \"The ocean is vast and blue. It's more than 20,000 feet deep. There are many fish in it.\",        Data(people=[]),    ),    (        \"Fiona traveled far from France to Spain.\",        Data(people=[Person(name=\"Fiona\", height_in_meters=None, hair_color=None)]),    ),]messages = []for text, tool_call in examples:    messages.extend(        tool_example_to_messages({\"input\": text, \"tool_calls\": [tool_call]})    )\n```\n\nLet's test out the prompt\n\n```\nexample_prompt = prompt.invoke({\"text\": \"this is some text\", \"examples\": messages})for message in example_prompt.messages:    print(f\"{message.type}: {message}\")\n```\n\n```\nsystem: content=\"You are an expert extraction algorithm. Only extract relevant information from the text. If you do not know the value of an attribute asked to extract, return null for the attribute's value.\" additional_kwargs={} response_metadata={}human: content=\"The ocean is vast and blue. It's more than 20,000 feet deep. There are many fish in it.\" additional_kwargs={} response_metadata={}ai: content='' additional_kwargs={} response_metadata={} tool_calls=[{'name': 'Data', 'args': {'people': []}, 'id': '240159b1-1405-4107-a07c-3c6b91b3d5b7', 'type': 'tool_call'}]tool: content='You have correctly called this tool.' tool_call_id='240159b1-1405-4107-a07c-3c6b91b3d5b7'human: content='Fiona traveled far from France to Spain.' additional_kwargs={} response_metadata={}ai: content='' additional_kwargs={} response_metadata={} tool_calls=[{'name': 'Data', 'args': {'people': [{'name': 'Fiona', 'hair_color': None, 'height_in_meters': None}]}, 'id': '3fc521e4-d1d2-4c20-bf40-e3d72f1068da', 'type': 'tool_call'}]tool: content='You have correctly called this tool.' tool_call_id='3fc521e4-d1d2-4c20-bf40-e3d72f1068da'human: content='this is some text' additional_kwargs={} response_metadata={}\n```", "url_final": "https://python.langchain.com/docs/how_to/extraction_examples/", "url_citable": "https://python.langchain.com/docs/how_to/extraction_examples/#define-reference-examples", "title": "How to use reference examples when doing extraction | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Define reference examples‚Äã", "section_anchor": "define-reference-examples", "fetched_at": "2025-10-06T21:55:37.599200", "has_code": true}
{"id": "c08d334777bf81b763f25c600aa92e0fe60d3545", "text": "Create an extractor‚Äã\n\nLet's select an LLM. Because we are using tool-calling, we will need a model that supports a tool-calling feature. See this table for available LLMs.\n\n```\npip install -qU \"langchain[google-genai]\"\n```\n\n```\nimport getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n```\n\nFollowing the extraction tutorial, we use the .with_structured_output method to structure model outputs according to the desired schema:\n\n```\nrunnable = prompt | llm.with_structured_output(    schema=Data,    method=\"function_calling\",    include_raw=False,)\n```", "url_final": "https://python.langchain.com/docs/how_to/extraction_examples/", "url_citable": "https://python.langchain.com/docs/how_to/extraction_examples/#create-an-extractor", "title": "How to use reference examples when doing extraction | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Create an extractor‚Äã", "section_anchor": "create-an-extractor", "fetched_at": "2025-10-06T21:55:37.599200", "has_code": true}
{"id": "455fcce10d37403bb92f3719d33d3fd134e84871", "text": "Without examples üòø‚Äã\n\nNotice that even capable models can fail with a very simple test case!\n\n```\nfor _ in range(5):    text = \"The solar system is large, but earth has only 1 moon.\"    print(runnable.invoke({\"text\": text, \"examples\": []}))\n```\n\n```\npeople=[Person(name='earth', hair_color='null', height_in_meters='null')]``````outputpeople=[Person(name='earth', hair_color='null', height_in_meters='null')]``````outputpeople=[]``````outputpeople=[Person(name='earth', hair_color='null', height_in_meters='null')]``````outputpeople=[]\n```", "url_final": "https://python.langchain.com/docs/how_to/extraction_examples/", "url_citable": "https://python.langchain.com/docs/how_to/extraction_examples/#without-examples-", "title": "How to use reference examples when doing extraction | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Without examples üòø‚Äã", "section_anchor": "without-examples-", "fetched_at": "2025-10-06T21:55:37.599200", "has_code": true}
{"id": "8917e709c57ac89ed8c16e0b822d36cbcfe80e33", "text": "With examples üòª‚Äã\n\nReference examples helps to fix the failure!\n\n```\nfor _ in range(5):    text = \"The solar system is large, but earth has only 1 moon.\"    print(runnable.invoke({\"text\": text, \"examples\": messages}))\n```\n\n```\npeople=[]``````outputpeople=[]``````outputpeople=[]``````outputpeople=[]``````outputpeople=[]\n```\n\nNote that we can see the few-shot examples as tool-calls in the Langsmith trace.\n\nAnd we retain performance on a positive sample:\n\n```\nrunnable.invoke(    {        \"text\": \"My name is Harrison. My hair is black.\",        \"examples\": messages,    })\n```\n\n```\nData(people=[Person(name='Harrison', hair_color='black', height_in_meters=None)])\n```\n\n- Define the schema\n- Define reference examples\n- Create an extractor\n- Without examples üòø\n- With examples üòª", "url_final": "https://python.langchain.com/docs/how_to/extraction_examples/", "url_citable": "https://python.langchain.com/docs/how_to/extraction_examples/#with-examples-", "title": "How to use reference examples when doing extraction | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "With examples üòª‚Äã", "section_anchor": "with-examples-", "fetched_at": "2025-10-06T21:55:37.599200", "has_code": true}
{"id": "e13230133f41079d3ed17cd40e54889e4bae9254", "text": "Fallback for LLM API Errors‚Äã\n\nThis is maybe the most common use case for fallbacks. A request to an LLM API can fail for a variety of reasons - the API could be down, you could have hit rate limits, any number of things. Therefore, using fallbacks can help protect against these types of things.\n\nIMPORTANT: By default, a lot of the LLM wrappers catch errors and retry. You will most likely want to turn those off when working with fallbacks. Otherwise the first wrapper will keep on retrying and not failing.\n\n```\n%pip install --upgrade --quiet  langchain langchain-openai\n```\n\n```\nfrom langchain_anthropic import ChatAnthropicfrom langchain_openai import ChatOpenAI\n```\n\nFirst, let's mock out what happens if we hit a RateLimitError from OpenAI\n\n```\nfrom unittest.mock import patchimport httpxfrom openai import RateLimitErrorrequest = httpx.Request(\"GET\", \"/\")response = httpx.Response(200, request=request)error = RateLimitError(\"rate limit\", response=response, body=\"\")\n```\n\n```\n# Note that we set max_retries = 0 to avoid retrying on RateLimits, etcopenai_llm = ChatOpenAI(model=\"gpt-4o-mini\", max_retries=0)anthropic_llm = ChatAnthropic(model=\"claude-3-haiku-20240307\")llm = openai_llm.with_fallbacks([anthropic_llm])\n```\n\n```\n# Let's use just the OpenAI LLm first, to show that we run into an errorwith patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):    try:        print(openai_llm.invoke(\"Why did the chicken cross the road?\"))    except RateLimitError:        print(\"Hit error\")\n```\n\n```\nHit error\n```\n\n```\n# Now let's try with fallbacks to Anthropicwith patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):    try:        print(llm.invoke(\"Why did the chicken cross the road?\"))    except RateLimitError:        print(\"Hit error\")\n```\n\n```\ncontent=' I don\\'t actually know why the chicken crossed the road, but here are some possible humorous answers:\\n\\n- To get to the other side!\\n\\n- It was too chicken to just stand there. \\n\\n- It wanted a change of scenery.\\n\\n- It wanted to show the possum it could be done.\\n\\n- It was on its way to a poultry farmers\\' convention.\\n\\nThe joke plays on the double meaning of \"the other side\" - literally crossing the road to the other side, or the \"other side\" meaning the afterlife. So it\\'s an anti-joke, with a silly or unexpected pun as the answer.' additional_kwargs={} example=False\n```\n\nWe can use our \"LLM with Fallbacks\" as we would a normal LLM.\n\n```\nfrom langchain_core.prompts import ChatPromptTemplateprompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"You're a nice assistant who always includes a compliment in your response\",        ),        (\"human\", \"Why did the {animal} cross the road\"),    ])chain = prompt | llmwith patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):    try:        print(chain.invoke({\"animal\": \"kangaroo\"}))    except RateLimitError:        print(\"Hit error\")\n```\n\n```\ncontent=\" I don't actually know why the kangaroo crossed the road, but I can take a guess! Here are some possible reasons:\\n\\n- To get to the other side (the classic joke answer!)\\n\\n- It was trying to find some food or water \\n\\n- It was trying to find a mate during mating season\\n\\n- It was fleeing from a predator or perceived threat\\n\\n- It was disoriented and crossed accidentally \\n\\n- It was following a herd of other kangaroos who were crossing\\n\\n- It wanted a change of scenery or environment \\n\\n- It was trying to reach a new habitat or territory\\n\\nThe real reason is unknown without more context, but hopefully one of those potential explanations does the joke justice! Let me know if you have any other animal jokes I can try to decipher.\" additional_kwargs={} example=False\n```", "url_final": "https://python.langchain.com/docs/how_to/fallbacks/", "url_citable": "https://python.langchain.com/docs/how_to/fallbacks/#fallback-for-llm-api-errors", "title": "How to add fallbacks to a runnable | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Fallback for LLM API Errors‚Äã", "section_anchor": "fallback-for-llm-api-errors", "fetched_at": "2025-10-06T21:55:40.708053", "has_code": true}
{"id": "fecc1efa0e98dc504a63a30b5635cd868bfc9aba", "text": "Fallback for Sequences‚Äã\n\nWe can also create fallbacks for sequences, that are sequences themselves. Here we do that with two different models: ChatOpenAI and then normal OpenAI (which does not use a chat model). Because OpenAI is NOT a chat model, you likely want a different prompt.\n\n```\n# First let's create a chain with a ChatModel# We add in a string output parser here so the outputs between the two are the same typefrom langchain_core.output_parsers import StrOutputParserchat_prompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"You're a nice assistant who always includes a compliment in your response\",        ),        (\"human\", \"Why did the {animal} cross the road\"),    ])# Here we're going to use a bad model name to easily create a chain that will errorchat_model = ChatOpenAI(model=\"gpt-fake\")bad_chain = chat_prompt | chat_model | StrOutputParser()\n```\n\n```\n# Now lets create a chain with the normal OpenAI modelfrom langchain_core.prompts import PromptTemplatefrom langchain_openai import OpenAIprompt_template = \"\"\"Instructions: You should always include a compliment in your response.Question: Why did the {animal} cross the road?\"\"\"prompt = PromptTemplate.from_template(prompt_template)llm = OpenAI()good_chain = prompt | llm\n```\n\n```\n# We can now create a final chain which combines the twochain = bad_chain.with_fallbacks([good_chain])chain.invoke({\"animal\": \"turtle\"})\n```\n\n```\n'\\n\\nAnswer: The turtle crossed the road to get to the other side, and I have to say he had some impressive determination.'\n```", "url_final": "https://python.langchain.com/docs/how_to/fallbacks/", "url_citable": "https://python.langchain.com/docs/how_to/fallbacks/#fallback-for-sequences", "title": "How to add fallbacks to a runnable | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Fallback for Sequences‚Äã", "section_anchor": "fallback-for-sequences", "fetched_at": "2025-10-06T21:55:40.708053", "has_code": true}
{"id": "a1f35a5500ebc0c2bd639f1e094ee118a2b20c2b", "text": "Fallback for Long Inputs‚Äã\n\nOne of the big limiting factors of LLMs is their context window. Usually, you can count and track the length of prompts before sending them to an LLM, but in situations where that is hard/complicated, you can fallback to a model with a longer context length.\n\n```\nshort_llm = ChatOpenAI()long_llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\")llm = short_llm.with_fallbacks([long_llm])\n```\n\n```\ninputs = \"What is the next number: \" + \", \".join([\"one\", \"two\"] * 3000)\n```\n\n```\ntry:    print(short_llm.invoke(inputs))except Exception as e:    print(e)\n```\n\n```\nThis model's maximum context length is 4097 tokens. However, your messages resulted in 12012 tokens. Please reduce the length of the messages.\n```\n\n```\ntry:    print(llm.invoke(inputs))except Exception as e:    print(e)\n```\n\n```\ncontent='The next number in the sequence is two.' additional_kwargs={} example=False\n```", "url_final": "https://python.langchain.com/docs/how_to/fallbacks/", "url_citable": "https://python.langchain.com/docs/how_to/fallbacks/#fallback-for-long-inputs", "title": "How to add fallbacks to a runnable | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Fallback for Long Inputs‚Äã", "section_anchor": "fallback-for-long-inputs", "fetched_at": "2025-10-06T21:55:40.708053", "has_code": true}
{"id": "19ee58e7e1740be81a48cead0cfe7ea91776fe59", "text": "Fallback to Better Model‚Äã\n\nOften times we ask models to output format in a specific format (like JSON). Models like GPT-3.5 can do this okay, but sometimes struggle. This naturally points to fallbacks - we can try with GPT-3.5 (faster, cheaper), but then if parsing fails we can use GPT-4.\n\n```\nfrom langchain.output_parsers import DatetimeOutputParser\n```\n\n```\nprompt = ChatPromptTemplate.from_template(    \"what time was {event} (in %Y-%m-%dT%H:%M:%S.%fZ format - only return this value)\")\n```\n\n```\n# In this case we are going to do the fallbacks on the LLM + output parser level# Because the error will get raised in the OutputParseropenai_35 = ChatOpenAI() | DatetimeOutputParser()openai_4 = ChatOpenAI(model=\"gpt-4\") | DatetimeOutputParser()\n```\n\n```\nonly_35 = prompt | openai_35fallback_4 = prompt | openai_35.with_fallbacks([openai_4])\n```\n\n```\ntry:    print(only_35.invoke({\"event\": \"the superbowl in 1994\"}))except Exception as e:    print(f\"Error: {e}\")\n```\n\n```\nError: Could not parse datetime string: The Super Bowl in 1994 took place on January 30th at 3:30 PM local time. Converting this to the specified format (%Y-%m-%dT%H:%M:%S.%fZ) results in: 1994-01-30T15:30:00.000Z\n```\n\n```\ntry:    print(fallback_4.invoke({\"event\": \"the superbowl in 1994\"}))except Exception as e:    print(f\"Error: {e}\")\n```\n\n```\n1994-01-30 15:30:00\n```\n\n- Fallback for LLM API Errors\n- Fallback for Sequences\n- Fallback for Long Inputs\n- Fallback to Better Model", "url_final": "https://python.langchain.com/docs/how_to/fallbacks/", "url_citable": "https://python.langchain.com/docs/how_to/fallbacks/#fallback-to-better-model", "title": "How to add fallbacks to a runnable | ü¶úÔ∏èüîó LangChain", "section_level": 2, "section_heading": "Fallback to Better Model‚Äã", "section_anchor": "fallback-to-better-model", "fetched_at": "2025-10-06T21:55:40.708053", "has_code": true}
