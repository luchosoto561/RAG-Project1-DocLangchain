{
  "provenance": {
    "url_final": "https://python.langchain.com/docs/how_to/callbacks_async/",
    "title": "How to use callbacks in async environments | ü¶úÔ∏èüîó LangChain",
    "fetched_at": "2025-10-06T21:54:57.832143"
  },
  "sections": [
    {
      "level": 1,
      "heading_text": "How to use callbacks in async environments | ü¶úÔ∏èüîó LangChain",
      "blocks": [
        {
          "type": "paragraph",
          "text": "This guide assumes familiarity with the following concepts:"
        },
        {
          "type": "list",
          "ordered": false,
          "items": [
            "Callbacks",
            "Custom callback handlers"
          ]
        },
        {
          "type": "paragraph",
          "text": "If you are planning to use the async APIs, it is recommended to use and extend AsyncCallbackHandler to avoid blocking the event."
        },
        {
          "type": "paragraph",
          "text": "If you use a sync CallbackHandler while using an async method to run your LLM / Chain / Tool / Agent, it will still work. However, under the hood, it will be called with run_in_executor which can cause issues if your CallbackHandler is not thread-safe."
        },
        {
          "type": "paragraph",
          "text": "If you're on python<=3.10, you need to remember to propagate config or callbacks when invoking other runnable from within a RunnableLambda, RunnableGenerator or @tool. If you do not do this, the callbacks will not be propagated to the child runnables being invoked."
        },
        {
          "type": "code",
          "code": "import asynciofrom typing import Any, Dict, Listfrom langchain_anthropic import ChatAnthropicfrom langchain_core.callbacks import AsyncCallbackHandler, BaseCallbackHandlerfrom langchain_core.messages import HumanMessagefrom langchain_core.outputs import LLMResultclass MyCustomSyncHandler(BaseCallbackHandler):    def on_llm_new_token(self, token: str, **kwargs) -> None:        print(f\"Sync handler being called in a `thread_pool_executor`: token: {token}\")class MyCustomAsyncHandler(AsyncCallbackHandler):    \"\"\"Async callback handler that can be used to handle callbacks from langchain.\"\"\"    async def on_llm_start(        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any    ) -> None:        \"\"\"Run when chain starts running.\"\"\"        print(\"zzzz....\")        await asyncio.sleep(0.3)        class_name = serialized[\"name\"]        print(\"Hi! I just woke up. Your llm is starting\")    async def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:        \"\"\"Run when chain ends running.\"\"\"        print(\"zzzz....\")        await asyncio.sleep(0.3)        print(\"Hi! I just woke up. Your llm is ending\")# To enable streaming, we pass in `streaming=True` to the ChatModel constructor# Additionally, we pass in a list with our custom handlerchat = ChatAnthropic(    model=\"claude-3-7-sonnet-20250219\",    max_tokens=25,    streaming=True,    callbacks=[MyCustomSyncHandler(), MyCustomAsyncHandler()],)await chat.agenerate([[HumanMessage(content=\"Tell me a joke\")]])"
        },
        {
          "type": "code",
          "code": "zzzz....Hi! I just woke up. Your llm is startingSync handler being called in a `thread_pool_executor`: token: Sync handler being called in a `thread_pool_executor`: token: WhySync handler being called in a `thread_pool_executor`: token:  don't scientists trust atoms?Because they make upSync handler being called in a `thread_pool_executor`: token:  everything!Sync handler being called in a `thread_pool_executor`: token: zzzz....Hi! I just woke up. Your llm is ending"
        },
        {
          "type": "code",
          "code": "LLMResult(generations=[[ChatGeneration(text=\"Why don't scientists trust atoms?\\n\\nBecause they make up everything!\", message=AIMessage(content=\"Why don't scientists trust atoms?\\n\\nBecause they make up everything!\", additional_kwargs={}, response_metadata={'model_name': 'claude-3-7-sonnet-20250219', 'stop_reason': 'end_turn', 'stop_sequence': None}, id='run--a596349d-8a7c-45fe-8691-bb1f9cfd6c08-0', usage_metadata={'input_tokens': 11, 'output_tokens': 17, 'total_tokens': 28, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}}))]], llm_output={}, run=[RunInfo(run_id=UUID('a596349d-8a7c-45fe-8691-bb1f9cfd6c08'))], type='LLMResult')"
        }
      ],
      "children": [
        {
          "level": 2,
          "heading_text": "Next steps‚Äã",
          "anchor": "next-steps",
          "blocks": [
            {
              "type": "paragraph",
              "text": "You've now learned how to create your own custom callback handlers."
            },
            {
              "type": "paragraph",
              "text": "Next, check out the other how-to guides in this section, such as how to attach callbacks to a runnable."
            },
            {
              "type": "list",
              "ordered": false,
              "items": [
                "Next steps"
              ]
            }
          ],
          "children": []
        }
      ]
    }
  ]
}