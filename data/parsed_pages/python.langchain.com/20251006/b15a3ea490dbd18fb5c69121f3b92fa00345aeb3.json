{
  "provenance": {
    "url_final": "https://python.langchain.com/docs/how_to/llm_token_usage_tracking/",
    "title": "How to track token usage for LLMs | ü¶úÔ∏èüîó LangChain",
    "fetched_at": "2025-10-06T21:55:47.992592"
  },
  "sections": [
    {
      "level": 1,
      "heading_text": "How to track token usage for LLMs | ü¶úÔ∏èüîó LangChain",
      "blocks": [
        {
          "type": "paragraph",
          "text": "Tracking token usage to calculate cost is an important part of putting your app in production. This guide goes over how to obtain this information from your LangChain model calls."
        },
        {
          "type": "paragraph",
          "text": "This guide assumes familiarity with the following concepts:"
        },
        {
          "type": "list",
          "ordered": false,
          "items": [
            "LLMs"
          ]
        }
      ],
      "children": [
        {
          "level": 2,
          "heading_text": "Using LangSmith‚Äã",
          "anchor": "using-langsmith",
          "blocks": [
            {
              "type": "paragraph",
              "text": "You can use LangSmith to help track token usage in your LLM application. See the LangSmith quick start guide."
            }
          ],
          "children": []
        },
        {
          "level": 2,
          "heading_text": "Using callbacks‚Äã",
          "anchor": "using-callbacks",
          "blocks": [
            {
              "type": "paragraph",
              "text": "There are some API-specific callback context managers that allow you to track token usage across multiple calls. You'll need to check whether such an integration is available for your particular model."
            },
            {
              "type": "paragraph",
              "text": "If such an integration is not available for your model, you can create a custom callback manager by adapting the implementation of the OpenAI callback manager."
            }
          ],
          "children": [
            {
              "level": 3,
              "heading_text": "OpenAI‚Äã",
              "anchor": "openai",
              "blocks": [
                {
                  "type": "paragraph",
                  "text": "Let's first look at an extremely simple example of tracking token usage for a single Chat model call."
                },
                {
                  "type": "paragraph",
                  "text": "The callback handler does not currently support streaming token counts for legacy language models (e.g., langchain_openai.OpenAI). For support in a streaming context, refer to the corresponding guide for chat models here."
                }
              ],
              "children": []
            },
            {
              "level": 3,
              "heading_text": "Single call‚Äã",
              "anchor": "single-call",
              "blocks": [
                {
                  "type": "code",
                  "code": "from langchain_community.callbacks import get_openai_callbackfrom langchain_openai import OpenAIllm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\")with get_openai_callback() as cb:    result = llm.invoke(\"Tell me a joke\")    print(result)    print(\"---\")print()print(f\"Total Tokens: {cb.total_tokens}\")print(f\"Prompt Tokens: {cb.prompt_tokens}\")print(f\"Completion Tokens: {cb.completion_tokens}\")print(f\"Total Cost (USD): ${cb.total_cost}\")"
                },
                {
                  "type": "code",
                  "code": "Why don't scientists trust atoms?Because they make up everything.---Total Tokens: 18Prompt Tokens: 4Completion Tokens: 14Total Cost (USD): $3.4e-05"
                }
              ],
              "children": []
            },
            {
              "level": 3,
              "heading_text": "Multiple calls‚Äã",
              "anchor": "multiple-calls",
              "blocks": [
                {
                  "type": "paragraph",
                  "text": "Anything inside the context manager will get tracked. Here's an example of using it to track multiple calls in sequence to a chain. This will also work for an agent which may use multiple steps."
                },
                {
                  "type": "code",
                  "code": "from langchain_community.callbacks import get_openai_callbackfrom langchain_core.prompts import PromptTemplatefrom langchain_openai import OpenAIllm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\")template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")chain = template | llmwith get_openai_callback() as cb:    response = chain.invoke({\"topic\": \"birds\"})    print(response)    response = chain.invoke({\"topic\": \"fish\"})    print(\"--\")    print(response)print()print(\"---\")print(f\"Total Tokens: {cb.total_tokens}\")print(f\"Prompt Tokens: {cb.prompt_tokens}\")print(f\"Completion Tokens: {cb.completion_tokens}\")print(f\"Total Cost (USD): ${cb.total_cost}\")"
                },
                {
                  "type": "code",
                  "code": "Why did the chicken go to the seance?To talk to the other side of the road!--Why did the fish need a lawyer?Because it got caught in a net!---Total Tokens: 50Prompt Tokens: 12Completion Tokens: 38Total Cost (USD): $9.400000000000001e-05"
                }
              ],
              "children": []
            }
          ]
        },
        {
          "level": 2,
          "heading_text": "Streaming‚Äã",
          "anchor": "streaming",
          "blocks": [
            {
              "type": "paragraph",
              "text": "get_openai_callback does not currently support streaming token counts for legacy language models (e.g., langchain_openai.OpenAI). If you want to count tokens correctly in a streaming context, there are a number of options:"
            },
            {
              "type": "list",
              "ordered": false,
              "items": [
                "Use chat models as described in this guide;",
                "Implement a custom callback handler that uses appropriate tokenizers to count the tokens;",
                "Use a monitoring platform such as LangSmith."
              ]
            },
            {
              "type": "paragraph",
              "text": "Note that when using legacy language models in a streaming context, token counts are not updated:"
            },
            {
              "type": "code",
              "code": "from langchain_community.callbacks import get_openai_callbackfrom langchain_openai import OpenAIllm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\")with get_openai_callback() as cb:    for chunk in llm.stream(\"Tell me a joke\"):        print(chunk, end=\"\", flush=True)    print(result)    print(\"---\")print()print(f\"Total Tokens: {cb.total_tokens}\")print(f\"Prompt Tokens: {cb.prompt_tokens}\")print(f\"Completion Tokens: {cb.completion_tokens}\")print(f\"Total Cost (USD): ${cb.total_cost}\")"
            },
            {
              "type": "code",
              "code": "Why don't scientists trust atoms?Because they make up everything!Why don't scientists trust atoms?Because they make up everything.---Total Tokens: 0Prompt Tokens: 0Completion Tokens: 0Total Cost (USD): $0.0"
            },
            {
              "type": "list",
              "ordered": false,
              "items": [
                "OpenAI",
                "Single call",
                "Multiple calls"
              ]
            }
          ],
          "children": []
        }
      ]
    }
  ]
}