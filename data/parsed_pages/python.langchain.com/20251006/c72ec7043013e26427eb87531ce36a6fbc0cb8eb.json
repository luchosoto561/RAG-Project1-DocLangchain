{
  "provenance": {
    "url_final": "https://python.langchain.com/docs/how_to/logprobs/",
    "title": "How to get log probabilities | ü¶úÔ∏èüîó LangChain",
    "fetched_at": "2025-10-06T21:55:49.981824"
  },
  "sections": [
    {
      "level": 1,
      "heading_text": "How to get log probabilities | ü¶úÔ∏èüîó LangChain",
      "blocks": [
        {
          "type": "paragraph",
          "text": "This guide assumes familiarity with the following concepts:"
        },
        {
          "type": "list",
          "ordered": false,
          "items": [
            "Chat models",
            "Tokens"
          ]
        },
        {
          "type": "paragraph",
          "text": "Certain chat models can be configured to return token-level log probabilities representing the likelihood of a given token. This guide walks through how to get this information in LangChain."
        }
      ],
      "children": [
        {
          "level": 2,
          "heading_text": "OpenAI‚Äã",
          "anchor": "openai",
          "blocks": [
            {
              "type": "paragraph",
              "text": "Install the LangChain x OpenAI package and set your API key"
            },
            {
              "type": "code",
              "code": "%pip install -qU langchain-openai"
            },
            {
              "type": "code",
              "code": "import getpassimport osif \"OPENAI_API_KEY\" not in os.environ:    os.environ[\"OPENAI_API_KEY\"] = getpass()"
            },
            {
              "type": "paragraph",
              "text": "For the OpenAI API to return log probabilities we need to configure the logprobs=True param. Then, the logprobs are included on each output AIMessage as part of the response_metadata:"
            },
            {
              "type": "code",
              "code": "from langchain_openai import ChatOpenAIllm = ChatOpenAI(model=\"gpt-4o-mini\").bind(logprobs=True)msg = llm.invoke((\"human\", \"how are you today\"))msg.response_metadata[\"logprobs\"][\"content\"][:5]"
            },
            {
              "type": "code",
              "code": "[{'token': 'I', 'bytes': [73], 'logprob': -0.26341408, 'top_logprobs': []}, {'token': \"'m\",  'bytes': [39, 109],  'logprob': -0.48584133,  'top_logprobs': []}, {'token': ' just',  'bytes': [32, 106, 117, 115, 116],  'logprob': -0.23484154,  'top_logprobs': []}, {'token': ' a',  'bytes': [32, 97],  'logprob': -0.0018291725,  'top_logprobs': []}, {'token': ' computer',  'bytes': [32, 99, 111, 109, 112, 117, 116, 101, 114],  'logprob': -0.052299336,  'top_logprobs': []}]"
            },
            {
              "type": "paragraph",
              "text": "And are part of streamed Message chunks as well:"
            },
            {
              "type": "code",
              "code": "ct = 0full = Nonefor chunk in llm.stream((\"human\", \"how are you today\")):    if ct < 5:        full = chunk if full is None else full + chunk        if \"logprobs\" in full.response_metadata:            print(full.response_metadata[\"logprobs\"][\"content\"])    else:        break    ct += 1"
            },
            {
              "type": "code",
              "code": "[][{'token': 'I', 'bytes': [73], 'logprob': -0.26593843, 'top_logprobs': []}][{'token': 'I', 'bytes': [73], 'logprob': -0.26593843, 'top_logprobs': []}, {'token': \"'m\", 'bytes': [39, 109], 'logprob': -0.3238896, 'top_logprobs': []}][{'token': 'I', 'bytes': [73], 'logprob': -0.26593843, 'top_logprobs': []}, {'token': \"'m\", 'bytes': [39, 109], 'logprob': -0.3238896, 'top_logprobs': []}, {'token': ' just', 'bytes': [32, 106, 117, 115, 116], 'logprob': -0.23778509, 'top_logprobs': []}][{'token': 'I', 'bytes': [73], 'logprob': -0.26593843, 'top_logprobs': []}, {'token': \"'m\", 'bytes': [39, 109], 'logprob': -0.3238896, 'top_logprobs': []}, {'token': ' just', 'bytes': [32, 106, 117, 115, 116], 'logprob': -0.23778509, 'top_logprobs': []}, {'token': ' a', 'bytes': [32, 97], 'logprob': -0.0022134194, 'top_logprobs': []}]"
            }
          ],
          "children": []
        },
        {
          "level": 2,
          "heading_text": "Next steps‚Äã",
          "anchor": "next-steps",
          "blocks": [
            {
              "type": "paragraph",
              "text": "You've now learned how to get logprobs from OpenAI models in LangChain."
            },
            {
              "type": "paragraph",
              "text": "Next, check out the other how-to guides chat models in this section, like how to get a model to return structured output or how to track token usage."
            },
            {
              "type": "list",
              "ordered": false,
              "items": [
                "OpenAI",
                "Next steps"
              ]
            }
          ],
          "children": []
        }
      ]
    }
  ]
}