{
  "provenance": {
    "url_final": "https://python.langchain.com/docs/how_to/custom_callbacks/",
    "title": "How to create custom callback handlers | 🦜️🔗 LangChain",
    "fetched_at": "2025-10-06T21:55:12.103077"
  },
  "sections": [
    {
      "level": 1,
      "heading_text": "How to create custom callback handlers | 🦜️🔗 LangChain",
      "blocks": [
        {
          "type": "paragraph",
          "text": "This guide assumes familiarity with the following concepts:"
        },
        {
          "type": "list",
          "ordered": false,
          "items": [
            "Callbacks"
          ]
        },
        {
          "type": "paragraph",
          "text": "LangChain has some built-in callback handlers, but you will often want to create your own handlers with custom logic."
        },
        {
          "type": "paragraph",
          "text": "To create a custom callback handler, we need to determine the event(s) we want our callback handler to handle as well as what we want our callback handler to do when the event is triggered. Then all we need to do is attach the callback handler to the object, for example via the constructor or at runtime."
        },
        {
          "type": "paragraph",
          "text": "In the example below, we'll implement streaming with a custom handler."
        },
        {
          "type": "paragraph",
          "text": "In our custom callback handler MyCustomHandler, we implement the on_llm_new_token handler to print the token we have just received. We then attach our custom handler to the model object as a constructor callback."
        },
        {
          "type": "code",
          "code": "from langchain_anthropic import ChatAnthropicfrom langchain_core.callbacks import BaseCallbackHandlerfrom langchain_core.prompts import ChatPromptTemplateclass MyCustomHandler(BaseCallbackHandler):    def on_llm_new_token(self, token: str, **kwargs) -> None:        print(f\"My custom handler, token: {token}\")prompt = ChatPromptTemplate.from_messages([\"Tell me a joke about {animal}\"])# To enable streaming, we pass in `streaming=True` to the ChatModel constructor# Additionally, we pass in our custom handler as a list to the callbacks parametermodel = ChatAnthropic(    model=\"claude-3-7-sonnet-20250219\", streaming=True, callbacks=[MyCustomHandler()])chain = prompt | modelresponse = chain.invoke({\"animal\": \"bears\"})"
        },
        {
          "type": "code",
          "code": "My custom handler, token: My custom handler, token: WhyMy custom handler, token:  don't bears wear shoes?Because theyMy custom handler, token:  prefer to go bear-foot!My custom handler, token:"
        },
        {
          "type": "paragraph",
          "text": "You can see this reference page for a list of events you can handle. Note that the handle_chain_* events run for most LCEL runnables."
        }
      ],
      "children": [
        {
          "level": 2,
          "heading_text": "Next steps​",
          "anchor": "next-steps",
          "blocks": [
            {
              "type": "paragraph",
              "text": "You've now learned how to create your own custom callback handlers."
            },
            {
              "type": "paragraph",
              "text": "Next, check out the other how-to guides in this section, such as how to attach callbacks to a runnable."
            },
            {
              "type": "list",
              "ordered": false,
              "items": [
                "Next steps"
              ]
            }
          ],
          "children": []
        }
      ]
    }
  ]
}