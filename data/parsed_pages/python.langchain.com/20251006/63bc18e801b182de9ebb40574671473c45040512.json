{
  "provenance": {
    "url_final": "https://python.langchain.com/docs/tutorials/classification/",
    "title": "Tagging | ü¶úÔ∏èüîó LangChain",
    "fetched_at": "2025-10-06T21:54:10.432485"
  },
  "sections": [
    {
      "level": 1,
      "heading_text": "Tagging | ü¶úÔ∏èüîó LangChain",
      "blocks": [
        {
          "type": "paragraph",
          "text": "Tagging means labeling a document with classes such as:"
        },
        {
          "type": "list",
          "ordered": false,
          "items": [
            "Sentiment",
            "Language",
            "Style (formal, informal etc.)",
            "Covered topics",
            "Political tendency"
          ]
        }
      ],
      "children": [
        {
          "level": 2,
          "heading_text": "Overview‚Äã",
          "anchor": "overview",
          "blocks": [
            {
              "type": "paragraph",
              "text": "Tagging has a few components:"
            },
            {
              "type": "list",
              "ordered": false,
              "items": [
                "function: Like extraction, tagging uses functions to specify how the model should tag a document",
                "schema: defines how we want to tag the document"
              ]
            }
          ],
          "children": []
        },
        {
          "level": 2,
          "heading_text": "Quickstart‚Äã",
          "anchor": "quickstart",
          "blocks": [
            {
              "type": "paragraph",
              "text": "Let's see a very straightforward example of how we can use OpenAI tool calling for tagging in LangChain. We'll use the with_structured_output method supported by OpenAI models."
            },
            {
              "type": "code",
              "code": "pip install -U langchain-core"
            },
            {
              "type": "paragraph",
              "text": "We'll need to load a chat model:"
            },
            {
              "type": "code",
              "code": "pip install -qU \"langchain[google-genai]\""
            },
            {
              "type": "code",
              "code": "import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")"
            },
            {
              "type": "paragraph",
              "text": "Let's specify a Pydantic model with a few properties and their expected type in our schema."
            },
            {
              "type": "code",
              "code": "from langchain_core.prompts import ChatPromptTemplatefrom pydantic import BaseModel, Fieldtagging_prompt = ChatPromptTemplate.from_template(    \"\"\"Extract the desired information from the following passage.Only extract the properties mentioned in the 'Classification' function.Passage:{input}\"\"\")class Classification(BaseModel):    sentiment: str = Field(description=\"The sentiment of the text\")    aggressiveness: int = Field(        description=\"How aggressive the text is on a scale from 1 to 10\"    )    language: str = Field(description=\"The language the text is written in\")# Structured LLMstructured_llm = llm.with_structured_output(Classification)"
            },
            {
              "type": "code",
              "code": "inp = \"Estoy increiblemente contento de haberte conocido! Creo que seremos muy buenos amigos!\"prompt = tagging_prompt.invoke({\"input\": inp})response = structured_llm.invoke(prompt)response"
            },
            {
              "type": "code",
              "code": "Classification(sentiment='positive', aggressiveness=1, language='Spanish')"
            },
            {
              "type": "paragraph",
              "text": "If we want dictionary output, we can just call .model_dump()"
            },
            {
              "type": "code",
              "code": "inp = \"Estoy muy enojado con vos! Te voy a dar tu merecido!\"prompt = tagging_prompt.invoke({\"input\": inp})response = structured_llm.invoke(prompt)response.model_dump()"
            },
            {
              "type": "code",
              "code": "{'sentiment': 'angry', 'aggressiveness': 8, 'language': 'Spanish'}"
            },
            {
              "type": "paragraph",
              "text": "As we can see in the examples, it correctly interprets what we want."
            },
            {
              "type": "paragraph",
              "text": "The results vary so that we may get, for example, sentiments in different languages ('positive', 'enojado' etc.)."
            },
            {
              "type": "paragraph",
              "text": "We will see how to control these results in the next section."
            }
          ],
          "children": []
        },
        {
          "level": 2,
          "heading_text": "Finer control‚Äã",
          "anchor": "finer-control",
          "blocks": [
            {
              "type": "paragraph",
              "text": "Careful schema definition gives us more control over the model's output."
            },
            {
              "type": "paragraph",
              "text": "Specifically, we can define:"
            },
            {
              "type": "list",
              "ordered": false,
              "items": [
                "Possible values for each property",
                "Description to make sure that the model understands the property",
                "Required properties to be returned"
              ]
            },
            {
              "type": "paragraph",
              "text": "Let's redeclare our Pydantic model to control for each of the previously mentioned aspects using enums:"
            },
            {
              "type": "code",
              "code": "class Classification(BaseModel):    sentiment: str = Field(..., enum=[\"happy\", \"neutral\", \"sad\"])    aggressiveness: int = Field(        ...,        description=\"describes how aggressive the statement is, the higher the number the more aggressive\",        enum=[1, 2, 3, 4, 5],    )    language: str = Field(        ..., enum=[\"spanish\", \"english\", \"french\", \"german\", \"italian\"]    )"
            },
            {
              "type": "code",
              "code": "tagging_prompt = ChatPromptTemplate.from_template(    \"\"\"Extract the desired information from the following passage.Only extract the properties mentioned in the 'Classification' function.Passage:{input}\"\"\")structured_llm = llm.with_structured_output(Classification)"
            },
            {
              "type": "paragraph",
              "text": "Now the answers will be restricted in a way we expect!"
            },
            {
              "type": "code",
              "code": "inp = \"Estoy increiblemente contento de haberte conocido! Creo que seremos muy buenos amigos!\"prompt = tagging_prompt.invoke({\"input\": inp})structured_llm.invoke(prompt)"
            },
            {
              "type": "code",
              "code": "Classification(sentiment='happy', aggressiveness=1, language='spanish')"
            },
            {
              "type": "code",
              "code": "inp = \"Estoy muy enojado con vos! Te voy a dar tu merecido!\"prompt = tagging_prompt.invoke({\"input\": inp})structured_llm.invoke(prompt)"
            },
            {
              "type": "code",
              "code": "Classification(sentiment='sad', aggressiveness=4, language='spanish')"
            },
            {
              "type": "code",
              "code": "inp = \"Weather is ok here, I can go outside without much more than a coat\"prompt = tagging_prompt.invoke({\"input\": inp})structured_llm.invoke(prompt)"
            },
            {
              "type": "code",
              "code": "Classification(sentiment='happy', aggressiveness=1, language='english')"
            },
            {
              "type": "paragraph",
              "text": "The LangSmith trace lets us peek under the hood:"
            }
          ],
          "children": [
            {
              "level": 3,
              "heading_text": "Going deeper‚Äã",
              "anchor": "going-deeper",
              "blocks": [
                {
                  "type": "list",
                  "ordered": false,
                  "items": [
                    "You can use the metadata tagger document transformer to extract metadata from a LangChain Document.",
                    "This covers the same basic functionality as the tagging chain, only applied to a LangChain Document."
                  ]
                },
                {
                  "type": "list",
                  "ordered": false,
                  "items": [
                    "Going deeper"
                  ]
                }
              ],
              "children": []
            }
          ]
        }
      ]
    }
  ]
}